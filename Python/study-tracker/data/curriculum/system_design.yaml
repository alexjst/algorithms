track: "system_design"
description: "28-day system design interview preparation with spaced repetition"
weeks:
  week1:
    - day: 1
      topic: "Scalability Fundamentals"
      activity: "Master scaling strategies, capacity planning, performance optimization, and bottleneck identification."
      detailed_content: |
        Scalability Concepts:
        - Horizontal scaling (Scale-out): Add more servers/instances
          * Pros: No single point of failure, cost-effective, unlimited scaling potential
          * Cons: Complex coordination, network overhead, data consistency challenges
          * Examples: Web servers, microservices, NoSQL databases
        - Vertical scaling (Scale-up): Increase power of existing servers (CPU, RAM, storage)
          * Pros: Simple implementation, no application changes, strong consistency
          * Cons: Hardware limits, single point of failure, expensive at high end
          * Examples: Traditional databases, legacy applications, compute-intensive tasks

        Scalability Dimensions:
        - Load scalability: Handle more concurrent users/requests
        - Data scalability: Store and process larger datasets
        - Geographic scalability: Serve users across multiple regions
        - Administrative scalability: Manage larger systems with same effort

        Performance Metrics:
        - Latency: Response time for single request (p50, p95, p99 percentiles)
        - Throughput: Requests processed per unit time (QPS, TPS)
        - Availability: System uptime percentage (99.9% = 8.77 hours downtime/year)
        - Consistency: Data accuracy across distributed components
        - Reliability: System's ability to perform correctly during failures

        Capacity Planning Process:
        1. Baseline measurement: Current traffic patterns and resource usage
        2. Growth projection: Expected user growth and feature expansion
        3. Load modeling: Peak traffic scenarios and seasonal variations
        4. Resource estimation: CPU, memory, storage, network requirements
        5. Testing validation: Load testing to verify assumptions
        6. Monitoring setup: Track actual vs predicted performance

        Common Bottlenecks:
        - CPU bound: Complex calculations, encryption, compression
        - Memory bound: Large datasets, caching, in-memory processing
        - I/O bound: Database queries, file operations, network calls
        - Network bound: Data transfer, API calls, distributed operations
        - Database bound: Complex queries, lock contention, slow storage

        Scalability Patterns:
        - Stateless design: No server-side session state for easy horizontal scaling
        - Asynchronous processing: Decouple time-consuming operations
        - Caching: Reduce expensive operations through data reuse
        - Data partitioning: Distribute data across multiple storage systems
        - Load balancing: Distribute traffic across multiple servers
        - Auto-scaling: Dynamically adjust resources based on demand

        Design Principles:
        - Design for failure: Assume components will fail and plan accordingly
        - Decouple components: Reduce dependencies between system parts
        - Scale horizontally: Prefer adding servers over upgrading hardware
        - Cache aggressively: Store frequently accessed data closer to users
        - Optimize for bottlenecks: Focus efforts on limiting resources
        - Monitor everything: Track metrics to identify issues early

        Load Estimation Calculations:
        - Daily Active Users (DAU) to QPS: DAU × daily requests ÷ 86,400 seconds
        - Peak traffic estimation: Average QPS × 2-10 (depends on usage patterns)
        - Storage estimation: Records × record size × growth factor × replication factor
        - Bandwidth estimation: QPS × average response size + growth buffer
        - Memory estimation: Active dataset size + buffer for operations

        Real-world Examples:
        - Netflix: Horizontal scaling with microservices, CDN for content delivery
        - Facebook: Massive horizontal scaling, custom database sharding
        - Amazon: Service-oriented architecture, auto-scaling infrastructure
        - Google: Distributed systems design, MapReduce for data processing
      resources:
        - title: "Designing Data-Intensive Applications - Chapter 1"
          url: "https://dataintensive.net/"
          description: "Fundamental concepts of scalable systems"
        - title: "High Scalability Blog"
          url: "https://highscalability.com/"
          description: "Real-world scalability case studies"
        - title: "The Art of Scalability"
          url: "https://theartofscalability.com/"
          description: "Comprehensive scalability principles"
        - title: "AWS Well-Architected Framework"
          url: "https://aws.amazon.com/architecture/well-architected/"
          description: "Cloud scalability best practices"
      practice_questions:
        estimation:
          - question: "Calculate QPS for 10M DAU with 50 requests per user per day. What's peak QPS assuming 5x multiplier?"
            answer: "Average QPS: (10M × 50) / 86,400s = 5,787 QPS. Peak QPS: 5,787 × 5 = 28,935 QPS. Need servers handling ~30K QPS. If each server handles 1K QPS, need 30 servers with load balancing."
          - question: "Social media app: 1M users, 10 posts/day average, 2KB per post. Calculate daily storage and annual growth."
            answer: "Daily: 1M × 10 × 2KB = 20GB/day. Annual: 20GB × 365 = 7.3TB/year. With 3x replication: 22TB/year. Plan for 30-40TB storage with buffer. Cost: ~$600/month on cloud storage."
          - question: "Video streaming: 100K concurrent streams at 1080p (5Mbps). Calculate total bandwidth and CDN requirements."
            answer: "Total bandwidth: 100K × 5Mbps = 500Gbps = 62.5GB/s. CDN egress: ~$0.08/GB = $5/sec = $432K/day. Need multi-tier CDN with regional POPs to reduce costs 70-80%."
          - question: "E-commerce site: 1M products, 100 bytes metadata each, 1000 QPS total traffic. With 10x read/write ratio and 95% cache hit rate, estimate database IOPS."
            answer: "Storage: 1M × 100B = 100MB (tiny). Traffic split: 1000 QPS = 900 reads + 100 writes. With 95% cache hit rate: 900 × 5% = 45 cache misses. Database IOPS: 45 reads + 100 writes = ~145 IOPS. Use Redis cache + standard SSD. Cost: ~$30/month RDS + $20/month Redis."
          - question: "Chat app: 50M users, average 100 messages/day, 200 bytes per message. Calculate storage for 1 year."
            answer: "Daily: 50M × 100 × 200B = 1TB/day. Annual: 365TB. With 3x replication: 1.1PB. Archive old messages to cheap storage after 90 days to save 70% costs. Active storage: ~300TB."
        concepts:
          - question: "A startup expects 100x user growth in 6 months. Should they optimize current code or plan for horizontal scaling?"
            answer: "Plan for horizontal scaling NOW. Optimization buys 2-5x improvement, but 100x needs architecture change. Design stateless services, decouple components, use message queues, implement caching. Vertical scaling has limits; horizontal scales infinitely."
          - question: "Your database CPU hits 90% during traffic spikes. Compare vertical scaling vs read replicas vs sharding solutions."
            answer: "Read replicas: Best if read-heavy (80%+ reads), fast to deploy, $500/month per replica. Vertical scaling: Quick fix, works for 6-12 months, hits hardware limits ($5K/month for large instance). Sharding: Complex but scales indefinitely, needed for write-heavy or >5TB data."
          - question: "When would you choose vertical scaling over horizontal scaling? Give 3 specific scenarios."
            answer: "1) Legacy monolith with tight coupling (refactoring cost > hardware), 2) Strong ACID requirements on single database, 3) Small to medium scale (<10K QPS) where simplicity matters. Vertical is simpler but has ceiling at ~96 cores/$20K month."
          - question: "What's the difference between scalability and performance? Can you have one without the other?"
            answer: "Performance: Speed for one user (latency). Scalability: Maintaining performance as users increase (throughput). Yes, can have one without other: Fast but doesn't scale (single-threaded), or scales but slow (inefficient algorithm). Need both for production systems."
          - question: "How do you identify if your bottleneck is CPU, memory, I/O, or network bound?"
            answer: "CPU: High CPU%, low I/O wait. Memory: Swapping, OOM errors, high cache misses. I/O: High disk queue, >10ms latencies. Network: High bandwidth usage, packet loss. Use: top, iostat, sar, netstat. Profile in production with <1% overhead tools."
          - question: "Explain the trade-offs between consistency and scalability in distributed systems."
            answer: "CAP theorem: Can't have all of Consistency, Availability, Partition-tolerance. Strong consistency (ACID) limits scalability, requires coordination (2PC, Raft). Eventual consistency scales better but has stale reads. Choose based on use case: banking=strong, social feeds=eventual."
        tradeoffs:
          - question: "Compare costs: 10 small servers vs 1 large server with same total capacity. Consider failure scenarios."
            answer: "10 small: $100/mo each = $1K total, lose 10% capacity on failure, easier updates. 1 large: $900/mo, lose 100% on failure, simpler ops. Choose small for: High availability needs. Choose large for: Simple workloads, strong consistency needs."
          - question: "Horizontal scaling with microservices vs vertical scaling with monolith: when to choose each?"
            answer: "Horizontal+microservices: Large teams (>50 eng), high scale (>100K QPS), independent deployments needed. Complex, 2-3x ops cost. Vertical+monolith: Small teams (<10), moderate scale (<10K QPS), rapid development. Simple, faster to market."
          - question: "Auto-scaling vs over-provisioning: cost and performance trade-offs"
            answer: "Auto-scaling: Saves 40-60% cost in variable traffic, 2-5 min scale-up lag, complex setup. Over-provisioning: Always fast, simple, wastes 40-60% capacity during off-peak. Use auto-scaling for: Predictable patterns with >3x variance."
          - question: "Strong consistency vs eventual consistency: impact on scalability"
            answer: "Strong consistency: Single-region only, <10K writes/sec typical, synchronous coordination slows writes. Eventual: Multi-region, 100K+ writes/sec, async replication, stale reads for seconds. Strong for: Financial, inventory. Eventual for: Social media, analytics."
          - question: "Synchronous vs asynchronous processing: latency and scalability implications"
            answer: "Synchronous: User waits, predictable, <200ms critical for UX, limits throughput to connection pool size. Asynchronous: Queue+workers, high throughput (100K+ jobs/sec), unpredictable timing. Use async for: Email, reports, batch processing. Sync for: Payments, bookings."
        scenarios:
          - "Your API response time degrades from 100ms to 2s as traffic doubles. Diagnose potential causes and solutions."
          - "Design a system for a viral social media feature that could get 100x normal traffic in 1 hour."
          - "A legacy monolith serves 1000 QPS. Design a migration strategy to handle 10,000 QPS in 6 months."
          - "Your database is at 80% capacity. Plan a scaling strategy that maintains < 100ms query latency."
          - "Traffic varies 10x between peak and off-peak hours. Design a cost-effective auto-scaling strategy."
      time_estimate: 60

    - day: 2
      topic: "Load Balancing"
      activity: "Master load balancing architecture, algorithms, health checks, and advanced routing strategies."
      detailed_content: |
        Load Balancer Types:
        - L4 (Transport/Network Layer): Routes based on IP address and port
          * Pros: High performance, low latency, protocol agnostic
          * Cons: Limited routing intelligence, no content-based decisions
          * Use cases: TCP/UDP traffic, high-throughput scenarios, database connections
        - L7 (Application Layer): Routes based on content (HTTP headers, URLs, cookies)
          * Pros: Advanced routing logic, SSL termination, content manipulation
          * Cons: Higher latency, resource intensive, protocol specific
          * Use cases: Web applications, microservices, API gateways

        Load Balancing Algorithms:
        - Round Robin: Sequential distribution to each server
          * Pros: Simple, fair distribution
          * Cons: Ignores server capacity and current load
          * Best for: Homogeneous servers with similar response times
        - Weighted Round Robin: Distribution based on assigned server weights
          * Pros: Accounts for different server capacities
          * Cons: Static weights don't adapt to real-time load
          * Best for: Heterogeneous server capacities
        - Least Connections: Route to server with fewest active connections
          * Pros: Considers current server load
          * Cons: Overhead of tracking connections
          * Best for: Long-lived connections, varying request processing times
        - Weighted Least Connections: Combines weights with connection count
          * Pros: Best of both weighted and least connections
          * Cons: More complex implementation
          * Best for: Mixed server capacities with varying load patterns
        - IP Hash: Consistent routing based on client IP hash
          * Pros: Session affinity without server-side state
          * Cons: Uneven distribution if clients cluster by IP
          * Best for: Applications requiring session persistence
        - Least Response Time: Route to server with fastest response
          * Pros: Optimizes for performance
          * Cons: Complex monitoring required
          * Best for: Performance-critical applications
        - Resource Based: Route based on server resource utilization
          * Pros: Dynamic adaptation to server health
          * Cons: Requires continuous monitoring
          * Best for: Cloud environments with auto-scaling

        Health Check Strategies:
        - Active Health Checks:
          * HTTP/HTTPS: GET requests to health endpoints
          * TCP: Connection establishment tests
          * Custom: Application-specific health scripts
        - Passive Health Checks: Monitor actual traffic for failures
        - Health Check Parameters:
          * Interval: Frequency of health checks (e.g., every 30 seconds)
          * Timeout: Max time to wait for response (e.g., 5 seconds)
          * Threshold: Consecutive failures before marking unhealthy (e.g., 3)
          * Recovery: Consecutive successes before marking healthy (e.g., 2)

        Advanced Features:
        - Session Affinity (Sticky Sessions):
          * Cookie-based: Use application cookies for routing
          * IP-based: Route based on client IP
          * Pros: Maintains session state
          * Cons: Uneven load distribution, scaling challenges
        - SSL Termination:
          * Offload SSL processing from backend servers
          * Centralized certificate management
          * Reduces backend server load
        - SSL Pass-through: Forward encrypted traffic without decryption
        - Connection Multiplexing: Reuse backend connections
        - Rate Limiting: Control request rates per client/IP
        - Geographic Routing: Route based on client location

        Load Balancer Placement:
        - Internet-facing: External traffic distribution
        - Internal: Service-to-service communication
        - Multi-tier: Load balancers at multiple application layers
        - Cross-zone: Distribution across availability zones
        - Global: DNS-based geographic distribution

        High Availability Patterns:
        - Active-Passive: Primary LB with standby failover
        - Active-Active: Multiple LBs sharing load
        - Cross-region: Load balancers in multiple regions
        - Anycast: Single IP announced from multiple locations

        Performance Considerations:
        - Throughput: Requests per second capacity
        - Latency: Additional delay introduced
        - Connection Limits: Maximum concurrent connections
        - CPU/Memory: Resource usage for different algorithms
        - Network Bandwidth: Throughput requirements

        Failure Scenarios:
        - Server failures: Automatic traffic redirection
        - Load balancer failures: Failover to secondary LB
        - Network partitions: Split-brain prevention
        - Cascading failures: Circuit breaker integration
        - Overload protection: Backpressure and throttling

        Implementation Types:
        - Hardware Load Balancers:
          * Pros: High performance, dedicated hardware, vendor support
          * Cons: Expensive, vendor lock-in, limited flexibility
          * Examples: F5 Big-IP, Citrix NetScaler
        - Software Load Balancers:
          * Pros: Cost-effective, flexible, programmable
          * Cons: Shared resources, performance limits
          * Examples: HAProxy, NGINX, Apache HTTPd
        - Cloud Load Balancers:
          * Pros: Managed service, auto-scaling, integrated monitoring
          * Cons: Vendor lock-in, limited customization
          * Examples: AWS ALB/NLB, Google Cloud Load Balancing, Azure Load Balancer

        Global Load Balancing:
        - DNS-based: Use DNS to route to different regions
        - GeoDNS: Route based on client geographic location
        - Anycast: Announce same IP from multiple locations
        - CDN integration: Combine with content delivery networks
      resources:
        - title: "AWS Application Load Balancer Guide"
          url: "https://docs.aws.amazon.com/elasticloadbalancing/latest/application/"
          description: "Practical load balancing concepts"
        - title: "NGINX Load Balancing"
          url: "https://nginx.org/en/docs/http/load_balancing.html"
          description: "Software load balancing implementation"
        - title: "HAProxy Configuration Guide"
          url: "https://www.haproxy.org/download/1.7/doc/configuration.txt"
          description: "Advanced load balancing configuration"
        - title: "Load Balancing Strategies"
          url: "https://kemptechnologies.com/load-balancer/load-balancing-algorithms-techniques/"
          description: "Comprehensive algorithm comparison"
      practice_questions:
        estimation:
          - question: "System serves 50K RPS. Each LB handles 10K RPS. How many LBs needed? Consider N+1 redundancy."
            answer: "Need: 50K / 10K = 5 LBs at capacity. With N+1 redundancy: 6 LBs total. During failure, 5 LBs handle 50K = 10K RPS each (at capacity). Use DNS round-robin or L4 LB in front. Cost: ~$300/month for 6 cloud LBs."
          - question: "LB adds 2ms latency. Backend processing takes 50ms. What's total response time and % overhead?"
            answer: "Total: 2ms + 50ms = 52ms. LB overhead: (2/52) × 100 = 3.8%. If targeting <100ms p99, this is acceptable. For <10ms systems, 2ms is 20% overhead - consider L4 or hardware LB (~0.5ms)."
          - question: "Global system: 100K RPS total, 3 regions (40%, 35%, 25% split). Calculate regional LB requirements."
            answer: "US: 40K RPS / 10K = 4 LBs (+1 redundancy) = 5. EU: 35K / 10K = 4. APAC: 25K / 10K = 3. Total: 12 LBs. Use GeoDNS for routing. Cost: ~$600/month + GeoDNS $50/month."
          - question: "Health checks: 100 servers, check every 30s, 5ms per check. What's monitoring overhead on LB?"
            answer: "Checks/sec: 100 / 30 = 3.33 checks/sec. CPU time: 3.33 × 5ms = 16.7ms/sec = 1.67% CPU. Minimal overhead. Network: 3.33 × 1KB = 3.33KB/sec. Health check overhead is negligible."
        concepts:
          - question: "E-commerce site needs session persistence for shopping carts. Compare sticky sessions vs stateless design."
            answer: "Sticky sessions: Simple, ties user to server, problematic on failures, limits scaling. Stateless: Store cart in Redis/DB, any server handles request, scales better, 2-5ms cache lookup cost. Recommendation: Stateless with Redis for e-commerce flexibility."
          - question: "Microservices architecture: when to use L4 vs L7 load balancing for service-to-service communication?"
            answer: "L4: High-throughput internal services (>10K RPS), uniform traffic, low latency needs (<1ms). L7: API gateway, path-based routing, canary deployments, need metrics/tracing. Service mesh (Envoy) provides L7 features with <5ms overhead."
          - question: "Your servers have different capacities (2-core, 4-core, 8-core). Which algorithm maximizes throughput?"
            answer: "Weighted Least Connections. Assign weights: 2-core=1, 4-core=2, 8-core=4. Considers both capacity AND current load. Achieves ~30% better utilization than round-robin on heterogeneous servers. Alternative: Resource-based (CPU/memory) for dynamic adaptation."
          - question: "How do you prevent a single slow server from affecting the entire system?"
            answer: "1) Timeout: Set 2-3x median response time, 2) Circuit breaker: Auto-remove after 3 consecutive failures, 3) Least Response Time algorithm, 4) Active health checks (every 5-10s), 5) Outlier detection: Remove servers at p99 >2x p50. Monitor with percentile metrics."
          - question: "Explain the difference between load balancing and service discovery."
            answer: "Service discovery: Finding available services (Consul, etcd, DNS). Load balancing: Distributing requests among found services. Pattern: Service discovery provides IPs → LB distributes traffic. Modern: Service mesh combines both (client-side LB with central registry)."
          - question: "How do you handle SSL termination in a microservices environment?"
            answer: "Option 1: Edge LB terminates SSL, internal HTTP (simple, central cert management). Option 2: End-to-end TLS with service mesh (secure but complex). Option 3: Hybrid - terminate at edge + re-encrypt for sensitive services. Edge termination saves CPU on backend servers."
        tradeoffs:
          - question: "Round Robin vs Least Connections: when does each perform better? Consider connection patterns."
            answer: "Round Robin: Short uniform requests (REST APIs <50ms), stateless services. 5-10% better throughput for predictable workloads. Least Connections: Long-lived connections (WebSockets, DB connections), variable request times. Prevents overload on slow servers."
          - question: "Hardware vs Software vs Cloud load balancers: cost, performance, and operational trade-offs"
            answer: "Hardware: $10K-50K, 1M+ RPS, complex ops, 99.999% uptime. Software: $0 (NGINX), 100K RPS, flexible, requires management. Cloud: $20/month, 50K RPS, auto-scale, managed. Choose: Cloud for startups, Hardware for high-perf, Software for customization."
          - question: "Active-Active vs Active-Passive LB setup: availability vs complexity analysis"
            answer: "Active-Active: 100% capacity used, no failover delay, even load distribution, more complex health checks. Active-Passive: 50% capacity idle, failover in 30-60s, simple, wastes resources. Use Active-Active for: High availability needs, variable traffic patterns."
          - question: "Session affinity vs stateless design: scalability and complexity implications"
            answer: "Session affinity: Simple app code, limits scaling, lost sessions on failure, uneven load. Stateless: Scales infinitely, survives failures, needs shared state (Redis), 2-5ms overhead. Modern systems: Stateless with sticky cookies for performance (best effort affinity)."
          - question: "L4 vs L7 load balancing: performance vs features comparison"
            answer: "L4: 1M+ RPS, <0.5ms latency, IP/port only, no SSL insights, simple. L7: 100K RPS, 2-5ms latency, path/header routing, WAF, metrics, SSL termination. Use L4 for: Internal services, throughput critical. L7 for: External APIs, microservices, security needs."
        scenarios:
          - "Design load balancing for a video streaming service with global users and varying content popularity."
          - "Your LB becomes a bottleneck at 50K RPS. Design a scaling strategy without service interruption."
          - "Backend servers randomly fail during traffic spikes. Design health checking to minimize impact."
          - "Multi-region deployment: design global load balancing with automatic failover between regions."
          - "Real-time gaming application: design load balancing to minimize latency and maintain session continuity."
      time_estimate: 60

    - day: 3
      topic: "Caching Strategies"
      activity: "Learn caching patterns, cache levels, eviction policies, and invalidation strategies for system performance."
      detailed_content: |
        Cache Patterns:
        - Cache-aside (Lazy Loading): Application manages cache, read miss triggers DB read + cache write
        - Write-through: Write to cache and DB simultaneously (synchronous, strong consistency)
        - Write-behind (Write-back): Write to cache first, DB later (asynchronous, better performance)
        - Write-around: Write directly to DB, bypass cache (prevents cache pollution from infrequent writes)
        - Refresh-ahead: Proactively refresh before expiration

        Cache Levels:
        - Browser cache: Client-side storage (HTML5 localStorage, sessionStorage)
        - CDN/Edge cache: Geographic distribution for static content
        - Reverse proxy cache: Server-side cache (Nginx, Varnish)
        - Application cache: In-memory cache within application (Redis, Memcached)
        - Database cache: Query result cache, buffer pools

        Eviction Policies:
        - LRU (Least Recently Used): Best for temporal locality
        - LFU (Least Frequently Used): Best for frequency-based access
        - FIFO (First In, First Out): Simple but less effective
        - TTL (Time To Live): Time-based expiration
        - Random: Simple, surprisingly effective for large caches

        Cache Invalidation Strategies:
        - TTL-based expiration: Automatic expiration after fixed time
        - Manual invalidation: Explicit cache deletion on data updates
        - Tag-based invalidation: Group related cache entries for bulk invalidation
        - Event-driven invalidation: Invalidate based on system events
        - Write-through invalidation: Remove stale data during writes

        Memcached vs Redis:
        Memcached:
        - Simple key-value store with string keys and binary values
        - Supports complex data structures only through client-side serialization
        - Multi-threaded, faster for simple operations
        - No persistence, pure in-memory cache
        - Simpler deployment and operation

        Redis:
        - Rich data types: strings, lists, sets, sorted sets, hashes, bitmaps, streams
        - Server-side operations on data structures (LPUSH, SADD, HINCRBY)
        - Single-threaded (6.0+ has I/O threading)
        - Optional persistence (RDB snapshots, AOF logs)
        - Advanced features: pub/sub, Lua scripting, clustering, replication

        Cache Warming and Cold Start:
        - Cache warming: Pre-populate cache with likely-to-be-accessed data
        - Lazy loading vs eager loading trade-offs
        - Cache priming strategies during deployment
        - Handling thundering herd problem during cold starts
      resources:
        - title: "Redis Caching Patterns"
          url: "https://redis.io/docs/manual/patterns/"
          description: "Common caching implementation patterns"
        - title: "Memcached vs Redis"
          url: "https://aws.amazon.com/elasticache/redis-vs-memcached/"
          description: "Choosing the right cache technology"
        - title: "Cache Invalidation Strategies"
          url: "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Invalidation.html"
          description: "Practical cache invalidation techniques"
      practice_questions:
        estimation:
          - question: "Cache hit ratio is 80% with 10ms cache lookup and 100ms DB query. What's average response time?"
            answer: "Average time = (0.8 × 10ms) + (0.2 × 100ms) = 8ms + 20ms = 28ms. Without cache: 100ms. Speedup: 100/28 = 3.57x. To reach <20ms avg, need 90% hit ratio: (0.9 × 10) + (0.1 × 100) = 19ms."
          - question: "1M cache entries, 1KB each. Using LRU, how much memory needed? What if 20% are hot data?"
            answer: "Total: 1M × 1KB = 1GB + overhead (~200MB for LRU metadata) = 1.2GB. If 20% hot data: 200K entries × 1KB = 200MB base + 40MB overhead = 240MB. Can serve 80% of traffic with 240MB cache if using LRU. Cost: ~$30/month for 1.2GB Redis."
          - question: "API serves 1000 RPS. Cache TTL is 60s. How many cache misses per minute for new data?"
            answer: "Requests/min: 1000 × 60 = 60K. With 60s TTL, each unique key misses once per minute. If 1000 unique keys: 1000 misses/min = 16.7 misses/sec = 1.67% miss rate. DB load: 16.7 QPS vs 1000 QPS without cache (60x reduction)."
        concepts:
          - question: "When would you choose write-through vs write-behind caching?"
            answer: "Write-through: When data consistency critical (financial, inventory). Writes are 2x slower (cache + DB), but always consistent, survives cache failures. Write-behind: High write throughput needs (social media, analytics), accepts eventual consistency, 10x faster writes, risk of data loss if cache fails before DB write."
          - question: "How do you handle cache invalidation in a distributed system?"
            answer: "1) Pub/Sub: Publish invalidation events to all cache nodes (Redis Pub/Sub, Kafka). 2) TTL: Short expiration (30-60s) for simple cases. 3) Cache tags: Group related entries, invalidate by tag. 4) Version keys: Append version to cache key, increment on update. Use Pub/Sub for low latency (<100ms) or TTL for simplicity."
          - question: "What's the thundering herd problem and how do you solve it?"
            answer: "Problem: Popular cache key expires, 1000s of requests hit DB simultaneously. Solutions: 1) Mutex lock: First request rebuilds cache, others wait (adds 50-100ms latency). 2) Probabilistic early expiration: Refresh before expiry with random jitter. 3) Background refresh: Separate job refreshes popular keys. Use probabilistic for <10ms overhead."
          - question: "When would you use Redis over Memcached?"
            answer: "Redis: Need data structures (lists, sets, sorted sets), persistence, pub/sub, atomic operations, single large values (512MB). Memcached: Simple key-value, multi-threaded performance (2x faster for simple GET/SET), no persistence needed, distributed environment. Use Redis for complex caching, Memcached for pure speed."
          - question: "How do you prevent cache stampede during high traffic?"
            answer: "1) Request coalescing: Deduplicate concurrent requests for same key (reduces DB load 100x). 2) Negative caching: Cache 'not found' for 30s. 3) Stale-while-revalidate: Serve stale data while refreshing. 4) Rate limiting: Limit DB queries per key. 5) Circuit breaker: Stop queries after failures. Combine for 99.9% stampede prevention."
        tradeoffs:
          - question: "Compare cache-aside vs write-through patterns for a read-heavy vs write-heavy system"
            answer: "Cache-aside (read-heavy): App controls caching, lazy loading, 90% hit rate typical, 2x DB load on misses. Best for: 10:1 read:write ratio. Write-through (write-heavy): Always consistent, every write updates cache+DB (2x write latency), no stale data. Best for: Strong consistency needs, 1:1 read:write ratio."
          - question: "TTL-based vs event-driven cache invalidation: pros and cons"
            answer: "TTL: Simple, no coordination, works across regions, but serves stale data (0-60s). Event-driven: Immediate consistency (<100ms), complex infrastructure (Kafka/SNS), coordination overhead. Use TTL for: Analytics, dashboards (stale OK). Event-driven for: Inventory, pricing (stale not OK)."
          - question: "Multi-level caching benefits vs complexity overhead"
            answer: "Benefits: Browser (0ms) → CDN (20ms) → App cache (2ms) → DB (100ms). Hit ratios multiply: 50% × 80% × 90% = 0.4% DB load. Complexity: 3 invalidation layers, consistency challenges, 5x ops overhead. Use for: >1M DAU, global users. Skip for: <100K DAU."
          - question: "Redis persistence options: RDB vs AOF trade-offs"
            answer: "RDB (snapshots): Fast recovery, compact, periodic (every 5min = up to 5min data loss), lower disk I/O. AOF (append log): Every write logged, <1s data loss, larger files, slower recovery. Hybrid: RDB hourly + AOF = best of both. Use RDB-only for: Recreatable data. AOF for: Critical data."
          - question: "Distributed cache vs local cache: when to use each?"
            answer: "Distributed (Redis): Shared across servers, 2-5ms latency, 100GB+ capacity, complex ops, $500/month. Local (in-process): 0.1ms latency, limited to server RAM (4-8GB), no network, free. Pattern: Local for hot data (10%), distributed for long-tail (90%). Achieve 0.5ms avg latency vs 2ms distributed-only."
      time_estimate: 45

    - day: 4
      topic: "Database Fundamentals"
      activity: "Master RDBMS vs NoSQL selection, ACID properties, transactions, and performance optimization."
      detailed_content: |
        RDBMS (Relational Database) Characteristics:
        - ACID Properties:
          * Atomicity: All operations in transaction succeed or fail together
          * Consistency: Database remains in valid state after transactions
          * Isolation: Concurrent transactions don't interfere with each other
          * Durability: Committed changes survive system failures
        - Strong consistency: All nodes see same data simultaneously
        - JOINS: Complex queries across multiple tables
        - Schema enforcement: Predefined structure with data types
        - Mature ecosystem: Extensive tooling and expertise
        - Examples: PostgreSQL, MySQL, Oracle, SQL Server

        SQL Transaction Isolation Levels:
        - Read Uncommitted: Lowest isolation, dirty reads possible
        - Read Committed: Prevents dirty reads, allows non-repeatable reads
        - Repeatable Read: Prevents dirty and non-repeatable reads
        - Serializable: Highest isolation, prevents all phenomena
        - Trade-off: Higher isolation = lower concurrency

        Database Normalization:
        - 1NF: Eliminate repeating groups, atomic values
        - 2NF: Remove partial dependencies on composite keys
        - 3NF: Remove transitive dependencies
        - Benefits: Reduces redundancy, prevents anomalies
        - Trade-offs: More JOINs, complex queries, potential performance impact

        NoSQL Database Types:
        - Document Stores (MongoDB, CouchDB):
          * Flexible JSON-like documents
          * Schema evolution without migration
          * Rich queries and indexing
          * Use cases: Content management, catalogs, user profiles
        - Key-Value Stores (Redis, DynamoDB, Riak):
          * Simple get/put operations
          * High performance and scalability
          * Limited querying capabilities
          * Use cases: Caching, session storage, real-time recommendations
        - Column-Family (Cassandra, HBase):
          * Wide column storage model
          * Excellent write performance
          * Time-series data optimization
          * Use cases: IoT data, logging, analytics
        - Graph Databases (Neo4j, Amazon Neptune):
          * Relationships as first-class citizens
          * Complex relationship queries
          * ACID transactions on graphs
          * Use cases: Social networks, fraud detection, recommendation engines
        - Time-Series Databases (InfluxDB, TimescaleDB, Prometheus):
          * Optimized for time-stamped data
          * Automatic data retention and rollups
          * Time-range queries and aggregations
          * Use cases: Monitoring, IoT sensors, financial data

        Database Selection Criteria:
        - Data Structure:
          * Structured, relational data → RDBMS
          * Semi-structured, evolving schema → Document stores
          * Simple key-value access → Key-value stores
          * Time-series data → Time-series databases
          * Complex relationships → Graph databases
        - Scalability Requirements:
          * Vertical scaling acceptable → RDBMS
          * Massive horizontal scaling needed → NoSQL
        - Consistency Requirements:
          * Strong consistency critical → RDBMS
          * Eventual consistency acceptable → NoSQL
        - Query Complexity:
          * Complex JOINs and analytics → RDBMS
          * Simple lookups and aggregations → NoSQL
        - Team Expertise:
          * SQL expertise → RDBMS
          * NoSQL experience → NoSQL

        Indexing Strategies:
        - Primary Index: Unique identifier for fast lookups
        - Secondary Index: Additional access paths for queries
        - Composite Index: Multiple columns for complex queries
        - Partial Index: Index subset of rows meeting conditions
        - Full-text Index: Search within text content
        - Trade-offs: Faster reads vs slower writes, storage overhead

        Performance Optimization:
        - Query Optimization:
          * Use EXPLAIN plans to analyze query execution
          * Avoid SELECT *, use specific columns
          * Optimize WHERE clauses and JOINs
          * Use appropriate indexes
        - Connection Pooling: Reuse database connections
        - Read Replicas: Distribute read traffic
        - Caching: Cache frequently accessed data
        - Partitioning: Distribute data across multiple nodes

        Consistency Models:
        - Strong Consistency: All nodes return same data
        - Eventual Consistency: Nodes converge to same state over time
        - Causal Consistency: Preserves order of causally related operations
        - Session Consistency: Consistency within user session
        - Monotonic Consistency: Never return older versions

        Database Trade-offs:
        - RDBMS vs NoSQL:
          * Consistency vs Availability (CAP theorem)
          * Query flexibility vs Performance
          * Schema rigidity vs Flexibility
          * Vertical vs Horizontal scaling
          * Mature tooling vs Modern features
        - SQL vs NoSQL Decision Matrix:
          * Complex queries → SQL
          * Simple operations → NoSQL
          * ACID requirements → SQL
          * High availability → NoSQL
          * Rapid development → NoSQL
          * Data integrity critical → SQL

        Common Anti-patterns:
        - Using NoSQL for complex relational data
        - Using RDBMS for simple key-value access
        - Over-normalization leading to complex JOINs
        - Under-indexing causing slow queries
        - Single database for all use cases
      resources:
        - title: "MongoDB vs PostgreSQL"
          url: "https://www.mongodb.com/compare/mongodb-postgresql"
          description: "Document vs relational database comparison"
        - title: "NoSQL Database Types"
          url: "https://aws.amazon.com/nosql/"
          description: "Overview of NoSQL database categories"
        - title: "Database Design Fundamentals"
          url: "https://www.postgresql.org/docs/current/tutorial.html"
          description: "SQL database design principles"
        - title: "CAP Theorem Explained"
          url: "https://en.wikipedia.org/wiki/CAP_theorem"
          description: "Understanding consistency, availability, partition tolerance"
      practice_questions:
        estimation:
          - question: "E-commerce site: 1M products, 10M orders/year. Estimate storage for RDBMS vs document store approach."
            answer: "RDBMS: Products (1M × 5KB = 5GB) + Orders (10M × 2KB = 20GB) + OrderItems (30M × 500B = 15GB) + Indexes (30%) = 52GB. Document store: Embed order items in order doc (10M × 5KB = 50GB) + Products (5GB) = 55GB. Similar storage, but document approach reduces JOINs, 3x faster reads, harder analytics."
          - question: "Social network: 100M users, average 500 friends each. Calculate storage for graph vs relational representation."
            answer: "Relational: Users (100M × 1KB = 100GB) + Friendships (100M × 500 × 16B = 800GB for edges) + Indexes (30%) = 1.17TB. Graph DB: Nodes (100GB) + Edges (800GB, but compressed) = ~700GB, 3x faster relationship queries. Graph wins for <3 hops traversal, SQL better for analytics."
          - question: "Time-series data: 1K metrics, 10K data points/minute. Storage and query performance comparison across database types."
            answer: "Data rate: 1K × 10K × 60 = 600M points/hour × 50B = 30GB/hour = 720GB/day. PostgreSQL: 720GB/day, slow aggregations (>10s). InfluxDB: 100GB/day (10x compression), <100ms aggregations. Cassandra: 500GB/day, fastest writes (1M/sec). Use InfluxDB for time-series workload."
          - question: "Banking system: 1M accounts, 100M transactions/day. Estimate IOPS and storage requirements."
            answer: "Storage: 1M × 2KB accounts = 2GB + 100M × 500B txns/day × 365 = 18TB/year. IOPS: 100M txns/day = 1,157 TPS × 10 IOPS/txn (reads/writes/logs) = 11,570 IOPS. Use SSD (10K IOPS each), need 2 SSDs + replication = 4 total. Cost: ~$2K/month for storage + IOPS."
        concepts:
          - question: "When would you choose MongoDB over PostgreSQL for a new project? Consider 3 scenarios."
            answer: "Choose MongoDB for: 1) Rapid prototyping with evolving schema (e.g., startup MVP, can add fields without migrations), 2) Document-heavy apps (CMS, catalogs where data naturally nested), 3) High write throughput (>10K writes/sec, horizontal sharding easier). Choose PostgreSQL for: Complex JOINs, strong consistency, mature ecosystem, data integrity critical."
          - question: "Your application needs both complex analytics and real-time lookups. Design a polyglot persistence strategy."
            answer: "Pattern: PostgreSQL for OLTP (transactional data, strong consistency) → Stream changes via CDC (Debezium) → Kafka → Load into: 1) Redis for real-time lookups (<1ms), 2) ClickHouse for analytics (columnar, fast aggregations). Keep PostgreSQL as source of truth. Cost: ~$1K/month for 3-system stack vs $300 single DB."
          - question: "Explain ACID properties with concrete examples. Why might you sacrifice some for NoSQL benefits?"
            answer: "ACID: Atomicity (transfer $100: debit+credit both succeed/fail), Consistency (balances sum unchanged), Isolation (concurrent transfers don't interfere), Durability (committed = survives crash). Sacrifice for: 10x write throughput (eventual consistency), horizontal scaling (partition tolerance), sub-10ms latency (no distributed transactions). Use for: Social media, not banking."
          - question: "How do database isolation levels affect application concurrency and consistency?"
            answer: "Read Uncommitted: Dirty reads possible, highest concurrency (1000 TPS), use for: analytics on replicas. Read Committed: Prevents dirty reads, good concurrency (800 TPS), default in PostgreSQL. Repeatable Read: Prevents non-repeatable reads, medium concurrency (500 TPS). Serializable: Full isolation, lowest concurrency (200 TPS), use for: financial transactions. Higher isolation = lower throughput."
          - question: "Design indexing strategy for an e-commerce product catalog with various search patterns."
            answer: "1) Primary: product_id (B-tree, unique), 2) category_id + price (composite, for browsing), 3) name, description (full-text GIN index for search), 4) created_at (B-tree for new arrivals), 5) Partial: WHERE in_stock=true (skip out-of-stock). Index overhead: 30% storage, 20% slower writes. Monitor query patterns, add indexes for slow queries (>100ms)."
          - question: "When would you denormalize a relational database? What are the trade-offs?"
            answer: "Denormalize when: 1) Read:Write ratio >10:1 (dashboards, reports), 2) Complex JOINs slow (>5 tables, >500ms), 3) Read replicas insufficient. Example: Embed user info in posts table. Trade-offs: 3x faster reads, 2x slower writes, data redundancy (larger storage), update anomalies. Use materialized views for query-time denormalization without storage penalty."
        tradeoffs:
          - question: "Strong consistency vs high availability: analyze trade-offs for different application types"
            answer: "Strong consistency (CP): Banking, inventory (cannot oversell), single-region writes, 99.9% availability, higher latency (50-100ms). High availability (AP): Social media, analytics, multi-region writes, 99.99% availability, eventual consistency (1-5s lag). Use CP for: Money, reservations. AP for: Posts, likes, views. Hybrid: CRDT for AP with convergence guarantees."
          - question: "Single large database vs multiple specialized databases: operational and performance implications"
            answer: "Single DB: Simpler ops (1 team, 1 backup strategy), ACID transactions, but limited scalability, one-size-fits-all compromises. Multi-DB: Optimized per workload (2x faster), independent scaling, but 3x ops complexity, eventual consistency between DBs, no cross-DB transactions. Use multi-DB when: >1M QPS, diverse workloads (OLTP + analytics + cache)."
          - question: "Normalized vs denormalized data models: query performance vs data integrity"
            answer: "Normalized (3NF): No redundancy, easier updates, data integrity, but 5-table JOINs slow (>500ms), complex queries. Denormalized: 3x faster reads (no JOINs), simpler queries, but 2x storage, update anomalies, data drift risk. Pattern: Normalize for writes (OLTP), denormalize for reads (OLAP). Use materialized views for best of both."
          - question: "SQL vs NoSQL for a startup: development speed vs long-term maintainability"
            answer: "SQL (PostgreSQL): Battle-tested, predictable performance, strong consistency, but schema migrations slow down iteration. NoSQL (MongoDB): Faster initial development (no migrations), flexible schema, easier horizontal scaling, but eventually need schema validation, harder analytics, less mature tooling. Recommendation: Start with PostgreSQL (JSON columns give flexibility), migrate to NoSQL only if scaling issues."
          - question: "Read replicas vs caching: cost, complexity, and consistency trade-offs"
            answer: "Read replicas: Full data copy (100GB replica = 100GB), 1-5s lag, SQL queries work, $200/month per replica. Cache (Redis): Hot data only (10GB = 90% hit rate), 0-60s staleness (TTL), limited queries, $50/month. Pattern: Use cache for hot data (landing pages), replicas for complex queries (reports). Replicas for: Strong consistency needs. Cache for: Performance (<10ms)."
        scenarios:
          - "Design data storage for a multi-tenant SaaS application. Consider isolation, scalability, and compliance."
          - "Your RDBMS reaches scaling limits. Plan migration strategy to NoSQL while maintaining data consistency."
          - "Choose database technology for: (a) Financial trading system (b) Content management (c) IoT sensor data (d) Social media feed."
          - "Application needs real-time analytics on transactional data. Design architecture avoiding impact on OLTP performance."
          - "Global application needs low-latency reads worldwide. Design distributed database strategy."
      time_estimate: 60

    - day: 5
      topic: "Consistency Models"
      activity: "Master consistency levels, implementation techniques, and trade-offs in distributed systems."
      detailed_content: |
        Consistency Spectrum (Strongest to Weakest):
        - Linearizability (Strong Consistency):
          * All operations appear instantaneous at some point between start and finish
          * Global ordering of all operations
          * Most expensive to implement
          * Example: Distributed consensus algorithms (Raft, Paxos)
        - Sequential Consistency:
          * All nodes see operations in same order
          * Operations can be reordered but maintain program order per process
          * Easier than linearizability but still strong
        - Causal Consistency:
          * Causally related operations appear in same order to all nodes
          * Concurrent operations can be seen in different orders
          * Preserves intuitive ordering relationships
        - Eventual Consistency:
          * All nodes converge to same state eventually
          * No timing guarantees for convergence
          * Common in AP systems (Amazon DynamoDB, Cassandra)
        - Weak Consistency:
          * No guarantees about when or if nodes converge
          * Best effort synchronization
          * Used in systems prioritizing performance over consistency

        Client-Centric Consistency Models:
        - Read-After-Write Consistency:
          * User sees their own writes immediately
          * Implementation: Route reads to write node or use timestamps
          * Example: User updates profile, immediately sees changes
        - Session Consistency:
          * Consistency guarantees within single session/connection
          * Different sessions may see different views
          * Implementation: Sticky sessions or session tokens
        - Monotonic Read Consistency:
          * User never sees older versions after seeing newer ones
          * Implementation: Read from same replica or use version vectors
          * Example: Reading email thread doesn't go backwards in time
        - Monotonic Write Consistency:
          * User's writes are applied in order they were issued
          * Implementation: Serialize writes per user
          * Example: Bank transfers processed in chronological order

        Implementation Techniques:
        - Quorum-Based Systems:
          * R + W > N ensures strong consistency (R=reads, W=writes, N=replicas)
          * R=1, W=N: Fast reads, slow writes
          * R=N, W=1: Fast writes, slow reads
          * R=W=(N+1)/2: Balanced approach
        - Vector Clocks:
          * Track causal relationships between events
          * Each node maintains counter for itself and others
          * Detect concurrent vs sequential operations
          * Used in Amazon DynamoDB, Riak
        - Logical Timestamps (Lamport):
          * Total ordering of events in distributed system
          * Simpler than vector clocks but less precise
        - Multi-Version Concurrency Control (MVCC):
          * Store multiple versions of data with timestamps
          * Readers don't block writers, writers don't block readers
          * Used in PostgreSQL, Oracle, MongoDB

        Conflict Resolution Strategies:
        - Last-Write-Wins (LWW):
          * Use timestamp to determine winning value
          * Simple but can lose data
          * Good for commutative operations
        - Application-Level Resolution:
          * Application logic decides how to merge conflicts
          * Most flexible but requires custom logic
          * Example: Shopping cart merges items from both versions
        - Convergent Replicated Data Types (CRDTs):
          * Data structures that automatically converge
          * No coordination needed for conflict resolution
          * Examples: G-Counter, PN-Counter, OR-Set
        - Operational Transform:
          * Transform operations to maintain consistency
          * Used in collaborative editing (Google Docs)

        Database Consistency Examples:
        - ACID Databases (PostgreSQL, MySQL):
          * Strong consistency within single database
          * ACID transactions guarantee consistency
          * Challenges in distributed deployments
        - NoSQL Eventual Consistency (Cassandra, DynamoDB):
          * Eventual consistency by default
          * Tunable consistency levels
          * Trade consistency for availability and performance
        - MongoDB:
          * Strong consistency for single document operations
          * Configurable read/write concerns
          * Eventual consistency for replica sets

        Consistency in Microservices:
        - Saga Pattern:
          * Manage consistency across service boundaries
          * Compensating actions for failure handling
          * Eventual consistency with business logic coordination
        - Event Sourcing:
          * Store events instead of current state
          * Replay events to rebuild state
          * Ensures consistency through event ordering
        - Two-Phase Commit (2PC):
          * Strong consistency across distributed services
          * High latency and availability issues
          * Generally avoided in microservices

        Performance vs Consistency Trade-offs:
        - Strong Consistency:
          * Pros: Simple application logic, data integrity
          * Cons: Higher latency, reduced availability, scalability limits
          * Use cases: Financial systems, inventory management
        - Eventual Consistency:
          * Pros: High performance, availability, scalability
          * Cons: Complex application logic, temporary inconsistencies
          * Use cases: Social media feeds, content delivery, analytics

        Monitoring Consistency:
        - Replication Lag: Time difference between primary and replicas
        - Consistency Violations: Detection of out-of-order operations
        - Convergence Time: How long for eventual consistency
        - Split-Brain Detection: Identifying network partition scenarios
      resources:
        - title: "Consistency Models Explained"
          url: "https://jepsen.io/consistency"
          description: "Detailed explanation of consistency models"
        - title: "DynamoDB Consistency"
          url: "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadConsistency.html"
          description: "Practical consistency implementation"
        - title: "Designing Data-Intensive Applications - Chapter 9"
          url: "https://dataintensive.net/"
          description: "Consistency and consensus in depth"
        - title: "Vector Clocks Explained"
          url: "https://en.wikipedia.org/wiki/Vector_clock"
          description: "Understanding causality tracking"
      practice_questions:
        estimation:
          - question: "DynamoDB: 5 replicas, R=2, W=3. What consistency guarantees? Calculate availability if 2 nodes fail."
            answer: "R+W=5 > N=5, guarantees strong consistency (reads always see latest write). With 2 nodes failed (3 remaining): W=3 still possible (write succeeds), R=2 still possible (read succeeds). Availability: 100% for up to 2 node failures. For 3 nodes failed: Can't achieve W=3, system becomes unavailable for writes (chooses C over A in CP)."
          - question: "Cassandra cluster: 1000 writes/sec, replication factor 3. Estimate convergence time with 10ms network latency."
            answer: "Writes: 1000/sec × RF=3 = 3000 replicas/sec. With async replication at 10ms latency: convergence in 10-50ms (depends on hinted handoff, gossip protocol). Under partition: Can take seconds to minutes. For consistency: Use QUORUM (R=2,W=2) sacrificing 10ms write latency for immediate consistency vs 50ms eventual consistency."
          - question: "Vector clock overhead: 100 nodes, 1KB messages. Calculate metadata overhead percentage."
            answer: "Vector clock size: 100 nodes × 8B (counter) = 800B per message. Overhead: 800B / 1KB = 80%. For large messages (100KB): 800B/100KB = 0.8%. High overhead for small messages. Optimization: Prune vector clocks (keep only recent/active nodes), reduce to 10 nodes = 80B = 8% overhead."
        concepts:
          - question: "Social media app: users post updates seen by followers. Design consistency model for timeline generation."
            answer: "Use eventual consistency with causal consistency for replies. Pattern: Post → Write to user's timeline (async fan-out to followers' timelines). Convergence time: 1-5s acceptable. For replies: Preserve causality (show post before reply). Use CRDT for likes/counts. Trade-off: Users see posts at slightly different times, but system scales to millions of posts/sec vs strong consistency limiting to 10K/sec."
          - question: "E-commerce inventory: prevent overselling while maintaining performance. Choose consistency approach."
            answer: "Use optimistic locking with version numbers. On purchase: Read inventory + version → Validate available → Write with version check. If conflict: Retry. Achieves strong consistency for inventory (no overselling) with 50ms latency vs 10ms eventual consistency. For hot items: Use distributed lock (Redis) or reserved inventory pool. Alternative: Saga pattern for multi-item carts."
          - question: "Banking system: transfer money between accounts. Why is strong consistency required? Design implementation."
            answer: "Strong consistency prevents: 1) Double-spending, 2) Negative balances, 3) Lost money in transfers. Implementation: Use 2PC or Saga pattern. 2PC: Prepare phase (lock both accounts) → Commit phase (transfer). Latency: 50-100ms. Alternative: Single DB with ACID transactions (simpler, 20ms latency). Event sourcing for audit trail. Never use eventual consistency for money."
          - question: "Global content management: editors in multiple regions. Design conflict resolution for concurrent edits."
            answer: "Use Operational Transform (OT) or CRDT. OT: Transform concurrent edits to maintain intent (used in Google Docs), complex algorithm, 10-50ms latency. CRDT: Automatically merge edits (simpler), may need manual conflict UI for complex cases. Implementation: Last-write-wins for metadata, OT for content. Lock documents during edit with 5min timeout for exclusive editing."
          - question: "Explain why read-after-write consistency is easier to implement than monotonic read consistency."
            answer: "Read-after-write: Route user's reads to same node that handled write, or use session token with timestamp. Simple: Track single write per user. Monotonic read: Must track ALL reads across all nodes, ensure never return older version. Harder: Needs version vectors, sticky sessions, or always read from primary. Monotonic read requires global coordination vs read-after-write is per-user local."
          - question: "When would you choose causal consistency over eventual consistency? Give specific scenarios."
            answer: "Choose causal when: 1) Comments/replies (reply shouldn't appear before post), 2) Chat apps (messages in conversation order), 3) Collaborative editing (see others' edits in logical order). Cost: 2x metadata overhead (vector clocks), 20-50ms latency vs 10ms eventual. Use eventual for: Independent events (likes, views, analytics) where order doesn't matter. Causal gives intuitive UX at moderate cost."
        tradeoffs:
          - question: "Strong vs eventual consistency for shopping cart: analyze user experience vs performance impact"
            answer: "Strong consistency: User always sees correct cart, 50ms add-to-cart latency, can't scale past 10K writes/sec, survives browser close. Eventual: <10ms latency, scales to 100K+ writes/sec, but temporary inconsistencies (added item appears after 1-2s), works across devices. Recommendation: Use eventual with read-after-write (user sees own changes immediately), eventual across devices. Best UX/performance balance."
          - question: "Quorum configurations: R=1,W=3 vs R=3,W=1 vs R=2,W=2. Compare latency, consistency, availability."
            answer: "R=1,W=3 (read-optimized): 10ms reads, 30ms writes, eventual consistency, survives 2 node failures. R=3,W=1 (write-optimized): 30ms reads, 10ms writes, may read stale, survives 2 failures. R=2,W=2 (balanced): 20ms both, strong consistency (R+W>N), survives 1 failure. Use R=1,W=3 for: Read-heavy apps. R=3,W=1 for: Write-heavy logging. R=2,W=2 for: Balanced workloads needing consistency."
          - question: "Vector clocks vs Lamport timestamps: accuracy vs overhead analysis"
            answer: "Vector clocks: Detect all causality (concurrent vs sequential), O(N) space per node, 100 nodes = 800B overhead. Lamport: Total order only (can't detect concurrent), O(1) space = 8B overhead, simpler. Use vector clocks for: Conflict detection (Riak, Dynamo). Lamport for: Event ordering (logs, audit trails). Trade-off: 100x space overhead for perfect causality detection."
          - question: "CRDT vs application-level conflict resolution: development complexity vs runtime overhead"
            answer: "CRDT: Zero conflict resolution code (automatic convergence), 2-5x storage overhead (tombstones, metadata), limited operations (counters, sets, registers). App-level: Custom merge logic (complex dev), minimal overhead, supports any operation. Use CRDT for: Counters, flags, sets (90% of conflicts). App-level for: Complex business logic (shopping cart merge, document reconciliation). CRDT saves dev time, app-level saves runtime costs."
          - question: "Session consistency vs global consistency: implementation complexity vs user experience"
            answer: "Session: Simple (sticky sessions or session token), per-user consistent, cheap (no global coordination), but different users see different views. Global: Complex (distributed consensus), all users see same data, expensive (2PC, Paxos), 3-5x higher latency. Use session for: Social media, dashboards (users don't compare views). Global for: Auctions, inventory (users compete for resources). Session gives 90% of UX at 10% of cost."
        scenarios:
          - "Design consistency model for collaborative document editor with real-time collaboration."
          - "Chat application: users in different regions send messages. Ensure message ordering per conversation."
          - "Online auction system: handle concurrent bids while preventing race conditions."
          - "Distributed counter (likes, views): design for high write volume with acceptable read consistency."
          - "Multi-tenant SaaS: ensure tenant data isolation while optimizing for performance."
      time_estimate: 60

    - day: 6
      topic: "CAP Theorem"
      activity: "Master CAP theorem trade-offs, PACELC extensions, and practical system design decisions."
      detailed_content: |
        CAP Theorem Fundamentals:
        You can only guarantee 2 of 3 properties simultaneously:
        - Consistency (C): All nodes return the same data at the same time
          * Every read receives the most recent write or an error
          * Also called "linearizability" or "atomic consistency"
          * Requires coordination between nodes
        - Availability (A): System remains operational and responsive
          * Every request receives a response (success or failure)
          * System continues to function despite node failures
          * No guarantees about data freshness
        - Partition Tolerance (P): System continues despite network failures
          * System functions when network connections fail between nodes
          * Essential for distributed systems (network partitions are inevitable)
          * Must be included in any distributed system design

        CAP Trade-off Scenarios:
        - CP (Consistency + Partition Tolerance):
          * Choose consistency over availability during network partitions
          * System becomes unavailable to maintain data consistency
          * Examples: Traditional RDBMS with synchronous replication, MongoDB, HBase
          * Use cases: Financial systems, inventory management, configuration stores
        - AP (Availability + Partition Tolerance):
          * Choose availability over consistency during network partitions
          * System remains available but may return stale data
          * Examples: Cassandra, DynamoDB, CouchDB, DNS
          * Use cases: Social media feeds, content delivery, analytics
        - CA (Consistency + Availability):
          * Only possible in absence of network partitions
          * Not realistic for distributed systems at scale
          * Examples: Single-node databases, systems within same datacenter
          * Use cases: Simple web applications, development environments

        PACELC Theorem Extension:
        "If Partition, choose between Availability and Consistency,
         Else (no partition), choose between Latency and Consistency"
        - PA/EL: Prioritize availability during partitions, latency during normal operation
          * Examples: Cassandra, DynamoDB
        - PA/EC: Prioritize availability during partitions, consistency during normal operation
          * Examples: MongoDB with eventual consistency settings
        - PC/EL: Prioritize consistency during partitions, latency during normal operation
          * Examples: BigTable, HBase
        - PC/EC: Prioritize consistency during partitions and normal operation
          * Examples: Traditional RDBMS, strongly consistent systems

        Real-world System Examples:
        - Amazon DynamoDB (AP/EL):
          * Eventually consistent by default
          * Remains available during partitions
          * Optimizes for low latency
          * Offers strongly consistent reads at higher latency
        - Google Spanner (CP/EC):
          * Strongly consistent across global deployment
          * May become unavailable during certain partition scenarios
          * Uses synchronized clocks for global consistency
        - Apache Cassandra (AP/EL):
          * Eventual consistency with tunable consistency levels
          * Always available for reads and writes
          * Optimizes for low latency and high throughput
        - MongoDB (CP/EL):
          * Strong consistency within replica sets
          * Primary becomes unavailable during partition
          * Chooses consistency over availability
        - Redis Cluster (AP/EL):
          * Eventually consistent between clusters
          * Individual nodes prioritize availability
          * Fast in-memory operations

        Implementation Strategies:
        - CP Systems Implementation:
          * Quorum-based writes: W > N/2
          * Master-slave with failover delays
          * Consensus protocols (Raft, Paxos)
          * Strong consistency at cost of availability
        - AP Systems Implementation:
          * Multi-master replication
          * Conflict resolution mechanisms
          * Read-repair and anti-entropy
          * Vector clocks for causality tracking
        - Tunable Consistency:
          * Allow applications to choose per-operation
          * Examples: Cassandra's consistency levels, DynamoDB's read consistency

        Practical Design Decisions:
        - Analyze Business Requirements:
          * Can business tolerate stale data? → AP
          * Must data always be accurate? → CP
          * Are network partitions common? → Must choose P
        - System Characteristics:
          * High read/write ratio → Consider AP with read-heavy optimization
          * Critical transactions → CP for data integrity
          * Global distribution → AP for latency
        - Hybrid Approaches:
          * Different components can make different CAP choices
          * Core data: CP (user accounts, transactions)
          * Derived data: AP (recommendations, analytics)
          * Caching layer: AP for performance

        Common Misconceptions:
        - "CAP forces binary choice": Can tune consistency levels
        - "CA systems are viable": Network partitions are inevitable
        - "AP means no consistency": Eventual consistency still provides guarantees
        - "Choose once globally": Different parts can make different choices

        Beyond CAP - Additional Considerations:
        - Performance: Latency and throughput requirements
        - Operational Complexity: Monitoring, debugging, maintenance
        - Data Model Fit: Relational vs document vs key-value
        - Ecosystem Maturity: Tools, expertise, community support
        - Cost: Infrastructure, development, operational costs

        CAP in Modern Architecture:
        - Microservices: Each service can make independent CAP choices
        - Event-Driven: Often AP with eventual consistency
        - CQRS: Write side CP, read side AP
        - Multi-region: Geographic regions may have different CAP profiles
      resources:
        - title: "CAP Theorem Explained"
          url: "https://www.ibm.com/topics/cap-theorem"
          description: "Comprehensive CAP theorem guide"
        - title: "PACELC Theorem"
          url: "https://en.wikipedia.org/wiki/PACELC_theorem"
          description: "Extension of CAP theorem with latency"
        - title: "CAP Twelve Years Later"
          url: "https://www.infoq.com/articles/cap-twelve-years-later-how-the-rules-have-changed/"
          description: "Eric Brewer's updated perspective on CAP"
        - title: "Jepsen Testing"
          url: "https://jepsen.io/"
          description: "Real-world consistency testing of distributed systems"
      practice_questions:
        estimation:
          - question: "Banking system: 99.99% availability requirement. Calculate downtime budget and analyze CAP implications."
            answer: "99.99% uptime = 52.56 min downtime/year = 4.38 min/month. For banking: Must choose CP (consistency critical for money). During partition: System unavailable but maintains data integrity. Alternative: Multi-region with async replication (99.99% per region, fail over in <1min). Cost: $10K/month for multi-region CP system vs $3K single region."
          - question: "Global CDN: 100ms latency target, 99.9% availability. Design CAP strategy for content delivery."
            answer: "CDN is classic AP system: Availability critical (serve stale content better than errors), eventual consistency acceptable (content changes infrequent). Edge nodes: 20-50ms regional latency, 1-5min cache staleness. On partition: Serve from cache indefinitely. TTL: 5min for dynamic, 24h for static. Cost: $500/month for 1TB/month globally distributed."
          - question: "E-commerce: Black Friday traffic 100x normal. Choose CAP properties for different system components."
            answer: "Product catalog: AP (stale prices OK for 1-5s, 100K QPS). Inventory: CP with reserved pool (prevent overselling, 10K TPS). Shopping cart: AP with session consistency (user sees own cart, 50K writes/sec). Checkout: CP (payment critical, 5K TPS). Analytics: AP (eventual, 1M events/sec). Trade-off: 95% AP for scale, 5% CP for money."
        concepts:
          - question: "Social media platform: design CAP strategy for user posts vs friend relationships vs activity feeds."
            answer: "Posts: AP (eventual consistency, 1-5s propagation acceptable, scales to 1M posts/sec). Friend relationships: CP (prevent double-adds, mutual consistency, 10K/sec sufficient). Activity feeds: AP (timelines can be eventually consistent, use fan-out on write). Use separate DBs: Posts in Cassandra (AP), Friendships in PostgreSQL (CP), Feeds in Redis (AP cache)."
          - question: "Online gaming: real-time leaderboard vs game state vs user profiles. Assign CAP choices per component."
            answer: "Game state: CP (prevent cheating, authoritative server, 10K updates/sec, 50ms latency acceptable). Leaderboard: AP (eventual consistency OK, 1-5s lag, CRDT counters, 100K updates/sec). User profiles: CP (consistent inventory, prevent dupes, 1K updates/sec). Pattern: Critical game logic CP, auxiliary features AP for scale."
          - question: "IoT sensor network: 10K sensors, occasional network issues. Design data collection strategy."
            answer: "Use AP: Sensors write to local buffer → Async to cloud when connected. Accept: 1) Duplicate data (dedupe server-side by timestamp+sensor_id), 2) Out-of-order arrival (sort by timestamp), 3) Temporary data loss (sensor buffer 1h = 3600 readings). Benefits: 100% uptime, 10K sensors × 1 reading/sec = 10K writes/sec to time-series DB (InfluxDB). CP would fail during network issues."
          - question: "Why are CA systems not viable in distributed environments? Provide concrete scenarios."
            answer: "Network partitions are inevitable: 1) Switch failure splits datacenter (happens annually), 2) Regional internet outage (AWS us-east-1 2021), 3) Software bugs cause split-brain. CA assumes no partitions = single point of failure. Example: 2-node CA system with network partition must: Stop serving (lose A) or risk split-brain (lose C). Reality: All distributed systems must choose CP or AP."
          - question: "Explain how Cassandra achieves AP while still providing some consistency guarantees."
            answer: "Cassandra is AP but offers tunable consistency: 1) Write with QUORUM (W=2/3): Majority write for consistency, 2) Read with QUORUM: Majority read ensures latest value, 3) Read-repair: Fix inconsistencies on read, 4) Hinted handoff: Queue writes during node failure. Trade-off: QUORUM consistency = 2x latency (20ms vs 10ms) but still available (tolerates 1/3 node failures)."
          - question: "How does Google Spanner achieve strong consistency across global deployment?"
            answer: "Spanner uses TrueTime API: Atomic clocks + GPS provide time uncertainty bounds (<7ms). For writes: Wait out uncertainty before commit = global order. Paxos consensus groups per region. Read from closest replica with version guarantee. Cost: 5-10ms write latency (2PC + TrueTime wait). Chooses CP (unavailable during partition) with optimizations for global scale. Alternative: CockroachDB uses hybrid logical clocks (cheaper, no atomic clocks)."
        tradeoffs:
          - question: "Financial trading system vs social media feed: compare CAP choices and justify decisions"
            answer: "Trading: CP mandatory (prevent double-trades, accurate order books, regulatory compliance). Cost: 50-100ms latency, single-region master, 99.9% availability acceptable. Social feed: AP optimal (stale posts OK, scale to 1M posts/sec, 99.99% availability). Cost: 1-5s eventual consistency. Trading loses $1M/min during downtime, justifies CP. Social loses engagement but survives outages, justifies AP."
          - question: "Strong consistency vs eventual consistency: quantify business impact for e-commerce inventory"
            answer: "Strong: No overselling (0 customer complaints), 50ms checkout latency, 10K TPS limit, complex ops. Eventual: 0.1% oversell rate = 100 complaints per 100K orders = $10K refunds/day, 10ms latency, 100K TPS, simple ops. Decision: Use strong for high-value items (>$1000), eventual for cheap items (<$20). Middle ground: Reserved inventory pool (95% strong, 5% eventual flexibility)."
          - question: "Single-region CP vs multi-region AP: availability, latency, and cost analysis"
            answer: "Single-region CP: 99.9% availability (regional outage = total failure), 20-50ms latency (within region), $3K/month. Multi-region AP: 99.99% availability (one region fails = others work), 20ms local + 200ms cross-region writes, $15K/month (5x cost). Use multi-region for: Global apps (>10M users), revenue >$1M/month (downtime cost justifies 5x infrastructure cost)."
          - question: "PACELC analysis: MongoDB vs Cassandra for content management system"
            answer: "MongoDB (PC/EC): Chooses consistency over availability during partition, strong consistency normally. CMS needs: Version control (strong consistency), moderate writes (1K/sec), complex queries. Cassandra (PA/EL): Always available, eventual consistency, 100K writes/sec. CMS benefit from MongoDB: ACID for multi-document updates, easier queries. Use Cassandra for: High-write CMS (news sites, 10K articles/hour)."
          - question: "Hybrid approach: different CAP choices for microservices vs monolithic design complexity"
            answer: "Hybrid microservices: User service (CP, account consistency), Posts service (AP, high throughput), Feed service (AP, eventual aggregation). Complexity: 3 databases, eventual consistency between services, saga patterns needed, 3x ops cost. Monolith: Single CP database, ACID transactions, simpler, but limited scale (10K QPS). Use hybrid when: >100K QPS, need independent scaling. Use monolith until: Scale bottleneck."
        scenarios:
          - "Design CAP strategy for multi-tenant SaaS with customers having different consistency requirements."
          - "Network partition splits your cluster 60-40. Design behavior for CP and AP system variants."
          - "Migrate from CP system (MySQL) to AP system (Cassandra). Plan transition strategy and challenges."
          - "Global messaging app: ensure message delivery while optimizing for local latency."
          - "Real-time analytics platform: balance between data freshness and query performance."
      time_estimate: 60

    - day: 7
      topic: "Week 1 Integration - URL Shortener Design"
      activity: "Apply Week 1 concepts to design a complete URL shortener system integrating scalability, caching, databases, and consistency."
      detailed_content: |
        System Requirements Analysis:
        - Functional Requirements:
          * Shorten long URLs to short aliases (e.g., bit.ly/abc123)
          * Redirect short URLs to original URLs
          * Custom aliases for premium users
          * URL expiration and deletion
          * Analytics (click tracking, geographic data)
          * User accounts and URL management
        - Non-functional Requirements:
          * Scale: 100M new URLs/day, 10B redirects/day (100:1 read/write ratio)
          * Latency: <100ms for redirects, <500ms for shortening
          * Availability: 99.9% uptime (8.77 hours downtime/year)
          * Durability: URLs must not be lost
          * Security: Prevent malicious URLs, abuse prevention

        Capacity Estimation:
        - Traffic Analysis:
          * Write QPS: 100M URLs/day = 1,157 URLs/sec (avg), 2,300/sec (peak 2x)
          * Read QPS: 10B redirects/day = 115,740/sec (avg), 231,480/sec (peak)
          * Total QPS: ~233K at peak (heavily read-dominated)
        - Storage Requirements:
          * URL record: original_url (2KB avg) + short_url (7 bytes) + metadata (100 bytes) = ~2.1KB
          * Daily storage: 100M × 2.1KB = 210GB/day
          * 5-year storage: 210GB × 365 × 5 = 383TB
          * With replication (3x): ~1.15PB total
        - Bandwidth:
          * Incoming: 100M × 2KB = 200GB/day writes
          * Outgoing: 10B × 300 bytes (redirect response) = 3TB/day reads
          * Peak bandwidth: 3TB/86400s × 2 = ~70MB/s
        - Memory (Cache):
          * Cache 20% of daily reads (80/20 rule): 2B URLs × 300 bytes = 600GB
          * Distributed across cache nodes: 600GB / 10 nodes = 60GB per node

        Short URL Generation Strategies:
        - Base62 Encoding:
          * Characters: [a-z, A-Z, 0-9] = 62 characters
          * URL length 7: 62^7 = 3.5 trillion unique URLs
          * Counter-based: Encode auto-incrementing ID
          * Pros: Simple, predictable length
          * Cons: Predictable sequence, scalability bottleneck
        - Hash-based Generation:
          * Hash original URL (MD5/SHA1) + take first 7 characters
          * Pros: Distributed generation, no coordination
          * Cons: Collision handling needed, longer to avoid collisions
        - Random Generation:
          * Generate random 7-character strings
          * Check for collisions in database
          * Pros: Unpredictable, distributed
          * Cons: Collision probability increases over time
        - Hybrid Approach:
          * Combine timestamp + node_id + counter for uniqueness
          * Base62 encode the result
          * Pros: Guaranteed uniqueness, distributed, time-ordered
          * Cons: Slightly more complex

        Database Design:
        - URL Mapping Table:
          ```sql
          CREATE TABLE url_mappings (
            short_url VARCHAR(7) PRIMARY KEY,
            original_url TEXT NOT NULL,
            user_id BIGINT,
            created_at TIMESTAMP,
            expires_at TIMESTAMP,
            click_count BIGINT DEFAULT 0,
            is_active BOOLEAN DEFAULT true
          );
          ```
        - Sharding Strategy:
          * Shard by short_url hash: consistent distribution
          * Range-based: A-G, H-N, O-T, U-Z, 0-9 (uneven distribution)
          * Hash-based sharding recommended for uniform distribution
        - Database Selection:
          * Read-heavy workload: Consider read replicas
          * NoSQL option: DynamoDB/Cassandra for horizontal scaling
          * SQL option: PostgreSQL with sharding for ACID properties

        Caching Architecture:
        - Cache Layers:
          * Browser cache: Cache 302 redirects (1 hour TTL)
          * CDN cache: Cache redirects geographically (1 hour TTL)
          * Application cache: Redis cluster for hot URLs (6 hour TTL)
          * Database cache: Query result caching
        - Cache Strategy:
          * Write-around: Don't cache on write (avoid cold URLs in cache)
          * Cache-aside: Load popular URLs into cache on read miss
          * LRU eviction: Remove least recently used URLs
        - Cache Sizing:
          * Monitor hit ratio: Target >90% for popular URLs
          * 20% of URLs generate 80% of traffic (Pareto principle)

        System Architecture:
        - Load Balancer Layer:
          * Global load balancer: DNS-based geographic routing
          * Regional load balancers: L7 for path-based routing
          * Algorithm: Least connections for backend services
        - Application Services:
          * URL Shortening Service: Handle URL creation
          * Redirect Service: Handle URL lookups and redirects
          * Analytics Service: Process click events asynchronously
          * User Management Service: Handle accounts and authentication
        - Data Storage:
          * Primary Database: Sharded SQL/NoSQL for URL mappings
          * Analytics Database: Time-series DB for click analytics
          * Cache: Redis cluster for hot URL lookups

        Consistency and Availability Trade-offs:
        - CAP Analysis:
          * Choose AP (Availability + Partition tolerance)
          * Eventual consistency acceptable for URL mappings
          * Strong consistency for user accounts and custom aliases
        - Conflict Resolution:
          * Custom aliases: Strong consistency to prevent duplicates
          * Auto-generated URLs: Last-write-wins for metadata updates
          * Analytics: Eventual consistency, aggregate asynchronously

        Advanced Features:
        - Custom Aliases:
          * Check availability in real-time
          * Reserve popular words for premium users
          * Implement with distributed locking or consensus
        - URL Expiration:
          * Background job to mark expired URLs as inactive
          * Grace period before actual deletion
          * Cleanup process for storage reclamation
        - Analytics Implementation:
          * Asynchronous event processing for click tracking
          * Real-time counters vs batch processing trade-offs
          * Geographic data collection and privacy considerations
        - Security Measures:
          * URL validation: Check for malicious content
          * Rate limiting: Prevent abuse and spam
          * Blacklist management: Block harmful domains
          * CAPTCHA for suspicious activity

        Scalability Optimizations:
        - Read Scaling:
          * CDN for global distribution
          * Read replicas for database scaling
          * Multi-level caching strategy
        - Write Scaling:
          * Database sharding for URL creation
          * Asynchronous processing for analytics
          * Batch processing for non-critical operations
        - Auto-scaling:
          * Horizontal scaling based on QPS metrics
          * Cache warm-up during scale events
          * Connection pooling for database efficiency

        Monitoring and Operations:
        - Key Metrics:
          * Latency: p95, p99 for redirects and URL creation
          * Throughput: QPS for reads and writes
          * Cache hit ratio: Monitor cache effectiveness
          * Error rates: 4xx/5xx response tracking
        - Alerting:
          * High latency alerts (>100ms redirects)
          * Cache hit ratio drops (<90%)
          * Database connection pool exhaustion
          * Disk space usage for URL storage
        - Logging:
          * Access logs for debugging and analytics
          * Error logs for system health monitoring
          * Audit logs for security and compliance

        Alternative Designs:
        - Serverless Architecture:
          * AWS Lambda for URL shortening and redirects
          * DynamoDB for storage with auto-scaling
          * CloudFront CDN for global distribution
          * Pros: No server management, automatic scaling
          * Cons: Cold start latency, vendor lock-in
        - Microservices vs Monolith:
          * Monolith: Simple deployment, shared cache
          * Microservices: Independent scaling, service isolation
          * Trade-off: Operational complexity vs scalability

        Migration and Deployment:
        - Blue-Green Deployment:
          * Zero-downtime deployment for critical service
          * Database migration strategies
          * Cache warming for new deployments
        - Data Migration:
          * Online migration for minimal downtime
          * Checksum validation for data integrity
          * Rollback procedures for failed migrations
      resources:
        - title: "System Design: URL Shortener"
          url: "https://www.educative.io/courses/grokking-system-design-fundamentals/url-shortener"
          description: "Complete URL shortener design walkthrough"
        - title: "Designing TinyURL"
          url: "https://github.com/donnemartin/system-design-primer/tree/master/solutions/system_design/pastebin"
          description: "Detailed system design with code examples"
        - title: "Base62 Encoding"
          url: "https://en.wikipedia.org/wiki/Base62"
          description: "Understanding URL encoding schemes"
      practice_questions:
        estimation:
          - "Calculate database size for 10 years: 500M URLs/day, avg 2KB per URL, 3x replication factor."
          - "Design cache size for 95% hit ratio: 1B daily redirects, 80/20 rule, 300 bytes per cached entry."
          - "Network bandwidth: Peak 500K QPS, avg response 500 bytes, calculate total bandwidth needs."
          - "Shard calculation: 100B total URLs, 10TB per shard limit, how many shards needed over 10 years?"
        concepts:
          - "Compare Base62 encoding vs hash-based generation: collision probability, scalability, predictability."
          - "Design URL shortener for 10x higher write traffic (1B URLs/day). What changes to architecture?"
          - "How would you handle custom aliases at scale while preventing collisions across regions?"
          - "Design analytics system to track clicks in real-time vs batch processing. Compare trade-offs."
          - "Implement URL expiration: design background cleanup without affecting read performance."
          - "How do you ensure shortened URLs are evenly distributed across database shards?"
        tradeoffs:
          - "SQL vs NoSQL for URL storage: consistency, scalability, query flexibility comparison"
          - "CDN caching vs application caching: cost, latency, cache invalidation complexity"
          - "Synchronous vs asynchronous analytics: real-time accuracy vs system performance"
          - "Monolithic vs microservices architecture: deployment simplicity vs independent scaling"
          - "Strong vs eventual consistency for custom aliases: user experience vs system complexity"
        scenarios:
          - "Viral URL gets 1M clicks in 1 hour (100x normal traffic). Design caching and scaling strategy."
          - "Database shard becomes unavailable. Design failover strategy maintaining service availability."
          - "Implement geographical URL shortening: different short domains per region with global analytics."
          - "Add URL preview feature: fetch page title/description without affecting redirect performance."
          - "Design A/B testing for redirect pages: route 10% traffic to experimental landing pages."
          - "Handle malicious URL detection: integrate security scanning without blocking legitimate URLs."
      time_estimate: 120

  week2:
    - day: 8
      topic: "Message Queues & Async Communication"
      activity: "Master message queuing patterns, delivery guarantees, system selection, and async architecture design."
      detailed_content: |
        Asynchronous Communication Benefits:
        - Decoupling: Services don't need to know about each other directly
        - Scalability: Handle traffic spikes by queuing requests
        - Reliability: Messages persist even if consumers are temporarily down
        - Performance: Non-blocking operations improve response times
        - Flexibility: Easy to add new consumers without changing producers

        Message Queue Patterns:
        - Point-to-Point (Queue Model):
          * One producer sends messages to one consumer
          * Message consumed by exactly one consumer
          * Load balancing: Multiple consumers compete for messages
          * Use cases: Task processing, job queues, work distribution
          * Example: E-commerce order processing pipeline
        - Publish/Subscribe (Topic Model):
          * One producer publishes to multiple subscribers
          * Message delivered to all interested consumers
          * Topic-based routing: Consumers subscribe to specific topics
          * Use cases: Event notifications, real-time updates, logging
          * Example: User activity events distributed to analytics, recommendations, notifications
        - Request/Reply Pattern:
          * Asynchronous version of synchronous request-response
          * Producer sends request and continues processing
          * Consumer processes and sends reply to callback queue
          * Correlation ID links requests to responses
          * Use cases: Long-running computations, external API calls

        Message Routing Strategies:
        - Direct Routing:
          * Messages routed based on exact routing key match
          * Simple and efficient for known message types
          * Example: Order messages routed to order-processing queue
        - Topic-based Routing:
          * Hierarchical routing using wildcards (user.*, user.created)
          * Flexible subscription patterns
          * Example: user.created, user.updated, user.deleted events
        - Content-based Routing:
          * Route based on message content/headers
          * Most flexible but computationally expensive
          * Example: Route based on geographic region, customer tier
        - Fanout:
          * Broadcast messages to all bound queues
          * No routing logic needed
          * Use cases: Broadcasting announcements, cache invalidation

        Delivery Guarantees:
        - At-Most-Once:
          * Messages delivered zero or one time (never duplicated)
          * Some messages may be lost during failures
          * Lowest overhead and highest performance
          * Use cases: Metrics, logs, non-critical notifications
        - At-Least-Once:
          * Messages delivered one or more times (may be duplicated)
          * No message loss but requires idempotent consumers
          * Most common approach in practice
          * Use cases: Payment processing, inventory updates
        - Exactly-Once:
          * Messages delivered exactly once (no loss, no duplication)
          * Highest complexity and overhead
          * Achieved through distributed transactions or deduplication
          * Use cases: Financial transactions, critical state changes

        Message Ordering Guarantees:
        - No Ordering: Messages may arrive in any order
        - Partition Ordering: Messages within same partition are ordered
        - Global Ordering: All messages across partitions are ordered
        - Trade-off: Stronger ordering = lower throughput and availability

        Popular Message Queue Systems:
        - Apache Kafka:
          * Distributed streaming platform with high throughput
          * Pull-based consumption model
          * Partition-based scaling with ordering guarantees
          * Excellent for event streaming and data pipelines
          * Pros: High throughput, durable, scalable, replay capability
          * Cons: Complex setup, no built-in routing, Java-centric
        - RabbitMQ:
          * Feature-rich message broker with flexible routing
          * Push-based delivery with acknowledgments
          * Exchange types: direct, topic, fanout, headers
          * Pros: Flexible routing, easy setup, multiple protocols
          * Cons: Lower throughput than Kafka, single point of failure
        - Amazon SQS:
          * Fully managed queue service with auto-scaling
          * Standard queues (at-least-once) and FIFO queues (exactly-once)
          * Dead letter queues for failed message handling
          * Pros: Managed service, auto-scaling, integration with AWS
          * Cons: Vendor lock-in, limited message size, eventual consistency
        - Apache Pulsar:
          * Multi-tenant messaging with geo-replication
          * Tiered storage architecture (hot/cold data)
          * Both queuing and streaming in single system
          * Pros: Multi-tenancy, geo-replication, unified model
          * Cons: Newer ecosystem, complex architecture
        - Redis Pub/Sub:
          * In-memory pub/sub with very low latency
          * Fire-and-forget delivery (no persistence)
          * Pros: Extremely fast, simple setup
          * Cons: No durability, limited scaling

        Message Durability and Persistence:
        - In-Memory: Fastest but messages lost on restart
        - Disk-based: Persistent but slower performance
        - Replicated: Multiple copies for high availability
        - Tiered Storage: Hot data in memory, cold data on disk

        Consumer Patterns:
        - Competing Consumer:
          * Multiple consumers compete for messages from same queue
          * Provides load balancing and fault tolerance
          * Messages processed by only one consumer
        - Message Dispatcher:
          * Single consumer distributes work to worker threads
          * Centralized control but potential bottleneck
        - Event-driven Consumer:
          * React to specific events or message types
          * Stateless processing of individual messages
        - Batch Consumer:
          * Process multiple messages together
          * Higher throughput but higher latency

        Error Handling and Resilience:
        - Dead Letter Queues (DLQ):
          * Storage for messages that failed processing repeatedly
          * Prevents poison messages from blocking queue
          * Manual investigation and reprocessing capability
        - Retry Logic:
          * Exponential backoff for temporary failures
          * Maximum retry limits to prevent infinite loops
          * Circuit breaker pattern for downstream failures
        - Message TTL (Time-to-Live):
          * Automatic expiration of old messages
          * Prevents queue buildup from slow consumers
        - Poison Message Handling:
          * Detection and isolation of malformed messages
          * Prevents single bad message from stopping processing

        Performance Considerations:
        - Throughput Optimization:
          * Batch message publishing and consuming
          * Asynchronous acknowledgments
          * Partition/shard messages for parallel processing
        - Latency Optimization:
          * In-memory queues for low latency
          * Minimize message serialization overhead
          * Co-locate producers and consumers
        - Capacity Planning:
          * Monitor queue depth and consumer lag
          * Auto-scaling based on queue metrics
          * Provision for traffic spikes

        Message Serialization:
        - JSON: Human-readable, widely supported, larger size
        - Protocol Buffers: Compact, fast, schema evolution
        - Avro: Schema evolution, data validation, Kafka integration
        - MessagePack: Compact binary format, cross-language support

        Monitoring and Observability:
        - Key Metrics:
          * Message throughput (messages/second)
          * Consumer lag (time behind latest message)
          * Queue depth (messages waiting to be processed)
          * Error rates and failed message counts
        - Distributed Tracing:
          * Track message flow across services
          * Identify bottlenecks and failures
          * Correlation IDs for request tracking
        - Alerting:
          * High consumer lag indicating processing issues
          * Queue depth growing faster than consumption
          * Error rate spikes indicating system problems

        Architecture Patterns:
        - Event-Driven Architecture:
          * Services communicate through events
          * Loose coupling and high scalability
          * Example: E-commerce with order, payment, inventory services
        - Saga Pattern:
          * Manage distributed transactions through events
          * Choreography vs orchestration approaches
          * Compensation events for rollback scenarios
        - CQRS with Event Sourcing:
          * Separate read and write models
          * Event store as source of truth
          * Replay events to rebuild state
        - Lambda Architecture:
          * Batch and stream processing layers
          * Speed layer for real-time processing
          * Batch layer for comprehensive analysis

        Anti-patterns to Avoid:
        - Synchronous Chain: Calling services synchronously in sequence
        - Shared Database: Multiple services sharing same database
        - Chatty Messaging: Too many small messages instead of batching
        - Message Ordering Dependency: Requiring global message ordering
        - Large Message Payloads: Sending large data through messages

        Selection Criteria:
        - Throughput Requirements: Kafka for high throughput, RabbitMQ for moderate
        - Ordering Requirements: Kafka for partition ordering, SQS FIFO for strict ordering
        - Durability Needs: Persistent queues for critical data, in-memory for performance
        - Operational Complexity: Managed services (SQS) vs self-hosted (Kafka)
        - Integration Ecosystem: Consider existing technology stack
      resources:
        - title: "Kafka vs RabbitMQ"
          url: "https://aws.amazon.com/compare/the-difference-between-rabbitmq-and-kafka/"
          description: "Message queue technology comparison"
        - title: "Message Queue Patterns"
          url: "https://www.enterpriseintegrationpatterns.com/patterns/messaging/"
          description: "Enterprise messaging patterns"
        - title: "Designing Event-Driven Systems"
          url: "https://www.confluent.io/designing-event-driven-systems/"
          description: "Event-driven architecture with Kafka"
        - title: "AWS SQS Best Practices"
          url: "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-best-practices.html"
          description: "Managed queue service optimization"
      practice_questions:
        estimation:
          - question: "E-commerce system: 1M orders/day, each order triggers 5 events. Calculate Kafka partition strategy and throughput requirements."
            answer: "Events: 1M × 5 = 5M events/day = 58 events/sec avg, 300 peak (5x). Kafka throughput: 1KB/event × 300 = 300KB/sec = 0.3MB/sec. Partitions: 10 partitions (each handles 30 events/sec), enables 10 parallel consumers. Replication factor 3 = 0.9MB/sec write. Storage: 5M × 1KB × 365 = 1.8TB/year. Cost: ~$200/month for 3-node Kafka cluster."
          - question: "Real-time analytics: 100K events/sec, 1KB per message. Estimate memory and storage for 7-day retention."
            answer: "Throughput: 100K × 1KB = 100MB/sec = 360GB/hour = 8.6TB/day. 7-day retention: 8.6TB × 7 = 60TB. Memory (hot data, 1 hour): 360GB across 10 Kafka brokers = 36GB each. Compression (5:1): 60TB → 12TB storage. With 3x replication: 36TB total. Cost: ~$5K/month for storage + compute."
          - question: "Message processing: 1000 msg/sec rate, 100ms processing time per message. How many consumers needed for real-time processing?"
            answer: "Processing capacity per consumer: 1000ms / 100ms = 10 msg/sec. Consumers needed: 1000 / 10 = 100 consumers. With headroom (80% utilization): 125 consumers. Cost: If 10 consumers per instance (1 core each), need 13 instances. Alternative: Reduce processing to 50ms (optimization) = 63 consumers needed."
          - question: "Queue capacity: 10K msg/sec peak, 2-hour processing lag tolerance. Calculate required queue capacity."
            answer: "Max messages in queue: 10K/sec × 2 hours × 3600s = 72M messages. At 1KB/message: 72GB. With overhead (30%): 94GB. Use disk-backed queue (SSD). Normal operation: 1K/sec × 5min lag = 300K messages = 300MB (memory-based). Design: Redis for normal (<5min lag), fall back to Kafka for spikes (>5min lag)."
        concepts:
          - question: "Design message flow for ride-sharing app: trip requests, driver matching, payment processing, trip completion."
            answer: "Flow: 1) User → trip.requested event → Kafka, 2) Matching service consumes → finds driver → trip.matched event, 3) Payment service → charges card → payment.completed, 4) Trip service → trip.completed. Use: At-least-once delivery with idempotency keys. Ordering: Partition by trip_id (per-trip ordering). Latency: 100ms total. Failure: DLQ for payment failures, manual review."
          - question: "Compare at-least-once vs exactly-once delivery for inventory updates. Design idempotent consumers."
            answer: "At-least-once: Simpler, 10ms latency, may duplicate (e.g., decrement inventory twice). Exactly-once: Complex (2PC), 50ms latency, guarantees no duplicates. Idempotent design: Store processed message IDs in DB, check before processing. Redis SET with 24h TTL for deduplication. Cost: 1ms lookup overhead. Recommendation: Use at-least-once + idempotency (best of both worlds)."
          - question: "Implement saga pattern for hotel booking: reserve room, charge payment, confirm booking with rollback capability."
            answer: "Choreography approach: 1) booking.initiated → Room service reserves (30s timeout) → room.reserved, 2) Payment service charges → payment.completed, 3) Confirmation service → booking.confirmed. Rollback: payment.failed → room.release-reservation. Use compensating transactions. Store saga state in DB. Timeout: Cancel after 2min total. Alternative: Orchestrator pattern with centralized workflow (simpler debugging, single point of failure)."
          - question: "Design event-driven microservices for social media: user posts, notifications, timeline updates, analytics."
            answer: "Events: post.created → Kafka (4 partitions by user_id). Consumers: 1) Notification service (sends push, 50ms), 2) Timeline service (fan-out to followers, 200ms), 3) Analytics service (batch processing, 5min lag). Use fanout exchange (RabbitMQ) or topic (Kafka). Throughput: 1000 posts/sec. Each event copied to 3 services = 3K msg/sec consumption. Cost: Kafka cluster $500/month."
          - question: "Handle message ordering in distributed system: per-user ordering vs global ordering trade-offs."
            answer: "Per-user: Partition by user_id, 100K users × 10 msg/sec = 1M msg/sec total across partitions. Parallelism: 100 partitions = 100 consumers. Latency: 10ms. Global ordering: Single partition = 10K msg/sec limit (bottleneck), single consumer, 50ms latency. Use per-user for: Chat apps, user feeds. Global for: Banking ledger, audit logs (less common). Per-user gives 100x throughput."
          - question: "Design dead letter queue strategy: retry logic, poison message detection, manual recovery process."
            answer: "Strategy: 1) Initial failure → Retry queue (exponential backoff: 1s, 5s, 30s), 2) After 3 retries → DLQ with error reason, 3) Monitor DLQ depth, alert if >100 messages, 4) Manual replay: Fix issue → Move messages back to main queue. Poison messages: Detect by error pattern (e.g., JSON parse error) → Auto-skip, log for investigation. Metrics: DLQ size, retry success rate."
        tradeoffs:
          - question: "Kafka vs RabbitMQ for real-time chat application: latency, throughput, complexity comparison"
            answer: "Kafka: 10-20ms latency, 100K msg/sec throughput, complex setup (ZooKeeper/KRaft), replay messages, partition ordering. RabbitMQ: 5-10ms latency, 20K msg/sec throughput, easier setup, flexible routing, no replay. For chat: Use RabbitMQ (lower latency matters more than ultra-high throughput, 20K msg/sec sufficient for 100K users). Use Kafka for: Chat history/analytics pipeline."
          - question: "Push vs pull consumption models: resource usage, flow control, scalability analysis"
            answer: "Push (RabbitMQ): Broker pushes to consumers, low latency (5ms), consumers may be overwhelmed (need flow control), broker tracks consumer state. Pull (Kafka): Consumers pull messages, slightly higher latency (10ms), consumers control rate (natural backpressure), consumers track offset. Use push for: Low-latency real-time. Pull for: High throughput, batch processing, independent consumer rates."
          - question: "Synchronous vs asynchronous API calls: consistency, performance, error handling trade-offs"
            answer: "Synchronous: Immediate response, strong consistency, 50-200ms latency, caller waits, simple error handling (HTTP status), tight coupling. Async: Immediate accept (202), eventual consistency, 10ms response + background processing, caller continues, complex error handling (callbacks/polling), loose coupling. Use sync for: User-facing APIs (<200ms). Async for: Long operations (reports, batch), decoupled systems."
          - question: "Message durability vs performance: in-memory vs persistent queues for different use cases"
            answer: "In-memory (Redis Pub/Sub): 1ms latency, 100K msg/sec, no durability (lost on restart), $50/month. Persistent (Kafka): 10ms latency, 50K msg/sec, durable (survives restart), $500/month. Use in-memory for: Real-time notifications, ephemeral data. Persistent for: Financial data, audit logs, critical events. Hybrid: In-memory primary + persistent backup for best performance + durability."
          - question: "Single large message vs multiple small messages: network overhead, processing complexity"
            answer: "Large message (1MB): 1 network call, 50ms transfer, harder to process (parse all), retry entire message on failure. Small messages (100 × 10KB): 100 network calls (overhead), 50ms total (parallel), easier to process (stream), retry only failed messages. Use large for: Batch uploads, reports. Small for: Real-time events, granular processing. Threshold: Messages >100KB should be split or use reference (store in S3, send URL)."
        scenarios:
          - "Design messaging for flash sale: handle 100x traffic spike, ensure inventory consistency, prevent overselling."
          - "Implement event sourcing for banking system: account transactions, balance calculations, audit requirements."
          - "Design notification system: email, SMS, push notifications with user preferences and delivery guarantees."
          - "Handle queue consumer failures: automatic failover, message reprocessing, duplicate detection."
          - "Design cross-region message replication: data consistency, network partitions, conflict resolution."
          - "Implement circuit breaker for message processing: detect downstream failures, graceful degradation."
      time_estimate: 60

    - day: 9
      topic: "API Design Best Practices"
      activity: "Master comprehensive API design including REST/GraphQL, security, performance, monitoring, and developer experience optimization."
      detailed_content: |
        RESTful API Design Principles:
        REST (Representational State Transfer) Core Constraints:
        - Client-Server: Separation of concerns between UI and data storage
        - Stateless: Each request contains all necessary information
        - Cacheable: Responses explicitly indicate cacheability
        - Uniform Interface: Consistent resource identification and manipulation
        - Layered System: Architecture composed of hierarchical layers
        - Code on Demand (optional): Server can extend client functionality

        HTTP Methods and Semantics:
        - GET: Retrieve resource representation (idempotent, safe)
        - POST: Create new resource or non-idempotent operations
        - PUT: Replace entire resource (idempotent)
        - PATCH: Partial resource update (may not be idempotent)
        - DELETE: Remove resource (idempotent)
        - HEAD: GET without response body (metadata only)
        - OPTIONS: Discover allowed methods for resource

        Resource Design and URL Structure:
        - Use nouns for resources, not verbs: /users/123/orders (not /getUserOrders)
        - Hierarchical relationships: /users/123/orders/456/items
        - Collection vs singleton: /users (collection), /users/123 (singleton)
        - Query parameters for filtering: /users?status=active&role=admin
        - Avoid deep nesting (max 2-3 levels): /users/123/orders/456
        - Consistent naming conventions: snake_case or camelCase

        HTTP Status Codes Strategy:
        Success Codes:
        - 200 OK: Standard successful response
        - 201 Created: Resource successfully created
        - 202 Accepted: Request accepted for processing
        - 204 No Content: Successful with no response body

        Client Error Codes:
        - 400 Bad Request: Invalid request format/parameters
        - 401 Unauthorized: Authentication required
        - 403 Forbidden: Access denied (authenticated but not authorized)
        - 404 Not Found: Resource doesn't exist
        - 409 Conflict: Resource conflict (duplicate creation)
        - 422 Unprocessable Entity: Valid format but semantic errors
        - 429 Too Many Requests: Rate limit exceeded

        Server Error Codes:
        - 500 Internal Server Error: Unexpected server condition
        - 502 Bad Gateway: Invalid response from upstream
        - 503 Service Unavailable: Temporary service overload
        - 504 Gateway Timeout: Upstream timeout

        GraphQL vs REST Comparison:
        REST Advantages:
        - Simple and widely understood
        - Leverages HTTP caching effectively
        - Better tooling and infrastructure support
        - Easier to cache at CDN/proxy level
        - Stateless and scalable by default

        GraphQL Advantages:
        - Single endpoint for all operations
        - Client specifies exact data requirements
        - Strongly typed schema with introspection
        - Real-time subscriptions support
        - Eliminates over-fetching and under-fetching
        - Better for mobile applications (bandwidth efficiency)

        When to Choose Each:
        - REST: Public APIs, simple CRUD operations, heavy caching requirements
        - GraphQL: Complex data relationships, mobile apps, rapid client development

        API Versioning Strategies:
        1. URL Path Versioning:
           - Format: /v1/users, /v2/users
           - Pros: Clear, explicit, cacheable
           - Cons: URL proliferation, tight coupling

        2. Header Versioning:
           - Format: Accept: application/vnd.api+json;version=2
           - Pros: Clean URLs, flexible content negotiation
           - Cons: Less visible, harder to test manually

        3. Query Parameter Versioning:
           - Format: /users?version=2
           - Pros: Simple, backward compatible
           - Cons: Easy to forget, query parameter pollution

        4. Custom Header Versioning:
           - Format: API-Version: 2.0
           - Pros: Explicit, doesn't affect URL structure
           - Cons: Additional complexity, tooling requirements

        Versioning Best Practices:
        - Semantic versioning: Major.Minor.Patch (2.1.3)
        - Deprecation timeline and communication
        - Sunset HTTP header for deprecated versions
        - Maintain backward compatibility within major versions
        - Version breaking changes only, not additions

        Authentication and Authorization:
        Authentication Methods:
        - API Keys: Simple but limited (no user context)
        - OAuth 2.0: Industry standard, supports various flows
        - JWT (JSON Web Tokens): Stateless, self-contained
        - Basic Authentication: Simple but requires HTTPS
        - Bearer Tokens: Flexible token-based approach

        OAuth 2.0 Flows:
        - Authorization Code: For web applications with backend
        - Client Credentials: For service-to-service communication
        - Resource Owner Password: Legacy apps (not recommended)
        - Implicit Flow: Deprecated for security reasons
        - PKCE: Enhanced security for mobile/SPA applications

        JWT Implementation:
        - Header: Algorithm and token type
        - Payload: Claims (user data, expiration)
        - Signature: Verification of token integrity
        - Stateless: No server-side session storage required
        - Security: Use strong secrets, short expiration times

        Authorization Strategies:
        - Role-Based Access Control (RBAC): Users have roles with permissions
        - Attribute-Based Access Control (ABAC): Fine-grained attribute evaluation
        - Scope-based: OAuth scopes define access boundaries
        - Resource-based: Per-resource permission checking

        Rate Limiting and Throttling:
        Rate Limiting Algorithms:
        1. Token Bucket:
           - Tokens added at fixed rate to bucket (max capacity)
           - Requests consume tokens
           - Allows burst traffic up to bucket capacity
           - Implementation: Redis with INCR and EXPIRE

        2. Leaky Bucket:
           - Requests enter bucket, processed at fixed rate
           - Smooth, consistent request rate
           - Drops requests when bucket overflows
           - Good for traffic shaping

        3. Fixed Window Counter:
           - Count requests in fixed time windows
           - Simple to implement and understand
           - Thundering herd problem at window boundaries
           - Implementation: Redis with time-based keys

        4. Sliding Window Log:
           - Track timestamps of all requests
           - Accurate but memory intensive
           - Good for strict rate limiting
           - Implementation: Redis sorted sets

        5. Sliding Window Counter:
           - Hybrid approach combining fixed window efficiency
           - Approximate sliding window accuracy
           - Memory efficient
           - Weighted calculation across windows

        Distributed Rate Limiting:
        - Centralized: Redis/database for global limits
        - Local + Sync: Local counters with periodic sync
        - Consistent Hashing: Distribute limits across nodes
        - Leader Election: Designated node tracks global state

        API Security Best Practices:
        Input Validation and Sanitization:
        - Validate all input parameters and headers
        - Use allow-lists over deny-lists
        - Implement proper data type checking
        - Sanitize outputs to prevent XSS
        - SQL injection prevention with parameterized queries

        Transport Security:
        - Always use HTTPS (TLS 1.2+ minimum)
        - HTTP Strict Transport Security (HSTS) headers
        - Certificate pinning for mobile applications
        - Secure cookie flags (HttpOnly, Secure, SameSite)

        Common Vulnerabilities:
        - Injection attacks (SQL, NoSQL, Command injection)
        - Broken authentication and session management
        - Insecure direct object references
        - Cross-Site Request Forgery (CSRF)
        - Server-Side Request Forgery (SSRF)
        - Mass assignment vulnerabilities

        Security Headers:
        - Content-Security-Policy: Prevent XSS attacks
        - X-Frame-Options: Prevent clickjacking
        - X-Content-Type-Options: Prevent MIME sniffing
        - X-XSS-Protection: Enable browser XSS filters

        Error Handling and Response Design:
        Error Response Structure:
        - Consistent error format across all endpoints
        - Machine-readable error codes
        - Human-readable error messages
        - Additional context for debugging (in development)
        - Correlation IDs for request tracing

        Example Error Response:
        ```json
        {
          "error": {
            "code": "VALIDATION_FAILED",
            "message": "Request validation failed",
            "details": [
              {
                "field": "email",
                "error": "Invalid email format"
              }
            ],
            "trace_id": "abc123xyz"
          }
        }
        ```

        Pagination Strategies:
        1. Offset-based (LIMIT/OFFSET):
           - Simple but inefficient for large datasets
           - Consistent with SQL databases
           - Problem: data shifts during pagination

        2. Cursor-based (keyset pagination):
           - Efficient for large datasets
           - Stable pagination with data changes
           - Uses unique, ordered field as cursor

        3. Token-based:
           - Opaque pagination tokens
           - Flexible implementation
           - Enables advanced pagination features

        Response Filtering and Field Selection:
        - Sparse fieldsets: ?fields=id,name,email
        - Resource expansion: ?expand=profile,orders
        - Filtering: ?filter[status]=active
        - Sorting: ?sort=-created_at,name

        API Documentation and Developer Experience:
        OpenAPI/Swagger Specification:
        - Machine-readable API definitions
        - Automatic documentation generation
        - Code generation for clients and servers
        - Interactive API exploration
        - Contract-first development approach

        Documentation Best Practices:
        - Complete request/response examples
        - Error scenarios and handling
        - Authentication requirements
        - Rate limiting information
        - SDK availability and code samples
        - Changelog and migration guides

        Developer Onboarding:
        - Quick start guides and tutorials
        - Sandbox environment for testing
        - Postman collections or similar tools
        - Community forums and support channels
        - Clear pricing and usage tiers

        API Monitoring and Observability:
        Key Metrics to Track:
        - Request latency (P50, P95, P99 percentiles)
        - Throughput (requests per second)
        - Error rates by status code
        - Authentication failure rates
        - Rate limiting trigger frequency
        - Payload sizes and response times

        Logging Best Practices:
        - Structured logging (JSON format)
        - Correlation IDs for request tracing
        - Log levels: ERROR, WARN, INFO, DEBUG
        - Sensitive data scrubbing
        - Centralized log aggregation

        Distributed Tracing:
        - Request flow across microservices
        - Performance bottleneck identification
        - Error propagation tracking
        - Tools: Jaeger, Zipkin, DataDog APM

        Health Checks and Status Pages:
        - Health endpoint (/health, /status)
        - Dependency health checking
        - Circuit breaker status
        - Public status page for incidents

        Performance Optimization:
        Caching Strategies:
        - HTTP caching headers (Cache-Control, ETag)
        - CDN integration for static responses
        - Application-level caching (Redis, Memcached)
        - Database query optimization

        Response Optimization:
        - Compression (gzip, Brotli)
        - Minification of response payloads
        - Efficient serialization formats
        - Connection pooling and keep-alive

        Database Performance:
        - Query optimization and indexing
        - Connection pooling
        - Read replicas for query distribution
        - Caching frequently accessed data

        API Gateway Patterns:
        Gateway Responsibilities:
        - Request routing and load balancing
        - Authentication and authorization
        - Rate limiting and throttling
        - Request/response transformation
        - API versioning and deprecation
        - Monitoring and analytics
        - Security policy enforcement

        Popular API Gateway Solutions:
        - Kong: Plugin-based, high performance
        - Ambassador: Kubernetes-native, Envoy-based
        - AWS API Gateway: Serverless, fully managed
        - Zuul: Netflix OSS, Java-based
        - Istio: Service mesh with gateway capabilities

        Gateway vs Service Mesh:
        - API Gateway: North-south traffic (client-to-service)
        - Service Mesh: East-west traffic (service-to-service)
        - Complementary patterns for complete traffic management

        API Testing Strategies:
        Unit Testing:
        - Controller/handler testing
        - Business logic validation
        - Mock external dependencies
        - Test data validation and serialization

        Integration Testing:
        - End-to-end API workflow testing
        - Database integration validation
        - External service integration
        - Authentication and authorization flows

        Contract Testing:
        - Provider contracts (API specification adherence)
        - Consumer contracts (client expectation validation)
        - Tools: Pact, Spring Cloud Contract

        Performance Testing:
        - Load testing under normal conditions
        - Stress testing at capacity limits
        - Spike testing with sudden traffic increases
        - Endurance testing for memory leaks

        Real-World Examples:
        E-commerce API Design:
        - Product catalog with search and filtering
        - Shopping cart management (session vs persistent)
        - Order processing workflow
        - Payment integration and webhooks
        - Inventory management and concurrency

        Social Media API:
        - User profile and relationship management
        - Content publishing and feed generation
        - Real-time notifications
        - Image/video upload handling
        - Privacy controls and content moderation

        Financial Services API:
        - Account and transaction management
        - Payment processing and settlements
        - Regulatory compliance (PCI DSS, SOX)
        - Fraud detection integration
        - High availability and consistency requirements
      resources:
        - title: "REST API Tutorial"
          url: "https://restfulapi.net/"
          description: "Comprehensive REST API design guide and best practices"
        - title: "OpenAPI Specification"
          url: "https://swagger.io/specification/"
          description: "Standard for REST API documentation and tooling"
        - title: "GraphQL Best Practices"
          url: "https://graphql.org/learn/best-practices/"
          description: "Official GraphQL implementation guidance"
        - title: "OAuth 2.0 Security Best Practices"
          url: "https://tools.ietf.org/html/draft-ietf-oauth-security-topics"
          description: "Security considerations for OAuth implementations"
        - title: "API Rate Limiting Patterns"
          url: "https://stripe.com/blog/rate-limiters"
          description: "Stripe's approach to distributed rate limiting"
        - title: "HTTP Status Code Guide"
          url: "https://httpstatuses.com/"
          description: "Complete reference for HTTP status codes"
      practice_questions:
        estimation:
          - question: "API serves 10M requests/day. With 1KB average response, what's daily bandwidth? How would you cache this?"
            answer: "Bandwidth: 10M × 1KB = 10GB/day = 115KB/sec avg, 575KB/sec peak (5x). Caching: 80/20 rule → cache 20% of endpoints = 2M requests × 1KB = 2GB cache (Redis). Hit rate: 80% → bandwidth reduced to 2GB/day (8x reduction). CDN for static assets: Additional 50% reduction = 1GB/day total. Cost: Redis $50/month + CDN $100/month."
          - question: "Social feed API has 100K users, each following 500 people. How do you design pagination for timeline queries?"
            answer: "Options: 1) Offset-based: Skip N, take 20 (simple but slow for large offsets, O(N) scan). 2) Cursor-based: Use last_seen_id, WHERE id < last_id LIMIT 20 (O(1) lookup with index). Recommendation: Cursor-based. Storage: 100K users × 500 follows = 50M edges × 16B = 800MB graph data. Timeline generation: Fan-out on write (pre-compute timelines), 1KB × 20 items = 20KB per page."
          - question: "Payment API requires 99.99% availability. Design rate limiting for 1000 TPS with burst capability."
            answer: "Rate limiting: Token bucket with 1000 tokens/sec refill, bucket size 5000 (5s burst). Implementation: Redis INCR with TTL. Availability: Multi-region deployment (3 regions) with failover. Each region handles 333 TPS normally, can handle 1000 TPS during failover. Monitoring: Alert if TPS >800 (80% capacity). Cost: $5K/month for 3-region setup + $100 for Redis rate limiter."
          - question: "File upload API handles 1GB files. How do you design chunked upload with resume capability?"
            answer: "Design: 1) Client splits into 10MB chunks, 2) POST /uploads → returns upload_id, 3) PUT /uploads/{id}/chunks/{chunk_num} for each chunk, 4) POST /uploads/{id}/complete to finalize. Resume: Store chunk metadata in Redis (uploaded_chunks bitmap). Timeout: 24h expiry for incomplete uploads. Storage: S3 multipart upload (min chunk 5MB). Bandwidth: 1GB/100s = 10MB/sec per upload. Support 100 concurrent uploads = 1GB/sec bandwidth needed."
        concepts:
          - question: "When would you choose GraphQL over REST for a mobile application?"
            answer: "Choose GraphQL when: 1) Mobile needs flexible queries (fetch exactly what's needed, save bandwidth), 2) Complex nested data (user + posts + comments in one query vs 3 REST calls), 3) Rapid iteration (mobile doesn't need new API endpoints). REST when: Simple CRUD, caching important (GraphQL queries harder to cache), team unfamiliar with GraphQL. Mobile benefit: 50% less data transfer, 3x fewer API calls."
          - question: "How do you implement idempotent operations for payment processing?"
            answer: "Use idempotency keys: 1) Client generates UUID for each payment request, 2) API checks if key exists in Redis/DB (TTL 24h), 3) If exists: Return cached response, 4) If new: Process payment + store result with key. Payment provider idempotency: Stripe accepts idempotency_key header. Prevents: Double charges from retries. Store: Redis SET with 24h expiry, or DB unique constraint. Cost: 1ms lookup overhead per request."
          - question: "What's the difference between authentication and authorization in API design?"
            answer: "Authentication: Who are you? Verify identity (login with password, OAuth, JWT token). Authorization: What can you do? Check permissions (role-based: admin/user, resource-based: own posts only). Implementation: Auth → JWT contains user_id + roles, 50ms. Authz → Check user permissions for resource, 10ms DB query or 1ms cache. Example: User authenticated (valid JWT) but unauthorized (can't access admin endpoint)."
          - question: "How do you handle API versioning when breaking changes are required?"
            answer: "Strategy: 1) URL versioning: /v1/users, /v2/users (clear, cacheable, multiple versions deployed), 2) Deprecation timeline: Announce → 6 months warning → Shut down v1. Migration: Run v1 and v2 in parallel (6-12 months), monitor v1 usage, contact active users. Breaking changes: New fields OK in v1, changed field types need v2. Cost: 2x infrastructure during migration. Alternative: Header versioning (Accept: application/vnd.api+json; version=2)."
          - question: "What are the security implications of using JWT vs session-based authentication?"
            answer: "JWT: Stateless (no DB lookup), scales horizontally, can't revoke (until expiry), token size 1KB (sent in every request), short expiry (15min) + refresh token (7 days). Session: Stateful (Redis/DB lookup 1ms), easy revocation, small cookie (32B session ID), can track concurrent sessions. JWT risks: Stolen token valid until expiry, mitigate with short expiry + HTTPS only. Session risks: Session fixation, mitigate with regeneration on login."
          - question: "How do you design APIs for eventual consistency in microservices?"
            answer: "Pattern: 1) API returns 202 Accepted (not 200 OK), 2) Provide status endpoint: GET /operations/{id}/status, 3) Webhook callback when complete. Example: POST /orders → 202 + order_id → Async processing (inventory, payment) → Webhook to client when done. Client polling: Every 1s for 30s. Alternative: WebSocket for real-time updates. Trade-off: Complex client vs simple API. Consistency: 1-5s lag acceptable for non-critical operations."
        tradeoffs:
          - question: "URL versioning vs header versioning: pros and cons for public APIs"
            answer: "URL (/v1/users): Clear, cacheable (different URLs), visible in logs, clutters URL space, easy testing (just change URL). Header (Accept: version=2): Clean URLs, same endpoint, harder caching (Vary header), harder testing (need custom headers), invisible in browser. Public API recommendation: URL versioning (easier for API consumers, Stripe/Twilio use this). Internal: Header versioning acceptable."
          - question: "Synchronous vs asynchronous API design for order processing"
            answer: "Sync: User waits for order confirmation (200 OK), 500ms-2s latency, simple client, tight coupling, scales to 1K orders/sec. Async: Immediate 202 Accepted, <50ms response, complex client (polling/webhooks), loose coupling, scales to 100K orders/sec. Use sync for: Simple orders (<500ms total). Async for: Complex orders (inventory check + payment + shipping = >2s), high throughput needed."
          - question: "JWT vs session tokens: security and scalability trade-offs"
            answer: "JWT: Stateless, no DB lookup (0ms), scales to 1M RPS, can't revoke (security risk), 1KB token size (bandwidth), horizontal scaling easy. Session: 1ms Redis lookup, limited to 100K RPS per Redis cluster, instant revocation (logout works immediately), 32B cookie (small), vertical scaling needed. Use JWT for: Microservices, stateless APIs. Session for: Traditional web apps, need instant logout."
          - question: "REST vs GraphQL for complex data relationships"
            answer: "REST: Over-fetching (get full user object when need just name), under-fetching (N+1 queries: users → posts for each user), predictable, easy caching. GraphQL: Fetch exactly what's needed, single query for nested data, flexible, but harder caching, N+1 problem on server (use DataLoader). Use GraphQL for: Mobile apps (minimize bandwidth), complex UIs (many variations). REST for: Simple CRUD, public APIs, caching critical."
          - question: "API Gateway vs direct service access: performance vs functionality"
            answer: "API Gateway: Single entry point, auth/rate limiting centralized, 5-10ms latency overhead, single point of failure (mitigate with HA), rich features (CORS, logging). Direct access: No latency overhead, simpler, but auth duplicated across services, harder rate limiting. Use Gateway for: External APIs (public, mobile), >10 services. Direct for: Internal service-to-service (service mesh instead), performance critical (<5ms)."
          - question: "Optimistic vs pessimistic locking in concurrent API operations"
            answer: "Optimistic: Assume no conflicts, use version numbers, retry on conflict, 10ms latency (no locks), 95% success rate at low contention, scales to 10K TPS. Pessimistic: Lock resource (SELECT FOR UPDATE), 50ms latency (lock wait), 100% success, scales to 1K TPS. Use optimistic for: Rare conflicts (inventory with 1000 units), high concurrency. Pessimistic for: Frequent conflicts (last ticket), financial transactions."
        scenarios:
          - "Design a rate limiting system for a multi-tenant SaaS API with different pricing tiers"
          - "How would you handle API deprecation for a public API with 1000+ integrations?"
          - "Design error handling for a payment API that interacts with multiple bank APIs"
          - "Create an API authentication strategy for a mobile app with offline capabilities"
          - "Design a file upload API that supports resumable uploads and virus scanning"
          - "How would you implement real-time notifications alongside a REST API?"
          - "Design API monitoring to detect and respond to DDoS attacks"
          - "Create a webhook system for event notifications with guaranteed delivery"
      time_estimate: 90

    - day: 10
      topic: "Microservices Architecture"
      activity: "Master comprehensive microservices design including decomposition strategies, communication patterns, data management, and operational concerns."
      detailed_content: |
        Microservices Architecture Fundamentals:
        Microservices Definition and Characteristics:
        - Independent deployability: Each service can be deployed separately
        - Business capability alignment: Services organized around business functions
        - Decentralized governance: Teams own their services end-to-end
        - Technology diversity: Different services can use different tech stacks
        - Failure isolation: Service failures don't cascade across the system
        - Evolutionary design: Services can evolve independently

        Monolith vs Microservices Trade-offs:
        Monolith Advantages:
        - Simpler development and testing initially
        - Easier debugging and monitoring
        - Better performance (in-process calls)
        - ACID transactions across entire application
        - Simpler deployment and operations

        Microservices Advantages:
        - Technology diversity and innovation
        - Independent scaling of components
        - Team autonomy and ownership
        - Fault isolation and resilience
        - Easier to understand individual services
        - Parallel development by multiple teams

        When to Choose Microservices:
        - Large, complex applications with multiple business domains
        - Multiple teams working on different features
        - Need for independent deployment and scaling
        - Different technology requirements per domain
        - Organizations with Conway's Law considerations

        Service Decomposition Strategies:
        Domain-Driven Design (DDD) Approach:
        1. Identify Business Domains:
           - Core domains: Primary business value
           - Supporting domains: Necessary but not core
           - Generic domains: Common functionality

        2. Bounded Contexts:
           - Define clear boundaries where domain models apply
           - Each bounded context becomes a potential microservice
           - Avoid sharing data models across contexts
           - Context mapping between domains

        3. Aggregate Design:
           - Identify business entities and their relationships
           - Group tightly coupled entities into aggregates
           - Each aggregate typically maps to one microservice
           - Ensure consistency within aggregates

        Decomposition Patterns:
        1. Decompose by Business Capability:
           - Organize services around what the business does
           - Examples: User Management, Order Processing, Payment
           - Services should be cohesive and loosely coupled

        2. Decompose by Sub-domain:
           - Based on DDD sub-domains
           - Each sub-domain has its own service
           - Clear boundaries and responsibilities

        3. Decompose by Transaction Boundaries:
           - Services handle complete business transactions
           - Avoid distributed transactions where possible
           - Design for eventual consistency

        4. Strangler Fig Pattern:
           - Gradually replace monolith components
           - Route new features to microservices
           - Incrementally migrate existing functionality

        Service Sizing Guidelines:
        - Single Responsibility: One service, one business capability
        - Team Size: Service should be owned by one team (2-pizza rule)
        - Data Consistency: Service owns its data and business rules
        - Deployment Independence: Service can be deployed without affecting others
        - Technology Boundaries: Different technology needs may justify separation

        Inter-Service Communication Patterns:
        Synchronous Communication:
        1. HTTP/REST APIs:
           - Simple and widely understood
           - Good for request-response patterns
           - Easy to test and debug
           - Challenges: Tight coupling, cascading failures

        2. gRPC:
           - High-performance RPC framework
           - Protocol buffers for serialization
           - Streaming support and bi-directional communication
           - Strong typing and code generation
           - Better performance than REST for high-frequency calls

        3. GraphQL:
           - Client specifies exact data requirements
           - Single endpoint for multiple services
           - Good for mobile and frontend applications
           - Challenges: Query complexity, caching

        Asynchronous Communication:
        1. Message Queues:
           - Point-to-point communication
           - Guaranteed delivery and ordering
           - Good for command processing
           - Examples: RabbitMQ, ActiveMQ

        2. Event Streaming:
           - Publish-subscribe pattern
           - Event sourcing and replay capabilities
           - Good for real-time data processing
           - Examples: Apache Kafka, Amazon Kinesis

        3. Event-Driven Architecture:
           - Services publish domain events
           - Other services subscribe to relevant events
           - Loose coupling and scalability
           - Eventual consistency model

        Communication Best Practices:
        - Prefer asynchronous communication for better resilience
        - Use synchronous calls only when immediate response needed
        - Implement circuit breakers for fault tolerance
        - Design for network failures and timeouts
        - Use correlation IDs for request tracing

        Data Management in Microservices:
        Database Per Service Pattern:
        Benefits:
        - Service independence and autonomy
        - Technology diversity (SQL, NoSQL, etc.)
        - Independent scaling of data layer
        - Clear ownership and responsibility

        Challenges:
        - No cross-service transactions
        - Data consistency across services
        - Complex queries spanning services
        - Data synchronization requirements

        Implementation Strategies:
        - Private database per service
        - Shared database anti-pattern (avoid)
        - Database interface per service
        - API-first approach to data access

        Distributed Transaction Patterns:
        1. Saga Pattern:
           - Sequence of local transactions
           - Compensating transactions for rollback
           - Two types: Choreography and Orchestration

        Choreography-based Saga:
        - Services communicate through events
        - Each service listens and reacts to events
        - Decentralized coordination
        - Good for simple workflows

        Orchestration-based Saga:
        - Central coordinator manages the saga
        - Explicit workflow definition
        - Better for complex business processes
        - Easier debugging and monitoring

        2. Two-Phase Commit (2PC):
           - Traditional distributed transaction approach
           - Coordinator manages prepare/commit phases
           - Problems: Blocking, single point of failure
           - Generally avoided in microservices

        Event Sourcing Pattern:
        - Store events instead of current state
        - Replay events to reconstruct state
        - Complete audit trail of changes
        - Supports temporal queries
        - Challenges: Event schema evolution, complexity

        CQRS (Command Query Responsibility Segregation):
        - Separate read and write models
        - Optimize read and write operations independently
        - Often combined with event sourcing
        - Improved performance and scalability
        - Challenges: Eventual consistency, complexity

        Data Consistency Patterns:
        1. Eventual Consistency:
           - Accept temporary inconsistency
           - System converges to consistent state
           - Better availability and performance
           - Requires careful design

        2. Strong Consistency:
           - Immediate consistency across services
           - May impact availability and performance
           - Use only when business requires it

        Service Discovery and Registration:
        Service Discovery Patterns:
        1. Client-Side Discovery:
           - Client queries service registry directly
           - Client handles load balancing
           - Examples: Netflix Eureka, Consul

        2. Server-Side Discovery:
           - Load balancer queries service registry
           - Client unaware of service locations
           - Examples: AWS ALB, Kubernetes Services

        3. Service Registry:
           - Central database of service instances
           - Health checking and monitoring
           - Examples: Consul, etcd, Zookeeper

        Service Registration Patterns:
        - Self-registration: Services register themselves
        - Third-party registration: Platform registers services
        - Health check integration
        - Graceful shutdown handling

        API Gateway Pattern:
        API Gateway Responsibilities:
        - Request routing to appropriate services
        - Authentication and authorization
        - Rate limiting and throttling
        - Request/response transformation
        - Protocol translation (HTTP to gRPC)
        - Monitoring and analytics
        - Circuit breaking and retries

        API Gateway Benefits:
        - Single entry point for clients
        - Cross-cutting concerns centralization
        - Protocol abstraction
        - Service composition

        API Gateway Challenges:
        - Potential bottleneck
        - Single point of failure
        - Additional complexity
        - Versioning and deployment coordination

        Service Mesh Architecture:
        Service Mesh Components:
        1. Data Plane:
           - Sidecar proxies (Envoy, Linkerd)
           - Handle service-to-service communication
           - Traffic management and security

        2. Control Plane:
           - Configuration and policy management
           - Service discovery integration
           - Security certificate management

        Service Mesh Benefits:
        - Traffic management (load balancing, routing)
        - Security (mTLS, authentication)
        - Observability (metrics, tracing, logging)
        - Policy enforcement
        - Language-agnostic features

        Popular Service Mesh Solutions:
        - Istio: Feature-rich, complex setup
        - Linkerd: Lightweight, easy to use
        - Consul Connect: HashiCorp ecosystem
        - AWS App Mesh: Managed service mesh

        Resilience Patterns:
        1. Circuit Breaker:
           - Prevent cascading failures
           - Fast failure when downstream unhealthy
           - Automatic recovery attempts
           - States: Closed, Open, Half-Open

        2. Retry Pattern:
           - Retry failed requests with backoff
           - Handle transient failures
           - Avoid retry storms
           - Exponential backoff and jitter

        3. Timeout Pattern:
           - Set maximum wait time for responses
           - Prevent resource exhaustion
           - Fail fast approach
           - Different timeouts for different operations

        4. Bulkhead Pattern:
           - Isolate critical resources
           - Separate thread pools or queues
           - Prevent one failure affecting others
           - Resource isolation

        5. Rate Limiting:
           - Control request rate to services
           - Prevent service overload
           - Different limits for different clients
           - Graceful degradation

        Monitoring and Observability:
        Three Pillars of Observability:
        1. Metrics:
           - Business metrics (orders, revenue)
           - Application metrics (latency, throughput)
           - Infrastructure metrics (CPU, memory)
           - Tools: Prometheus, InfluxDB

        2. Logging:
           - Structured logging (JSON format)
           - Correlation IDs for request tracing
           - Centralized log aggregation
           - Tools: ELK Stack, Splunk

        3. Distributed Tracing:
           - Request flow across services
           - Performance bottleneck identification
           - Error propagation tracking
           - Tools: Jaeger, Zipkin

        Health Monitoring:
        - Health check endpoints (/health)
        - Liveness vs readiness probes
        - Dependency health checking
        - Circuit breaker integration

        Security in Microservices:
        Authentication and Authorization:
        - OAuth 2.0 and OpenID Connect
        - JWT tokens for stateless authentication
        - API keys for service-to-service
        - Service identity and mTLS

        Network Security:
        - Private networks and VPCs
        - Service mesh security features
        - Zero-trust network model
        - Encryption in transit and at rest

        Secret Management:
        - Centralized secret storage
        - Secret rotation and lifecycle
        - Environment-specific secrets
        - Tools: HashiCorp Vault, AWS Secrets Manager

        Deployment and DevOps:
        Deployment Patterns:
        1. Blue-Green Deployment:
           - Two identical production environments
           - Switch traffic between environments
           - Zero-downtime deployment
           - Easy rollback capability

        2. Canary Deployment:
           - Gradual rollout to subset of users
           - Monitor metrics and rollback if needed
           - Risk mitigation for new releases
           - A/B testing capabilities

        3. Rolling Deployment:
           - Update instances one by one
           - Maintain service availability
           - Slower than blue-green
           - Resource efficient

        Container Orchestration:
        - Docker containers for packaging
        - Kubernetes for orchestration
        - Service discovery and load balancing
        - Auto-scaling and self-healing

        CI/CD Pipeline:
        - Independent service pipelines
        - Automated testing and deployment
        - Service contract testing
        - Database migration handling

        Configuration Management:
        - Externalized configuration
        - Environment-specific configs
        - Dynamic configuration updates
        - Tools: Spring Cloud Config, Consul

        Performance and Scalability:
        Scaling Patterns:
        1. Horizontal Scaling:
           - Add more service instances
           - Load balancing across instances
           - Stateless service design
           - Auto-scaling based on metrics

        2. Vertical Scaling:
           - Increase resources per instance
           - Simpler but limited scalability
           - Temporary solution

        3. Data Partitioning:
           - Shard data across services
           - Service-specific data stores
           - Geographic distribution

        Caching Strategies:
        - Service-level caching
        - Distributed caching
        - Cache invalidation coordination
        - CDN for static content

        Migration Strategies:
        Monolith to Microservices Migration:
        1. Strangler Fig Pattern:
           - Gradually replace monolith components
           - Route new features to microservices
           - Extract services incrementally

        2. Database Decomposition:
           - Start with shared database
           - Gradually separate data stores
           - Handle data migration carefully

        3. Big Bang Migration:
           - Complete rewrite approach
           - High risk but clean architecture
           - Suitable for small applications

        Migration Best Practices:
        - Start with least risky services
        - Maintain backward compatibility
        - Monitor and measure progress
        - Team training and skills development

        Testing Strategies:
        Testing Pyramid for Microservices:
        1. Unit Tests:
           - Test individual service components
           - Fast feedback and high coverage
           - Mock external dependencies

        2. Integration Tests:
           - Test service interactions
           - Database and external service integration
           - Test contracts and APIs

        3. Contract Tests:
           - Provider and consumer contracts
           - Prevent breaking changes
           - Tools: Pact, Spring Cloud Contract

        4. End-to-End Tests:
           - Test complete user journeys
           - Expensive and slow
           - Focus on critical business flows

        5. Chaos Engineering:
           - Test system resilience
           - Inject failures deliberately
           - Tools: Chaos Monkey, Gremlin

        Real-World Implementation Examples:
        E-commerce Microservices:
        - User Service: Authentication and profiles
        - Product Catalog: Product information and search
        - Inventory Service: Stock management
        - Order Service: Order processing and workflow
        - Payment Service: Payment processing
        - Shipping Service: Logistics and tracking
        - Notification Service: Email and SMS
        - Analytics Service: Business intelligence

        Financial Services Architecture:
        - Account Service: Account management
        - Transaction Service: Payment processing
        - Risk Service: Fraud detection
        - Compliance Service: Regulatory requirements
        - Reporting Service: Financial reporting
        - Integration Service: External bank APIs

        Common Anti-Patterns:
        1. Distributed Monolith:
           - Services tightly coupled
           - Cannot deploy independently
           - Shared database and schemas

        2. Chatty Interfaces:
           - Too many service-to-service calls
           - Performance degradation
           - Network latency issues

        3. Shared Database:
           - Multiple services access same database
           - Tight coupling through data schema
           - Difficult to evolve independently

        4. God Service:
           - Service with too many responsibilities
           - Violates single responsibility principle
           - Difficult to maintain and scale

        5. Data Inconsistency:
           - Ignoring eventual consistency
           - Poor error handling
           - Lack of compensation mechanisms
      resources:
        - title: "Microservices Patterns"
          url: "https://microservices.io/patterns/microservices.html"
          description: "Comprehensive catalog of microservice patterns and solutions"
        - title: "Building Microservices (2nd Edition)"
          url: "https://samnewman.io/books/building_microservices_2nd_edition/"
          description: "Authoritative guide to microservices architecture"
        - title: "Domain-Driven Design"
          url: "https://domainlanguage.com/ddd/"
          description: "Eric Evans' foundational work on DDD principles"
        - title: "Microservices on AWS"
          url: "https://aws.amazon.com/microservices/"
          description: "AWS guide to implementing microservices"
        - title: "Martin Fowler on Microservices"
          url: "https://martinfowler.com/articles/microservices.html"
          description: "Foundational article defining microservices architecture"
        - title: "Service Mesh Comparison"
          url: "https://servicemesh.es/"
          description: "Comparison of service mesh technologies"
      practice_questions:
        estimation:
          - question: "E-commerce system with 1M users needs microservices decomposition. Estimate service count and communication patterns."
            answer: "Services: User (auth, profile), Product (catalog, search), Order (checkout, fulfillment), Payment, Inventory, Notification, Analytics = 7 core services. Communication: Order → Payment (sync, 50ms), Order → Inventory (sync, 30ms), Order → Notification (async, event). Traffic: 10K orders/day = 116 orders/sec peak × 5 service calls = 580 inter-service calls/sec. Cost: $2K/month for infrastructure."
          - question: "Order processing system handles 10K orders/hour. Design service boundaries and calculate inter-service call volume."
            answer: "Order service receives order → calls Inventory (check stock, 20ms) → calls Payment (charge, 100ms) → publishes order.completed event → Shipping service subscribes (async). Calls/hour: 10K orders × 2 sync calls = 20K sync calls + 10K async events. Peak: 3 orders/sec × 2 = 6 sync RPS. Services handle 100 RPS each, plenty of headroom."
          - question: "Social media platform with 100M users. Design microservices for feed generation with scalability estimates."
            answer: "Services: Post (create/store, 10K posts/sec), Timeline (fan-out, 50K updates/sec), Feed (read, 100K reads/sec), User Graph (follows, 1K updates/sec). Feed generation: Fan-out on write (popular users) + fan-out on read (normal users). Storage: 100M users × 1KB timeline = 100GB Redis. Cost: $5K/month for infrastructure."
          - question: "Banking system with 24/7 availability requirement. Design service resilience and estimate downtime costs."
            answer: "99.99% uptime = 52 min downtime/year. Multi-region (3 regions), each handles 33% traffic, can handle 100% during failover. Circuit breakers: 5 consecutive failures → open (30s timeout). Downtime cost: $10K/min (transactions blocked). Investment: $50K/month infrastructure to prevent $520K annual downtime. ROI: 10x."
        concepts:
          - question: "How do you handle distributed transactions in microservices without 2PC?"
            answer: "Use Saga pattern: Choreography (event-driven, services react to events, decentralized) or Orchestration (central coordinator, easier debugging). Example: Order saga → Reserve inventory → Charge payment → Confirm order. Rollback: Compensating transactions (refund payment, release inventory). Store saga state in DB. Timeout: 2min max. Alternative: Event sourcing for audit trail."
          - question: "What's the difference between API Gateway and Service Mesh?"
            answer: "API Gateway: North-south traffic (external → internal), routing, auth, rate limiting, single entry point, 5-10ms latency. Service Mesh: East-west traffic (service → service), mTLS, observability, circuit breakers, distributed across sidecars, 1-3ms latency. Use both: Gateway for external APIs, mesh for internal. Examples: Kong Gateway + Istio mesh."
          - question: "How do you ensure data consistency across microservices?"
            answer: "Eventual consistency with patterns: 1) Saga for transactions, 2) Event sourcing for audit, 3) CQRS (separate read/write models), 4) Idempotency for retries, 5) Outbox pattern (publish events reliably). Strong consistency: Only within service boundary. Cross-service: Accept 1-5s lag. Monitor: Track event delivery, saga completion rates."
          - question: "When would you choose synchronous vs asynchronous communication?"
            answer: "Sync (REST/gRPC): User-facing workflows (<500ms response needed), immediate feedback required, simple error handling, low latency (50ms). Async (Kafka/RabbitMQ): Long-running operations (>500ms), high throughput (>10K/sec), decoupling needed, eventual consistency OK, complex error handling. Example: Checkout uses sync (payment), order fulfillment uses async (shipping)."
          - question: "How do you handle service discovery in a dynamic environment?"
            answer: "Use service registry (Consul, Eureka) with health checks. Services register on startup (IP + port), deregister on shutdown. Clients query registry every 30s (cache). Health check: HTTP GET /health every 10s, 3 failures → mark unhealthy. Auto-scaling: New instances register automatically. DNS as fallback (60s TTL). Kubernetes: Native service discovery via DNS."
          - question: "What are the trade-offs of database per service pattern?"
            answer: "Pros: Independent scaling, technology choice (Postgres + Mongo + Redis), clear ownership, failure isolation. Cons: No ACID across services, cross-service queries hard (need aggregation service), data duplication (eventual consistency), 3x ops complexity. Use when: >5 services, different data models needed. Avoid when: Complex transactions, team <10 developers."
        tradeoffs:
          - question: "Monolith vs microservices for a startup: pros and cons"
            answer: "Monolith: Faster initial development (1 codebase), easier debugging, ACID transactions, 1 deployment, team <10 works well. Microservices: Complex from day 1, distributed debugging, eventual consistency, 10+ deployments, need team >20. Startup recommendation: Start monolith, extract services when: Scaling bottlenecks, team >20, clear boundaries emerge. Netflix extracted 500+ services over 5 years."
          - question: "Event-driven architecture vs request-response: when to use each"
            answer: "Event-driven: Loose coupling, 1-N communication (1 event → many consumers), eventual consistency, harder debugging, scales to 100K events/sec. Request-response: Tight coupling, 1-1 communication, strong consistency, easier debugging, scales to 10K RPS. Use events for: Notifications, analytics, audit logs. Request-response for: User-facing queries, transactional operations."
          - question: "Service mesh vs API gateway for cross-cutting concerns"
            answer: "Service mesh (Istio): Service-to-service, distributed (sidecar per pod), mTLS, observability, 1-3ms overhead, complex setup. API gateway (Kong): Client-to-service, centralized, API key/OAuth, rate limiting, 5-10ms overhead, simple setup. Use mesh when: >20 services, need mTLS everywhere. Gateway when: External APIs, simpler architecture. Can use both."
          - question: "Strong consistency vs eventual consistency in distributed data"
            answer: "Strong: All reads see latest write (CP), 50-100ms latency (consensus), limited scale (10K TPS), use for: Money, inventory. Eventual: Reads may be stale (AP), <10ms latency, unlimited scale (1M TPS), use for: Social feeds, analytics. Middle ground: Session consistency (user sees own writes). Cost: Strong consistency = 5x infrastructure for same throughput."
          - question: "Client-side vs server-side service discovery"
            answer: "Client-side: Client queries registry directly (Consul, Eureka), faster (1 hop less), client complexity, language-specific libraries. Server-side: Load balancer queries registry, simpler client (just HTTP), extra hop (+5ms), centralized logic. Use client-side for: Internal services, performance critical. Server-side for: External clients, polyglot environments. Kubernetes: Native server-side."
          - question: "Choreography vs orchestration for saga pattern"
            answer: "Choreography: Services listen for events, react independently, decentralized, scales better (no bottleneck), harder to debug (distributed state). Orchestration: Central coordinator, explicit workflow, easier debugging (single state machine), coordinator = bottleneck. Use choreography for: Simple workflows (<5 steps), high throughput. Orchestration for: Complex workflows (>5 steps), need visibility."
        scenarios:
          - "Design microservices decomposition for a ride-sharing platform"
          - "Handle payment processing across multiple services with failure scenarios"
          - "Migrate a monolithic e-commerce system to microservices incrementally"
          - "Design service communication for real-time chat application"
          - "Implement distributed logging and tracing across 20+ services"
          - "Design data synchronization between microservices for inventory management"
          - "Handle service deployment with zero downtime across interconnected services"
          - "Design authentication and authorization for microservices architecture"
      time_estimate: 120

    - day: 11
      topic: "Service Discovery & Health Monitoring"
      activity: "Master comprehensive service discovery mechanisms, health monitoring strategies, circuit breaker patterns, and failure detection systems."
      detailed_content: |
        Service Discovery Fundamentals:
        Why Service Discovery is Needed:
        - Dynamic service environments: Services start, stop, scale up/down
        - Network address changes: IP addresses and ports not static
        - Load balancing requirements: Distribute traffic across instances
        - Failure handling: Remove unhealthy instances from rotation
        - Service composition: Services need to find dependencies
        - Cross-region deployment: Services in different data centers

        Service Discovery Challenges:
        - Service registry consistency and availability
        - Network partitions and split-brain scenarios
        - Service registration and deregistration timing
        - Load balancing strategy integration
        - Health check accuracy and frequency
        - Configuration propagation delays

        Service Discovery Architectural Patterns:
        1. Client-Side Discovery:
           How it Works:
           - Client queries service registry directly
           - Client obtains list of available service instances
           - Client implements load balancing logic
           - Client handles failure detection and retries

           Advantages:
           - Client has full control over load balancing
           - No additional network hop for discovery
           - Rich load balancing algorithms possible
           - Better performance (cached service locations)

           Disadvantages:
           - Service registry coupling in client code
           - Language-specific client libraries required
           - Complex client implementation
           - Service registry becomes critical dependency

           Examples: Netflix Eureka, Apache Zookeeper

        2. Server-Side Discovery:
           How it Works:
           - Client makes request to load balancer/proxy
           - Load balancer queries service registry
           - Load balancer routes request to healthy instance
           - Client unaware of individual service instances

           Advantages:
           - Simpler client implementation
           - Language-agnostic approach
           - Centralized load balancing logic
           - Better security (internal topology hidden)

           Disadvantages:
           - Additional network hop overhead
           - Load balancer becomes potential bottleneck
           - Limited load balancing algorithm options
           - Load balancer needs high availability

           Examples: AWS ALB, NGINX Plus, HAProxy

        3. Service Mesh Discovery:
           How it Works:
           - Sidecar proxy handles service discovery
           - Control plane manages service registry
           - Data plane (proxies) implement load balancing
           - Transparent to application code

           Advantages:
           - Language-agnostic solution
           - Advanced traffic management features
           - Security features (mTLS, authentication)
           - Observability built-in

           Disadvantages:
           - Additional infrastructure complexity
           - Resource overhead (sidecar proxies)
           - Learning curve and operational overhead
           - Vendor lock-in potential

        Service Registry Technologies:
        1. HashiCorp Consul:
           Features:
           - Distributed service registry with consensus
           - Built-in health checking capabilities
           - DNS interface for service discovery
           - Key-value store for configuration
           - Multi-datacenter support with WAN gossip
           - Service mesh capabilities (Consul Connect)

           Architecture:
           - Raft consensus protocol for consistency
           - Gossip protocol for failure detection
           - HTTP and DNS APIs for service access
           - Agent deployment on each node

           Use Cases:
           - Multi-cloud deployments
           - Traditional and containerized workloads
           - Service mesh implementations
           - Configuration management

        2. Netflix Eureka:
           Features:
           - REST-based service registry
           - Client-side load balancing integration
           - Self-preservation mode during network partitions
           - Zone-aware service discovery
           - Integration with Netflix OSS stack

           Architecture:
           - Eventually consistent replication model
           - Client-side heartbeat mechanism
           - Peer-to-peer replication between servers
           - Lease-based service registration

           Use Cases:
           - AWS cloud deployments
           - Spring Boot applications
           - Microservices in Java ecosystem

        3. Apache Zookeeper:
           Features:
           - Highly available coordination service
           - Strong consistency guarantees
           - Hierarchical namespace organization
           - Watch mechanism for configuration changes
           - Leader election capabilities

           Architecture:
           - ZAB (Zookeeper Atomic Broadcast) protocol
           - Ensemble of servers for high availability
           - Sequential consistency model
           - Session-based client connections

           Use Cases:
           - Configuration management
           - Distributed coordination
           - Service discovery for Kafka, Hadoop
           - Legacy system integration

        4. etcd:
           Features:
           - Distributed key-value store
           - Raft consensus algorithm
           - Watch API for real-time updates
           - TTL (Time-To-Live) for automatic cleanup
           - Multi-version concurrency control

           Architecture:
           - Leader-follower replication model
           - gRPC API for client communication
           - Cluster membership management
           - Consistent and partition-tolerant

           Use Cases:
           - Kubernetes cluster coordination
           - Configuration storage
           - Service discovery in container environments
           - Distributed locking

        5. Kubernetes Services:
           Features:
           - Native service discovery in Kubernetes
           - DNS-based service resolution
           - Service types (ClusterIP, NodePort, LoadBalancer)
           - Endpoint management and health checking
           - Integration with ingress controllers

           Architecture:
           - kube-proxy for traffic routing
           - CoreDNS for service name resolution
           - Endpoint controllers for instance tracking
           - Service mesh integration (Istio, Linkerd)

        Service Registration Patterns:
        1. Self-Registration:
           How it Works:
           - Service instances register themselves
           - Service sends heartbeats to maintain registration
           - Service deregisters on shutdown
           - Health checks performed by service registry

           Implementation Steps:
           - Service startup: Register with service registry
           - Periodic heartbeats: Maintain active status
           - Health endpoint: Expose health check endpoint
           - Graceful shutdown: Deregister before stopping

           Advantages:
           - Simple and direct approach
           - Service controls its own lifecycle
           - No external dependencies for registration
           - Fast registration and deregistration

           Disadvantages:
           - Service registry coupling in application code
           - Need for registration logic in every service
           - Potential for failed deregistration
           - Language-specific implementation required

        2. Third-Party Registration:
           How it Works:
           - External component handles registration
           - Service deployment platform manages lifecycle
           - Health checks performed by external agent
           - No application code changes required

           Implementation Examples:
           - Kubernetes: Service and endpoint controllers
           - Docker Swarm: Built-in service discovery
           - AWS ECS: Service discovery integration
           - Consul: Registrator or consul-template

           Advantages:
           - No application code changes needed
           - Consistent registration across services
           - Platform handles failure scenarios
           - Language-agnostic approach

           Disadvantages:
           - Additional infrastructure complexity
           - Potential delays in registration updates
           - Platform-specific implementation
           - Less control over registration process

        DNS-Based Service Discovery:
        DNS Service Discovery Implementation:
        - SRV records: Service location information
        - A/AAAA records: IP address resolution
        - TXT records: Additional service metadata
        - PTR records: Reverse DNS lookup support

        SRV Record Format:
        ```
        _service._protocol.domain TTL class SRV priority weight port target
        _http._tcp.example.com. 60 IN SRV 10 20 8080 server1.example.com.
        ```

        Advantages:
        - Universal DNS support across platforms
        - No additional client libraries required
        - Built-in caching at multiple levels
        - Standard protocol with wide tooling support

        Disadvantages:
        - Limited metadata support
        - DNS caching can delay updates
        - No built-in health checking
        - Security considerations (DNS spoofing)

        Health Monitoring Strategies:
        Health Check Types and Implementation:
        1. Shallow Health Checks:
           - Basic connectivity and responsiveness
           - HTTP endpoint returning 200 OK
           - Minimal resource consumption
           - Fast execution (< 1 second)

           Example Implementation:
           ```
           GET /health
           Response: 200 OK
           Body: {"status": "UP"}
           ```

        2. Deep Health Checks:
           - Verify critical dependencies
           - Database connectivity checks
           - External service availability
           - Resource utilization validation

           Example Implementation:
           ```
           GET /health/detailed
           Response: 200 OK
           Body: {
             "status": "UP",
             "components": {
               "database": {"status": "UP", "responseTime": "45ms"},
               "cache": {"status": "UP", "hitRatio": 0.85},
               "externalAPI": {"status": "DOWN", "error": "timeout"}
             }
           }
           ```

        3. Readiness vs Liveness Probes:
           Liveness Probe:
           - Determines if service should be restarted
           - Detects deadlocks and unrecoverable errors
           - Should not check external dependencies
           - Failure results in service restart

           Readiness Probe:
           - Determines if service can accept traffic
           - Checks if service is ready to serve requests
           - May include dependency checks
           - Failure removes from load balancer

        4. Custom Health Indicators:
           - Business-specific health metrics
           - Performance threshold monitoring
           - Resource availability checks
           - Circuit breaker state monitoring

        Health Check Best Practices:
        - Timeout configuration: 5-30 seconds typically
        - Check frequency: Every 10-30 seconds
        - Failure threshold: 3-5 consecutive failures
        - Success threshold: 1-2 consecutive successes
        - Graceful degradation: Partial functionality indication

        Circuit Breaker Pattern Deep Dive:
        Circuit Breaker States:
        1. Closed State:
           - Normal operation mode
           - All requests passed through
           - Failure count tracking
           - Transition to Open on threshold breach

        2. Open State:
           - Fast-fail mode activated
           - All requests immediately rejected
           - Timeout period before testing recovery
           - Prevents cascading failures

        3. Half-Open State:
           - Testing recovery mode
           - Limited number of test requests allowed
           - Success transitions to Closed
           - Failure returns to Open

        Circuit Breaker Configuration:
        - Failure threshold: Number of failures to open circuit
        - Timeout period: Time to wait before testing recovery
        - Success threshold: Successes needed to close circuit
        - Request timeout: Maximum wait time for responses
        - Sliding window: Time period for failure counting

        Circuit Breaker Implementation Patterns:
        1. Request-Based Circuit Breaker:
           - Tracks success/failure rate over request count
           - Good for high-traffic services
           - Responsive to sudden failure spikes

        2. Time-Based Circuit Breaker:
           - Tracks success/failure rate over time windows
           - Good for varying traffic patterns
           - Smoother response to intermittent failures

        3. Hybrid Circuit Breaker:
           - Combines request and time-based approaches
           - More sophisticated failure detection
           - Better suited for complex scenarios

        Advanced Health Monitoring:
        Dependency Health Aggregation:
        - Service health depends on dependency health
        - Weighted dependency importance
        - Partial functionality with degraded dependencies
        - Cascade failure prevention strategies

        Health Check Optimization:
        - Parallel dependency checking
        - Cached health status with TTL
        - Asynchronous health reporting
        - Health check result aggregation

        Monitoring and Alerting:
        Key Metrics to Monitor:
        1. Service Discovery Metrics:
           - Service registration/deregistration rate
           - Service registry query latency
           - Service registry availability
           - Stale service instance detection

        2. Health Check Metrics:
           - Health check success rate
           - Health check response time
           - Failed health check reasons
           - Health state transition frequency

        3. Circuit Breaker Metrics:
           - Circuit breaker state changes
           - Request success/failure rates
           - Circuit breaker trigger frequency
           - Recovery time measurements

        Alerting Strategies:
        - Service unavailability alerts
        - High failure rate notifications
        - Service registry partition warnings
        - Circuit breaker state change alerts
        - Health check timeout notifications

        Failure Detection and Recovery:
        Failure Modes and Detection:
        1. Service Instance Failure:
           - Process crashes or becomes unresponsive
           - Health check failures indicate problems
           - Automatic deregistration from service registry
           - Load balancer removes from rotation

        2. Network Partition:
           - Service registry becomes unreachable
           - Services continue with cached information
           - Split-brain scenarios in distributed registries
           - Partition tolerance strategies required

        3. Service Registry Failure:
           - Central registry becomes unavailable
           - Client-side caching provides resilience
           - Fallback to static configuration
           - Peer-to-peer discovery mechanisms

        Recovery Strategies:
        - Exponential backoff for retry attempts
        - Jitter to prevent thundering herd
        - Circuit breaker integration
        - Graceful degradation mechanisms

        Configuration Management Integration:
        Dynamic Configuration:
        - Feature flags for service behavior control
        - A/B testing configuration management
        - Runtime configuration updates
        - Configuration validation and rollback

        Configuration Sources:
        - Environment variables for deployment-specific settings
        - Configuration files for static settings
        - Service registry for dynamic configuration
        - External configuration services (Spring Cloud Config)

        Real-World Implementation Examples:
        E-commerce Platform Service Discovery:
        Services:
        - Product Catalog Service: 10 instances across 3 AZs
        - User Service: 5 instances with database dependency
        - Order Service: 8 instances with multiple dependencies
        - Payment Service: 3 instances with external API dependency

        Discovery Implementation:
        - Consul for service registry with multi-DC setup
        - Health checks every 15 seconds with 3-failure threshold
        - Circuit breakers for external payment gateway
        - DNS interface for legacy service integration

        Microfinance Application:
        Services:
        - Account Service: Critical with 99.9% availability requirement
        - Transaction Service: High throughput with rate limiting
        - Risk Assessment Service: ML-based with variable response times
        - Notification Service: Non-critical with graceful degradation

        Discovery Implementation:
        - Kubernetes native service discovery
        - Istio service mesh for advanced traffic management
        - Custom health checks for ML model availability
        - Circuit breakers for third-party credit checks

        Common Anti-Patterns and Solutions:
        1. Health Check Dependency Cascade:
           Problem: Deep health checks create dependency chains
           Solution: Shallow checks for liveness, deep for readiness

        2. Chatty Health Checks:
           Problem: Too frequent health checks consume resources
           Solution: Optimize check frequency and implement caching

        3. Ignored Circuit Breaker States:
           Problem: Applications don't respect circuit breaker decisions
           Solution: Proper integration and fallback mechanisms

        4. Static Service Configuration:
           Problem: Hardcoded service endpoints resist change
           Solution: Dynamic service discovery with configuration management

        Troubleshooting and Debugging:
        Common Issues:
        - Service registration delays causing 404 errors
        - Health check false positives during startup
        - Circuit breaker premature opening
        - Service registry split-brain scenarios
        - DNS caching preventing service updates

        Debugging Tools:
        - Service registry health dashboards
        - Circuit breaker state monitoring
        - Health check trace logging
        - Network connectivity testing
        - Configuration validation tools
      resources:
        - title: "Service Discovery Patterns"
          url: "https://microservices.io/patterns/service-registry.html"
          description: "Comprehensive guide to service discovery implementation patterns"
        - title: "Circuit Breaker Pattern"
          url: "https://martinfowler.com/bliki/CircuitBreaker.html"
          description: "Martin Fowler's explanation of circuit breaker fault tolerance pattern"
        - title: "Consul Service Discovery"
          url: "https://www.consul.io/docs/discovery"
          description: "HashiCorp Consul service discovery documentation and best practices"
        - title: "Kubernetes Service Discovery"
          url: "https://kubernetes.io/docs/concepts/services-networking/service/"
          description: "Native Kubernetes service discovery and networking concepts"
        - title: "Netflix Eureka Guide"
          url: "https://github.com/Netflix/eureka/wiki"
          description: "Netflix Eureka service registry implementation guide"
        - title: "Health Check Patterns"
          url: "https://microservices.io/patterns/observability/health-check-api.html"
          description: "Health check API implementation patterns and best practices"
      practice_questions:
        estimation:
          - question: "E-commerce platform with 50 microservices needs service discovery. Estimate registry query load and design capacity planning."
            answer: "Each service queries registry every 30s: 50 services × 2 QPS (per service) = 100 QPS to registry. Health checks: 50 services × 1 check/10s = 5 checks/sec. Registry: Consul cluster (3 nodes, CP model), handles 10K QPS easily. Storage: 50 services × 1KB metadata = 50KB total. Failover: 3-node cluster survives 1 node failure. Cost: ~$300/month for registry cluster."
          - question: "Financial system requiring 99.99% availability. Design health monitoring strategy with failure detection times."
            answer: "Health checks: Shallow (every 5s, <10ms response) + deep (every 60s, <100ms, checks DB connections). Failure detection: 3 consecutive shallow failures = 15s to mark unhealthy. Alert immediately, auto-failover in 30s. Circuit breaker: Open after 5 failures, half-open after 30s, close after 3 successes. Downtime: 52 min/year max. Cost: $10K/month for multi-region HA."
          - question: "Global application with 5 regions. Design service discovery architecture with cross-region failover."
            answer: "Per-region registry (Consul cluster), replicate metadata globally (async, 1-5s lag). Services query local registry (10ms latency). Cross-region failover: If regional registry down, fall back to nearest region (adds 50-200ms latency). Traffic: 50 services/region × 5 regions = 250 services total. Registry storage: 250 × 1KB = 250KB. Cost: ~$1.5K/month for 5 regional clusters."
          - question: "High-traffic API gateway serving 100K RPS. Design circuit breaker configuration and estimate failure scenarios."
            answer: "Circuit breaker: Per-service circuit, 10% error rate threshold, 5s evaluation window, 30s timeout. At 100K RPS: If service fails, circuit opens, 10K requests/sec to that service fail fast (<1ms) instead of timing out (1s), saves 10K connections. Recovery: Half-open tries 10 requests, if successful, close circuit. Failure scenario: 5% of requests fail = 5K errors/sec handled gracefully."
        concepts:
          - question: "What's the difference between liveness and readiness probes in health monitoring?"
            answer: "Liveness: Is service alive? Check process running. If fails: Restart container. Use for: Deadlock detection. Readiness: Is service ready to serve traffic? Check dependencies (DB, cache). If fails: Remove from load balancer (don't restart). Use for: Gradual rollout, startup checks. Example: Service alive but DB down = liveness passes, readiness fails, no traffic sent."
          - question: "How do you handle service discovery during network partitions?"
            answer: "CP registry (Consul): Minority partition becomes unavailable (prefers consistency), services cache last known state, continue with stale data (degraded mode). AP registry (Eureka): Both partitions accept registrations (availability over consistency), may route to unreachable services, eventual consistency on heal. Recommendation: CP for critical services (banking), AP for resilient services (web apps). Cache TTL: 60s for graceful degradation."
          - question: "When would you choose client-side vs server-side service discovery?"
            answer: "Client-side (Consul, Eureka): Service queries registry, gets IP list, load balances client-side. Pros: Faster (no proxy), flexible LB. Cons: Client complexity, language libraries needed. Server-side (Kubernetes, AWS ELB): DNS lookup or LB. Pros: Simple client (just HTTP), language-agnostic. Cons: Extra hop (+5ms), LB = single point. Use client-side for: Performance, internal services. Server-side for: External, polyglot."
          - question: "How do circuit breakers prevent cascading failures in microservices?"
            answer: "Service A calls Service B. B slows down (DB overload). Without breaker: A waits 1s per call, threads pile up, A becomes unresponsive, cascades to callers. With breaker: After 5 failures, circuit opens, A fails fast (<1ms), threads freed, A stays healthy. B gets time to recover. Half-open state tests recovery. Prevents: Thread exhaustion, resource starvation, cascading timeouts."
          - question: "What are the trade-offs between DNS-based and registry-based service discovery?"
            answer: "DNS: Simple (no extra infra), works everywhere, but slow updates (60s TTL), no health checks, no metadata. Registry (Consul): Real-time updates (<1s), health-aware, rich metadata (version, tags), but needs extra infra ($300/month), client integration. Use DNS for: Simple, static environments. Registry for: Dynamic, cloud-native, frequent changes. Kubernetes: DNS for simplicity + registry for advanced features."
          - question: "How do you implement graceful degradation with health monitoring?"
            answer: "Health check returns: Healthy (100%), Degraded (50% capacity, DB slow), Unhealthy (0%, DB down). Load balancer adjusts: Send less traffic to degraded instances. Service implementation: Degraded mode = disable non-critical features (recommendations OFF, core functions ON). Example: E-commerce checkout works, recommendations disabled. Monitoring: Alert on degraded, page on unhealthy. Prevents: Complete outage, maintains core functionality."
        tradeoffs:
          - question: "Consul vs Eureka for service registry: consistency vs availability trade-offs"
            answer: "Consul (CP): Raft consensus, strong consistency, minority partition = unavailable, 99.9% availability, financial/critical systems. Eureka (AP): Peer replication, eventual consistency (1-30s lag), always available (even during partition), 99.99% availability, web applications. Consul: Accurate routing, slower writes (50ms). Eureka: Fast writes (10ms), may route to dead instances. Use Consul for: Consistency matters. Eureka for: Availability matters."
          - question: "Shallow vs deep health checks: performance vs accuracy"
            answer: "Shallow (ping/HTTP 200): 1ms response, every 5s, CPU check only, fast but inaccurate (service up but DB down). Deep (check DB/cache): 50-100ms response, every 60s, validates dependencies, accurate but expensive (DB load). Pattern: Shallow for liveness (restart container), deep for readiness (remove from LB). At 50 services: Shallow = 10 checks/sec, Deep = 0.8 checks/sec. Use both for comprehensive health."
          - question: "Self-registration vs third-party registration patterns"
            answer: "Self-registration: Service registers itself on startup, simple, needs service awareness of registry, tightly coupled. Third-party: Sidecar/agent registers service (Kubernetes service mesh), service unaware of registry, loosely coupled, extra component. Self: Good for simple apps, direct control. Third-party: Good for polyglot, legacy apps, consistent registration. Kubernetes: Uses third-party (kubelet registers pods)."
          - question: "Circuit breaker vs retry patterns for fault tolerance"
            answer: "Retry: Transient failures (network blip), exponential backoff (1s, 2s, 4s), max 3 attempts, fixes temporary issues, but amplifies load on struggling service. Circuit breaker: Persistent failures (service down), fast fail after threshold, gives service time to recover, prevents cascade. Use both: Retry for transient errors (5XX with retry-able codes), circuit breaker for repeated failures. Together: 3 retries → if keeps failing → open circuit."
          - question: "DNS caching vs real-time service discovery updates"
            answer: "DNS caching: 60s TTL, simple, works everywhere, but stale for 60s after change, no health awareness, eventual consistency. Real-time (Consul): <1s updates, health-aware, accurate, but needs client integration, extra infra. Use DNS for: Stable services (databases, rarely change). Real-time for: Dynamic services (auto-scaling, frequent deployments). Hybrid: DNS SRV records with short TTL (10s) for compromise."
          - question: "Centralized vs distributed health monitoring approaches"
            answer: "Centralized (single health checker): Simple, consistent view, single point of failure, scales to ~1000 services, 50ms latency (network to central). Distributed (each LB checks): No SPOF, scales infinitely, independent decisions, but inconsistent view (split-brain risk), <5ms latency (local). Use centralized for: Small deployments (<100 services), need consistency. Distributed for: Large scale (>1000 services), regional deployments."
        scenarios:
          - "Design service discovery for a multi-cloud deployment spanning AWS and Azure"
          - "Implement health monitoring for a payment service with external bank API dependencies"
          - "Handle service discovery during a rolling deployment with zero downtime"
          - "Design circuit breaker strategy for a recommendation service with ML models"
          - "Create health monitoring for a microservices system with 100+ services"
          - "Implement service discovery for a hybrid environment with VMs and containers"
          - "Design failure detection for services with variable response times"
          - "Handle service discovery in a Kubernetes cluster with auto-scaling"
      time_estimate: 100

    - day: 12
      topic: "Data Partitioning Strategies"
      activity: "Master comprehensive data partitioning including horizontal/vertical sharding, consistent hashing, rebalancing strategies, and cross-shard operations."
      detailed_content: |
        Data Partitioning Fundamentals:
        Why Data Partitioning is Needed:
        - Scalability: Single database performance limits
        - Storage capacity: Data grows beyond single machine capacity
        - Performance: Distribute load across multiple machines
        - Availability: Isolate failures to specific partitions
        - Geographic distribution: Data locality for global applications
        - Cost optimization: Use appropriate hardware for different data types

        When to Consider Partitioning:
        - Database size exceeding single server capacity (>1TB)
        - Query performance degrading despite optimization
        - Write throughput hitting database limits
        - Need for geographic data distribution
        - Different data access patterns requiring optimization
        - Regulatory requirements for data sovereignty

        Partitioning vs Scaling Alternatives:
        - Vertical scaling: Increase hardware resources (temporary solution)
        - Read replicas: Scale read operations only
        - Caching: Reduce database load but doesn't solve storage
        - Partitioning: Scale both storage and throughput horizontally

        Types of Data Partitioning:
        1. Horizontal Partitioning (Sharding):
           Definition: Split rows across multiple database instances

           How it Works:
           - Each shard contains subset of total data
           - Same schema across all shards
           - Application routes queries to appropriate shard
           - Data distribution based on partitioning key

           Benefits:
           - Linear scalability for both reads and writes
           - Improved query performance (smaller datasets)
           - Failure isolation (one shard down doesn't affect others)
           - Geographic distribution possible

           Challenges:
           - Complex application logic for shard routing
           - Cross-shard queries and transactions
           - Uneven data distribution (hotspots)
           - Schema changes across multiple shards

        2. Vertical Partitioning:
           Definition: Split columns across different systems

           Column-Level Vertical Partitioning:
           - Frequently accessed columns on fast storage
           - Infrequently accessed columns on cheaper storage
           - Large BLOB/text fields separated from main table
           - Different access patterns optimized separately

           Example:
           ```
           User table split into:
           - user_core: id, username, email (frequent access)
           - user_profile: bio, preferences, settings (occasional access)
           - user_media: profile_picture, documents (rare access)
           ```

           Service-Level Vertical Partitioning:
           - Different services own different data domains
           - Microservices architecture pattern
           - Domain-driven design principles
           - Independent scaling and development

           Benefits:
           - Optimized storage and performance per data type
           - Reduced I/O for frequently accessed data
           - Better cache utilization
           - Technology choice flexibility

           Challenges:
           - Joins across partitions become complex
           - Application complexity increases
           - Data consistency across partitions
           - Additional network calls

        3. Functional Partitioning:
           Definition: Split data by feature or business domain

           Implementation Approaches:
           - Feature-based: Orders, Users, Products in separate databases
           - Geographic: US customers, EU customers, APAC customers
           - Temporal: Current data, archived data separation
           - Tenant-based: Multi-tenant application isolation

           Benefits:
           - Clear ownership and responsibility boundaries
           - Independent scaling per domain
           - Technology choice per domain requirements
           - Easier compliance and data governance

           Challenges:
           - Cross-domain operations complexity
           - Data synchronization requirements
           - Reporting across domains
           - Maintaining referential integrity

        Horizontal Sharding Strategies:
        1. Range-Based Sharding:
           How it Works:
           - Partition data based on value ranges
           - Example: Users A-M on shard1, N-Z on shard2
           - Date ranges: 2023 data on shard1, 2024 on shard2

           Implementation:
           ```
           def get_shard(user_id):
               if user_id < 1000000:
                   return "shard1"
               elif user_id < 2000000:
                   return "shard2"
               else:
                   return "shard3"
           ```

           Advantages:
           - Simple to understand and implement
           - Range queries efficiently handled
           - Good for time-series data
           - Sequential data stays together

           Disadvantages:
           - Uneven data distribution possible
           - Hotspots on active ranges
           - Difficult to rebalance
           - Requires knowledge of data distribution

        2. Hash-Based Sharding:
           How it Works:
           - Apply hash function to partitioning key
           - Use hash result to determine shard
           - Uniform distribution across shards

           Implementation:
           ```
           def get_shard(user_id, num_shards):
               return hash(user_id) % num_shards
           ```

           Advantages:
           - Even data distribution
           - Simple implementation
           - No hotspot issues
           - Works well for unknown data patterns

           Disadvantages:
           - Range queries require checking all shards
           - Difficult to rebalance (all data reshuffled)
           - Loss of data locality
           - Fixed number of shards

        3. Directory-Based Sharding:
           How it Works:
           - Maintain lookup service for shard locations
           - Directory service maps keys to shard locations
           - Application queries directory before data access

           Architecture:
           ```
           Client -> Directory Service -> Appropriate Shard
           Directory Service maintains:
           - Key ranges -> Shard mappings
           - Shard health status
           - Rebalancing operations
           ```

           Advantages:
           - Flexible sharding strategies
           - Dynamic rebalancing possible
           - Complex routing logic centralized
           - Can combine multiple strategies

           Disadvantages:
           - Additional network hop (latency)
           - Directory service becomes critical dependency
           - Consistency challenges in directory
           - More complex architecture

        4. Consistent Hashing:
           How it Works:
           - Hash both data keys and shard identifiers
           - Map both to points on a hash ring
           - Data assigned to next shard clockwise on ring
           - Adding/removing shards only affects neighbors

           Virtual Nodes:
           - Each physical shard has multiple virtual nodes
           - Better load distribution
           - Reduces impact of shard additions/removals

           Implementation Concepts:
           ```
           Hash Ring: 0 -------- 2^32
           Virtual Nodes: Each shard gets multiple positions
           Data Placement: Key hashed, assigned to next shard
           Rebalancing: Only adjacent virtual nodes affected
           ```

           Advantages:
           - Minimal data movement during rebalancing
           - Excellent for distributed systems
           - Handles node failures gracefully
           - Scales elastically

           Disadvantages:
           - Complex implementation
           - Potential for uneven distribution
           - Requires virtual nodes for balance
           - Range queries still problematic

        Sharding Key Design:
        Characteristics of Good Sharding Keys:
        1. High Cardinality:
           - Many possible values for even distribution
           - Avoid low-cardinality keys (gender, status)
           - Ensure sufficient values for future growth

        2. Even Distribution:
           - Avoid skewed access patterns
           - Consider data growth patterns
           - Monitor and adjust over time

        3. Query Pattern Alignment:
           - Most queries should target single shard
           - Avoid frequent cross-shard operations
           - Balance between reads and writes

        4. Stability:
           - Key values shouldn't change frequently
           - Avoid keys that can be updated
           - Consider immutable identifiers

        Common Sharding Key Patterns:
        1. User ID-Based:
           - Works well for user-centric applications
           - Good for social media, e-commerce
           - Potential celebrity user hotspots

        2. Tenant ID-Based:
           - Perfect for multi-tenant applications
           - Clear isolation boundaries
           - Uneven tenant sizes can cause imbalance

        3. Geographic-Based:
           - Good for location-aware applications
           - Reduces latency for regional users
           - Population density affects distribution

        4. Time-Based:
           - Excellent for time-series data
           - Archive old shards easily
           - Recent data hotspots common

        5. Hash of Natural Key:
           - Even distribution guaranteed
           - Lost semantic meaning
           - Range queries become expensive

        Anti-Patterns in Sharding Key Selection:
        - Monotonically increasing keys (creates hotspots)
        - Low cardinality keys (uneven distribution)
        - Frequently updated keys (data movement)
        - Keys requiring joins across shards
        - Business-critical fields that might change

        Cross-Shard Operations:
        Distributed Queries:
        1. Scatter-Gather Pattern:
           - Send query to all relevant shards
           - Gather and merge results
           - Application-level aggregation

           Implementation Challenges:
           - Partial failures handling
           - Result set pagination
           - Performance optimization
           - Memory management for large results

        2. Two-Phase Queries:
           - Phase 1: Identify relevant shards
           - Phase 2: Execute targeted queries
           - Optimization for selective queries

        3. Aggregation Strategies:
           - Pre-computed aggregates per shard
           - Real-time aggregation across shards
           - Approximate algorithms (HyperLogLog, Bloom filters)

        Distributed Transactions:
        1. Two-Phase Commit (2PC):
           - Coordinator manages commit protocol
           - All shards must agree to commit
           - Blocking protocol with availability issues

        2. Saga Pattern:
           - Sequence of local transactions
           - Compensating actions for rollback
           - Better availability but eventual consistency

        3. Distributed Locks:
           - Coordinate access across shards
           - Prevent concurrent modifications
           - Deadlock detection and resolution

        Data Rebalancing Strategies:
        When Rebalancing is Needed:
        - Uneven data distribution detected
        - Shard capacity limits reached
        - Performance degradation observed
        - New shards added to cluster
        - Failed shards need redistribution

        Rebalancing Approaches:
        1. Stop-and-Copy:
           - Stop all writes to affected shards
           - Copy data to new distribution
           - Update routing configuration
           - Resume operations

           Advantages: Simple, ensures consistency
           Disadvantages: Downtime required

        2. Live Migration:
           - Gradually move data while serving requests
           - Double-write during migration period
           - Switch routing after migration complete

           Phases:
           - Phase 1: Copy existing data
           - Phase 2: Sync incremental changes
           - Phase 3: Switch traffic routing
           - Phase 4: Cleanup old data

        3. Virtual Node Redistribution:
           - Move virtual nodes between physical shards
           - Gradual rebalancing process
           - Minimal data movement required

        Rebalancing Best Practices:
        - Monitor shard metrics continuously
        - Automate rebalancing triggers
        - Implement circuit breakers during migration
        - Test rebalancing procedures regularly
        - Plan for rollback scenarios

        Database-Specific Partitioning:
        SQL Database Partitioning:
        1. Built-in Partitioning:
           - PostgreSQL: Table partitioning (range, hash, list)
           - MySQL: Partition by range, hash, key, list
           - SQL Server: Horizontal, vertical, functional partitioning

        2. Application-Level Sharding:
           - Multiple database instances
           - Application routing logic
           - Connection pooling per shard

        NoSQL Database Partitioning:
        1. MongoDB:
           - Automatic sharding with shard keys
           - Range and hash-based strategies
           - Balancer for automatic rebalancing

        2. Cassandra:
           - Consistent hashing by default
           - Partition key determines data placement
           - Virtual nodes for load distribution

        3. DynamoDB:
           - Automatic partitioning by partition key
           - Adaptive capacity for hot partitions
           - Global secondary indexes for different access patterns

        Monitoring and Observability:
        Key Metrics to Monitor:
        1. Shard-Level Metrics:
           - Data size per shard
           - Query latency per shard
           - CPU and memory utilization
           - Disk I/O patterns

        2. Distribution Metrics:
           - Data distribution skew
           - Query distribution across shards
           - Hotspot detection
           - Rebalancing progress

        3. Cross-Shard Operation Metrics:
           - Cross-shard query frequency
           - Distributed transaction success rates
           - Aggregate query performance
           - Data consistency lag

        Alerting Strategies:
        - Shard capacity thresholds
        - Performance degradation alerts
        - Rebalancing operation status
        - Cross-shard operation failures

        Real-World Implementation Examples:
        Instagram Photo Storage Sharding:
        - Sharding key: Photo ID (time-based + shard ID)
        - Strategy: Custom ID generation ensuring even distribution
        - Challenge: Handling celebrity accounts with high activity
        - Solution: Additional hash layer for viral content

        Uber Trip Data Partitioning:
        - Primary sharding: Geographic (city-based)
        - Secondary partitioning: Time-based for historical data
        - Cross-shard operations: Global analytics and ML models
        - Real-time routing: GPS-based shard selection

        Slack Message Storage:
        - Sharding key: Team ID
        - Strategy: Hash-based distribution
        - Challenge: Large teams creating hotspots
        - Solution: Sub-sharding for large teams by channel

        Netflix Viewing History:
        - Sharding key: User ID
        - Strategy: Consistent hashing with virtual nodes
        - Time-based archival: Old data moved to different storage
        - Cross-shard analytics: Recommendation engine queries

        Common Pitfalls and Solutions:
        1. Celebrity Problem:
           Problem: Popular entities create hotspots
           Solutions:
           - Additional hash layer for viral content
           - Caching layer for popular data
           - Read replicas for high-traffic shards

        2. Thundering Herd During Rebalancing:
           Problem: All clients hit same shard during migration
           Solutions:
           - Gradual traffic shifting
           - Circuit breakers and rate limiting
           - Client-side backoff and retry

        3. Cross-Shard Join Explosion:
           Problem: Complex queries require multiple shard hits
           Solutions:
           - Denormalization and data duplication
           - Separate analytical database
           - Pre-computed result caching

        4. Shard Key Changes:
           Problem: Business requirements change sharding strategy
           Solutions:
           - Design for multiple sharding strategies
           - Gradual migration to new key
           - Hybrid routing during transition

        Advanced Partitioning Patterns:
        1. Multi-Level Partitioning:
           - Geographic -> Tenant -> Time-based
           - Each level optimizes for different access patterns
           - Increased complexity but better performance

        2. Partition Pruning:
           - Query optimizer eliminates irrelevant shards
           - Metadata-driven shard selection
           - Significant performance improvements

        3. Dynamic Repartitioning:
           - Automatic shard splitting based on load
           - Machine learning for partition prediction
           - Self-healing partition strategies

        4. Federated Partitioning:
           - Different data types use different strategies
           - Optimized partitioning per data access pattern
           - Complex but highly optimized approach
      resources:
        - title: "Database Sharding Explained"
          url: "https://aws.amazon.com/what-is/database-sharding/"
          description: "AWS comprehensive guide to database sharding strategies"
        - title: "Consistent Hashing"
          url: "https://en.wikipedia.org/wiki/Consistent_hashing"
          description: "Detailed explanation of consistent hashing algorithm"
        - title: "PostgreSQL Partitioning"
          url: "https://www.postgresql.org/docs/current/ddl-partitioning.html"
          description: "PostgreSQL built-in partitioning features and best practices"
        - title: "MongoDB Sharding"
          url: "https://docs.mongodb.com/manual/sharding/"
          description: "MongoDB automatic sharding implementation guide"
        - title: "Cassandra Data Modeling"
          url: "https://cassandra.apache.org/doc/latest/data_modeling/"
          description: "Cassandra partitioning and data modeling best practices"
        - title: "Sharding Pinterest"
          url: "https://medium.com/pinterest-engineering/sharding-pinterest-how-we-scaled-our-mysql-fleet-3f341e96ca6f"
          description: "Pinterest's real-world MySQL sharding experience"
      practice_questions:
        estimation:
          - question: "Social media platform with 1B users needs data partitioning. Design sharding strategy and estimate shard count."
            answer: "Shard by user_id (hash-based). Data per user: 1KB profile + 10KB posts = 11KB. Total: 1B × 11KB = 11TB. Shard size: 100GB per shard = 110 shards. Use consistent hashing with 1000 virtual nodes per server to enable smooth rebalancing. Servers: 55 physical servers (2 shards each). Cross-shard queries: User graph stored in separate graph DB. Cost: $20K/month for storage + compute."
          - question: "E-commerce database growing 100GB/month. When to shard and estimate timeline for implementation?"
            answer: "Shard when: 1) Single DB >1TB (query performance degrades), 2) Write throughput >10K TPS, 3) Read replicas insufficient. Current: 100GB/month = 1TB in 10 months. Sharding timeline: 3 months planning + 2 months migration = 5 months total. Start at 6-month mark (600GB). Strategy: Shard by customer_id, 10 shards initially, room to scale to 100 shards. Cost: $5K migration + $2K/month extra operational."
          - question: "Global chat application needs geographic partitioning. Design strategy for 5 regions with data sovereignty."
            answer: "Partition by region (US, EU, APAC, LATAM, Africa). Users assigned to region on signup (immutable). Cross-region chats: Replicate messages to both regions (eventual consistency, 50-200ms lag). Storage per region: 200M users × 5KB = 1TB. Compliance: EU data stays in EU (GDPR). Latency: 20ms within region, 200ms cross-region. Cost: $1K/month per region = $5K total."
          - question: "Time-series database storing 1M events/second. Design partitioning for efficient queries and archival."
            answer: "Partition by time (daily partitions): 1M events/sec × 86,400s × 100B = 8.6TB/day. Query pattern: Recent data (last 7 days) = hot, old data = cold. Hot partitions: SSD, 7 × 8.6TB = 60TB. Cold: S3, compressed 10:1 = 300TB/year → 30TB storage. Query: WHERE time > '2025-10-03' (partition pruning, scans 1 day = 8.6TB instead of full dataset). Cost: $5K/month hot + $1K/month cold storage."
        concepts:
          - question: "When would you choose range-based over hash-based sharding?"
            answer: "Range (by date, ID ranges): Enables range queries (WHERE date BETWEEN), sequential access, but prone to hotspots (current month hot). Use for: Time-series, audit logs, incremental IDs. Hash: Uniform distribution, prevents hotspots, but no range queries (must query all shards). Use for: User data, random access patterns. Hybrid: Hash for users, range for their time-series data (orders by date within user shard)."
          - question: "How do you handle cross-shard transactions in a partitioned system?"
            answer: "Avoid cross-shard transactions (design around them). If unavoidable: 1) Two-phase commit (2PC): Slow (100-200ms), blocks resources, use rarely. 2) Saga pattern: Eventual consistency, compensating transactions, 50ms. 3) Denormalize: Duplicate data to avoid cross-shard joins. Example: Order service stores customer name (denormalized) to avoid joining customer shard. 90% of transactions should be single-shard."
          - question: "What are the trade-offs between consistent hashing and directory-based sharding?"
            answer: "Consistent hashing: Automatic rebalancing (add/remove nodes), minimal data movement (1/N keys), no central directory, but complex implementation, not range-query friendly. Directory: Central mapping (key → shard), simple, supports range queries, but manual rebalancing, directory = SPOF (mitigate with replication). Use consistent for: Dynamic environments (cloud, auto-scaling). Directory for: Static, range queries important."
          - question: "How do you detect and resolve hotspots in a sharded database?"
            answer: "Detection: Monitor per-shard metrics (QPS, latency, CPU). Hotspot: One shard >2x average load. Causes: Celebrity user (millions of followers), seasonal data (current month hot), poor shard key. Resolution: 1) Split hot shard (if range-based), 2) Add read replicas for hot shard, 3) Cache hot data (Redis), 4) Rebalance with better key (add salt to celebrity IDs). Monitor every 5min, auto-alert if >80% capacity."
          - question: "What factors determine good vs bad sharding keys?"
            answer: "Good key: 1) High cardinality (millions of values, not just 10 countries), 2) Uniform distribution (no hotspots), 3) Stable (doesn't change), 4) Query-aligned (most queries filter by this key). Examples: user_id, order_id. Bad key: 1) Low cardinality (country = 200 values = uneven shards), 2) Temporal (created_date = current month hot), 3) Mutable (status changes). Worst: Boolean (active/inactive = only 2 shards)."
          - question: "How do you implement zero-downtime data rebalancing?"
            answer: "Dual-write approach: 1) Write to both old and new shards, 2) Background copy existing data (batch 1000 rows/sec), 3) Verify consistency, 4) Switch reads to new shard, 5) Stop writes to old shard, 6) Drop old shard. Duration: 1TB at 1GB/hour = 1000 hours = 6 weeks. Use consistent hashing to minimize data movement (only affected keys). Monitor: Replication lag, query latency, error rates. Rollback plan: Keep old shard for 1 week."
        tradeoffs:
          - question: "Horizontal vs vertical partitioning for analytics workloads"
            answer: "Horizontal (row-based): Split by user_id, scales writes, enables parallel queries, but cross-shard aggregations expensive. Vertical (column-based): Split by column groups (user profile vs user activity), reduces I/O for columnar queries, optimizes for specific query patterns, but complex joins. Analytics: Use vertical (columnar storage like Redshift), 10x faster scans. OLTP: Use horizontal, scales transactions. Hybrid: Horizontal for users + vertical for analytics (separate OLAP DB)."
          - question: "Single vs multi-level partitioning strategies"
            answer: "Single (by user_id): Simple, one routing decision, but limited to one dimension. Multi-level (by region then user_id): Supports multiple query patterns (by region, by user), more flexible, but 2x routing complexity, harder rebalancing. Use single for: Uniform access patterns. Multi-level for: Geographic data (region/country/user), hierarchical data (tenant/user/document). Cost: Multi-level = 2x operational complexity."
          - question: "Automatic vs manual sharding rebalancing"
            answer: "Automatic (Cassandra, MongoDB): Detects imbalance, triggers rebalance, hands-off, but unpredictable timing (may happen during peak), less control, potential disruption. Manual: Planned during low-traffic (3 AM), controlled, tested, but requires monitoring and intervention. Use automatic for: Dev/test, self-service teams (<10 people). Manual for: Production, critical systems, enterprise (>100M users). Automatic risks: Rebalancing during Black Friday."
          - question: "Application-level vs database-level partitioning"
            answer: "Application-level: App routes queries to correct shard, full control, works with any DB, but complexity in app code, must update all clients. Database-level (Citus, Vitess): DB handles routing, transparent to app, simpler app code, but DB lock-in, less flexibility. Use app-level for: Polyglot, custom logic, multiple DB types. DB-level for: Single DB product, simpler operations, prefer managed solutions."
          - question: "Strong vs eventual consistency in cross-shard operations"
            answer: "Strong (2PC): Cross-shard transactions ACID, 100-200ms latency, locks held across shards, limited to 1K TPS. Eventual (saga): 50ms latency, scales to 100K TPS, but temporary inconsistencies (1-5s), complex error handling. Use strong for: Money, inventory (can't have inconsistency). Eventual for: Social features, analytics, denormalized data. Cost: Strong = 5x infrastructure for same throughput."
          - question: "Complex sharding vs read replicas for scaling reads"
            answer: "Read replicas: Simple (1 master + N replicas), scales reads to 10x, 1-5s replication lag, SPOF = master write bottleneck. Sharding: Complex (10-100 shards), scales both reads and writes to 100x, no replication lag (eventual per shard), no SPOF, but operational complexity. Use replicas when: Read:write = 10:1, <1TB data, <10K writes/sec. Shard when: Write-heavy, >1TB, need horizontal write scaling."
        scenarios:
          - "Design partitioning strategy for a multi-tenant SaaS with varying tenant sizes"
          - "Handle data rebalancing for a gaming platform with seasonal traffic spikes"
          - "Implement cross-shard analytics for a financial trading platform"
          - "Design sharding for a content management system with global distribution"
          - "Handle partition key changes for an evolving e-commerce platform"
          - "Implement efficient cross-shard search for a social media platform"
          - "Design temporal partitioning for regulatory compliance in healthcare"
          - "Handle celebrity user problem in a social networking application"
      time_estimate: 110

    - day: 13
      topic: "Database Replication"
      activity: "Master comprehensive database replication including replication topologies, consistency models, conflict resolution strategies, and failover mechanisms."
      detailed_content: |
        Database Replication Fundamentals:
        Why Database Replication is Needed:
        - High availability: Survive individual server failures
        - Read scalability: Distribute read load across replicas
        - Disaster recovery: Geographically distributed backups
        - Data locality: Reduce latency for global applications
        - Offline access: Local copies for disconnected operations
        - Load distribution: Separate read and write workloads

        When to Implement Replication:
        - Single point of failure concerns
        - Read traffic exceeding single server capacity
        - Geographic distribution requirements
        - Business continuity and disaster recovery needs
        - Regulatory compliance for data availability
        - Development/testing environment isolation

        Replication vs Other Scaling Solutions:
        - Vertical scaling: Limited by hardware constraints
        - Horizontal partitioning: Complex for non-partitionable data
        - Caching: Temporary solution, doesn't solve availability
        - Replication: Provides both availability and read scalability

        Database Replication Topologies:
        1. Master-Slave (Primary-Replica) Replication:
           Architecture:
           - Single master node handles all writes
           - Multiple read-only slave nodes receive updates
           - Slaves replicate master's transaction log
           - Read requests distributed across slaves

           Data Flow:
           ```
           Write Request -> Master -> Slave 1
                                  -> Slave 2
                                  -> Slave N
           Read Request -> Any Slave (load balanced)
           ```

           Advantages:
           - Simple to understand and implement
           - Strong consistency for writes (single source)
           - Read scalability through multiple replicas
           - Clear separation of read and write workloads

           Disadvantages:
           - Master is single point of failure for writes
           - Write scalability limited to master capacity
           - Replica lag can cause read inconsistency
           - Manual intervention required for master failure

           Use Cases:
           - Read-heavy applications (blogs, news sites)
           - Reporting and analytics workloads
           - Development/testing environment separation
           - Geographic read distribution

        2. Master-Master (Multi-Master) Replication:
           Architecture:
           - Multiple nodes accept write operations
           - Bidirectional replication between masters
           - Each master maintains full copy of data
           - Conflict resolution mechanisms required

           Data Flow:
           ```
           Master 1 <-> Master 2
           Master 1 <-> Master 3
           Master 2 <-> Master 3
           (Full mesh or ring topology)
           ```

           Advantages:
           - No single point of failure for writes
           - Write scalability across multiple nodes
           - Geographic write distribution possible
           - Better availability during node failures

           Disadvantages:
           - Complex conflict resolution required
           - Potential for inconsistent data states
           - Higher network overhead
           - More difficult to debug and monitor

           Use Cases:
           - Geographically distributed applications
           - High-availability write requirements
           - Collaborative editing systems
           - Multi-region deployments

        3. Cascade Replication:
           Architecture:
           - Master replicates to primary slaves
           - Primary slaves replicate to secondary slaves
           - Hierarchical replication topology
           - Reduces load on master for many replicas

           Benefits:
           - Reduced network load on master
           - Scalable for large numbers of replicas
           - Geographic distribution optimization
           - Bandwidth usage optimization

           Challenges:
           - Increased replication lag for deeper levels
           - Complexity in failure handling
           - Dependency chains create fragility

        4. Ring Replication:
           Architecture:
           - Nodes arranged in circular topology
           - Each node replicates to next node in ring
           - Eventually consistent across all nodes
           - Good for distributed systems

           Benefits:
           - Balanced load distribution
           - Fault tolerance (multiple paths)
           - Scalable architecture
           - Geographic distribution support

           Challenges:
           - Longer convergence time
           - Complex failure detection
           - Potential for split rings

        Replication Methods and Consistency:
        1. Synchronous Replication:
           How it Works:
           - Master waits for acknowledgment from all replicas
           - Transaction commits only after replica confirmation
           - Strong consistency guarantee
           - Higher latency due to network coordination

           Implementation:
           ```
           1. Client sends write to master
           2. Master forwards to all replicas
           3. Replicas acknowledge write completion
           4. Master commits transaction
           5. Master responds to client
           ```

           Advantages:
           - Strong consistency across all nodes
           - No data loss during failures
           - Immediate consistency for read operations
           - Simplified application logic

           Disadvantages:
           - Higher write latency
           - Reduced availability (all nodes must be up)
           - Network partition sensitivity
           - Slower performance under load

           Use Cases:
           - Financial systems requiring accuracy
           - Critical business data
           - Compliance-sensitive applications
           - Small numbers of replicas

        2. Asynchronous Replication:
           How it Works:
           - Master commits locally without waiting for replicas
           - Replication happens in background
           - Better performance but eventual consistency
           - Potential for data loss during failures

           Implementation:
           ```
           1. Client sends write to master
           2. Master commits transaction immediately
           3. Master responds to client
           4. Master asynchronously replicates to slaves
           ```

           Advantages:
           - Low write latency
           - High availability (master independent)
           - Better performance under load
           - Network partition tolerance

           Disadvantages:
           - Eventual consistency only
           - Potential data loss during master failure
           - Replica lag complexity
           - Read-after-write consistency issues

           Use Cases:
           - High-performance applications
           - Social media platforms
           - Content management systems
           - Non-critical data scenarios

        3. Semi-Synchronous Replication:
           How it Works:
           - Configurable number of replicas must acknowledge
           - Hybrid approach balancing consistency and performance
           - Timeout mechanisms for unresponsive replicas
           - Flexible consistency guarantees

           Configuration Options:
           - Minimum replica acknowledgments required
           - Timeout for replica responses
           - Fallback to asynchronous on timeout
           - Priority-based replica selection

           Advantages:
           - Balanced consistency and performance
           - Configurable based on requirements
           - Better availability than full synchronous
           - Reduced data loss risk

           Disadvantages:
           - More complex configuration
           - Potential for partial consistency
           - Monitoring complexity
           - Tuning requirements

        Replication Lag and Monitoring:
        Causes of Replication Lag:
        - Network latency between nodes
        - High write volume on master
        - Slow replica hardware or configuration
        - Long-running transactions
        - Lock contention on replicas

        Measuring Replication Lag:
        - Log sequence number (LSN) differences
        - Timestamp-based lag calculation
        - Row count verification
        - Checksum comparison

        Handling Replication Lag:
        - Read preference configuration
        - Lag-aware load balancing
        - Master fallback for critical reads
        - Cached results for tolerance

        Conflict Resolution Strategies:
        1. Last-Write-Wins (LWW):
           How it Works:
           - Use timestamp to determine latest write
           - Simple but may lose concurrent updates
           - Clock synchronization critical
           - Often combined with vector clocks

           Advantages:
           - Simple to implement and understand
           - Deterministic resolution
           - No human intervention required
           - Scales well

           Disadvantages:
           - Data loss for concurrent writes
           - Clock synchronization dependency
           - May not reflect business intent
           - Poor for collaborative scenarios

        2. Vector Clocks:
           How it Works:
           - Track causality between updates
           - Each node maintains version vector
           - Detect concurrent vs sequential updates
           - Preserve all concurrent versions

           Implementation:
           ```
           Vector Clock: [Node1: 3, Node2: 1, Node3: 2]
           Update increments local node counter
           Merge vectors on conflict detection
           ```

           Advantages:
           - Preserves causality information
           - No data loss for concurrent updates
           - Distributed system friendly
           - Language/platform agnostic

           Disadvantages:
           - Complex implementation
           - Storage overhead grows
           - Requires conflict resolution UI
           - Vector size management needed

        3. Operational Transformation (OT):
           How it Works:
           - Transform operations based on context
           - Resolve conflicts at operation level
           - Maintain operation semantics
           - Common in collaborative editing

           Example:
           ```
           Initial: "Hello"
           Op1: Insert "X" at position 2 -> "HeXllo"
           Op2: Insert "Y" at position 3 -> "HelYlo"
           Transform: Adjust positions for both operations
           ```

           Advantages:
           - Preserves user intent
           - Real-time collaboration support
           - Automatic conflict resolution
           - User-friendly resolution

           Disadvantages:
           - Complex algorithm implementation
           - Operation-specific logic required
           - Performance overhead
           - Limited to certain data types

        4. Custom Business Logic Resolution:
           How it Works:
           - Application-specific conflict resolution
           - Business rules determine winner
           - May require human intervention
           - Context-aware resolution strategies

           Examples:
           - Financial: Always prefer larger amount
           - CRM: Merge customer information
           - Inventory: Sum quantities from all sources
           - User preferences: Most recent wins

           Advantages:
           - Business logic alignment
           - Context-aware decisions
           - Flexible resolution strategies
           - Optimal user experience

           Disadvantages:
           - Complex implementation
           - Requires domain expertise
           - May need manual intervention
           - Testing complexity

        Failover Strategies and High Availability:
        Automatic Failover:
        Components:
        - Health monitoring and detection
        - Replica promotion algorithms
        - Client connection redirection
        - Data consistency verification

        Health Monitoring:
        - Heartbeat mechanisms
        - Query response monitoring
        - Resource utilization tracking
        - Network connectivity checks

        Promotion Process:
        ```
        1. Detect master failure
        2. Select best replica candidate
        3. Promote replica to master
        4. Update DNS/load balancer configuration
        5. Redirect write traffic
        6. Monitor new master health
        ```

        Replica Selection Criteria:
        - Most up-to-date data (lowest lag)
        - Best hardware resources
        - Network proximity to clients
        - Administrative preferences

        Manual Failover:
        When Manual is Preferred:
        - Critical business decisions
        - Data integrity concerns
        - Complex conflict situations
        - Planned maintenance scenarios

        Manual Process:
        - Administrator verification
        - Data consistency checks
        - Controlled traffic redirection
        - Rollback preparation

        Split-Brain Prevention:
        Problem: Multiple nodes believe they are master
        Solutions:
        - Quorum-based decisions
        - Witness nodes for tie-breaking
        - Fencing mechanisms
        - Coordinator-based systems

        Quorum Implementation:
        ```
        Nodes: 3 total
        Quorum: 2 nodes minimum
        Network partition: [Node1] vs [Node2, Node3]
        Result: Node2-Node3 partition becomes active master
        ```

        Database-Specific Replication:
        MySQL Replication:
        - Binary log-based replication
        - Statement vs row-based formats
        - GTID (Global Transaction ID) support
        - Multi-source replication capabilities

        PostgreSQL Replication:
        - WAL (Write-Ahead Log) streaming
        - Hot standby replicas
        - Logical replication for partial data
        - Built-in failover mechanisms

        MongoDB Replica Sets:
        - Automatic failover with elections
        - Oplog-based replication
        - Read preference configuration
        - Arbiter nodes for odd numbers

        Redis Replication:
        - Master-slave with Sentinel
        - Redis Cluster for sharding
        - Pub/sub replication patterns
        - Memory-optimized replication

        Cloud Database Replication:
        AWS RDS:
        - Read replicas across regions
        - Multi-AZ deployments
        - Automated backup and recovery
        - Cross-region disaster recovery

        Google Cloud SQL:
        - Regional persistent disks
        - Read replicas and high availability
        - Point-in-time recovery
        - Cross-region replication

        Azure Database:
        - Geo-replication capabilities
        - Active-passive configurations
        - Automatic failover groups
        - Zone redundancy options

        Monitoring and Observability:
        Key Metrics to Monitor:
        1. Replication Health:
           - Replication lag measurements
           - Replica connectivity status
           - Error rates and retry counts
           - Throughput and latency metrics

        2. Data Consistency:
           - Checksum verification results
           - Row count comparisons
           - Timestamp drift measurements
           - Conflict resolution frequency

        3. Performance Metrics:
           - Query response times per replica
           - Resource utilization (CPU, memory, disk)
           - Network bandwidth usage
           - Connection pool status

        Alerting Strategies:
        - Replica lag threshold violations
        - Failed replica connections
        - Master-replica data divergence
        - Failover event notifications
        - Performance degradation alerts

        Real-World Implementation Examples:
        Netflix Database Replication:
        - Global master-slave topology
        - Cross-region read replicas
        - Automatic failover with Hystrix
        - Custom lag monitoring and alerting

        Instagram Photo Metadata:
        - Master-slave with geographic distribution
        - Async replication for performance
        - Custom sharding with replication
        - Cassandra multi-datacenter setup

        Airbnb Booking System:
        - Multi-master for global writes
        - Conflict resolution for concurrent bookings
        - Regional read replicas for performance
        - Custom failover orchestration

        WhatsApp Message Storage:
        - Master-slave with real-time replication
        - Custom conflict resolution for message ordering
        - Erlang-based cluster management
        - Geographic message distribution

        Best Practices and Common Pitfalls:
        Replication Best Practices:
        - Monitor replication lag continuously
        - Test failover procedures regularly
        - Plan for network partition scenarios
        - Implement proper backup strategies
        - Use connection pooling efficiently

        Common Pitfalls:
        1. Ignoring Replication Lag:
           Problem: Stale data served to users
           Solution: Lag-aware routing and monitoring

        2. Inadequate Failover Testing:
           Problem: Failures during actual outages
           Solution: Regular disaster recovery drills

        3. Poor Conflict Resolution:
           Problem: Data corruption or loss
           Solution: Comprehensive conflict testing

        4. Insufficient Monitoring:
           Problem: Undetected replication issues
           Solution: Comprehensive metrics and alerting

        5. Over-Synchronization:
           Problem: Performance degradation
           Solution: Balance consistency needs with performance

        Advanced Replication Patterns:
        1. Multi-Tier Replication:
           - Production -> Staging -> Development
           - Data masking at each tier
           - Different lag tolerances
           - Security boundary enforcement

        2. Selective Replication:
           - Replicate only specific tables/columns
           - Filter based on business rules
           - Reduce bandwidth and storage
           - Privacy and compliance benefits

        3. Bidirectional Replication:
           - Two-way sync between regions
           - Complex conflict resolution
           - Active-active configurations
           - Geographic load distribution

        4. Event-Driven Replication:
           - Trigger-based replication
           - Business event sourcing
           - Real-time analytics feeds
           - Audit trail maintenance

        Security Considerations:
        Replication Security:
        - Encrypted connections between nodes
        - Certificate-based authentication
        - Role-based access control
        - Data masking for non-production replicas

        Compliance and Governance:
        - Data residency requirements
        - GDPR compliance for EU data
        - Audit logging for data access
        - Retention policy enforcement
      resources:
        - title: "Database Replication Patterns"
          url: "https://en.wikipedia.org/wiki/Replication_(database)"
          description: "Comprehensive overview of database replication concepts and patterns"
        - title: "MySQL Replication Guide"
          url: "https://dev.mysql.com/doc/refman/8.0/en/replication.html"
          description: "MySQL replication implementation and best practices"
        - title: "PostgreSQL High Availability"
          url: "https://www.postgresql.org/docs/current/high-availability.html"
          description: "PostgreSQL replication and failover strategies"
        - title: "MongoDB Replica Sets"
          url: "https://docs.mongodb.com/manual/replication/"
          description: "MongoDB replica set configuration and management"
        - title: "Redis Replication"
          url: "https://redis.io/topics/replication"
          description: "Redis master-slave replication and high availability"
        - title: "Conflict-Free Replicated Data Types"
          url: "https://crdt.tech/"
          description: "Advanced conflict resolution techniques for distributed systems"
      practice_questions:
        estimation:
          - question: "E-commerce platform needs 99.9% availability across 3 regions. Design replication strategy and estimate RTO/RPO."
            answer: "Use master-slave with async replication per region + cross-region async replication. RTO: 1-2 minutes (auto-failover), RPO: 5-10 seconds (replication lag). Deploy master + 2 replicas per region (9 nodes total). Cost: ~$5K/month for 3 regions with monitoring."
          - question: "Social media app with 100M users requires read scalability. How many read replicas needed for 10K RPS?"
            answer: "Assume 80% reads, 20% writes. Read load: 8K RPS. If each replica handles 1K RPS, need 8 read replicas + 1 master. With safety margin (2x), deploy 16 read replicas across regions. Cost: ~$2K/month."
          - question: "Financial system needs zero data loss replication. Design synchronous replication architecture."
            answer: "Use synchronous replication with 2-phase commit. Master + 2 sync replicas (quorum=2). Write latency increases ~50ms. Use nearby data centers (<10ms network latency). Requires strong failover mechanism with fencing to prevent split-brain."
          - question: "Global chat application requires <100ms replication lag. Design network topology and estimate costs."
            answer: "Deploy master-slave in each region (US, EU, APAC). Use dedicated network links (AWS Direct Connect / GCP Interconnect). Async replication between regions with binlog streaming. Network cost: ~$1K/month per region link. Monitor lag with alerts at 50ms threshold."
        concepts:
          - question: "When would you choose synchronous vs asynchronous replication?"
            answer: "Synchronous: When data loss is unacceptable (financial transactions, critical data). Trade-off: Higher latency, reduced availability. Asynchronous: When performance matters more than consistency (social media, analytics). Trade-off: Possible data loss, better performance."
          - question: "How do you handle the split-brain problem in master-master replication?"
            answer: "Use consensus algorithm (Raft/Paxos) or quorum-based writes. Implement fencing (STONITH) to isolate failed master. Use external coordination service (ZooKeeper, etcd) for leader election. Prevent writes to minority partition."
          - question: "What are the trade-offs between statement-based and row-based replication?"
            answer: "Statement-based: Compact, efficient for bulk operations. Issues: Non-deterministic functions (NOW(), RAND()), trigger differences. Row-based: Deterministic, exact data replication. Issues: Large binary logs for bulk updates, more network bandwidth."
          - question: "How do you implement conflict resolution for concurrent writes?"
            answer: "Strategies: 1) Last-write-wins (timestamp-based, simple but loses data), 2) Vector clocks (track causality, complex), 3) Application-level merge (business logic), 4) CRDT (conflict-free data types). Choose based on data semantics and consistency requirements."
          - question: "What factors affect replication lag and how do you minimize it?"
            answer: "Factors: Network latency, write volume, replica load, large transactions. Solutions: Parallel replication threads, dedicated network links, partition data, batching, SSD storage, monitor and alert on lag spikes. Use read-your-writes pattern for consistency."
          - question: "How do you ensure data consistency during failover scenarios?"
            answer: "1) Wait for replica to catch up before promoting (increases downtime), 2) Use semi-sync replication (at least one replica in sync), 3) Implement fencing to prevent old master from accepting writes, 4) Use GTID (Global Transaction ID) to track replication position accurately."
        tradeoffs:
          - question: "Master-slave vs master-master replication for global applications"
            answer: "Master-slave: Simpler, consistent writes, single source of truth. Cons: Write bottleneck, higher latency for distant users. Master-master: Local writes, better availability, lower latency. Cons: Conflict resolution complexity, eventual consistency. Choose master-master for write-heavy global apps."
          - question: "Synchronous vs asynchronous replication: consistency vs performance"
            answer: "Synchronous: Strong consistency, no data loss, higher latency (~2x), reduced availability (all replicas must respond). Asynchronous: Better performance, higher availability, eventual consistency, possible data loss during failures. Use semi-sync as middle ground."
          - question: "Read replicas vs caching for read scalability"
            answer: "Read replicas: Fresh data (seconds lag), complex queries supported, SQL interface. Higher cost per query. Caching: Very fast (<1ms), cheaper, stale data, limited query complexity. Use both: Cache for hot data, replicas for complex queries."
          - question: "Automatic vs manual failover strategies"
            answer: "Automatic: Fast recovery (1-2 min), reduces downtime, risk of split-brain if misconfigured. Manual: Safer, human verification, slower (10-30 min), requires on-call. Use automatic with careful monitoring and fencing. Manual for critical financial systems."
          - question: "Last-write-wins vs vector clocks for conflict resolution"
            answer: "LWW: Simple, works with timestamps, loses concurrent writes. Use for: Non-critical data, shopping carts. Vector clocks: Preserves causality, detects conflicts, complex to implement. Use for: Collaborative editing, distributed databases (Riak, Cassandra)."
          - question: "Single-master vs multi-master for high availability"
            answer: "Single-master: Simple, consistent, write bottleneck, master failure stops writes. Multi-master: No single point of failure, geographic distribution, complex conflict resolution. Choose multi-master when: Write availability > consistency, geographic distribution required."
        scenarios:
          - "Design replication strategy for a banking system with regulatory compliance"
          - "Handle failover for an e-commerce platform during Black Friday traffic"
          - "Implement cross-region replication for a social media platform"
          - "Design conflict resolution for a collaborative document editing system"
          - "Handle replication lag in a real-time analytics dashboard"
          - "Implement disaster recovery for a mission-critical healthcare system"
          - "Design replication for a multi-tenant SaaS with data isolation requirements"
          - "Handle master failure during peak traffic with zero downtime"
      time_estimate: 105

    - day: 14
      topic: "Week 2 Integration - Chat System Design"
      activity: "Design a comprehensive real-time chat system integrating microservices architecture, message queues, API design, service discovery, data partitioning, and replication strategies."
      detailed_content: |
        System Requirements Analysis:
        Functional Requirements:
        - 1-on-1 messaging: Real-time text message exchange
        - Group messaging: Support up to 256 members per group
        - Online presence: Real-time user status (online/offline/away)
        - Message history: Persistent storage and retrieval
        - Message delivery status: Sent, delivered, read receipts
        - File sharing: Images, documents, and media files
        - Push notifications: Offline message delivery
        - Search functionality: Message and user search
        - User profiles: Basic user information management

        Non-Functional Requirements:
        - Scale: 500M daily active users (DAU)
        - Volume: 100B messages per day
        - Latency: <100ms message delivery
        - Availability: 99.99% uptime (52 minutes downtime/year)
        - Consistency: Strong consistency for message ordering
        - Security: End-to-end encryption support
        - Global: Multi-region deployment support

        Capacity Estimation:
        Message Volume Calculations:
        - 500M DAU × 40 messages/user/day = 20B messages/day
        - Peak traffic: 3x average = 60B messages/day
        - Messages per second: 60B / (24 × 3600) = ~694K messages/second
        - Peak messages per second: 2M messages/second

        Storage Requirements:
        - Average message size: 100 bytes (text) + metadata
        - Daily storage: 20B × 100 bytes = 2TB/day
        - Annual storage: 2TB × 365 = 730TB/year
        - With replication (3x): 2.2PB/year

        Network Bandwidth:
        - Per message: 100 bytes
        - Peak bandwidth: 2M messages/second × 100 bytes = 200MB/second
        - With protocol overhead: ~400MB/second

        WebSocket Connections:
        - Concurrent users: 500M DAU × 10% = 50M concurrent
        - Connections per server: 10K connections
        - Servers needed: 50M / 10K = 5,000 WebSocket servers

        High-Level Architecture:
        Client Applications:
        - Mobile apps (iOS, Android): Primary user interface
        - Web applications: Browser-based access
        - Desktop applications: Native desktop clients

        API Gateway Layer:
        - Load balancing across microservices
        - Authentication and authorization
        - Rate limiting and throttling
        - Protocol translation (HTTP/WebSocket)
        - Request routing and service discovery

        Microservices Architecture:
        Service Decomposition Strategy:
        1. User Service:
           - User registration and authentication
           - Profile management
           - Contact list management
           - User preferences and settings

        2. Chat Service:
           - Message processing and routing
           - Chat room management
           - Message persistence
           - Message ordering and deduplication

        3. Presence Service:
           - Online/offline status tracking
           - Last seen timestamps
           - Activity status updates
           - Real-time presence broadcasting

        4. Notification Service:
           - Push notification delivery
           - Offline message queuing
           - Notification preferences
           - Multi-platform notification support

        5. Media Service:
           - File upload and download
           - Image and video processing
           - Content moderation
           - CDN integration

        6. Search Service:
           - Message content indexing
           - User and group search
           - Full-text search capabilities
           - Search result ranking

        7. Analytics Service:
           - Usage metrics collection
           - Performance monitoring
           - Business intelligence
           - Real-time dashboards

        Real-Time Communication Design:
        WebSocket vs Alternatives Comparison:
        WebSocket Advantages:
        - Full-duplex communication
        - Low latency (<50ms)
        - Efficient for high-frequency messaging
        - Native browser support
        - Reduced server overhead

        HTTP Long Polling:
        - Simpler implementation
        - Better for low-frequency updates
        - Higher latency (100-500ms)
        - More server resources required

        Server-Sent Events (SSE):
        - One-way communication only
        - Good for notifications
        - Limited browser support
        - Not suitable for chat applications

        Selected Approach: WebSocket with fallback to long polling

        WebSocket Connection Management:
        Connection Pool Architecture:
        ```
        Client -> Load Balancer -> WebSocket Gateway
                                -> Connection Manager
                                -> Message Router
        ```

        Connection Lifecycle:
        1. Authentication: JWT token validation
        2. Connection establishment: WebSocket handshake
        3. Session management: User session tracking
        4. Heartbeat: Keep-alive mechanism
        5. Graceful disconnect: Cleanup resources

        Connection Distribution:
        - Consistent hashing for user assignment
        - Session affinity for WebSocket connections
        - Connection failover handling
        - Load balancing across WebSocket servers

        Message Flow Architecture:
        Message Processing Pipeline:
        1. Message Ingestion:
           - Client sends message via WebSocket
           - API Gateway validates and authenticates
           - Message queued for processing

        2. Message Processing:
           - Content validation and filtering
           - Spam and abuse detection
           - Message encryption
           - Metadata enrichment

        3. Message Routing:
           - Determine recipients (1-on-1 or group)
           - Fan-out for group messages
           - Real-time delivery via WebSocket
           - Offline user handling

        4. Message Persistence:
           - Store in partitioned message database
           - Update delivery status
           - Index for search functionality

        Message Queue Integration:
        Apache Kafka Implementation:
        - Topic per chat type (1-on-1, group)
        - Partitioning by chat_id for ordering
        - Consumer groups for horizontal scaling
        - Message retention for replay capability

        Queue Architecture:
        ```
        Message Producer -> Kafka Topic -> Consumer Groups
                                      -> WebSocket Delivery
                                      -> Database Persistence
                                      -> Push Notifications
        ```

        Message Ordering Guarantees:
        - Per-chat ordering using Kafka partitions
        - Vector clocks for conflict resolution
        - Idempotency tokens for deduplication
        - Sequence numbers for message ordering

        Data Storage and Partitioning:
        Database Partitioning Strategy:
        1. User Data Partitioning:
           - Partition key: hash(user_id)
           - Consistent hashing for distribution
           - User profile, contacts, preferences
           - Cross-partition joins minimized

        2. Message Data Partitioning:
           - Primary partition: hash(chat_id)
           - Secondary partition: timestamp ranges
           - Hot data in SSD, cold data in HDD
           - Time-based archival strategy

        3. Presence Data:
           - Redis cluster for real-time data
           - TTL-based expiration
           - Geographic distribution
           - Backup to persistent storage

        Database Schema Design:
        Messages Table Partitioning:
        ```sql
        CREATE TABLE messages (
            message_id UUID PRIMARY KEY,
            chat_id VARCHAR(64) NOT NULL,
            sender_id VARCHAR(64) NOT NULL,
            content TEXT,
            message_type ENUM('text', 'image', 'file'),
            timestamp TIMESTAMP NOT NULL,
            delivery_status ENUM('sent', 'delivered', 'read')
        ) PARTITION BY HASH(chat_id);
        ```

        Chat Metadata Table:
        ```sql
        CREATE TABLE chats (
            chat_id VARCHAR(64) PRIMARY KEY,
            chat_type ENUM('direct', 'group'),
            participants JSON,
            created_at TIMESTAMP,
            updated_at TIMESTAMP
        ) PARTITION BY HASH(chat_id);
        ```

        Database Replication Strategy:
        Master-Slave Replication:
        - Write operations to master nodes
        - Read operations distributed across replicas
        - Geographic distribution for latency
        - Automatic failover for high availability

        Replication Configuration:
        - Synchronous replication for critical data
        - Asynchronous replication for analytics
        - Cross-region replication for disaster recovery
        - Read preference routing by geography

        Service Discovery and Health Monitoring:
        Service Registry Implementation:
        - Consul for service discovery
        - Health checks every 10 seconds
        - Automatic service deregistration
        - DNS-based service resolution

        Health Monitoring Strategy:
        - Service health endpoints (/health)
        - Dependency health aggregation
        - Circuit breakers for external services
        - Real-time alerting for failures

        Circuit Breaker Configuration:
        - Failure threshold: 5 consecutive failures
        - Timeout period: 30 seconds
        - Success threshold: 3 successful calls
        - Fallback mechanisms for degraded service

        API Design and Versioning:
        RESTful API Design:
        ```
        POST /api/v1/chats/{chat_id}/messages
        GET /api/v1/chats/{chat_id}/messages?page=1&limit=50
        PUT /api/v1/users/{user_id}/presence
        GET /api/v1/users/{user_id}/profile
        ```

        WebSocket API:
        ```json
        {
          "type": "message",
          "payload": {
            "chat_id": "chat_123",
            "content": "Hello World",
            "timestamp": "2024-01-01T12:00:00Z"
          }
        }
        ```

        API Versioning Strategy:
        - URL path versioning: /api/v1/, /api/v2/
        - Backward compatibility maintenance
        - Deprecation timeline communication
        - Client SDK version management

        Group Chat Scalability:
        Fan-Out Strategies:
        1. Push Model (Fan-Out on Write):
           - Message copied to each recipient's queue
           - Fast read operations
           - High write overhead for large groups
           - Storage duplication

        2. Pull Model (Fan-Out on Read):
           - Single message storage
           - Recipients fetch on demand
           - Efficient storage usage
           - Slower read operations

        Hybrid Approach:
        - Push model for small groups (<50 members)
        - Pull model for large groups (>50 members)
        - Configurable threshold based on activity
        - Dynamic switching based on group size

        Large Group Optimization:
        - Message sampling for inactive users
        - Lazy loading for message history
        - Compression for bulk operations
        - Hierarchical fan-out for massive groups

        Online Presence System:
        Presence State Management:
        States: online, away, busy, offline
        Implementation:
        - Redis for real-time presence data
        - TTL-based automatic offline detection
        - Heartbeat mechanism for online users
        - Last seen timestamp tracking

        Presence Broadcasting:
        - Contact list filtering
        - Presence change notifications
        - Batch updates for efficiency
        - Rate limiting for presence updates

        Presence Data Structure:
        ```json
        {
          "user_id": "user_123",
          "status": "online",
          "last_seen": "2024-01-01T12:00:00Z",
          "device_type": "mobile",
          "location": "us-east-1"
        }
        ```

        Message Delivery Guarantees:
        Delivery Status Tracking:
        1. Sent: Message stored in sender's outbox
        2. Delivered: Message reached recipient's device
        3. Read: Message opened by recipient

        Implementation Strategy:
        - Acknowledgment system for each stage
        - Retry mechanism for failed deliveries
        - Exponential backoff for retries
        - Dead letter queues for failed messages

        Push Notification System:
        Offline Message Handling:
        - Message queuing for offline users
        - Push notification triggers
        - Notification batching and summarization
        - Platform-specific notification formatting

        Notification Service Architecture:
        ```
        Message Queue -> Notification Service -> FCM/APNS
                                             -> Email/SMS
                                             -> In-app Notifications
        ```

        Security and Privacy:
        End-to-End Encryption:
        - Signal Protocol implementation
        - Key exchange mechanisms
        - Forward secrecy guarantees
        - Message content encryption

        Authentication and Authorization:
        - JWT token-based authentication
        - OAuth 2.0 for third-party integration
        - Role-based access control
        - Session management and timeout

        Data Privacy:
        - GDPR compliance for EU users
        - Data retention policies
        - User data deletion capabilities
        - Audit logging for compliance

        Monitoring and Observability:
        Key Metrics:
        1. Performance Metrics:
           - Message delivery latency (P95, P99)
           - WebSocket connection stability
           - API response times
           - Database query performance

        2. Business Metrics:
           - Daily/monthly active users
           - Message volume and growth
           - User retention rates
           - Feature adoption rates

        3. System Health:
           - Service availability
           - Error rates by service
           - Resource utilization
           - Circuit breaker state

        Alerting Strategy:
        - Critical: Service downtime, data loss
        - Warning: High latency, error rate spikes
        - Info: Capacity thresholds, deployment events

        Scaling and Performance Optimization:
        Horizontal Scaling Strategies:
        - Auto-scaling based on connection count
        - Load balancing across regions
        - Database read replica scaling
        - CDN for static content delivery

        Caching Strategy:
        - Redis for frequently accessed data
        - Application-level caching
        - CDN caching for media files
        - Browser caching for static assets

        Performance Optimizations:
        - Message compression for large groups
        - Lazy loading for chat history
        - Connection pooling for databases
        - Batch operations for efficiency

        Disaster Recovery and Backup:
        Backup Strategy:
        - Daily incremental backups
        - Weekly full backups
        - Cross-region backup replication
        - Point-in-time recovery capability

        Disaster Recovery:
        - Multi-region active-passive setup
        - Automated failover procedures
        - RTO: 5 minutes, RPO: 1 minute
        - Regular disaster recovery testing

        Cost Optimization:
        Infrastructure Cost Management:
        - Reserved instances for predictable workloads
        - Spot instances for batch processing
        - Storage tiering for message archives
        - Cost monitoring and alerting

        Real-World Implementation Examples:
        WhatsApp Architecture Insights:
        - Erlang for high concurrency
        - FreeBSD for network optimization
        - XMPP protocol customization
        - Message routing optimization

        Discord Scalability:
        - Elixir for fault tolerance
        - Cassandra for message storage
        - Redis for presence and caching
        - Custom voice chat infrastructure

        Slack's Approach:
        - MySQL for message storage
        - Redis for real-time features
        - Elasticsearch for search
        - WebSocket connection management

        Deployment and DevOps:
        Containerization:
        - Docker containers for all services
        - Kubernetes for orchestration
        - Helm charts for deployment
        - Service mesh for communication

        CI/CD Pipeline:
        - Automated testing for all services
        - Blue-green deployment strategy
        - Feature flags for gradual rollouts
        - Automated rollback procedures

        Testing Strategy:
        1. Unit Testing:
           - Service-level testing
           - Mock external dependencies
           - Test coverage >90%

        2. Integration Testing:
           - API contract testing
           - Database integration tests
           - End-to-end user flows

        3. Load Testing:
           - WebSocket connection testing
           - Message throughput testing
           - Database performance testing
           - Chaos engineering for resilience

        Common Challenges and Solutions:
        1. Message Ordering:
           Challenge: Ensuring global message ordering
           Solution: Kafka partitioning + vector clocks

        2. Connection Management:
           Challenge: Handling millions of WebSocket connections
           Solution: Connection pooling + horizontal scaling

        3. Group Chat Performance:
           Challenge: Fan-out performance for large groups
           Solution: Hybrid push/pull model

        4. Presence Accuracy:
           Challenge: Accurate online/offline detection
           Solution: Heartbeat + TTL + graceful disconnect

        5. Search Scalability:
           Challenge: Searching across billions of messages
           Solution: Elasticsearch sharding + caching

        Implementation Checklist:
        Phase 1 - MVP (Months 1-3):
        - [ ] Basic 1-on-1 messaging
        - [ ] User authentication and profiles
        - [ ] Real-time message delivery
        - [ ] Basic mobile and web clients

        Phase 2 - Growth (Months 4-6):
        - [ ] Group messaging functionality
        - [ ] File and image sharing
        - [ ] Push notifications
        - [ ] Message search capabilities

        Phase 3 - Scale (Months 7-12):
        - [ ] Multi-region deployment
        - [ ] Advanced security features
        - [ ] Analytics and monitoring
        - [ ] Performance optimizations

        Design Trade-offs Summary:
        1. Consistency vs Availability:
           - Chose eventual consistency for better availability
           - Strong consistency for message ordering within chats

        2. Storage vs Performance:
           - Duplicated data for faster reads
           - SSD for hot data, HDD for archives

        3. Complexity vs Scalability:
           - Microservices for independent scaling
           - Increased operational complexity

        4. Cost vs Performance:
           - Premium instances for real-time services
           - Cost optimization for batch processing
      resources:
        - title: "WhatsApp System Design Analysis"
          url: "https://www.educative.io/courses/grokking-system-design-fundamentals/whatsapp"
          description: "Comprehensive analysis of WhatsApp's architecture and scaling strategies"
        - title: "Discord Engineering Blog"
          url: "https://discord.com/category/engineering"
          description: "Real-world insights into Discord's chat system architecture"
        - title: "Slack Engineering on Scaling"
          url: "https://slack.engineering/"
          description: "Slack's approach to building and scaling real-time messaging"
        - title: "Apache Kafka for Messaging"
          url: "https://kafka.apache.org/documentation/"
          description: "Kafka documentation for implementing message queuing"
        - title: "WebSocket Best Practices"
          url: "https://www.ably.io/topic/websockets"
          description: "Best practices for WebSocket implementation and scaling"
        - title: "Signal Protocol Documentation"
          url: "https://signal.org/docs/"
          description: "End-to-end encryption implementation for secure messaging"
      practice_questions:
        estimation:
          - question: "Chat system needs to handle 1B messages/day. Calculate required Kafka partitions and consumer groups."
            answer: "Messages: 1B/day = 11.6K msg/sec avg, 58K peak (5x). Kafka throughput: 1KB/msg × 58K = 58MB/sec. Partitions: 60 partitions (each handles 1K msg/sec), enables 60 parallel consumers. Consumer groups: 3 groups (persistence, notification, analytics) × 60 consumers = 180 total. Replication factor 3 = 174MB/sec write. Storage: 1B × 1KB × 30 days = 30TB. Cost: $3K/month for 10-node Kafka cluster."
          - question: "Real-time chat with 100M concurrent users. Estimate WebSocket server requirements and connection distribution."
            answer: "Connections: 100M users. Per server: 10K connections (memory limit). Servers needed: 100M / 10K = 10,000 WebSocket servers. Each server: 8GB RAM (80KB/connection) + 2 vCPU. Distribution: Use consistent hashing by user_id. Load balancer: 100 L7 load balancers (1M connections each). Bandwidth: 100B/msg × 10 msg/min × 100M = 1.67GB/sec = 13.3Gbps. Cost: $300K/month for infrastructure."
          - question: "Group chat with 10K members needs message delivery. Compare fan-out strategies and calculate costs."
            answer: "Write fan-out: Write 10K copies per message to user inboxes. Cost: 10K writes × $0.001 = $10 per message. Read latency: 10ms (direct read). Read fan-out: Write 1 copy to group feed. Cost: 1 write × $0.001 = $0.001 per message. Read: Each user reads from group feed. Read latency: 50ms (need to filter). Hybrid: Write fan-out for <100 members, read fan-out for >100. Balance: Cost vs latency. For 10K members: Use read fan-out (1000x cheaper), cache recent messages."
          - question: "Global chat application across 5 regions. Design network architecture and estimate cross-region latency."
            answer: "Regions: US-East, US-West, EU, Asia, Brazil. Architecture: Active-active, each region has full stack (WebSocket, Kafka, DB). Cross-region: Message replication via Kafka MirrorMaker 2.0. Latency: US-East to EU = 80ms, US-East to Asia = 200ms, Asia to Brazil = 250ms. Strategy: Send to nearest region (5ms), async replicate globally. User sees message in 5ms, other regions in 50-250ms. Cost: 5 regions × $100K/month = $500K. Bandwidth: 100GB/day cross-region = $3K/month."
        concepts:
          - question: "How do you ensure message ordering in a distributed chat system?"
            answer: "Use Kafka partitioning by conversation_id: All messages in same conversation go to same partition = total order. Within partition: Kafka guarantees order. Problem: Multiple consumers. Solution: Assign partition to single consumer. For 1:1 chat: Use hash(min(user1_id, user2_id)) for consistent partition. For group: Use group_id. Alternative: Lamport timestamps + vector clocks for causal ordering. Cost: Single consumer per partition = 60 consumers for 60 partitions."
          - question: "What are the trade-offs between push and pull models for group chat?"
            answer: "Push (WebSocket): Pros: Real-time (5ms), no polling. Cons: Stateful connections (10K/server), expensive at scale ($300K/month). Use for: 1:1 chat, small groups (<100). Pull (HTTP polling): Pros: Stateless, cheaper ($30K/month). Cons: Latency (30s-1min), higher client battery usage. Use for: Large groups (>1000), notification channels. Hybrid: Push for active users, pull for inactive. Switch at 5min idle. Result: 80% push, 20% pull = $250K/month."
          - question: "How do you implement presence detection with high accuracy and low latency?"
            answer: "Heartbeat: Client sends heartbeat every 30s via WebSocket. Server: Redis TTL = 60s (2× heartbeat). If TTL expires = offline. Pub/Sub: Broadcast presence changes to friends. Latency: <100ms. Scalability: 100M users × 1 heartbeat/30s = 3.3M/sec. Redis cluster: 10 nodes (each 300K ops/sec). Cost: $5K/month. Optimization: Batch updates every 5s, reduce Redis writes by 10x. Alternative: Last-seen timestamp (eventual consistency, cheaper)."
          - question: "What strategies ensure reliable message delivery in mobile networks?"
            answer: "Use at-least-once delivery: 1) Client sends message with UUID, 2) Server ACKs with msg_id, 3) Client retries if no ACK (exponential backoff: 1s, 2s, 4s), 4) Server deduplicates by UUID. Offline: Queue messages in local SQLite. When online: Sync with server. Network switch: Use TCP keepalive + WebSocket ping/pong (30s). Connection lost: Reconnect with last_seen_msg_id, server sends missed messages. Cost: 5% retry rate × 1B msg/day = 50M retries/day."
          - question: "How do you handle WebSocket connection failover and session management?"
            answer: "Stateless servers: Store session in Redis (user_id, server_id, last_msg_id). On disconnect: Client reconnects to any server via load balancer. New server: Reads session from Redis, resumes from last_msg_id. Failover time: <5s. Kafka consumer rebalancing: 10s. Total: 15s downtime. Health check: Load balancer pings every 10s. If server fails: Remove from pool in 30s. Session TTL: 24h. Cost: Redis cluster $3K/month for 100M sessions."
          - question: "What are the security considerations for end-to-end encrypted messaging?"
            answer: "Use Signal Protocol: Double Ratchet (forward secrecy + break-in recovery). Key exchange: X3DH (Extended Triple Diffie-Hellman). Server: Never sees plaintext, only encrypted messages. Metadata: Server knows who talks to whom, not content. Multi-device: Each device has own identity key, messages encrypted per device. Group chat: Sender Key protocol (MLS for >100 members). Backup: Encrypted backup with user passphrase (PBKDF2, 100K rounds). Cost: 10ms encryption overhead per message."
        tradeoffs:
          - question: "WebSocket vs HTTP long polling for real-time messaging"
            answer: "WebSocket: Pros: Real-time (5ms), bidirectional, less overhead (no HTTP headers). Cons: Stateful (10K connections/server), harder to load balance, firewall issues. Cost: $300K/month for 100M users. Long polling: Pros: Stateless, works everywhere, simpler. Cons: Higher latency (500ms-1s), more bandwidth (HTTP headers), server load. Cost: $100K/month. Hybrid: WebSocket for active users (80%), long polling for fallback (20%) = $250K/month."
          - question: "Microservices vs monolith for chat system architecture"
            answer: "Microservices: Services: Gateway, Chat, User, Group, Notification, Media, Presence. Pros: Independent scaling (chat 100x, user 10x), team autonomy. Cons: Complex (7 services), inter-service latency (10-50ms), distributed tracing needed. Cost: $500K/month (10 services × $50K). Monolith: Pros: Simple, low latency (in-memory), easier debugging. Cons: Single scaling unit, deployment risk. Cost: $200K/month. Start: Monolith. Scale: Microservices at 10M users."
          - question: "SQL vs NoSQL for message storage and retrieval"
            answer: "SQL (PostgreSQL): Pros: ACID, complex queries (search), indexing. Cons: Scaling (sharding complex), 10K TPS/shard. Use for: Audit logs, analytics. NoSQL (Cassandra): Pros: Horizontal scaling (1M TPS), partition by conversation_id. Cons: No joins, eventual consistency. Use for: Message history. Hybrid: Write to Cassandra (low latency), async sync to PostgreSQL (analytics). Storage: 1B msg/day × 1KB × 365 = 365TB. Cassandra: $10K/month, PostgreSQL: $30K/month."
          - question: "Push vs pull model for large group message distribution"
            answer: "Push (fan-out on write): Write to all 10K member inboxes. Pros: Fast reads (10ms). Cons: Slow writes (10s), expensive ($10/message). Use for: <100 members. Pull (fan-out on read): Write once to group feed. Pros: Fast writes (10ms), cheap ($0.001/message). Cons: Slow reads (50ms), need filtering. Use for: >1000 members. Hybrid: <100 = push, 100-1000 = push to online + pull for offline, >1000 = pull. Result: 95% of groups use push, but 95% of messages use pull (due to large groups)."
          - question: "Strong vs eventual consistency for message ordering"
            answer: "Strong consistency: Use Kafka single partition per conversation. Pros: Total order guaranteed. Cons: Single consumer bottleneck (10K msg/sec max), higher latency (50ms). Use for: Banking, legal chat. Cost: 60 partitions = 60 parallel conversations max. Eventual consistency: Multiple partitions, Lamport timestamps. Pros: Higher throughput (100K msg/sec), lower latency (10ms). Cons: Out-of-order delivery (1%), need client-side reordering. Use for: Social chat. 99% of apps choose eventual (fast > perfect order)."
          - question: "Horizontal vs vertical scaling for WebSocket connections"
            answer: "Horizontal: Add more servers (10K connections each). Pros: Linear scaling (add server = +10K users), fault tolerance. Cons: Complex load balancing, session affinity. Cost: $30/server/month × 10K servers = $300K. Vertical: Bigger servers (100K connections each). Pros: Simpler, less overhead. Cons: Hardware limits (128GB RAM max = 100K connections), single point of failure. Cost: $3K/server/month × 1K servers = $3M. Choose: Horizontal (10x cheaper, better reliability)."
        scenarios:
          - question: "Design message delivery guarantees for a banking chat application"
            answer: "Requirements: No message loss, total ordering, audit trail. Architecture: 1) Client sends with idempotency key (UUID), 2) Kafka with acks=all (all replicas confirm), 3) Store in PostgreSQL with transaction, 4) ACK to client. Ordering: Single Kafka partition per conversation. Audit: Immutable append-only log with signatures. Encryption: TLS 1.3 + E2E encryption. Delivery: At-least-once (retry until ACK). Latency: 100ms (vs 10ms social chat). Compliance: GDPR (right to delete = soft delete with hash). Cost: $50K/month for 1M messages/day."
          - question: "Handle a viral message in a group with 1M members efficiently"
            answer: "Problem: Fan-out to 1M users = $1K per message (write fan-out). Solution: Use read fan-out + caching. 1) Write message once to group feed, 2) CDN cache message for 1h (99% hit rate), 3) Users pull from CDN (1ms), 4) Push notification to online users (100K) = $100. Total cost: $100 vs $1K (10x savings). Scalability: CDN handles 1M concurrent reads. Alternative: Tree-based propagation (each user forwards to 10 peers) = decentralized, harder to implement."
          - question: "Implement offline message synchronization when user comes back online"
            answer: "Client stores last_seen_msg_id in local SQLite. On reconnect: 1) Send last_seen_msg_id to server, 2) Server queries messages WHERE id > last_seen_msg_id ORDER BY id LIMIT 1000, 3) Send batch to client, 4) Client ACKs, requests next batch. Pagination: 1000 messages/batch. For 10K missed: 10 batches × 100ms = 1s. Optimization: Compress batch (gzip 5x) = 200KB/batch. If >7 days offline: Reset sync (download all conversations). Cost: 1% of users offline >1 day = 1M users × 10KB = 10GB sync/day."
          - question: "Design cross-platform real-time typing indicators"
            answer: "Protocol: 1) User types → send typing event every 3s (debounce), 2) Server broadcasts to conversation members via WebSocket, 3) Display 'User is typing...' for 5s (timeout). Scalability: 100K typing events/sec. Don't persist: In-memory only (Redis Pub/Sub). Multi-device: Include device_id in event. Mobile optimization: Only send if >3 chars typed (reduce events 80%). Cost: Ephemeral (no storage), but uses WebSocket bandwidth. Alternative: Coalesce typing events (batch updates every 1s) = reduce load 3x."
          - question: "Handle message encryption/decryption in a multi-device environment"
            answer: "Use Signal Protocol with multi-device support: 1) Each device has identity key pair, 2) Sender encrypts message per device (N encryptions for N devices), 3) Store encrypted copies in server, 4) Each device decrypts with own key. Key management: X3DH for initial key exchange, Double Ratchet for forward secrecy. New device: Uses QR code + verify existing device. Removed device: Rotate group keys. Cost: N× encryption overhead (10ms × 3 devices = 30ms). Storage: 3× messages (one per device). Alternative: Use envelope encryption (encrypt message once with data key, encrypt data key per device) = faster."
          - question: "Implement message search across billions of encrypted messages"
            answer: "Problem: Server can't search encrypted content. Solutions: 1) Client-side search: Download all messages to device, index locally (SQLite FTS5). Pros: Privacy. Cons: Slow, limited to device storage. 2) Encrypted search (searchable encryption): Client encrypts search keywords with deterministic encryption, server searches encrypted index. Pros: Fast (100ms). Cons: Keyword leakage to server. 3) Hybrid: Store message hashes for exact match, client-side for full-text. Cost: 1B messages × 100B/hash = 100GB index. Use: Elasticsearch with encrypted fields. 99% apps: Client-side search only (privacy first)."
          - question: "Design disaster recovery for chat system during datacenter failure"
            answer: "Multi-region active-active: 3 regions (US, EU, Asia). Each region: Full stack (WebSocket, Kafka, Cassandra). Replication: Kafka MirrorMaker 2.0 (async, <1s lag), Cassandra multi-DC (quorum writes). Failure: Region failure → DNS/load balancer redirects to nearest region in 30s. Users: Reconnect to new region, resume from last_msg_id. Data loss: Max 1s of messages (async replication lag). RTO: 30s, RPO: 1s. Cost: 3 regions × $200K = $600K/month (2× single region for 3× availability). Alternative: Active-passive (cheaper, 5min RTO)."
          - question: "Handle gradual deployment of new chat features without downtime"
            answer: "Use feature flags + canary deployment: 1) Deploy new version to 1% of servers, 2) Monitor metrics (latency, error rate) for 1h, 3) If stable: Increase to 10%, 50%, 100% over 24h. Feature flags: Enable new feature for 1% of users (A/B test). Backward compatibility: Old clients must work with new servers (version negotiation). Database migrations: Dual-write pattern (write to old + new schema), migrate background, then switch reads. Rollback: Instant (disable feature flag). Cost: No downtime, 10% extra capacity during deployment. Alternative: Blue-green deployment (requires 2× infrastructure)."
      time_estimate: 150

  week3:
    - day: 15
      topic: "Advanced Database Sharding"
      activity: "Master advanced sharding architectures, resharding strategies, cross-shard operations, and production-grade sharding solutions for massive scale."
      detailed_content: |
        Advanced Sharding Architectures:

        1. Sharding Strategies Deep Dive:
        - Hash-based Sharding:
          * Consistent hashing with virtual nodes
          * Handling hash collisions and distribution
          * Pros: Simple, good distribution
          * Cons: Hard to range queries, resharding complexity
        - Range-based Sharding:
          * Ordered data distribution across shards
          * Hot spot management for sequential keys
          * Pros: Range queries, ordered scans
          * Cons: Uneven distribution, hot shards
        - Directory-based Sharding:
          * Centralized shard mapping service
          * Dynamic shard assignment and routing
          * Pros: Flexible, easy resharding
          * Cons: Single point of failure, lookup overhead
        - Hybrid Approaches:
          * Combining multiple strategies
          * Geographic + hash-based sharding
          * Time-based + range sharding

        2. Shard Key Design and Evolution:
        - Shard Key Selection Criteria:
          * High cardinality for even distribution
          * Query pattern alignment
          * Avoiding hot spots and monotonic keys
          * Immutability considerations
        - Composite Shard Keys:
          * Multi-column sharding strategies
          * Hierarchical key structures
          * Balancing distribution vs query efficiency
        - Shard Key Migration:
          * Gradual key transformation
          * Backward compatibility during migration
          * Dual-key support periods

        3. Hot Shard Detection and Mitigation:
        - Detection Mechanisms:
          * Real-time metrics monitoring
          * Query pattern analysis
          * Resource utilization tracking
          * Automated alerting systems
        - Mitigation Strategies:
          * Shard splitting techniques
          * Read replica creation
          * Data redistribution algorithms
          * Application-level load balancing
        - Prevention Techniques:
          * Better shard key design
          * Pre-splitting strategies
          * Randomization techniques
          * Temporal data distribution

        4. Resharding and Data Migration:
        - Live Migration Strategies:
          * Online resharding without downtime
          * Incremental data movement
          * Consistency during migration
          * Rollback mechanisms
        - Double-write Pattern:
          * Writing to both old and new shards
          * Synchronization challenges
          * Conflict resolution
          * Performance implications
        - Pre-splitting Techniques:
          * Creating more shards than needed
          * Virtual shard management
          * Elastic shard allocation
          * Capacity planning integration

        5. Cross-shard Operations:
        - Distributed Transactions:
          * Two-phase commit (2PC) protocol
          * Three-phase commit improvements
          * Saga pattern for long transactions
          * Compensation-based transactions
        - Cross-shard Queries:
          * Scatter-gather query execution
          * Query planning and optimization
          * Result aggregation strategies
          * Performance considerations
        - Global Secondary Indexes:
          * Cross-shard index maintenance
          * Consistency guarantees
          * Update propagation
          * Query routing optimization

        6. Consistency and ACID Properties:
        - Shard-level ACID:
          * Maintaining ACID within shards
          * Cross-shard consistency challenges
          * Eventual consistency models
          * Conflict resolution strategies
        - Distributed Locking:
          * Cross-shard lock management
          * Deadlock detection and resolution
          * Lock timeout handling
          * Performance optimization
        - Consensus Protocols:
          * Raft for shard leadership
          * Paxos for distributed decisions
          * Byzantine fault tolerance
          * Leader election mechanisms

        Real-world Implementation Examples:

        1. YouTube Vitess:
        - MySQL sharding at massive scale
        - Transparent query routing
        - Online schema migrations
        - Automatic failover and recovery
        - Horizontal and vertical sharding

        2. Pinterest Sharding:
        - UUID-based shard keys
        - Consistent hashing implementation
        - Hot shard detection system
        - Gradual migration strategies
        - Cross-shard analytics challenges

        3. Instagram Sharding:
        - Photo and user data sharding
        - Geographic distribution strategies
        - Timeline generation across shards
        - Follower graph partitioning
        - Real-time notification challenges

        4. Discord Sharding:
        - Message history partitioning
        - Guild-based sharding strategies
        - Real-time message routing
        - Voice chat data distribution
        - Cross-guild operations

        Monitoring and Operations:

        1. Performance Metrics:
        - Query latency per shard
        - Throughput distribution
        - Resource utilization tracking
        - Hot spot identification
        - Connection pool monitoring

        2. Data Distribution Analysis:
        - Shard size monitoring
        - Key distribution analysis
        - Growth pattern tracking
        - Rebalancing triggers
        - Capacity planning metrics

        3. Operational Tools:
        - Automated resharding systems
        - Migration progress tracking
        - Health check frameworks
        - Alerting and notification
        - Performance optimization tools

        Advanced Patterns and Techniques:

        1. Federated Queries:
        - Cross-database querying
        - Query federation engines
        - Performance optimization
        - Result caching strategies
        - Security and access control

        2. Materialized Views:
        - Cross-shard view maintenance
        - Incremental view updates
        - Consistency guarantees
        - Query acceleration
        - Storage optimization

        3. Event Sourcing with Sharding:
        - Event stream partitioning
        - Aggregate root boundaries
        - Event replay across shards
        - Snapshot management
        - Temporal querying

        4. CQRS with Sharding:
        - Command and query separation
        - Write model sharding
        - Read model denormalization
        - Projection management
        - Eventual consistency handling

        Production Considerations:

        1. Deployment Strategies:
        - Blue-green deployments
        - Canary releases
        - Rolling updates
        - Feature flags integration
        - Rollback procedures

        2. Disaster Recovery:
        - Cross-region replication
        - Backup and restore strategies
        - Point-in-time recovery
        - Failover procedures
        - Data integrity verification

        3. Security Considerations:
        - Shard-level access control
        - Encryption at rest and transit
        - Audit logging
        - Compliance requirements
        - Attack vector mitigation

        4. Cost Optimization:
        - Resource allocation strategies
        - Auto-scaling policies
        - Storage optimization
        - Network cost management
        - Performance vs cost trade-offs

        Practice Questions:

        Capacity Estimation:
        1. "Design sharding for a social media platform with 1B users, 100B posts. Calculate optimal shard count, considering 10TB per shard limit and 50% growth yearly."
        2. "Estimate resharding timeline for migrating 500TB database from 50 to 500 shards with 100MB/s network, including downtime windows."
        3. "Calculate cross-shard query performance impact: 1000 QPS queries hitting average 5 shards with 10ms per-shard latency plus 2ms aggregation overhead."

        Conceptual Understanding:
        1. "Compare consistent hashing vs range-based sharding for time-series data. Analyze query patterns, hot spots, and resharding complexity."
        2. "Explain trade-offs between shard-level ACID vs cross-shard eventual consistency. When would you choose each approach?"
        3. "Design global secondary indexes for sharded e-commerce inventory. Handle real-time updates, query routing, and consistency challenges."

        Trade-off Analysis:
        1. "Analyze pre-splitting strategy: 10x more shards than needed vs just-in-time splitting. Consider resource overhead, complexity, and performance."
        2. "Compare federation vs materialized views for cross-shard analytics. Evaluate latency, consistency, storage, and maintenance costs."
        3. "Design shard key evolution strategy balancing query performance, distribution, and migration complexity for user behavior analytics."

        Scenario-based Design:
        1. "Design resharding strategy for chat application during 10x user growth spike. Handle active conversations, message history, and real-time delivery."
        2. "Handle celebrity user causing hot shard in social media platform. Design immediate mitigation, long-term prevention, and monitoring systems."
        3. "Implement cross-shard transaction for transferring money between users in different shards. Ensure ACID properties and handle failures."
        4. "Design sharding strategy for IoT sensor data: 1M devices, 1 datapoint/second each. Optimize for time-range queries and real-time aggregations."

      resources:
        - title: "Vitess: Production MySQL Sharding"
          url: "https://vitess.io/docs/overview/"
          description: "Complete sharding framework by YouTube"
        - title: "Sharding Pinterest: Lessons from Scale"
          url: "https://medium.com/@Pinterest_Engineering/sharding-pinterest-how-we-scaled-our-mysql-fleet-3f341e96ca6f"
          description: "Real-world sharding implementation case study"
        - title: "Instagram Sharding Architecture"
          url: "https://instagram-engineering.com/sharding-ids-at-instagram-1cf5a71e5a5c"
          description: "ID sharding and distributed systems design"
        - title: "Discord: Storing Billions of Messages"
          url: "https://discord.com/blog/how-discord-stores-billions-of-messages"
          description: "Message sharding and Cassandra at scale"
        - title: "Consistent Hashing in Practice"
          url: "https://medium.com/@sent0hil/consistent-hashing-a-guide-go-implementation-fe3421ac3e8f"
          description: "Implementation details and best practices"
        - title: "Two-Phase Commit Protocol"
          url: "https://en.wikipedia.org/wiki/Two-phase_commit_protocol"
          description: "Distributed transaction coordination"
        - title: "Saga Pattern for Distributed Transactions"
          url: "https://microservices.io/patterns/data/saga.html"
          description: "Alternative to distributed transactions"
        - title: "Spanner: Google's Globally Distributed Database"
          url: "https://research.google/pubs/pub39966/"
          description: "Advanced sharding with global consistency"
      time_estimate: 120

    - day: 16
      topic: "Content Delivery Networks (CDN)"
      activity: "Master CDN architecture, global content distribution, edge computing, and performance optimization for modern web applications at scale."
      detailed_content: |
        CDN Architecture Fundamentals:

        1. Network Topology and Infrastructure:
        - Edge Locations (POPs):
          * Geographic distribution strategies
          * Proximity-based routing algorithms
          * Edge server capacity planning
          * Network interconnection and peering
        - Origin Servers:
          * Primary and secondary origins
          * Origin shielding strategies
          * Load balancing between origins
          * Failover and disaster recovery
        - Hierarchical Cache Structure:
          * Edge → Regional → Origin hierarchy
          * Multi-tier caching strategies
          * Cache miss handling and forwarding
          * Bandwidth optimization between tiers

        2. Routing and Request Direction:
        - DNS-based Routing:
          * GeoDNS for geographic routing
          * Weighted round-robin algorithms
          * Health check integration
          * Latency-based routing decisions
        - Anycast Routing:
          * IP anycast implementation
          * BGP route optimization
          * Network topology considerations
          * Failover and traffic shifting
        - HTTP Redirects:
          * 302 redirect mechanisms
          * Location header optimization
          * Client-side routing logic
          * Performance implications

        3. Advanced Caching Strategies:
        - Cache Hierarchies:
          * L1 (Edge) → L2 (Regional) → L3 (Origin)
          * Cache replacement algorithms (LRU, LFU, FIFO)
          * Cache warming and pre-population
          * Hit ratio optimization strategies
        - Content Classification:
          * Static Assets: Images, CSS, JS, fonts
          * Semi-dynamic: User-generated content, thumbnails
          * Dynamic Content: API responses, personalized data
          * Streaming Media: Video chunks, live streams
        - Cache Key Design:
          * URL normalization strategies
          * Query parameter handling
          * Header-based cache variations
          * User-specific cache segmentation

        4. Cache Invalidation and Purging:
        - Time-based Expiration:
          * Cache-Control and Expires headers
          * TTL strategy optimization
          * Stale-while-revalidate patterns
          * Grace period handling
        - Event-driven Invalidation:
          * Tag-based purging systems
          * Real-time invalidation APIs
          * Batch invalidation operations
          * Selective cache clearing
        - Version-based Strategies:
          * Cache busting with versioned URLs
          * Immutable content patterns
          * Deployment-triggered invalidation
          * Rollback considerations

        5. Content Optimization Techniques:
        - Compression and Encoding:
          * Gzip vs Brotli compression algorithms
          * Real-time compression at edge
          * Pre-compressed content delivery
          * Compression ratio optimization
        - Image and Media Optimization:
          * Format selection (WebP, AVIF, JPEG XL)
          * Responsive image serving
          * Real-time image resizing
          * Progressive loading strategies
        - HTTP Protocol Optimization:
          * HTTP/2 multiplexing and server push
          * HTTP/3 QUIC protocol benefits
          * Connection reuse and pooling
          * Header compression techniques

        Edge Computing and Processing:

        1. Edge Functions and Serverless:
        - Edge Workers/Functions:
          * JavaScript execution at edge locations
          * Request/response manipulation
          * A/B testing and feature flags
          * Security and authentication
        - Edge-side Includes (ESI):
          * Fragment-based caching
          * Dynamic content assembly
          * Personalization at edge
          * Template processing
        - WebAssembly at Edge:
          * High-performance edge computing
          * Custom logic execution
          * Language-agnostic processing
          * Security sandboxing

        2. Real-time Processing:
        - Stream Processing:
          * Live video transcoding
          * Real-time analytics
          * Log processing and aggregation
          * Event stream manipulation
        - Edge Analytics:
          * Real-time metrics collection
          * User behavior tracking
          * Performance monitoring
          * Security threat detection

        Security and Protection:

        1. DDoS Protection:
        - Traffic Analysis:
          * Baseline traffic pattern establishment
          * Anomaly detection algorithms
          * Attack signature recognition
          * Rate limiting mechanisms
        - Mitigation Strategies:
          * Traffic scrubbing and filtering
          * Distributed defense across POPs
          * Capacity scaling during attacks
          * Origin protection measures

        2. Web Application Firewall (WAF):
        - Rule-based Filtering:
          * OWASP Top 10 protection
          * Custom security rules
          * Geoblocking and IP filtering
          * Bot detection and mitigation
        - Advanced Security:
          * SSL/TLS termination and optimization
          * Certificate management
          * HTTP security headers
          * Content Security Policy enforcement

        Real-world Implementation Examples:

        1. Netflix CDN Strategy:
        - Open Connect Appliances:
          * ISP-embedded cache servers
          * Content pre-positioning
          * Adaptive bitrate streaming
          * Global content distribution
        - Content Delivery Optimization:
          * Predictive caching algorithms
          * Regional content libraries
          * Peak-hour traffic management
          * Quality of experience optimization

        2. YouTube Global Distribution:
        - Video Content Delivery:
          * Multi-resolution video caching
          * Adaptive streaming protocols
          * Geographic content preferences
          * Live streaming optimization
        - Edge Storage Strategy:
          * Popular content pre-caching
          * Long-tail content optimization
          * Storage cost optimization
          * Bandwidth utilization patterns

        3. Shopify Commerce CDN:
        - E-commerce Optimization:
          * Product image delivery
          * Dynamic inventory updates
          * Checkout process acceleration
          * Mobile-first optimization
        - Global Scaling:
          * Multi-region deployment
          * Currency and language adaptation
          * Compliance and data residency
          * Performance monitoring

        4. Cloudflare Network:
        - Global Anycast Network:
          * 200+ city presence
          * Edge computing platform
          * Security service integration
          * Developer-friendly APIs
        - Innovation Features:
          * Workers distributed computing
          * Stream video platform
          * R2 object storage
          * Zero Trust security

        Performance Monitoring and Analytics:

        1. Core Metrics:
        - Cache Performance:
          * Hit ratio by content type
          * Origin offload percentage
          * Average response times
          * Bandwidth savings analysis
        - User Experience:
          * Time to First Byte (TTFB)
          * Page load time distributions
          * Geographic performance variations
          * Mobile vs desktop performance

        2. Advanced Analytics:
        - Real-time Monitoring:
          * Traffic pattern analysis
          * Error rate tracking
          * Security threat detection
          * Capacity utilization monitoring
        - Business Intelligence:
          * Cost optimization insights
          * User behavior analysis
          * Content popularity trends
          * Revenue impact measurement

        Cost Optimization and Business Considerations:

        1. Pricing Models:
        - Bandwidth-based Pricing:
          * Data transfer cost optimization
          * Peak vs off-peak pricing
          * Regional pricing variations
          * Commitment-based discounts
        - Request-based Pricing:
          * Cost per request analysis
          * Caching impact on costs
          * Edge function execution costs
          * Security service pricing

        2. ROI and Business Impact:
        - Performance Benefits:
          * Conversion rate improvements
          * User engagement metrics
          * SEO ranking benefits
          * Customer satisfaction scores
        - Operational Savings:
          * Origin server load reduction
          * Bandwidth cost savings
          * Infrastructure scaling benefits
          * Operational complexity reduction

        Practice Questions:

        Capacity Estimation:
        1. "Design CDN for global video streaming service: 100M users, 2 hours average watch time, 1080p average quality. Calculate edge storage, bandwidth requirements, and optimal POP placement."
        2. "Estimate cache hit ratio improvement for e-commerce site: 1M products, 100K daily users, 20% cache hit ratio currently. Design caching strategy to achieve 85% hit ratio."
        3. "Calculate CDN cost optimization for news website: 50TB monthly traffic, 80% cacheable content, $0.05/GB bandwidth cost. Compare origin vs CDN delivery costs."

        Conceptual Understanding:
        1. "Compare pull vs push CDN models for different content types. Analyze latency, storage costs, and complexity trade-offs."
        2. "Explain edge computing vs traditional CDN caching. When would you use edge functions vs simple content caching?"
        3. "Design cache invalidation strategy for social media platform. Handle user posts, profile updates, and real-time content feeds."

        Trade-off Analysis:
        1. "Analyze geographic CDN placement strategy: Many small POPs vs fewer large regional centers. Consider costs, performance, and maintenance."
        2. "Compare CDN providers: CloudFlare vs AWS CloudFront vs Fastly. Evaluate features, pricing, performance, and integration capabilities."
        3. "Design caching strategy balancing freshness vs performance for stock trading platform with real-time price updates."

        Scenario-based Design:
        1. "Handle viral content surge: News article getting 100x normal traffic in 1 hour. Design cache warming, capacity scaling, and origin protection."
        2. "Design CDN for live sports streaming: 10M concurrent viewers, multiple camera angles, global audience. Handle peak traffic and low latency requirements."
        3. "Implement CDN for mobile app with offline capabilities: Image caching, delta updates, and background synchronization across global user base."
        4. "Design secure CDN for banking application: Sensitive document delivery, compliance requirements, and DDoS protection."

      resources:
        - title: "Cloudflare: How CDNs Work"
          url: "https://www.cloudflare.com/learning/cdn/what-is-a-cdn/"
          description: "Comprehensive CDN concepts and implementation"
        - title: "AWS CloudFront Developer Guide"
          url: "https://docs.aws.amazon.com/cloudfront/"
          description: "Production CDN service implementation"
        - title: "Fastly Edge Computing Platform"
          url: "https://docs.fastly.com/"
          description: "Edge computing and real-time processing"
        - title: "Netflix Open Connect"
          url: "https://openconnect.netflix.com/en/"
          description: "Global content delivery infrastructure"
        - title: "Akamai Edge Computing"
          url: "https://www.akamai.com/resources"
          description: "Enterprise CDN and edge services"
        - title: "HTTP Caching Best Practices"
          url: "https://developers.google.com/web/fundamentals/performance/optimizing-content-efficiency/http-caching"
          description: "Web performance optimization techniques"
        - title: "Edge-side Includes (ESI) Specification"
          url: "https://www.w3.org/TR/esi-lang"
          description: "Dynamic content assembly at edge"
        - title: "WebAssembly at the Edge"
          url: "https://webassembly.org/"
          description: "High-performance edge computing platform"
      time_estimate: 120

    - day: 17
      topic: "Search Systems & Information Retrieval"
      activity: "Master search engine architecture, advanced indexing strategies, ranking algorithms, and relevance optimization for large-scale information retrieval systems."
      detailed_content: |
        Search System Architecture:

        1. High-level Search Pipeline:
        - Document Ingestion:
          * Web crawling and discovery
          * Content extraction and parsing
          * Document classification and filtering
          * Quality assessment and spam detection
        - Index Construction:
          * Text processing and normalization
          * Tokenization and linguistic analysis
          * Inverted index building
          * Distributed index sharding
        - Query Processing:
          * Query parsing and analysis
          * Intent classification and understanding
          * Query rewriting and expansion
          * Result ranking and presentation

        2. Crawling and Data Acquisition:
        - Web Crawling Strategies:
          * Breadth-first vs depth-first crawling
          * Politeness policies and rate limiting
          * Duplicate detection and deduplication
          * Incremental crawling for fresh content
        - Content Discovery:
          * Sitemap and robots.txt processing
          * Link graph analysis
          * Social media and API integration
          * User-submitted content handling
        - Quality Control:
          * Content quality assessment
          * Spam and malicious content filtering
          * Language detection and processing
          * Copyright and legal compliance

        Advanced Indexing Strategies:

        1. Inverted Index Design:
        - Index Structure:
          * Term dictionary with posting lists
          * Document frequency and term positions
          * Index compression techniques
          * Delta encoding and variable-byte encoding
        - Advanced Features:
          * Phrase indexing for exact match queries
          * Proximity indexing for near-match queries
          * Field-specific indexing (title, body, metadata)
          * Multi-language indexing and analysis
        - Distribution Strategies:
          * Document-based partitioning
          * Term-based partitioning
          * Hybrid partitioning approaches
          * Replication for fault tolerance

        2. Real-time Indexing:
        - Incremental Updates:
          * Log-structured merge trees
          * Delta indexing strategies
          * Memory-mapped index structures
          * Background merging and compaction
        - Near Real-time Search:
          * Buffer-based indexing
          * Commit and refresh strategies
          * Index warming techniques
          * Consistency guarantees

        3. Specialized Indexes:
        - Geospatial Indexing:
          * R-trees and spatial partitioning
          * Geohash and spatial grid systems
          * Location-based search optimization
          * Distance and proximity calculations
        - Temporal Indexing:
          * Time-series data indexing
          * Time-range queries optimization
          * Historical data archiving
          * Temporal relevance scoring
        - Multimedia Indexing:
          * Image feature extraction and indexing
          * Video content analysis
          * Audio fingerprinting
          * Cross-modal search capabilities

        Ranking and Relevance Algorithms:

        1. Classical Ranking Models:
        - TF-IDF (Term Frequency-Inverse Document Frequency):
          * Term frequency normalization strategies
          * Document frequency calculations
          * Variants and improvements
          * Implementation optimizations
        - BM25 and Okapi BM25:
          * Parameter tuning (k1, b, k3)
          * Document length normalization
          * Field-specific BM25 variants
          * Performance characteristics
        - Vector Space Models:
          * Cosine similarity calculations
          * Term weighting schemes
          * Dimensionality reduction techniques
          * Latent semantic analysis

        2. Link-based Algorithms:
        - PageRank:
          * Random walk model
          * Damping factor and convergence
          * Personalized PageRank
          * Computation and storage optimizations
        - HITS (Hyperlink-Induced Topic Search):
          * Authority and hub scores
          * Topic-specific HITS
          * Web graph analysis
          * Link spam detection
        - Modern Link Analysis:
          * TrustRank for spam detection
          * Social signals integration
          * Citation analysis for academic search
          * Temporal link dynamics

        3. Machine Learning for Ranking:
        - Learning to Rank (LTR):
          * Pointwise approach (classification/regression)
          * Pairwise approach (ranking SVM, RankNet)
          * Listwise approach (ListNet, AdaRank)
          * Feature engineering for ranking
        - Deep Learning Models:
          * Neural ranking models
          * BERT for search relevance
          * Transformer-based architectures
          * Embedding-based semantic matching
        - Personalization:
          * User behavior modeling
          * Collaborative filtering integration
          * Context-aware ranking
          * Privacy-preserving personalization

        Query Processing and Understanding:

        1. Query Analysis:
        - Query Parsing:
          * Tokenization and normalization
          * Phrase detection and handling
          * Boolean query processing
          * Wildcard and regex support
        - Intent Classification:
          * Navigational vs informational queries
          * Commercial intent detection
          * Entity recognition in queries
          * Ambiguity resolution
        - Query Expansion:
          * Synonym expansion techniques
          * Stemming and lemmatization
          * Thesaurus-based expansion
          * Automatic query suggestion

        2. Advanced Query Features:
        - Faceted Search:
          * Dynamic facet generation
          * Hierarchical faceting
          * Range and numerical facets
          * Facet value ordering and counting
        - Auto-complete and Suggestions:
          * Trie-based completion
          * Popular query completion
          * Personalized suggestions
          * Typo-tolerant completion
        - Spell Correction:
          * Edit distance algorithms
          * Phonetic matching techniques
          * Context-aware correction
          * Machine learning-based correction

        Performance Optimization:

        1. Query Performance:
        - Index Optimization:
          * Index warming strategies
          * Cache-friendly data structures
          * Parallel query processing
          * Query result caching
        - Distributed Query Processing:
          * Scatter-gather query execution
          * Query routing and load balancing
          * Result merging and ranking
          * Fault tolerance and retries

        2. Scalability Patterns:
        - Horizontal Scaling:
          * Shard management and routing
          * Cross-shard result aggregation
          * Load balancing strategies
          * Auto-scaling based on query load
        - Caching Layers:
          * Query result caching
          * Index segment caching
          * Distributed cache coordination
          * Cache invalidation strategies

        Real-world Implementation Examples:

        1. Google Search Architecture:
        - Web Scale Indexing:
          * MapReduce for distributed indexing
          * Bigtable for metadata storage
          * Distributed file system integration
          * Real-time index updates
        - Ranking Innovation:
          * PageRank and authority signals
          * RankBrain machine learning
          * BERT for query understanding
          * Knowledge graph integration

        2. Elasticsearch at Scale:
        - Netflix Search Infrastructure:
          * Multi-cluster deployment
          * Content recommendation search
          * Real-time analytics integration
          * Performance monitoring
        - GitHub Code Search:
          * Programming language analysis
          * Repository indexing strategies
          * Code structure understanding
          * Developer workflow integration

        3. Amazon Product Search:
        - E-commerce Optimization:
          * Product catalog indexing
          * Inventory-aware search results
          * Conversion-optimized ranking
          * Personalized product recommendations
        - Search Features:
          * Visual search capabilities
          * Voice search integration
          * Multi-language support
          * Mobile search optimization

        4. Slack Search System:
        - Workplace Search:
          * Message history indexing
          * Real-time message search
          * Channel and user filtering
          * File content search
        - Performance Requirements:
          * Sub-second response times
          * High availability guarantees
          * Privacy and security compliance
          * Cross-workspace search

        Search Quality and Evaluation:

        1. Relevance Metrics:
        - Traditional Metrics:
          * Precision and recall
          * F-measure and F1 score
          * Mean Average Precision (MAP)
          * Normalized Discounted Cumulative Gain (NDCG)
        - User-centric Metrics:
          * Click-through rates (CTR)
          * Dwell time and engagement
          * Session success rate
          * User satisfaction surveys

        2. A/B Testing and Experimentation:
        - Ranking Experiments:
          * Statistical significance testing
          * Interleaving experiments
          * Online evaluation frameworks
          * Long-term impact analysis
        - Quality Assurance:
          * Query-result pair evaluation
          * Human relevance judgments
          * Automated quality checks
          * Regression testing frameworks

        Specialized Search Domains:

        1. Enterprise Search:
        - Content Sources:
          * Document repositories
          * Email and collaboration tools
          * Database and CRM systems
          * Intranet and knowledge bases
        - Security and Access Control:
          * User permission integration
          * Content security classification
          * Audit trail and compliance
          * GDPR and privacy requirements

        2. Academic and Scientific Search:
        - Citation Analysis:
          * Paper ranking by citations
          * Author authority metrics
          * Journal impact factors
          * Cross-reference networks
        - Content Processing:
          * PDF and paper parsing
          * Formula and equation handling
          * Scientific notation processing
          * Multi-language academic content

        Practice Questions:

        Capacity Estimation:
        1. "Design search system for 1B web pages, 1M queries/second peak. Calculate index size, memory requirements, and server count for sub-100ms response time."
        2. "Estimate storage for e-commerce search: 100M products, 50 attributes each, 10 languages. Include inverted indexes, forward indexes, and caching layers."
        3. "Calculate ranking computation cost: BM25 scoring for 10K candidate documents, 50-term queries, considering CPU and memory constraints."

        Conceptual Understanding:
        1. "Compare inverted index vs forward index structures. Analyze storage efficiency, query performance, and update complexity for different use cases."
        2. "Explain trade-offs between real-time indexing vs batch indexing. Consider consistency, performance, and resource utilization."
        3. "Design ranking algorithm combining TF-IDF, PageRank, and user behavior signals. Handle signal normalization and weight optimization."

        Trade-off Analysis:
        1. "Analyze document-based vs term-based index partitioning for distributed search. Consider query latency, load balancing, and fault tolerance."
        2. "Compare Elasticsearch vs custom search solution for startup. Evaluate development time, operational complexity, and scalability limits."
        3. "Design personalization strategy balancing relevance improvement vs privacy concerns and computational overhead."

        Scenario-based Design:
        1. "Design search for code repository with 10M files: Handle syntax highlighting, code structure understanding, and developer workflow integration."
        2. "Implement real-time search for social media platform: Index 1000 posts/second, handle viral content, and ensure content freshness."
        3. "Build enterprise search across 100+ data sources: Handle heterogeneous schemas, access control, and query federation."
        4. "Design multilingual search for global e-commerce: Handle 20 languages, cultural preferences, and localized ranking factors."

      resources:
        - title: "Elasticsearch: The Definitive Guide"
          url: "https://www.elastic.co/guide/en/elasticsearch/guide/current/"
          description: "Comprehensive guide to distributed search"
        - title: "Introduction to Information Retrieval"
          url: "https://nlp.stanford.edu/IR-book/"
          description: "Academic foundation of search systems"
        - title: "Modern Search Engines: Architecture and Engineering"
          url: "https://www.manning.com/books/relevant-search"
          description: "Practical search system implementation"
        - title: "Apache Lucene Documentation"
          url: "https://lucene.apache.org/core/documentation.html"
          description: "Core search library implementation"
        - title: "Google's Original PageRank Paper"
          url: "http://ilpubs.stanford.edu:8090/422/1/1999-66.pdf"
          description: "Foundational link analysis algorithm"
        - title: "Learning to Rank for Information Retrieval"
          url: "https://www.microsoft.com/en-us/research/publication/learning-to-rank-for-information-retrieval/"
          description: "Machine learning approaches to ranking"
        - title: "BM25 and Beyond: Relevance Scoring"
          url: "https://opensourceconnections.com/blog/2015/10/16/bm25-the-next-generation-of-lucene-relevation/"
          description: "Modern relevance scoring techniques"
        - title: "Designing Data-Intensive Applications: Search"
          url: "https://dataintensive.net/"
          description: "Search systems in distributed architectures"
      time_estimate: 120

    - day: 18
      topic: "Big Data Processing Systems"
      activity: "Master distributed data processing architectures, MapReduce paradigms, stream processing frameworks, and modern big data platforms for petabyte-scale analytics."
      detailed_content: |
        Big Data Processing Fundamentals:

        1. Processing Paradigms and Architectures:
        - Batch Processing:
          * High-throughput, high-latency processing
          * Complete dataset availability for analysis
          * Complex transformations and aggregations
          * Cost-effective for large historical datasets
          * Examples: Daily ETL pipelines, monthly reports
        - Stream Processing:
          * Low-latency, continuous data processing
          * Incremental processing of data streams
          * Real-time alerts and monitoring
          * Event-driven architectures
          * Examples: Fraud detection, real-time analytics
        - Micro-batch Processing:
          * Hybrid approach combining batch and stream
          * Small batches processed frequently
          * Balance between latency and throughput
          * Fault tolerance through batch boundaries
          * Examples: Near real-time reporting

        2. Lambda Architecture:
        - Architecture Components:
          * Batch Layer: Comprehensive, accurate processing
          * Speed Layer: Real-time approximate results
          * Serving Layer: Merges results from both layers
          * Master Dataset: Immutable, append-only storage
        - Benefits and Challenges:
          * Comprehensive data processing coverage
          * Fault tolerance and data recovery
          * Complex operational overhead
          * Code duplication between layers
        - Implementation Patterns:
          * Data ingestion strategies
          * Batch view materialization
          * Stream processing integration
          * Query serving optimization

        3. Kappa Architecture:
        - Unified Stream Processing:
          * Single processing engine for all data
          * Stream-first approach to data processing
          * Reprocessing through stream replay
          * Simplified operational model
        - Advantages over Lambda:
          * Reduced complexity and maintenance
          * Single codebase for all processing
          * Easier debugging and testing
          * Real-time and historical unified view
        - Implementation Considerations:
          * Stream storage and replay capabilities
          * Exactly-once processing guarantees
          * State management and checkpointing
          * Backpressure and flow control

        MapReduce and Distributed Computing:

        1. MapReduce Programming Model:
        - Core Concepts:
          * Map Function: Transform input key-value pairs
          * Reduce Function: Aggregate values by key
          * Partitioning: Distribute data across reducers
          * Combiners: Local aggregation optimization
        - Execution Flow:
          * Input splitting and distribution
          * Map task execution and intermediate output
          * Shuffle and sort phases
          * Reduce task execution and output
        - Optimization Techniques:
          * Input format optimization
          * Combiner function usage
          * Custom partitioning strategies
          * Output compression techniques

        2. Hadoop Ecosystem:
        - HDFS (Hadoop Distributed File System):
          * Block-based distributed storage
          * Replication for fault tolerance
          * NameNode and DataNode architecture
          * Block placement and load balancing
        - YARN (Yet Another Resource Negotiator):
          * Resource management and job scheduling
          * ApplicationMaster and NodeManager
          * Multi-tenancy and resource isolation
          * Dynamic resource allocation
        - MapReduce Engine:
          * Job execution framework
          * Task scheduling and monitoring
          * Fault tolerance and recovery
          * Performance tuning and optimization

        3. Apache Spark Architecture:
        - Core Components:
          * Driver Program: Application coordination
          * Cluster Manager: Resource allocation
          * Executors: Task execution and data storage
          * RDD (Resilient Distributed Datasets)
        - Processing Models:
          * Batch processing with Spark Core
          * Stream processing with Spark Streaming
          * SQL processing with Spark SQL
          * Machine learning with MLlib
          * Graph processing with GraphX
        - Optimization Features:
          * In-memory computing capabilities
          * Lazy evaluation and query optimization
          * Dynamic resource allocation
          * Adaptive query execution

        Stream Processing Systems:

        1. Apache Flink Architecture:
        - Core Features:
          * True streaming with low latency
          * Event-time processing semantics
          * Exactly-once state consistency
          * Advanced windowing capabilities
        - State Management:
          * Operator state and keyed state
          * State backends and checkpointing
          * Savepoints for application evolution
          * State schema evolution
        - Time and Windowing:
          * Event time vs processing time
          * Watermarks for out-of-order events
          * Window types (tumbling, sliding, session)
          * Custom window functions

        2. Apache Kafka Streams:
        - Stream Processing Library:
          * Client-side stream processing
          * No separate cluster management
          * Integration with Kafka ecosystem
          * Microservices-friendly architecture
        - Processing Topology:
          * Source and sink processors
          * Stream and table abstractions
          * Stateful and stateless operations
          * Joins and aggregations
        - Exactly-once Semantics:
          * Transactional processing guarantees
          * Idempotent producers and consumers
          * State store consistency
          * Failure recovery mechanisms

        3. Apache Storm (Real-time Computing):
        - Architecture Components:
          * Nimbus: Master node coordination
          * Supervisors: Worker node management
          * Spouts: Data source components
          * Bolts: Processing components
        - Topology Design:
          * Stream grouping strategies
          * Parallelism and scaling
          * Fault tolerance guarantees
          * At-least-once processing

        Modern Big Data Platforms:

        1. Cloud-Native Solutions:
        - Google Cloud Dataflow:
          * Unified batch and stream processing
          * Auto-scaling and managed infrastructure
          * Apache Beam programming model
          * Integration with GCP services
        - Amazon EMR and Kinesis:
          * Managed Hadoop and Spark clusters
          * Real-time stream processing
          * Integration with AWS ecosystem
          * Cost optimization and spot instances
        - Azure Data Factory and Stream Analytics:
          * ETL orchestration and data movement
          * Real-time stream processing
          * Integration with Azure services
          * Visual pipeline development

        2. Modern Unified Platforms:
        - Databricks Unified Analytics:
          * Collaborative data science platform
          * Managed Spark and Delta Lake
          * MLOps and AutoML capabilities
          * Multi-cloud deployment options
        - Snowflake Data Cloud:
          * Cloud-native data warehouse
          * Separation of compute and storage
          * Auto-scaling and concurrency
          * Data sharing and marketplace

        Real-world Implementation Examples:

        1. Netflix Big Data Platform:
        - Data Processing Pipeline:
          * Real-time event collection and processing
          * Batch processing for recommendation models
          * A/B testing data analysis
          * Content performance analytics
        - Technology Stack:
          * Kafka for data ingestion
          * Spark for batch processing
          * Flink for real-time analytics
          * Cassandra for data storage

        2. Uber Data Platform:
        - Real-time Processing:
          * Trip matching and pricing
          * Fraud detection systems
          * Real-time city operations
          * Dynamic supply-demand balancing
        - Architecture:
          * Kafka for event streaming
          * Flink for stream processing
          * Hadoop for historical analysis
          * Presto for interactive queries

        3. LinkedIn Data Infrastructure:
        - Kafka Origin Story:
          * Activity stream processing
          * Real-time feed generation
          * Metrics and monitoring
          * Change data capture
        - Processing Evolution:
          * Voldemort for serving
          * Hadoop for batch analytics
          * Samza for stream processing
          * Brooklin for data replication

        4. Spotify Music Recommendations:
        - Data Processing Pipeline:
          * User behavior stream processing
          * Music feature extraction
          * Collaborative filtering algorithms
          * Real-time playlist generation
        - Technology Choices:
          * Google Cloud Dataflow
          * BigQuery for analytics
          * Pub/Sub for messaging
          * TensorFlow for ML models

        Performance Optimization and Scalability:

        1. Resource Management:
        - Cluster Sizing:
          * CPU, memory, and storage planning
          * Network bandwidth considerations
          * Cost optimization strategies
          * Auto-scaling policies
        - Task Scheduling:
          * Fair vs FIFO vs capacity schedulers
          * Resource pools and queues
          * Priority-based scheduling
          * Speculative execution

        2. Data Locality and Partitioning:
        - Data Locality:
          * Co-locating computation and data
          * Rack-aware scheduling
          * Network topology optimization
          * Storage tier optimization
        - Partitioning Strategies:
          * Hash-based partitioning
          * Range-based partitioning
          * Custom partitioning functions
          * Dynamic repartitioning

        3. Fault Tolerance and Recovery:
        - Checkpointing Strategies:
          * Periodic state snapshots
          * Incremental checkpointing
          * Distributed checkpointing protocols
          * Recovery time optimization
        - Failure Handling:
          * Task failure recovery
          * Node failure detection
          * Data corruption handling
          * Network partition tolerance

        Practice Questions:

        Capacity Estimation:
        1. "Design big data processing system for 1PB daily data: Calculate cluster size, processing time, and resource requirements for batch and stream processing."
        2. "Estimate costs for processing 100TB monthly data using cloud services: Compare Spark on EMR vs Dataflow vs on-premises Hadoop."
        3. "Calculate stream processing latency: 1M events/second through 10-stage pipeline with 5ms per stage, including network and serialization overhead."

        Conceptual Understanding:
        1. "Compare Lambda vs Kappa architectures for e-commerce analytics. Analyze complexity, consistency, and operational overhead."
        2. "Explain exactly-once processing in distributed systems. Compare implementations in Kafka Streams, Flink, and Spark Streaming."
        3. "Design data partitioning strategy for time-series analytics. Balance query performance, parallelism, and storage efficiency."

        Trade-off Analysis:
        1. "Analyze MapReduce vs Spark for large-scale ETL: Consider memory usage, fault tolerance, development complexity, and performance."
        2. "Compare batch vs stream processing for fraud detection: Evaluate accuracy, latency, cost, and operational complexity."
        3. "Design resource allocation strategy: Dedicated clusters vs shared multi-tenant environment. Consider isolation, utilization, and cost."

        Scenario-based Design:
        1. "Design real-time recommendation system: Process 100K user events/second, update ML models, and serve recommendations under 100ms."
        2. "Build data pipeline for IoT sensor network: Handle 1M sensors, 1 datapoint/second each, with real-time alerting and historical analytics."
        3. "Implement financial risk calculation system: Process market data streams, calculate portfolio risk in real-time, and generate regulatory reports."
        4. "Design log analytics platform: Ingest 10TB daily logs, provide real-time dashboards, and enable ad-hoc queries for debugging."

      resources:
        - title: "Designing Data-Intensive Applications"
          url: "https://dataintensive.net/"
          description: "Comprehensive guide to big data systems design"
        - title: "Apache Spark Documentation"
          url: "https://spark.apache.org/docs/latest/"
          description: "Unified analytics engine for big data processing"
        - title: "Apache Flink Documentation"
          url: "https://flink.apache.org/documentation.html"
          description: "Stream processing framework with low latency"
        - title: "Kafka Streams Developer Guide"
          url: "https://kafka.apache.org/documentation/streams/"
          description: "Stream processing library for Apache Kafka"
        - title: "MapReduce: Simplified Data Processing"
          url: "https://research.google/pubs/pub62/"
          description: "Original Google MapReduce paper"
        - title: "The Lambda Architecture"
          url: "http://lambda-architecture.net/"
          description: "Big data architecture pattern"
        - title: "Streaming Systems: O'Reilly Book"
          url: "https://www.oreilly.com/library/view/streaming-systems/9781491983867/"
          description: "Comprehensive guide to stream processing"
        - title: "Google Cloud Dataflow Model"
          url: "https://cloud.google.com/dataflow/docs/concepts/beam-programming-model"
          description: "Unified batch and stream processing model"
      time_estimate: 130

    - day: 19
      topic: "NoSQL Database Deep Dive"
      activity: "Master NoSQL database architectures, data models, consistency patterns, and selection criteria for document, key-value, column-family, graph, and time-series databases."
      detailed_content: |
        NoSQL Database Categories and Architecture:

        1. Document Stores (MongoDB, CouchDB, Amazon DocumentDB):
        - Data Model and Structure:
          * JSON/BSON document storage with nested structures
          * Schema flexibility and schema-on-read capabilities
          * Collections and databases organization
          * Document size limitations and optimization
        - Query Capabilities:
          * Rich query language (MongoDB Query Language)
          * Index support (compound, text, geospatial, partial)
          * Aggregation framework for complex operations
          * MapReduce for large-scale data processing
        - Scaling and Distribution:
          * Horizontal sharding with shard keys
          * Replica sets for high availability
          * Automatic failover and read preference
          * Zones and cross-datacenter replication
        - Use Cases and Examples:
          * Content Management Systems (CMS)
          * Product catalogs and inventory management
          * User profiles and personalization
          * Real-time analytics and logging
          * Mobile and web application backends

        2. Key-Value Stores (Redis, DynamoDB, Riak, Amazon ElastiCache):
        - Architecture Patterns:
          * Simple key-value pair storage model
          * In-memory vs persistent storage options
          * Distributed hash tables and consistent hashing
          * Partitioning and replication strategies
        - Advanced Features:
          * Data structures (strings, hashes, lists, sets, sorted sets)
          * Expiration and TTL mechanisms
          * Atomic operations and transactions
          * Pub/Sub messaging capabilities
          * Lua scripting for complex operations
        - Performance Characteristics:
          * Sub-millisecond latency for in-memory systems
          * High throughput for read and write operations
          * Predictable performance scaling
          * Memory vs disk trade-offs
        - Use Cases and Patterns:
          * Session storage and user state management
          * Real-time caching and application acceleration
          * Gaming leaderboards and counters
          * Shopping carts and temporary data
          * Rate limiting and throttling systems

        3. Column-Family Databases (Cassandra, HBase, Amazon Keyspaces):
        - Data Model Deep Dive:
          * Wide column storage with row keys and column families
          * Dynamic column creation and sparse data handling
          * Time-stamped values and versioning
          * Composite keys and clustering columns
        - Architecture and Distribution:
          * Ring-based architecture with consistent hashing
          * Tunable consistency levels (ONE, QUORUM, ALL)
          * Multi-datacenter replication strategies
          * Gossip protocol for cluster membership
        - Performance Optimization:
          * Write-optimized storage with LSM trees
          * Bloom filters for efficient reads
          * Compaction strategies and tombstone handling
          * Compression algorithms for storage efficiency
        - Use Cases and Scenarios:
          * Time-series data and event logging
          * IoT sensor data collection
          * Large-scale analytics and data warehousing
          * Content delivery and media storage
          * Financial trading and market data

        4. Graph Databases (Neo4j, Amazon Neptune, ArangoDB, TigerGraph):
        - Graph Data Models:
          * Nodes, edges, and properties representation
          * Labeled property graph model
          * RDF (Resource Description Framework) support
          * Multi-model capabilities (document + graph)
        - Query Languages and Processing:
          * Cypher query language for graph traversal
          * SPARQL for RDF queries
          * Gremlin for graph computing
          * Native graph processing vs graph layers
        - Graph Algorithms and Analytics:
          * Shortest path and pathfinding algorithms
          * Centrality measures (betweenness, closeness, PageRank)
          * Community detection and clustering
          * Graph neural networks integration
        - Scaling and Performance:
          * Graph partitioning challenges
          * Horizontal vs vertical scaling approaches
          * Caching strategies for graph traversals
          * ACID transactions and consistency
        - Use Cases and Applications:
          * Social networks and relationship mapping
          * Fraud detection and risk analysis
          * Recommendation engines and collaborative filtering
          * Knowledge graphs and semantic search
          * Supply chain and network optimization

        5. Time-Series Databases (InfluxDB, TimescaleDB, Prometheus, OpenTSDB):
        - Specialized Architecture:
          * Time-based partitioning and data organization
          * Columnar storage for temporal data
          * Retention policies and data lifecycle management
          * Continuous queries and real-time processing
        - Query and Analytics Features:
          * Time-range queries and windowing functions
          * Downsampling and data aggregation
          * Gap-filling and interpolation functions
          * Statistical functions and trend analysis
        - Performance Optimizations:
          * Compression algorithms for time-series data
          * Index strategies for temporal queries
          * Batch ingestion and write optimization
          * Memory management for recent data
        - Use Cases and Domains:
          * Application performance monitoring (APM)
          * Infrastructure and system metrics
          * IoT sensor data and industrial telemetry
          * Financial market data and trading systems
          * Scientific data collection and analysis

        Advanced NoSQL Concepts:

        1. Consistency Models and CAP Theorem:
        - Consistency Levels:
          * Strong consistency (immediate consistency)
          * Eventual consistency (BASE properties)
          * Causal consistency and session consistency
          * Tunable consistency per operation
        - CAP Theorem Trade-offs:
          * Consistency vs Availability during partitions
          * CP systems (MongoDB, HBase) vs AP systems (Cassandra, DynamoDB)
          * Practical implications for system design
          * PACELC theorem extensions

        2. Replication and Partitioning Strategies:
        - Replication Patterns:
          * Master-slave vs master-master replication
          * Synchronous vs asynchronous replication
          * Multi-region replication for global systems
          * Conflict resolution strategies
        - Partitioning Techniques:
          * Hash-based partitioning with consistent hashing
          * Range-based partitioning for ordered data
          * Directory-based partitioning for flexibility
          * Hybrid approaches and repartitioning

        3. Transaction Support and ACID Properties:
        - NoSQL Transaction Models:
          * Single-document vs multi-document transactions
          * Cross-partition transaction complexity
          * Distributed transaction protocols (2PC, Saga)
          * Eventual consistency vs immediate consistency
        - ACID Alternatives:
          * BASE (Basically Available, Soft state, Eventual consistency)
          * CALM theorem and monotonic operations
          * Idempotent operations and retry safety
          * Compensation patterns for failure handling

        Real-world Implementation Examples:

        1. Netflix NoSQL Strategy:
        - Cassandra for Viewing History:
          * Storing billions of viewing events
          * Time-series data modeling
          * Multi-region replication
          * Tuned consistency for performance
        - MongoDB for Content Metadata:
          * Rich content descriptions and schemas
          * Search and recommendation integration
          * Global content distribution
          * Dynamic schema evolution

        2. Uber's Database Architecture:
        - Redis for Real-time Operations:
          * Driver location caching
          * Session management
          * Rate limiting and throttling
          * Pub/Sub for real-time updates
        - Cassandra for Trip Data:
          * Historical trip storage
          * Analytics and reporting
          * Geographic partitioning
          * Time-based retention policies

        3. LinkedIn's Data Platform:
        - Voldemort (Key-Value):
          * User profile storage
          * Social graph data
          * High-availability requirements
          * Consistent hashing distribution
        - Graph Database for Social Connections:
          * Connection recommendations
          * Network analysis and insights
          * Real-time relationship queries
          * Privacy and access control

        4. Discord Message Storage:
        - Cassandra for Message History:
          * Billions of messages storage
          * Channel-based partitioning
          * Real-time message delivery
          * Cross-datacenter replication
        - Redis for Real-time Features:
          * Online presence tracking
          * Message caching
          * Rate limiting and spam protection
          * Real-time notifications

        Database Selection Framework:

        1. Data Model Considerations:
        - Structured vs Semi-structured Data:
          * Relational data with fixed schema → RDBMS
          * Flexible documents with varying fields → Document stores
          * Simple key-value pairs → Key-value stores
          * Time-stamped metrics → Time-series databases
          * Relationship-heavy data → Graph databases
        - Query Pattern Analysis:
          * Simple lookups → Key-value stores
          * Complex queries and joins → Document stores or RDBMS
          * Graph traversals → Graph databases
          * Time-range analytics → Time-series databases
          * Write-heavy workloads → Column-family stores

        2. Scalability and Performance Requirements:
        - Read vs Write Patterns:
          * Read-heavy: Document stores with read replicas
          * Write-heavy: Column-family databases
          * Balanced: Key-value stores with caching
          * Complex queries: Graph databases with indexing
        - Consistency Requirements:
          * Strong consistency: RDBMS or CP NoSQL systems
          * Eventual consistency: AP NoSQL systems
          * Tunable consistency: Cassandra or DynamoDB
          * Real-time consistency: In-memory databases

        3. Operational Considerations:
        - Team Expertise and Learning Curve:
          * SQL familiarity → SQL-like NoSQL (DocumentDB)
          * DevOps complexity → Managed services
          * Operational overhead → Cloud-native solutions
          * Monitoring and debugging capabilities
        - Cost and Resource Planning:
          * Storage costs for different data models
          * Compute requirements for query processing
          * Network costs for distributed systems
          * Operational overhead and maintenance

        Practice Questions:

        Capacity Estimation:
        1. "Design NoSQL storage for social media platform: 1B users, 100B posts, 1TB daily growth. Choose appropriate databases for user profiles, posts, and analytics."
        2. "Estimate DynamoDB costs for gaming leaderboard: 10M players, 1000 updates/second, 100 reads/second. Calculate RCU/WCU requirements and monthly costs."
        3. "Calculate Cassandra cluster size for IoT platform: 1M sensors, 1 datapoint/second each, 2-year retention. Consider replication factor and growth."

        Conceptual Understanding:
        1. "Compare document databases vs relational databases for e-commerce product catalog. Analyze schema flexibility, query capabilities, and scaling patterns."
        2. "Explain eventual consistency in Cassandra. How would you handle read-after-write consistency for critical operations?"
        3. "Design data model for graph database storing social network. Handle mutual connections, privacy settings, and recommendation queries."

        Trade-off Analysis:
        1. "Compare Redis vs DynamoDB for session storage: Analyze latency, durability, cost, and operational complexity for global web application."
        2. "Choose between MongoDB and Cassandra for logging system: 1TB daily logs, real-time queries, 3-month retention. Consider write performance and query needs."
        3. "Analyze graph database vs document database for recommendation engine: Product recommendations based on user behavior and social connections."

        Scenario-based Design:
        1. "Design NoSQL architecture for multiplayer game: Player profiles, match history, leaderboards, and real-time game state. Handle 1M concurrent players."
        2. "Build time-series database solution for financial trading: Store tick data, calculate moving averages, and detect anomalies. Handle 100K updates/second."
        3. "Design multi-tenant SaaS database architecture: Isolated customer data, flexible schemas, and cross-tenant analytics. Choose appropriate NoSQL solutions."
        4. "Implement content delivery system: Global content distribution, user preferences, and real-time recommendations. Handle diverse content types and access patterns."

      resources:
        - title: "NoSQL Distilled: A Brief Guide"
          url: "https://martinfowler.com/books/nosql.html"
          description: "Comprehensive guide to NoSQL database concepts"
        - title: "MongoDB Architecture Guide"
          url: "https://docs.mongodb.com/manual/"
          description: "Document database design patterns and best practices"
        - title: "Cassandra: The Complete Reference"
          url: "https://cassandra.apache.org/doc/latest/"
          description: "Column-family database architecture and operations"
        - title: "Redis Documentation and Patterns"
          url: "https://redis.io/documentation"
          description: "Key-value store advanced features and use cases"
        - title: "Neo4j Graph Database Concepts"
          url: "https://neo4j.com/docs/"
          description: "Graph database modeling and query optimization"
        - title: "Time-Series Database Comparison"
          url: "https://blog.timescale.com/blog/what-the-heck-is-time-series-data-and-why-do-i-need-a-time-series-database-dcf3b6dfd73/"
          description: "Time-series database architecture and use cases"
        - title: "CAP Theorem and NoSQL"
          url: "https://en.wikipedia.org/wiki/CAP_theorem"
          description: "Consistency, availability, and partition tolerance trade-offs"
        - title: "Amazon DynamoDB Design Patterns"
          url: "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/best-practices.html"
          description: "NoSQL design patterns and optimization techniques"
      time_estimate: 130

    - day: 20
      topic: "Week 3 Integration - Social Media Feed Design"
      activity: "Design a comprehensive social media platform integrating advanced sharding, CDN, search systems, big data processing, and NoSQL databases for global scale."
      detailed_content: |
        Week 3 Integration Project: Global Social Media Platform

        This capstone project integrates all Week 3 concepts:
        - Advanced Database Sharding (Day 15)
        - Content Delivery Networks (Day 16)
        - Search Systems & Information Retrieval (Day 17)
        - Big Data Processing Systems (Day 18)
        - NoSQL Database Deep Dive (Day 19)

        System Requirements and Scale:

        Functional Requirements:
        - User registration and profile management
        - Post creation (text, images, videos, stories)
        - Timeline/feed generation with personalization
        - Social interactions (likes, comments, shares, reactions)
        - User relationships (follow, unfollow, block)
        - Content search and discovery
        - Real-time notifications
        - Direct messaging and chat
        - Trending topics and hashtag tracking
        - Content moderation and safety

        Non-functional Requirements:
        - 1.5 billion daily active users
        - 500 million posts created daily
        - 50 billion interactions daily (likes, comments, shares)
        - 10TB of media uploaded daily
        - Timeline load time: <200ms globally
        - Search response time: <100ms
        - 99.9% availability (8.76 hours downtime/year)
        - Global deployment across 6 regions
        - Real-time feed updates within 5 seconds

        Comprehensive Architecture Design:

        1. User Service (Advanced Sharding Applied):
        - Database Sharding Strategy:
          * Shard by user_id using consistent hashing
          * 1000 shards across 200 database servers
          * Each shard handles ~1.5M users
          * Cross-shard queries for social graph operations
        - NoSQL Implementation:
          * MongoDB for user profiles (flexible schema)
          * Redis for session management and online status
          * Cassandra for activity logs and audit trails
        - Data Model:
          * User profiles with rich metadata
          * Social graph relationships (bidirectional)
          * Privacy settings and preferences
          * Account verification and security settings

        2. Post Service (Multi-Database Strategy):
        - Content Storage Sharding:
          * Time-based + user_id composite sharding
          * Recent posts (last 30 days) in hot shards
          * Older posts archived to cold storage
          * Media metadata stored separately from content
        - NoSQL Database Selection:
          * Cassandra for post metadata (high write throughput)
          * MongoDB for rich content objects (flexible schema)
          * Time-series database for engagement metrics
          * Graph database for content relationships
        - Data Architecture:
          * Post content with versioning support
          * Media attachments and processing status
          * Engagement metrics (likes, shares, views)
          * Content classification and moderation flags

        3. Timeline Service (Hybrid Feed Generation):
        - Feed Generation Strategy:
          * Push model for users with <10K followers
          * Pull model for celebrities with >1M followers
          * Hybrid model for mid-tier influencers
          * Pre-computed feeds for active users
        - Big Data Processing Integration:
          * Apache Kafka for real-time post streaming
          * Apache Flink for real-time feed updates
          * Apache Spark for batch personalization
          * Lambda architecture for comprehensive processing
        - Caching and Performance:
          * Redis for timeline caches (last 500 posts)
          * CDN for timeline API responses
          * Preload next page content predictively
          * Cache warming for returning users

        4. Search Service (Comprehensive Search System):
        - Search Architecture (Day 17 Applied):
          * Elasticsearch cluster with 100+ nodes
          * Real-time indexing with <5 second latency
          * Multi-language support and analysis
          * Autocomplete and spell correction
        - Index Design:
          * Post content with full-text search
          * User profiles with fuzzy matching
          * Hashtag and mention extraction
          * Media content analysis and tagging
        - Advanced Features:
          * Personalized search ranking
          * Trending topics detection
          * Content similarity and recommendations
          * Real-time search suggestions

        5. Recommendation Service (ML-Driven Personalization):
        - Big Data Pipeline (Day 18 Applied):
          * Kafka Streams for real-time feature extraction
          * Spark MLlib for model training and inference
          * Feature store for user and content features
          * A/B testing framework for algorithm optimization
        - Recommendation Algorithms:
          * Collaborative filtering for user similarity
          * Content-based filtering for post similarity
          * Deep learning models for engagement prediction
          * Graph neural networks for social influence
        - Real-time Processing:
          * Stream processing for immediate personalization
          * Feature vector updates within 1 second
          * Model serving with <10ms inference time
          * Continuous learning from user interactions

        6. Media Service (Global Content Distribution):
        - CDN Architecture (Day 16 Applied):
          * Multi-tier CDN with edge locations globally
          * Adaptive bitrate streaming for videos
          * Image optimization and format selection
          * Progressive loading and lazy loading
        - Media Processing Pipeline:
          * Async video transcoding with multiple resolutions
          * Image compression and thumbnail generation
          * Content analysis for auto-tagging
          * NSFW detection and content moderation
        - Storage Strategy:
          * Object storage for original media files
          * CDN caching for frequently accessed content
          * Geographically distributed storage
          * Intelligent storage tiering based on access patterns

        7. Analytics and Insights Service:
        - Time-Series Database Implementation:
          * InfluxDB for user engagement metrics
          * Prometheus for system performance monitoring
          * Custom dashboards for business intelligence
          * Real-time alerting for anomaly detection
        - Big Data Analytics:
          * Data lake for historical analysis
          * ETL pipelines for data warehouse
          * Machine learning for trend prediction
          * Privacy-compliant data processing

        Advanced Implementation Details:

        Database Sharding Strategy (Day 15 Integration):
        - User Sharding:
          * Consistent hashing with virtual nodes
          * Cross-shard relationship queries optimization
          * Hot celebrity shard splitting mechanisms
          * Global secondary indexes for search
        - Content Sharding:
          * Time + geography based partitioning
          * Content locality for regional preferences
          * Automated resharding based on growth
          * Cross-shard analytics aggregation

        CDN and Global Distribution (Day 16 Integration):
        - Edge Computing:
          * Timeline generation at edge locations
          * Image resizing and optimization at edge
          * Personalization logic deployment
          * Real-time content filtering
        - Performance Optimization:
          * Prefetching based on user behavior
          * Intelligent caching with ML predictions
          * Network-aware content delivery
          * Mobile-optimized content serving

        Search System Architecture (Day 17 Integration):
        - Distributed Indexing:
          * Real-time document indexing pipeline
          * Incremental index updates
          * Cross-datacenter index replication
          * Search result personalization
        - Advanced Search Features:
          * Multi-modal search (text, image, video)
          * Semantic search with embeddings
          * Real-time trending search suggestions
          * Privacy-preserving search personalization

        Big Data Processing (Day 18 Integration):
        - Stream Processing:
          * Real-time engagement scoring
          * Trend detection and viral content identification
          * Abuse detection and content filtering
          * User behavior pattern analysis
        - Batch Processing:
          * Daily recommendation model retraining
          * Historical analytics and reporting
          * Data quality checks and validation
          * Long-term trend analysis

        NoSQL Database Strategy (Day 19 Integration):
        - Multi-Database Architecture:
          * MongoDB: User profiles, post content, rich objects
          * Cassandra: Activity feeds, time-series data, high-write workloads
          * Redis: Caching, session management, real-time features
          * Neo4j: Social graph, recommendation graphs, influence networks
          * InfluxDB: Metrics, monitoring, time-series analytics
        - Consistency and Performance:
          * Eventually consistent user feeds
          * Strong consistency for financial transactions
          * Tunable consistency based on use case
          * Cross-database transaction coordination

        Scalability and Performance Optimizations:

        1. Hot User Problem Resolution:
        - Celebrity Timeline Optimization:
          * Dedicated infrastructure for high-follower accounts
          * Fan-out limitations and sampling strategies
          * Push notification rate limiting
          * Separate processing queues for viral content
        - Load Distribution:
          * Geographic load balancing
          * Peak hour traffic prediction and scaling
          * Elastic resource allocation
          * Circuit breakers for cascade failure prevention

        2. Real-time Processing Challenges:
        - Stream Processing Optimization:
          * Exactly-once processing guarantees
          * Late-arriving data handling
          * Backpressure management
          * Stateful processing with checkpoints
        - Data Consistency:
          * Event sourcing for audit trails
          * CQRS for read/write optimization
          * Saga pattern for distributed transactions
          * Conflict resolution for concurrent updates

        3. Global Distribution Strategy:
        - Multi-Region Architecture:
          * Active-active deployment across regions
          * Data locality optimization
          * Cross-region replication strategies
          * Disaster recovery and failover procedures
        - Network Optimization:
          * Anycast routing for global load balancing
          * TCP optimization for long-distance connections
          * Compression and protocol optimization
          * Mobile network adaptation

        Security and Privacy Considerations:

        1. Data Protection:
        - Privacy by Design:
          * GDPR and CCPA compliance
          * User data anonymization
          * Right to be forgotten implementation
          * Consent management systems
        - Content Security:
          * End-to-end encryption for private messages
          * Content moderation and abuse detection
          * NSFW content filtering
          * Spam and bot detection systems

        2. System Security:
        - Access Control:
          * OAuth 2.0 for third-party integrations
          * Multi-factor authentication
          * API rate limiting and DDoS protection
          * Zero-trust security architecture
        - Infrastructure Security:
          * Network segmentation and firewalls
          * Infrastructure as code with security scanning
          * Secrets management and rotation
          * Security monitoring and incident response

        Monitoring and Observability:

        1. System Monitoring:
        - Comprehensive Metrics:
          * Application performance monitoring (APM)
          * Infrastructure monitoring and alerting
          * Business metric tracking and dashboards
          * User experience monitoring
        - Distributed Tracing:
          * Request tracing across microservices
          * Performance bottleneck identification
          * Error tracking and debugging
          * Capacity planning and optimization

        2. Business Intelligence:
        - Analytics Pipeline:
          * Real-time business dashboards
          * User engagement analysis
          * Content performance tracking
          * Revenue and growth metrics
        - Machine Learning Operations:
          * Model performance monitoring
          * Feature drift detection
          * A/B testing statistical analysis
          * Recommendation system optimization

        Cost Optimization Strategies:

        1. Infrastructure Efficiency:
        - Resource Optimization:
          * Auto-scaling based on traffic patterns
          * Spot instance utilization for batch jobs
          * Storage tiering for cost optimization
          * Network traffic optimization
        - Technology Choices:
          * Open source vs managed service trade-offs
          * Multi-cloud strategy for cost arbitrage
          * Reserved capacity planning
          * Development vs production environment optimization

        2. Operational Efficiency:
        - Automation and DevOps:
          * Infrastructure as code deployment
          * Automated testing and validation
          * Continuous integration and deployment
          * Chaos engineering for resilience testing
        - Team Productivity:
          * Developer productivity tools
          * Monitoring and alerting automation
          * Self-service platforms for teams
          * Documentation and knowledge sharing

        Performance Metrics and SLAs:

        Service Level Objectives:
        - Timeline Loading: 95th percentile < 200ms globally
        - Search Response: 99th percentile < 100ms
        - Post Publishing: 95th percentile < 500ms end-to-end
        - Notification Delivery: 90% within 5 seconds
        - System Availability: 99.9% uptime (8.76 hours/year downtime)
        - Data Durability: 99.999999999% (11 9's)

        Capacity Planning:
        - User Growth: 20% year-over-year growth planning
        - Content Growth: 25% year-over-year growth planning
        - Peak Traffic Handling: 3x average traffic capacity
        - Storage Scaling: Automatic expansion with 6-month forecasting
        - Network Capacity: Global backbone with 2x overprovisioning

        Future Enhancements and Innovation:

        1. Emerging Technologies:
        - AI/ML Integration:
          * GPT-powered content generation assistance
          * Computer vision for automated content tagging
          * Natural language processing for sentiment analysis
          * Reinforcement learning for optimization
        - Next-Generation Features:
          * AR/VR content support and processing
          * Blockchain integration for creator monetization
          * Edge AI for real-time content analysis
          * Quantum-resistant cryptography preparation

        2. Platform Evolution:
        - Technical Debt Management:
          * Legacy system migration strategies
          * API versioning and backward compatibility
          * Database migration with zero downtime
          * Technology stack modernization roadmap
        - Innovation Pipeline:
          * Experimental feature development framework
          * Technology evaluation and adoption process
          * Open source contribution strategy
          * Research and development partnerships

        Practice Questions and Exercises:

        Architecture Design Questions:
        1. "Design database sharding strategy for 2B users with uneven geographic distribution. Handle hot celebrity accounts and cross-shard social graph queries."
        2. "Implement CDN strategy for global media delivery with 10TB daily uploads. Optimize for mobile users in emerging markets with limited bandwidth."
        3. "Design search system handling 1M queries/second with personalized results. Support real-time indexing of 500M daily posts with sub-second latency."
        4. "Build recommendation pipeline processing 50B daily interactions. Integrate collaborative filtering, content analysis, and social signals for personalization."

        Scalability Challenges:
        1. "Handle viral content spreading to 100M users in 1 hour. Design systems to prevent cascade failures while maintaining real-time updates."
        2. "Implement timeline generation for user with 100M followers. Balance fan-out costs, latency requirements, and storage constraints."
        3. "Design cross-region consistency strategy for global social media platform. Handle network partitions while maintaining user experience."
        4. "Optimize search performance for trending queries during major events. Handle 10x traffic spikes while maintaining sub-100ms response times."

        Technology Integration:
        1. "Compare NoSQL database choices for different components: user profiles, content storage, activity feeds, and analytics. Justify selection criteria."
        2. "Design big data processing pipeline integrating Kafka, Flink, and Spark. Handle real-time features, batch analytics, and ML model training."
        3. "Implement multi-tier caching strategy using Redis, CDN, and application caches. Optimize for different content types and access patterns."
        4. "Build monitoring and alerting system for distributed social media platform. Include business metrics, technical metrics, and user experience monitoring."

      resources:
        - title: "Instagram Engineering Blog"
          url: "https://instagram-engineering.com/"
          description: "Real-world social media scaling challenges and solutions"
        - title: "Facebook (Meta) Engineering Blog"
          url: "https://engineering.fb.com/"
          description: "Large-scale social platform architecture insights"
        - title: "Twitter Engineering Blog"
          url: "https://blog.twitter.com/engineering"
          description: "Real-time social media system design patterns"
        - title: "Pinterest Engineering Blog"
          url: "https://medium.com/@Pinterest_Engineering"
          description: "Visual content platform scaling strategies"
        - title: "LinkedIn Engineering Blog"
          url: "https://engineering.linkedin.com/"
          description: "Professional social network architecture"
        - title: "High Scalability: Social Media Cases"
          url: "http://highscalability.com/"
          description: "Collection of social media scaling case studies"
        - title: "System Design Interview Book"
          url: "https://www.amazon.com/System-Design-Interview-insiders-Second/dp/B08CMF2CQF"
          description: "Social media system design interview preparation"
        - title: "Designing Data-Intensive Applications"
          url: "https://dataintensive.net/"
          description: "Foundational concepts for large-scale system design"
      time_estimate: 180

  week4:
    - day: 22
      topic: "Security & Authentication"
      activity: "Master enterprise-grade security architecture, authentication systems, authorization frameworks, threat modeling, and defense strategies for large-scale distributed systems."
      detailed_content: |
        Enterprise Security Architecture:

        1. Security-First Design Principles:
        - Zero Trust Architecture:
          * Never trust, always verify principle
          * Network segmentation and micro-perimeters
          * Identity-based security boundaries
          * Continuous verification and monitoring
        - Defense in Depth:
          * Multiple security layers and controls
          * Fail-safe and fail-secure designs
          * Redundant security mechanisms
          * Compartmentalization and isolation
        - Principle of Least Privilege:
          * Minimum necessary access rights
          * Time-bound and context-aware permissions
          * Regular access reviews and audits
          * Automatic privilege escalation controls

        Authentication Systems and Mechanisms:

        1. Multi-Factor Authentication (MFA):
        - Authentication Factors:
          * Something you know (passwords, PINs)
          * Something you have (tokens, phones, cards)
          * Something you are (biometrics, behavior)
          * Somewhere you are (location, device)
        - Implementation Strategies:
          * TOTP (Time-based One-Time Passwords)
          * Push notifications and mobile approval
          * Hardware security keys (FIDO2/WebAuthn)
          * Biometric authentication integration
        - Risk-based Authentication:
          * Adaptive authentication based on context
          * Device fingerprinting and recognition
          * Behavioral analytics and anomaly detection
          * Geographic and temporal risk assessment

        2. Enterprise Identity Management:
        - Single Sign-On (SSO):
          * SAML 2.0 for enterprise federation
          * OpenID Connect for modern applications
          * Active Directory and LDAP integration
          * Cross-domain authentication and trust
        - Identity Providers (IdP):
          * Centralized identity management
          * User provisioning and deprovisioning
          * Group membership and role assignment
          * Identity lifecycle management
        - Federation and Trust:
          * Cross-organizational authentication
          * Identity provider federation
          * Trust relationships and metadata exchange
          * Certificate-based authentication

        3. Modern Authentication Protocols:
        - OAuth 2.0 and Extensions:
          * Authorization code flow with PKCE
          * Client credentials for service-to-service
          * Device authorization flow for IoT
          * Token introspection and revocation
        - JWT (JSON Web Tokens):
          * Stateless authentication and claims
          * Token structure (header, payload, signature)
          * Signing algorithms (RS256, ES256, HS256)
          * Token validation and security considerations
        - OpenID Connect:
          * Identity layer on top of OAuth 2.0
          * ID tokens and userinfo endpoint
          * Discovery and dynamic registration
          * Session management and logout

        Authorization and Access Control:

        1. Access Control Models:
        - Role-Based Access Control (RBAC):
          * Role hierarchy and inheritance
          * Permission assignment and management
          * Static vs dynamic role assignment
          * Role explosion and management challenges
        - Attribute-Based Access Control (ABAC):
          * Policy-based access decisions
          * Subject, resource, action, and environment attributes
          * Fine-grained and context-aware permissions
          * XACML (eXtensible Access Control Markup Language)
        - Relationship-Based Access Control (ReBAC):
          * Graph-based access control models
          * Social relationships and ownership
          * Dynamic permission calculation
          * Zanzibar-inspired authorization systems

        2. Authorization Patterns and Implementation:
        - Policy Decision Points (PDP):
          * Centralized policy evaluation
          * Policy engines and rule management
          * Decision caching and performance optimization
          * Policy versioning and deployment
        - Policy Enforcement Points (PEP):
          * Distributed enforcement mechanisms
          * API gateways and middleware integration
          * Fine-grained resource protection
          * Audit trail and compliance logging
        - Resource-Level Authorization:
          * Object-level permissions
          * Field-level access controls
          * Dynamic resource ownership
          * Hierarchical resource structures

        Cryptography and Data Protection:

        1. Encryption at Rest and in Transit:
        - Data-at-Rest Encryption:
          * Database-level encryption (TDE)
          * File system and disk encryption
          * Application-level field encryption
          * Key management and rotation strategies
        - Data-in-Transit Protection:
          * TLS 1.3 for secure communications
          * Perfect Forward Secrecy (PFS)
          * Certificate management and validation
          * End-to-end encryption for sensitive data
        - Advanced Encryption Techniques:
          * Homomorphic encryption for computation on encrypted data
          * Format-preserving encryption (FPE)
          * Searchable encryption for databases
          * Secure multi-party computation

        2. Key Management and PKI:
        - Key Management Systems (KMS):
          * Hardware Security Modules (HSM)
          * Key generation and distribution
          * Key rotation and lifecycle management
          * Envelope encryption and key hierarchy
        - Public Key Infrastructure (PKI):
          * Certificate authorities and trust chains
          * Certificate lifecycle management
          * Certificate transparency and monitoring
          * Code signing and document encryption
        - Secrets Management:
          * Centralized secrets storage (Vault, AWS Secrets Manager)
          * Dynamic secrets and short-lived credentials
          * Secrets rotation and versioning
          * Application integration patterns

        Security Threats and Mitigation:

        1. Common Attack Vectors:
        - OWASP Top 10 Mitigation:
          * Injection attacks (SQL, NoSQL, LDAP, OS)
          * Broken authentication and session management
          * Sensitive data exposure and inadequate encryption
          * XML external entities (XXE) and deserialization
          * Security misconfiguration and known vulnerabilities
        - Advanced Persistent Threats (APT):
          * Multi-stage attack detection
          * Lateral movement prevention
          * Command and control communication blocking
          * Data exfiltration monitoring
        - Application Security:
          * Input validation and sanitization
          * Output encoding and CSRF protection
          * Secure coding practices and code review
          * Dynamic and static application security testing

        2. DDoS Protection and Rate Limiting:
        - DDoS Mitigation Strategies:
          * Traffic analysis and pattern recognition
          * Geo-blocking and IP reputation filtering
          * Content delivery network (CDN) protection
          * Upstream provider and ISP coordination
        - Rate Limiting Implementation:
          * Token bucket and leaky bucket algorithms
          * Distributed rate limiting across services
          * User-based and IP-based limiting
          * Adaptive rate limiting based on system load
        - API Security:
          * API authentication and authorization
          * Request signing and verification
          * API versioning and backward compatibility
          * API gateway security features

        Real-world Security Implementation Examples:

        1. Google's Zero Trust Model (BeyondCorp):
        - Architecture Components:
          * Device inventory and trust assessment
          * User and device authentication
          * Network access proxy and enforcement
          * Risk-based access decisions
        - Implementation Principles:
          * Remove network location trust
          * Encrypt and authenticate all communications
          * Grant access based on device and user state
          * Continuous monitoring and validation

        2. Netflix Security Architecture:
        - Cloud Security Strategy:
          * Immutable infrastructure and deployment
          * Security monitoring and incident response
          * Data classification and protection
          * Third-party risk management
        - Application Security:
          * Security-focused microservices design
          * Automated security testing in CI/CD
          * Runtime application self-protection (RASP)
          * Security chaos engineering

        3. Facebook (Meta) Security Systems:
        - Scale Security Challenges:
          * 3 billion user authentication
          * Real-time threat detection and response
          * Privacy-preserving data processing
          * Global regulatory compliance
        - Technical Solutions:
          * Machine learning for fraud detection
          * Differential privacy for analytics
          * Secure computation for cross-platform data
          * Automated security policy enforcement

        4. Amazon Web Services (AWS) Security:
        - Shared Responsibility Model:
          * AWS infrastructure security responsibilities
          * Customer application and data security
          * Service-specific security configurations
          * Compliance framework alignment
        - Security Services Integration:
          * Identity and Access Management (IAM)
          * Key Management Service (KMS)
          * Web Application Firewall (WAF)
          * Security Hub and GuardDuty threat detection

        Security Monitoring and Incident Response:

        1. Security Information and Event Management (SIEM):
        - Log Collection and Analysis:
          * Centralized log aggregation and correlation
          * Real-time security event detection
          * Threat intelligence integration
          * Machine learning for anomaly detection
        - Incident Detection and Response:
          * Security orchestration and automated response (SOAR)
          * Threat hunting and investigation workflows
          * Incident classification and prioritization
          * Forensic data collection and preservation

        2. Compliance and Governance:
        - Regulatory Compliance:
          * GDPR, CCPA, and privacy regulations
          * SOX, HIPAA, and industry-specific requirements
          * PCI DSS for payment card data
          * ISO 27001 and security management systems
        - Security Governance:
          * Security policy development and enforcement
          * Risk assessment and management processes
          * Security awareness training and culture
          * Third-party security assessments

        Performance and Scalability Considerations:

        1. Authentication Performance Optimization:
        - Caching Strategies:
          * Token validation caching
          * User session state management
          * Permission evaluation caching
          * Distributed cache consistency
        - Load Balancing:
          * Authentication service scaling
          * Session affinity and stateless design
          * Geographic distribution of auth services
          * Failover and disaster recovery

        2. Security vs Performance Trade-offs:
        - Optimization Techniques:
          * Asymmetric vs symmetric encryption choices
          * Hardware acceleration for cryptographic operations
          * Batching and bulk operations for authorization
          * Lazy loading and just-in-time permission evaluation
        - Monitoring and Alerting:
          * Security metric collection and analysis
          * Performance impact measurement
          * Security debt tracking and remediation
          * Cost-benefit analysis for security investments

        DevSecOps and Security Automation:

        1. Security in CI/CD Pipelines:
        - Shift-Left Security:
          * Static Application Security Testing (SAST)
          * Dynamic Application Security Testing (DAST)
          * Software Composition Analysis (SCA)
          * Infrastructure as Code security scanning
        - Automated Security Testing:
          * Unit tests for security functions
          * Integration tests for authentication flows
          * End-to-end security scenario testing
          * Penetration testing automation

        2. Infrastructure Security Automation:
        - Configuration Management:
          * Security baseline enforcement
          * Compliance scanning and remediation
          * Vulnerability management automation
          * Patch management and deployment
        - Runtime Security:
          * Container security and image scanning
          * Runtime application protection
          * Network security monitoring
          * Behavioral analysis and alerting

        Practice Questions:

        Architecture Design:
        1. "Design authentication system for global SaaS platform: 100M users, 1000 API calls/second, multi-region deployment. Include SSO, MFA, and fraud detection."
        2. "Implement authorization framework for microservices architecture: 50 services, fine-grained permissions, cross-service calls. Design policy engine and enforcement."
        3. "Build security architecture for financial trading platform: Real-time transactions, regulatory compliance, fraud prevention, and audit requirements."

        Security Implementation:
        1. "Design DDoS protection for e-commerce platform during Black Friday: Handle 100x traffic increase while maintaining legitimate user access."
        2. "Implement zero-trust network architecture for remote workforce: Device trust, identity verification, and secure access to internal resources."
        3. "Build data privacy system for social media platform: GDPR compliance, user consent management, and data anonymization at scale."

        Threat Modeling:
        1. "Analyze attack vectors for mobile banking application: Identify threats, assess risks, and design countermeasures for each attack path."
        2. "Design security monitoring for cloud infrastructure: Detect insider threats, advanced persistent threats, and automated attacks."
        3. "Implement secure communication protocol for IoT devices: Handle millions of devices, over-the-air updates, and device authentication."

        Compliance and Governance:
        1. "Design audit system for healthcare platform: HIPAA compliance, access logging, data lineage tracking, and regulatory reporting."
        2. "Implement security governance framework for startup scaling from 100 to 10,000 employees: Policy development, training, and enforcement."
        3. "Build privacy-preserving analytics system: Collect user behavior data while maintaining individual privacy and regulatory compliance."

      resources:
        - title: "OWASP Top 10 Security Risks"
          url: "https://owasp.org/www-project-top-ten/"
          description: "Most critical web application security risks"
        - title: "NIST Cybersecurity Framework"
          url: "https://www.nist.gov/cyberframework"
          description: "Comprehensive cybersecurity guidance and standards"
        - title: "OAuth 2.0 Security Best Practices"
          url: "https://tools.ietf.org/html/draft-ietf-oauth-security-topics"
          description: "Security considerations for OAuth 2.0 implementations"
        - title: "Zero Trust Architecture - NIST SP 800-207"
          url: "https://csrc.nist.gov/publications/detail/sp/800-207/final"
          description: "Zero trust architecture principles and implementation"
        - title: "Google BeyondCorp Papers"
          url: "https://research.google/pubs/?area=security"
          description: "Zero trust network architecture research and implementation"
        - title: "SANS Security Architecture"
          url: "https://www.sans.org/white-papers/"
          description: "Enterprise security architecture best practices"
        - title: "Designing Secure Systems (Microsoft)"
          url: "https://docs.microsoft.com/en-us/azure/architecture/framework/security/"
          description: "Cloud security architecture patterns and practices"
        - title: "Application Security Verification Standard (ASVS)"
          url: "https://owasp.org/www-project-application-security-verification-standard/"
          description: "Comprehensive application security requirements framework"
      time_estimate: 140

    - day: 23
      topic: "Monitoring & Observability"
      activity: "Master comprehensive observability architecture, monitoring strategies, SLI/SLO frameworks, alerting systems, and operational excellence for large-scale distributed systems."
      detailed_content: |
        Modern Observability Architecture:

        1. Three Pillars of Observability:
        - Metrics (What is Happening):
          * Quantitative measurements of system behavior
          * Time-series data for trend analysis
          * Aggregatable data for statistical analysis
          * Low storage overhead and fast queries
          * Examples: CPU usage, request rate, error count
        - Logs (What Happened):
          * Discrete events with timestamps and context
          * Detailed information about specific occurrences
          * Searchable and filterable event records
          * High storage overhead but rich information
          * Examples: Error messages, audit trails, debug info
        - Traces (How Things Happen):
          * Request flow through distributed systems
          * Causal relationships between service calls
          * Performance bottleneck identification
          * End-to-end transaction visibility
          * Examples: User request journey, dependency mapping

        2. Modern Extensions to Observability:
        - Profiles (Why Things Happen):
          * Continuous profiling of application performance
          * CPU, memory, and resource usage analysis
          * Code-level performance optimization insights
          * Production performance debugging
        - Service Maps and Topology:
          * Real-time service dependency visualization
          * Service health and performance mapping
          * Critical path identification
          * Impact analysis for incidents
        - User Experience Monitoring:
          * Real user monitoring (RUM) data
          * Synthetic transaction monitoring
          * Core web vitals and performance metrics
          * Business-impact correlation

        Metrics and Time-Series Systems:

        1. Metrics Design and Collection:
        - Metric Types and Patterns:
          * Counter: Monotonically increasing values (requests, errors)
          * Gauge: Point-in-time values (memory usage, queue size)
          * Histogram: Distribution of values (latency, request size)
          * Summary: Quantiles and aggregations over time windows
        - High-Cardinality Challenges:
          * Tag explosion and storage costs
          * Query performance degradation
          * Cardinality limits and management
          * Sampling and aggregation strategies
        - Instrumentation Best Practices:
          * Consistent naming conventions
          * Meaningful labels and dimensions
          * Appropriate metric granularity
          * Performance impact considerations

        2. Time-Series Database Architecture:
        - Prometheus Ecosystem:
          * Pull-based metrics collection model
          * Service discovery and target configuration
          * PromQL for flexible querying and alerting
          * Federation for multi-cluster monitoring
        - Alternative Solutions:
          * InfluxDB for high-write throughput scenarios
          * Datadog for unified observability platform
          * New Relic for application performance monitoring
          * Grafana Cloud for managed time-series storage
        - Scaling Strategies:
          * Prometheus sharding and federation
          * Long-term storage solutions (Cortex, Thanos)
          * Downsampling and retention policies
          * Cross-datacenter replication

        Logging Architecture and Management:

        1. Structured Logging Design:
        - Log Format Standards:
          * JSON structured logs for machine parsing
          * Consistent field naming and data types
          * Hierarchical context and nested objects
          * Timestamp standardization (ISO 8601)
        - Contextual Information:
          * Correlation IDs for request tracing
          * User and session identifiers
          * Service and version information
          * Environment and deployment context
        - Log Levels and Categorization:
          * Debug: Detailed diagnostic information
          * Info: General operational information
          * Warn: Potentially harmful situations
          * Error: Error events with potential impact
          * Fatal: Severe errors causing termination

        2. Log Aggregation and Processing:
        - ELK Stack (Elasticsearch, Logstash, Kibana):
          * Logstash for log processing and transformation
          * Elasticsearch for distributed search and storage
          * Kibana for visualization and dashboards
          * Beats for lightweight data shippers
        - Modern Alternatives:
          * Fluent Bit and Fluentd for log forwarding
          * Vector for high-performance log processing
          * Loki for logs-as-metrics approach
          * Splunk for enterprise log management
        - Cloud-Native Solutions:
          * AWS CloudWatch Logs and Insights
          * Google Cloud Logging (formerly Stackdriver)
          * Azure Monitor and Log Analytics
          * Datadog Log Management

        3. Log Storage and Retention:
        - Storage Optimization:
          * Log compression and archival strategies
          * Hot, warm, and cold storage tiers
          * Index lifecycle management
          * Cost optimization techniques
        - Retention Policies:
          * Compliance and regulatory requirements
          * Business value vs storage costs
          * Automated deletion and archival
          * Data sovereignty considerations

        Distributed Tracing Systems:

        1. OpenTelemetry Standards:
        - Instrumentation Framework:
          * Auto-instrumentation for popular frameworks
          * Manual instrumentation APIs
          * Semantic conventions for consistent data
          * Multi-language SDK support
        - Trace Data Model:
          * Spans: Individual units of work
          * Trace context propagation
          * Baggage for cross-cutting concerns
          * Sampling strategies and decisions
        - Backend Integration:
          * Jaeger for trace collection and analysis
          * Zipkin for distributed tracing
          * AWS X-Ray for cloud-native tracing
          * Datadog APM for application monitoring

        2. Tracing Implementation Patterns:
        - Sampling Strategies:
          * Head-based sampling at trace start
          * Tail-based sampling after trace completion
          * Adaptive sampling based on system load
          * Priority sampling for critical transactions
        - Context Propagation:
          * HTTP header propagation (W3C Trace Context)
          * Message queue context passing
          * Database and cache correlation
          * Cross-language context sharing
        - Performance Considerations:
          * Instrumentation overhead minimization
          * Asynchronous trace export
          * Batching and compression
          * Circuit breakers for trace collection

        Service Level Management:

        1. SLI/SLO/SLA Framework:
        - Service Level Indicators (SLIs):
          * Request latency: 95th percentile response time
          * Availability: Successful requests ratio
          * Throughput: Requests processed per second
          * Quality: Accuracy and correctness metrics
        - Service Level Objectives (SLOs):
          * Target performance levels for SLIs
          * Error budgets and burn rate calculations
          * Time window considerations (rolling vs calendar)
          * Multi-SLO objectives and dependencies
        - Service Level Agreements (SLAs):
          * Customer-facing commitments
          * Financial penalties and credits
          * Legal and contractual implications
          * SLA vs SLO alignment strategies

        2. Error Budget Management:
        - Error Budget Calculation:
          * Available vs consumed error budget
          * Burn rate analysis and alerting
          * Multi-window burn rate policies
          * Budget recovery and reset strategies
        - Policy Implementation:
          * Development velocity vs reliability trade-offs
          * Release freezes and rollback procedures
          * Post-incident improvement actions
          * Reliability investment decisions

        Alerting and Incident Management:

        1. Alerting Strategy and Design:
        - Alert Types and Patterns:
          * Symptom-based alerting (user impact)
          * Cause-based alerting (system failures)
          * Threshold-based static alerts
          * Anomaly detection dynamic alerts
        - Alert Quality Optimization:
          * Signal vs noise ratio improvement
          * Alert fatigue prevention
          * Actionable and specific alert content
          * Context-rich alert information
        - Escalation and Routing:
          * Multi-tier escalation policies
          * Team-based alert routing
          * Time-based escalation rules
          * Holiday and vacation coverage

        2. Advanced Alerting Techniques:
        - Machine Learning for Anomaly Detection:
          * Baseline establishment and drift detection
          * Seasonal pattern recognition
          * Multi-dimensional anomaly detection
          * False positive reduction strategies
        - Correlation and Suppression:
          * Related alert grouping
          * Root cause vs symptom identification
          * Dependency-based alert suppression
          * Maintenance window handling
        - Predictive Alerting:
          * Trend-based capacity alerts
          * Resource exhaustion prediction
          * Seasonal demand forecasting
          * Proactive scaling recommendations

        Real-world Implementation Examples:

        1. Google SRE Monitoring Practices:
        - Four Golden Signals:
          * Latency: Time to process requests
          * Traffic: Demand on the system
          * Errors: Rate of failed requests
          * Saturation: Resource utilization levels
        - Implementation Strategies:
          * Borgmon monitoring system (inspiration for Prometheus)
          * Distributed tracing with Dapper
          * SLO-based alerting and error budgets
          * Postmortem culture and learning

        2. Netflix Observability Platform:
        - Atlas Monitoring:
          * Dimensional time-series data platform
          * High-cardinality metrics support
          * Real-time streaming analytics
          * Automated anomaly detection
        - Distributed Tracing:
          * Zipkin for request flow analysis
          * Chaos engineering observability
          * Microservices dependency mapping
          * Performance optimization insights

        3. Uber's M3 Monitoring Stack:
        - M3 Time-Series Platform:
          * Distributed time-series database
          * High availability and scalability
          * Multi-region replication
          * Cost-efficient storage aggregation
        - Observability Architecture:
          * Jaeger for distributed tracing
          * ELK stack for log analysis
          * Real-time alerting and dashboards
          * Business metric correlation

        4. Shopify Observability Evolution:
        - Monitoring Modernization:
          * Migration from legacy monitoring
          * Kubernetes-native observability
          * Multi-cloud monitoring strategy
          * Developer self-service platforms
        - Business Metric Integration:
          * Revenue and conversion tracking
          * Customer experience monitoring
          * Merchant success metrics
          * Platform reliability correlation

        Observability Platform Architecture:

        1. Unified Observability Platforms:
        - Single Pane of Glass:
          * Correlated metrics, logs, and traces
          * Unified query and visualization
          * Cross-signal navigation and drilling
          * Consistent user experience
        - Data Correlation Strategies:
          * Trace ID propagation across signals
          * Service and resource tagging
          * Time-based correlation windows
          * Machine learning for relationship discovery
        - Multi-Tenancy and Access Control:
          * Team-based data isolation
          * Role-based access controls
          * Data retention policies per tenant
          * Cost allocation and chargeback

        2. Observability Data Pipeline:
        - Data Collection Architecture:
          * Agent-based vs agentless collection
          * Push vs pull data models
          * Edge processing and filtering
          * Data format standardization
        - Stream Processing:
          * Real-time data transformation
          * Aggregation and enrichment
          * Anomaly detection pipelines
          * Alert generation and routing
        - Storage and Indexing:
          * Time-series database optimization
          * Log storage and search indexing
          * Trace sampling and storage
          * Data lifecycle management

        Performance and Cost Optimization:

        1. Observability Data Management:
        - Sampling and Aggregation:
          * Intelligent sampling strategies
          * Pre-aggregation for common queries
          * Data downsampling over time
          * Cardinality reduction techniques
        - Storage Optimization:
          * Compression algorithms and techniques
          * Storage tiering and archival
          * Query optimization and caching
          * Infrastructure cost monitoring

        2. Operational Efficiency:
        - Self-Service Capabilities:
          * Developer-friendly instrumentation
          * Automated dashboard generation
          * Template-based monitoring setup
          * Documentation and best practices
        - Automation and Integration:
          * Infrastructure as code for monitoring
          * CI/CD pipeline integration
          * Automated testing for observability
          * Continuous improvement processes

        Practice Questions:

        Architecture Design:
        1. "Design observability platform for microservices architecture: 100 services, 1M requests/second. Include metrics, logs, traces, and alerting with <5% overhead."
        2. "Build monitoring system for global e-commerce platform: Multi-region deployment, business metrics integration, and real-time anomaly detection."
        3. "Implement SLO framework for cloud platform: Define SLIs, error budgets, alerting policies, and incident response for 99.9% availability target."

        Scalability and Performance:
        1. "Design log aggregation system for 10TB daily logs: Handle peak ingestion, enable real-time search, and optimize storage costs."
        2. "Implement distributed tracing for high-throughput system: 1M traces/second with 1% sampling rate, minimize performance impact."
        3. "Build metrics platform for IoT monitoring: 1M devices, 100 metrics/device/minute, real-time dashboards and alerting."

        Operational Excellence:
        1. "Design alerting strategy for reducing alert fatigue: Current 1000 alerts/day with 90% false positive rate, target actionable alerts only."
        2. "Implement observability for chaos engineering: Monitor system behavior during failure injection, measure blast radius and recovery time."
        3. "Build correlation system linking business metrics to technical metrics: Revenue impact analysis for system outages and performance degradation."

        Cost and Efficiency:
        1. "Optimize observability costs for startup scaling 10x: Balance monitoring coverage with budget constraints, prioritize critical signals."
        2. "Design multi-tenant observability platform: Isolated data access, shared infrastructure, cost allocation per team/product."
        3. "Implement observability data lifecycle: Hot/warm/cold storage strategy, automated archival, compliance requirements."

      resources:
        - title: "Site Reliability Engineering (Google)"
          url: "https://sre.google/sre-book/table-of-contents/"
          description: "Comprehensive SRE practices including monitoring and alerting"
        - title: "Observability Engineering (O'Reilly)"
          url: "https://www.oreilly.com/library/view/observability-engineering/9781492076438/"
          description: "Modern observability practices and tooling"
        - title: "Prometheus Monitoring Documentation"
          url: "https://prometheus.io/docs/"
          description: "Open-source monitoring system and time series database"
        - title: "OpenTelemetry Specification"
          url: "https://opentelemetry.io/docs/"
          description: "Vendor-neutral observability framework and standards"
        - title: "Distributed Systems Observability (O'Reilly)"
          url: "https://www.oreilly.com/library/view/distributed-systems-observability/9781492033431/"
          description: "Observability strategies for distributed architectures"
        - title: "The Art of Monitoring"
          url: "https://artofmonitoring.com/"
          description: "Practical monitoring implementation guide"
        - title: "Netflix Atlas Monitoring Platform"
          url: "https://netflix.github.io/atlas-docs/"
          description: "Dimensional time-series monitoring system"
        - title: "Google SRE Workbook: SLI/SLO Implementation"
          url: "https://sre.google/workbook/implementing-slos/"
          description: "Practical SLI/SLO implementation guidance"
      time_estimate: 130

    - day: 24
      topic: "Rate Limiting & Traffic Management"
      activity: "Master advanced rate limiting algorithms, traffic shaping, DDoS protection, and resilience patterns for high-scale distributed systems."
      detailed_content: |
        Rate Limiting Fundamentals and Algorithms:

        1. Core Rate Limiting Algorithms:
        - Token Bucket Algorithm:
          * Bucket capacity and token refill rate
          * Burst handling and sustained rate control
          * Implementation with timestamps and counters
          * Memory efficiency and performance characteristics
          * Use cases: API rate limiting, network traffic shaping
        - Leaky Bucket Algorithm:
          * Constant output rate regardless of input bursts
          * Queue-based implementation with overflow handling
          * Smooth traffic shaping and congestion control
          * Comparison with token bucket trade-offs
          * Use cases: Network QoS, traffic smoothing
        - Fixed Window Counter:
          * Simple time-window based counting
          * Reset behavior and boundary spike issues
          * Memory and computational efficiency
          * Implementation with expiring counters
          * Use cases: Basic API limiting, simple quotas
        - Sliding Window Log:
          * Precise rate limiting with event timestamps
          * Memory overhead for storing request logs
          * Accurate burst detection and control
          * Scalability challenges with high request rates
          * Use cases: Premium API tiers, strict compliance
        - Sliding Window Counter:
          * Hybrid approach combining fixed windows
          * Weighted calculation for smoother transitions
          * Balance between accuracy and efficiency
          * Implementation with multiple time buckets
          * Use cases: Production API gateways, fair usage

        2. Advanced Rate Limiting Techniques:
        - Adaptive Rate Limiting:
          * Dynamic rate adjustment based on system load
          * Machine learning for traffic pattern recognition
          * Predictive scaling and proactive limiting
          * Performance metrics integration
        - Hierarchical Rate Limiting:
          * Multi-level rate limiting (global, tenant, user)
          * Priority-based rate allocation
          * Quota distribution and inheritance
          * Fair sharing algorithms
        - Contextual Rate Limiting:
          * Request type and endpoint-specific limits
          * Geographic and time-based variations
          * User tier and subscription-based limits
          * Business rule integration

        Distributed Rate Limiting Architecture:

        1. Centralized vs Distributed Approaches:
        - Centralized Rate Limiting:
          * Single source of truth for rate decisions
          * Consistent global rate enforcement
          * Single point of failure concerns
          * Network latency and bottleneck issues
        - Distributed Rate Limiting:
          * Local rate limiting with global coordination
          * Eventually consistent rate enforcement
          * Improved performance and availability
          * Synchronization and drift challenges
        - Hybrid Approaches:
          * Local rate limiting with periodic synchronization
          * Emergency local limits with global overrides
          * Multi-tier rate limiting strategies
          * Graceful degradation during network partitions

        2. Implementation Patterns:
        - Redis-Based Distributed Limiting:
          * Lua scripts for atomic rate operations
          * Redis Cluster for high availability
          * Key design and expiration strategies
          * Performance optimization techniques
        - Database-Based Rate Limiting:
          * Time-series tables for rate tracking
          * Optimistic locking for concurrent updates
          * Batch processing for efficiency
          * Cleanup and maintenance strategies
        - In-Memory Grid Solutions:
          * Hazelcast and Apache Ignite integration
          * Distributed counters and maps
          * Replication and consistency models
          * Performance and scalability characteristics

        DDoS Protection and Mitigation:

        1. DDoS Attack Types and Characteristics:
        - Volumetric Attacks:
          * UDP floods and amplification attacks
          * ICMP floods and network saturation
          * Bandwidth exhaustion strategies
          * Network-level detection and mitigation
        - Protocol Attacks:
          * SYN floods and TCP state exhaustion
          * Ping of death and fragmentation attacks
          * Connection table exhaustion
          * Protocol anomaly detection
        - Application Layer Attacks:
          * HTTP floods and slowloris attacks
          * SQL injection and XSS amplification
          * Resource exhaustion attacks
          * Application-specific vulnerabilities

        2. Multi-Layer DDoS Protection:
        - Network Layer Protection:
          * BGP blackholing and null routing
          * Upstream provider coordination
          * Anycast distribution and load spreading
          * Network appliance and firewall rules
        - Transport Layer Protection:
          * SYN cookies and TCP optimization
          * Connection rate limiting
          * Proxy and load balancer protection
          * Geographic and IP reputation filtering
        - Application Layer Protection:
          * Request validation and filtering
          * CAPTCHA and human verification
          * Behavioral analysis and bot detection
          * Rate limiting and traffic shaping

        3. Advanced Protection Techniques:
        - Machine Learning for Attack Detection:
          * Traffic pattern analysis and anomaly detection
          * Behavioral fingerprinting and classification
          * Real-time threat intelligence integration
          * Adaptive threshold adjustment
        - Proof of Work Challenges:
          * Client-side computational challenges
          * Dynamic difficulty adjustment
          * Resource exhaustion protection
          * User experience balance
        - Traffic Fingerprinting:
          * Request signature analysis
          * Device and browser fingerprinting
          * Geolocation and timing analysis
          * Reputation scoring systems

        Traffic Management and Resilience Patterns:

        1. Circuit Breaker Pattern:
        - Circuit States and Transitions:
          * Closed: Normal operation with monitoring
          * Open: Fast-fail to prevent cascade failures
          * Half-open: Gradual recovery testing
          * State transition thresholds and timeouts
        - Implementation Strategies:
          * Failure rate and latency-based triggers
          * Exponential backoff and jitter
          * Graceful degradation responses
          * Service mesh integration
        - Advanced Circuit Breaker Features:
          * Per-endpoint and per-user circuit breakers
          * Bulkhead isolation for different resources
          * Priority-based circuit breaking
          * Monitoring and alerting integration

        2. Bulkhead Pattern:
        - Resource Isolation Strategies:
          * Thread pool isolation for different services
          * Connection pool separation
          * Memory and CPU resource partitioning
          * Network bandwidth allocation
        - Implementation Approaches:
          * Container-based isolation
          * Process-level separation
          * Application-level resource pools
          * Cloud resource grouping
        - Capacity Planning:
          * Resource allocation optimization
          * Load testing and capacity modeling
          * Dynamic resource scaling
          * Cost-performance trade-offs

        3. Load Shedding and Graceful Degradation:
        - Load Shedding Techniques:
          * Request priority classification
          * Queue management and dropping policies
          * Sampling-based load reduction
          * Feature toggle-based shedding
        - Graceful Degradation Strategies:
          * Non-essential feature disabling
          * Simplified response modes
          * Cached response serving
          * Fallback service integration
        - Quality of Service (QoS):
          * Service level prioritization
          * Resource reservation systems
          * Dynamic priority adjustment
          * SLA enforcement mechanisms

        Real-world Implementation Examples:

        1. Netflix Traffic Management:
        - Hystrix Circuit Breaker:
          * Service-to-service protection
          * Real-time failure detection
          * Fallback mechanisms
          * Dashboard and monitoring
        - Zuul API Gateway:
          * Dynamic routing and filtering
          * Rate limiting and throttling
          * Request transformation
          * Security policy enforcement

        2. Twitter Rate Limiting:
        - Multi-Tier Rate Limiting:
          * User-based and application-based limits
          * Endpoint-specific rate limits
          * Time window variations
          * Premium tier allowances
        - Real-time Rate Enforcement:
          * Redis-based distributed limiting
          * Sub-second rate calculations
          * Burst handling strategies
          * Rate limit headers and communication

        3. Cloudflare DDoS Protection:
        - Global Anycast Network:
          * Traffic distribution across edge locations
          * Local DDoS mitigation
          * Capacity aggregation
          * Attack source isolation
        - Intelligent Attack Detection:
          * Machine learning classification
          * Real-time threat intelligence
          * Behavioral analysis
          * Automated mitigation rules

        4. GitHub API Rate Limiting:
        - Sophisticated Rate Limiting:
          * Multiple rate limit categories
          * Search API special handling
          * Authenticated vs anonymous limits
          * Abuse detection and response
        - User Experience Optimization:
          * Rate limit status communication
          * Predictive rate limit warnings
          * Best practice documentation
          * Developer tooling integration

        Traffic Shaping and Quality of Service:

        1. Traffic Shaping Algorithms:
        - Weighted Fair Queuing (WFQ):
          * Bandwidth allocation based on weights
          * Fair sharing among competing flows
          * Latency optimization for high-priority traffic
          * Implementation complexity considerations
        - Class-Based Queuing (CBQ):
          * Hierarchical traffic classification
          * Multi-level priority handling
          * Bandwidth guarantees and limits
          * Policy-based traffic management
        - Random Early Detection (RED):
          * Proactive congestion avoidance
          * Probabilistic packet dropping
          * Queue length management
          * TCP-friendly behavior

        2. Service Mesh Traffic Management:
        - Envoy Proxy Configuration:
          * Rate limiting filter configuration
          * Circuit breaker implementation
          * Load balancing algorithms
          * Traffic routing policies
        - Istio Traffic Policies:
          * DestinationRule rate limiting
          * VirtualService routing rules
          * Fault injection for testing
          * Security policy integration
        - Linkerd Traffic Management:
          * Service profiles and routes
          * Request-level load balancing
          * Retry and timeout policies
          * Traffic splitting and canary deployments

        Performance Optimization and Scalability:

        1. Rate Limiting Performance:
        - Memory Optimization:
          * Efficient data structures for counters
          * Memory pooling and recycling
          * Garbage collection optimization
          * Cache-friendly algorithms
        - CPU Optimization:
          * Lock-free data structures
          * Batching and amortization
          * Algorithmic complexity reduction
          * Hardware acceleration opportunities
        - Network Optimization:
          * Local rate limiting to reduce latency
          * Batched coordination messages
          * Compression and protocol optimization
          * Edge-based rate limiting

        2. Scalability Patterns:
        - Horizontal Scaling:
          * Stateless rate limiter design
          * Consistent hashing for distribution
          * Auto-scaling based on traffic patterns
          * Cross-region coordination
        - Vertical Scaling:
          * Multi-core optimization
          * Memory and storage scaling
          * Hardware acceleration
          * Performance profiling and tuning

        Monitoring and Operations:

        1. Rate Limiting Metrics:
        - Key Performance Indicators:
          * Rate limit hit rates and violations
          * Request latency distribution
          * Throughput and capacity utilization
          * Error rates and false positives
        - Business Impact Metrics:
          * User experience degradation
          * Revenue impact analysis
          * Customer satisfaction correlation
          * Competitive benchmark comparison

        2. Operational Excellence:
        - Configuration Management:
          * Dynamic rate limit adjustment
          * A/B testing for rate policies
          * Emergency rate limit overrides
          * Policy versioning and rollback
        - Incident Response:
          * Automated DDoS response procedures
          * Escalation policies and runbooks
          * Communication and coordination
          * Post-incident analysis and improvement

        Practice Questions:

        Algorithm Design:
        1. "Design rate limiting system for API gateway: 10K requests/second, per-user and global limits, distributed across 50 servers. Choose appropriate algorithm and justify."
        2. "Implement adaptive rate limiting for e-commerce platform: Handle traffic spikes during sales events, maintain performance for legitimate users."
        3. "Build multi-tier rate limiting for SaaS platform: Free, premium, enterprise tiers with different limits, graceful degradation strategies."

        DDoS Protection:
        1. "Design DDoS protection for online gaming platform: Handle 1Tbps attacks, maintain <50ms latency for legitimate players during attacks."
        2. "Implement intelligent bot detection system: Distinguish between legitimate crawlers, malicious bots, and human traffic at scale."
        3. "Build traffic analysis system: Real-time anomaly detection, automatic mitigation, and false positive minimization."

        Resilience Patterns:
        1. "Design circuit breaker system for microservices: 100 services, cascading failure prevention, service dependency management."
        2. "Implement bulkhead pattern for multi-tenant system: Resource isolation, fair sharing, performance guarantees per tenant."
        3. "Build load shedding mechanism for video streaming: Priority-based request handling, quality degradation, capacity management."

        Performance and Scale:
        1. "Optimize rate limiting performance: Handle 1M requests/second with <1ms overhead, memory-efficient distributed counters."
        2. "Design global rate limiting system: Multi-region deployment, eventual consistency, network partition handling."
        3. "Implement traffic shaping for CDN: Bandwidth allocation, QoS policies, customer SLA enforcement across global edge locations."

      resources:
        - title: "Rate Limiting Strategies (Google Cloud)"
          url: "https://cloud.google.com/architecture/rate-limiting-strategies-techniques"
          description: "Comprehensive rate limiting implementation strategies"
        - title: "AWS Shield DDoS Protection"
          url: "https://aws.amazon.com/shield/"
          description: "DDoS protection strategies and implementation"
        - title: "Netflix Hystrix Circuit Breaker"
          url: "https://github.com/Netflix/Hystrix/wiki"
          description: "Circuit breaker pattern implementation and best practices"
        - title: "Cloudflare DDoS Protection System"
          url: "https://blog.cloudflare.com/cloudflare-ddos-protection-system/"
          description: "Large-scale DDoS protection architecture"
        - title: "Envoy Rate Limiting Configuration"
          url: "https://www.envoyproxy.io/docs/envoy/latest/configuration/http/http_filters/rate_limit_filter"
          description: "Service mesh rate limiting implementation"
        - title: "Redis Rate Limiting Patterns"
          url: "https://redis.com/redis-best-practices/basic-rate-limiting/"
          description: "Distributed rate limiting with Redis"
        - title: "Traffic Management in Istio"
          url: "https://istio.io/latest/docs/concepts/traffic-management/"
          description: "Service mesh traffic management and policies"
        - title: "Resilience Engineering Patterns"
          url: "https://www.oreilly.com/library/view/release-it-2nd/9781680504552/"
          description: "Comprehensive resilience patterns and practices"
      time_estimate: 120

    - day: 25
      topic: "Distributed Systems Consensus"
      activity: "Master consensus algorithms, leader election, distributed coordination, and Byzantine fault tolerance for building reliable distributed systems."
      detailed_content: |
        Consensus Fundamentals:
        - The Consensus Problem: Achieving agreement among distributed processes despite failures
        - Safety Property: Never returning incorrect results (consistency)
        - Liveness Property: Eventually returning correct results (availability)
        - FLP Impossibility Theorem: Consensus impossible in asynchronous systems with one faulty process
        - Partial Synchrony: Real-world assumption for practical consensus algorithms

        Raft Consensus Algorithm:
        - Leader Election: Randomized timeouts, term numbers, majority votes
        - Log Replication: Append-only logs, consistency checks, commit index
        - Safety Guarantees: Leader completeness, state machine safety
        - Configuration Changes: Joint consensus for membership updates
        - Log Compaction: Snapshots to prevent unbounded log growth
        - Implementation Considerations: Heartbeats, election timeouts, batching
        - Real-world Usage: etcd (Kubernetes), TiKV (TiDB), CockroachDB Raft groups

        Paxos Family of Algorithms:
        - Basic Paxos: Single-decree consensus with prepare/promise/accept/accepted phases
        - Multi-Paxos: Optimization for multiple decisions with stable leader
        - Fast Paxos: Reduced latency by skipping prepare phase when possible
        - Flexible Paxos: Separate quorums for phase 1 and phase 2
        - EPaxos (Egalitarian): Leaderless consensus for geographic distribution
        - Roles and Phases: Proposers (leaders), Acceptors (voters), Learners (followers)
        - Production Examples: Google Chubby, Apache Cassandra (lightweight Paxos)

        Byzantine Fault Tolerance:
        - Byzantine Failures: Arbitrary behavior including malicious attacks
        - PBFT (Practical Byzantine Fault Tolerance): 3f+1 nodes to tolerate f failures
        - Three-phase Protocol: Pre-prepare, prepare, commit phases
        - View Changes: Leader replacement when primary is suspected faulty
        - Tendermint: Modern BFT consensus for blockchain applications
        - HotStuff: Linear communication complexity BFT with pipelining
        - Applications: Blockchain systems, critical infrastructure, financial systems

        Leader Election Patterns:
        - Bully Algorithm: Highest ID node becomes leader
        - Ring Algorithm: Token passing in logical ring topology
        - Raft Leader Election: Randomized timeouts, term-based voting
        - Apache Zookeeper: Ephemeral sequential nodes for leader election
        - Consul: Raft-based leadership with service discovery integration
        - Kubernetes Leader Election: Lease-based coordination using etcd

        Distributed Coordination Services:
        - Apache Zookeeper: Configuration management, service discovery, leader election
        - etcd: Kubernetes backing store, distributed key-value store
        - Consul: Service mesh coordination, health checking, KV store
        - Amazon DynamoDB Global Tables: Multi-region consensus via vector clocks
        - Google Spanner: TrueTime API for globally consistent timestamps
        - Coordination Primitives: Locks, barriers, queues, configuration management

        Implementation Challenges:
        - Network Partitions: Split-brain prevention, quorum-based decisions
        - Clock Synchronization: Logical clocks (Lamport, vector), physical clock skew
        - Failure Detection: Heartbeats, timeouts, phi-accrual failure detectors
        - Recovery and Catchup: Log replay, snapshot transfer, incremental updates
        - Performance Optimization: Batching, pipelining, read leases
        - Testing: Jepsen-style partition testing, chaos engineering, linearizability

        Trade-offs and Design Decisions:
        - Throughput vs Latency: Batching and pipelining effects
        - Fault Tolerance vs Performance: Replication factor impact
        - Consistency Models: Strong vs eventual consistency requirements
        - Geographical Distribution: Wide-area network considerations
        - Read vs Write Performance: Read replicas, leader leases
        - Operational Complexity: Monitoring, debugging, capacity planning

        Production Architecture Patterns:
        - Multi-Raft: Separate Raft groups per partition (TiKV, CockroachDB)
        - Hierarchical Consensus: Region-level and global-level coordination
        - Cross-Region Replication: Consensus across data centers
        - State Machine Replication: Application-level consistency via consensus
        - Consensus as a Service: Managed coordination services (etcd, Consul)
        - Hybrid Approaches: Combining different algorithms for different layers

      practice_questions:
        capacity_estimation:
          - question: "Design consensus system for 1000-node cluster with 10K ops/sec"
            answer: "Use Multi-Raft with 200 Raft groups (each 5 nodes). Per group: 50 ops/sec = 1 leader + 4 followers. Leaders: 200 nodes (handle writes). Followers: 800 nodes (replicas). Write amplification: 5x (1 leader + 4 followers) = 50K total writes/sec. Log size: 1KB/op × 10K × 86400 = 864GB/day. Replication bandwidth: 1KB × 50K = 50MB/sec. Quorum: 3/5 nodes (majority). Latency: 2 RTT (propose + commit) = 10ms same-region. Cost: $50K/month (1000 nodes × $50)."
          - question: "Calculate quorum sizes for 5-region deployment with 3 nodes per region"
            answer: "Total: 15 nodes (5 regions × 3). For majority: Need ⌈15/2⌉ + 1 = 8 nodes. Problem: Network partition splits regions. Solution: Use 3 nodes per region + cross-region quorum. Write quorum: 2 nodes in leader region + 1 node in 1 other region = 3 nodes (same as single-region, but cross-region latency). Read quorum: 1 node (leader serves reads with lease). Alternative: 2-1-1-1-1 topology (2 nodes in primary, 1 in each other) = faster writes (only 2 local nodes needed for quorum), but less fault tolerance."
          - question: "Estimate bandwidth for Raft log replication with 1MB/sec write load"
            answer: "Replication factor: 3 (1 leader + 2 followers). Bandwidth: 1MB/sec × 3 = 3MB/sec total. Per follower: 1MB/sec. Network: Leader → Follower1 (1MB/sec) + Leader → Follower2 (1MB/sec) = 2MB/sec outbound from leader. Compression: Gzip 3x = 667KB/sec per follower = 1.3MB/sec total (2.3x savings). Batching: 100 ops per batch reduces overhead by 10x. Snapshot transfer: Daily snapshot (10GB) = 116KB/sec avg. Total: 2MB/sec + 116KB/sec = 2.1MB/sec."
          - question: "Size etcd cluster for Kubernetes control plane with 10K nodes"
            answer: "etcd usage: Store cluster state (nodes, pods, services). Data: 10K nodes × 10KB = 100MB. Pods: 50K pods × 5KB = 250MB. ConfigMaps/Secrets: 50MB. Total: 400MB. With history (1000 revisions): 400GB. Cluster: 5 nodes (leader + 4 followers). Quorum: 3/5. Hardware: 8 vCPU, 16GB RAM, 100GB SSD per node. Write rate: 5K ops/sec (pod updates). Read rate: 50K ops/sec (watch streams). Latency: 10ms writes (2 RTT), 1ms reads (local). Cost: $2K/month (5 nodes × $400)."

        conceptual:
          - question: "Explain why FLP theorem doesn't prevent practical consensus systems"
            answer: "FLP theorem: Impossible to achieve consensus in asynchronous system with even 1 faulty node (proven by Fischer, Lynch, Paterson). Why practical systems work: 1) Assumption relaxation: Use partially synchronous model (bounded delays, not infinite). 2) Timeouts: Detect failures via heartbeats (not perfect but works 99.9%). 3) Randomization: Break determinism (e.g., random leader election timeouts). 4) Practical trade-off: Accept temporary unavailability during partitions (CP systems) or accept weaker consistency (AP systems). Raft/Paxos: Partially synchronous + leader election with timeouts. Result: Consensus works in practice despite FLP."
          - question: "Compare Raft vs Paxos trade-offs for different use cases"
            answer: "Raft: Pros: Simpler to understand (log-based, strong leader), easier to implement, better for single-datacenter. Cons: Leader bottleneck, slower leader election (timeout-based). Use for: etcd, Consul, distributed databases (TiKV). Latency: 10ms. Paxos (Multi-Paxos): Pros: Faster leader election (no timeout), leader can be any node. Cons: Complex, harder to implement correctly. Use for: Google Chubby, Spanner. Latency: 5ms. EPaxos (Egalitarian Paxos): No leader, lower latency for cross-region (1 RTT). Use for: WAN consensus. Choose: Raft for simplicity, Paxos for performance."
          - question: "How does Byzantine fault tolerance change consensus requirements?"
            answer: "Crash fault tolerance (CFT): Nodes fail by stopping (Raft, Paxos). Tolerates f failures with 2f+1 nodes (majority). Byzantine fault tolerance (BFT): Nodes fail by sending malicious/incorrect messages. Tolerates f failures with 3f+1 nodes (vs 2f+1). Example: 1 Byzantine node needs 4 total nodes (vs 3 for crash). Reason: Need majority of honest nodes to override Byzantine nodes. PBFT algorithm: 3-phase commit (pre-prepare, prepare, commit) vs 2-phase (Raft). Latency: 3-5× slower. Cost: 1.5× more nodes. Use cases: Blockchain (untrusted nodes), financial systems. Most systems: Use CFT (cheaper, faster, trusted environment)."
          - question: "Why do most consensus algorithms require majority quorums?"
            answer: "Majority quorum (⌈N/2⌉ + 1): Ensures any two quorums overlap in at least 1 node = guarantees consistency. Example: 5 nodes, quorum = 3. Any 2 quorums of 3 must share ≥1 node. Why critical: Prevents split-brain (two leaders with different state). Alternative: Minority quorums allow split-brain. Example: 2/5 quorum → 2 nodes accept write A, different 2 nodes accept write B = inconsistency. Trade-off: Majority = higher availability cost (need >50% nodes alive). Quorum intersection theorem: R + W > N guarantees consistency (read quorum + write quorum > total nodes)."
          - question: "Explain the role of logical vs physical time in consensus"
            answer: "Physical time (wall clock): Actual time (NTP). Problem: Clock skew (10-100ms), clock drift. Can't trust for ordering. Use: Timeouts, leases (with skew bounds). Logical time: Lamport timestamps, vector clocks. Guarantees: Causal ordering (if event A causes B, timestamp(A) < timestamp(B)). Use: Conflict resolution, ordering events without synchronized clocks. Hybrid: TrueTime (Google Spanner) = physical time with uncertainty bounds (<7ms). Use: Global ordering with bounded wait. Raft: Uses logical log indexes for ordering (not time). Result: No clock synchronization needed for correctness (only for performance/timeouts)."

        trade_offs:
          - question: "Multi-Paxos vs Raft: when to choose each for your system?"
            answer: "Multi-Paxos: Pros: Flexible leader (any node), faster leader election (no timeout), optimized for low latency (2 RTT vs 3). Cons: Complex implementation, edge cases hard to handle. Use for: Google (Chubby, Spanner), high-performance systems, cross-region WAN. Latency: 5ms. Raft: Pros: Simple, well-documented, easier to implement correctly, strong leader model. Cons: Slower leader election (timeout-based: 150-300ms), leader bottleneck. Use for: Most systems (etcd, Consul, TiKV, CockroachDB). Latency: 10ms. Choose Raft unless: Need absolute lowest latency or have expert team to implement Paxos correctly."
          - question: "Strong consistency vs performance: where to make compromises?"
            answer: "Strong consistency (linearizable): Every read sees latest write. Cost: 2 RTT latency (10-50ms), leader bottleneck (10K writes/sec max per leader). Use for: Financial transactions, inventory, leader election. Compromise 1: Read from followers (stale reads, but faster: 1ms vs 10ms). Use for: Analytics, dashboards. Compromise 2: Eventual consistency (0 RTT coordination). Use for: Social media feeds, view counts. Compromise 3: Session consistency (user sees own writes). Use for: User profiles. Hybrid: 5% strong (critical paths) + 95% eventual (everything else) = 10× throughput. Choose: Strong only when data loss/inconsistency costs > latency costs."
          - question: "Cross-region consensus: trade-offs between latency and availability"
            answer: "Single-region: Latency 10ms, Availability 99.9% (1 DC failure = downtime). Multi-region (3 regions): Latency 100-200ms (cross-region RTT), Availability 99.99% (survives 1 region failure). Trade-off: 10-20× higher latency for 10× better availability. Mitigation: 1) Use read-only replicas in each region (reads 10ms, writes 100ms). 2) Partition data by geography (US users → US DC). 3) Async replication (eventual consistency, 1s lag). 4) Raft groups per region + global coordinator. Cost: 3× infrastructure. Choose: Single-region (latency critical, can tolerate downtime), multi-region (availability critical, can tolerate latency)."
          - question: "Leader-based vs leaderless: pros and cons for different workloads"
            answer: "Leader-based (Raft, Paxos): Pros: Simpler (single writer), strong consistency, total ordering. Cons: Leader bottleneck (10K writes/sec), single point of failure (need election on failure: 150ms). Use for: Metadata stores (etcd, ZooKeeper), configuration, locks. Leaderless (Dynamo, Cassandra): Pros: Higher throughput (all nodes accept writes), no election delay, multi-datacenter writes. Cons: Eventual consistency, conflict resolution needed, more complex. Use for: High-write workloads (IoT, logs), multi-region active-active. Hybrid: Leader per partition (Multi-Raft) = 100× throughput with strong consistency. Choose: Leader for consistency, leaderless for throughput/availability."
          - question: "Byzantine vs crash fault tolerance: cost-benefit analysis"
            answer: "Crash fault tolerance (CFT - Raft, Paxos): Nodes fail by stopping. Cost: 2f+1 nodes to tolerate f failures = 3 nodes for 1 failure. Latency: 2-3 RTT (10ms). Use: Trusted environment (same org). Byzantine fault tolerance (BFT - PBFT): Nodes fail by lying/attacking. Cost: 3f+1 nodes to tolerate f failures = 4 nodes for 1 failure (33% more). Latency: 3-5 RTT (25ms, 2.5× slower). Use: Untrusted environment (blockchain, multi-org). When to use BFT: Blockchain (untrusted miners), financial systems (regulatory). When CFT enough: Internal systems (trusted infrastructure). Cost: BFT = 1.5× more nodes + 2.5× latency = 3.75× total cost. Choose BFT only if Byzantine failures are real threat."

        scenario_based:
          - question: "Design leader election for microservices in Kubernetes environment"
            answer: "Use etcd (built into K8s) + leader election library. Architecture: 1) Each service instance tries to create lease in etcd with TTL=10s, 2) First instance to create = leader, 3) Leader renews lease every 5s (heartbeat), 4) If leader dies: Lease expires in 10s, new election starts. Library: Use client-go LeaderElector (Go) or kubernetes-client (Python). Failover: 10s (TTL expiry). Load: 1 write every 5s per service = negligible. Cost: Free (uses existing etcd). Alternative: Use distributed lock (Redis, ZooKeeper), but etcd already in K8s. Edge case: Network partition splits cluster → majority side continues, minority side blocks (safe)."
          - question: "Handle network partition in 5-node consensus cluster"
            answer: "Partition: 3 nodes (Group A) + 2 nodes (Group B). Raft behavior: Group A (majority) → Continues serving requests (can form quorum: 3 > 5/2). Group B (minority) → Blocks writes (cannot form quorum: 2 < 3). Reads: Group A serves reads, Group B rejects or serves stale. When partition heals: Group B syncs from Group A (log replay). Edge case: 2-2-1 split → If leader in 2-node group, new election in other 2-node group (both fail quorum). Node 1 joins either side → 3-node group wins. Mitigation: Odd number of nodes (5, 7) reduces tie probability. Monitoring: Detect partition via heartbeat failures, alert operators."
          - question: "Migrate from single-region to multi-region consensus safely"
            answer: "Steps: 1) Deploy new region nodes (3 regions × 3 nodes = 9 total). 2) Add nodes as followers (replication only, no voting). 3) Wait for full sync (check lag < 1s). 4) Update quorum config to include new nodes (9 nodes, quorum = 5). 5) Verify cross-region writes work (test failover). 6) Gradual traffic shift (10% → 50% → 100% over 1 week). Rollback: Keep old region nodes for 1 month (can revert config). Zero downtime: Yes (followers don't need quorum, just replication). Challenges: Cross-region latency (100ms writes), network partitions (test with chaos engineering). Cost: 3× infrastructure. Time: 2 months (testing, rollout)."
          - question: "Implement configuration management service using consensus"
            answer: "Use etcd/Consul for consensus + versioned key-value store. Architecture: 1) Store configs as versioned keys (namespace/service/config → version N), 2) Watch API for real-time updates (long polling), 3) Clients cache locally + watch for changes. Write: Client → Leader → Raft log → Replicate → Commit → Watchers notified (50ms e2e). Read: Local cache (1ms) or read from follower (10ms). Features: Atomic updates (compare-and-swap), rollback (revert to version N-1), audit log (who changed what when). Scale: 10K services × 10 configs = 100K keys = 100MB. Cost: $2K/month (5-node etcd cluster). Alternative: ZooKeeper (older, less user-friendly)."
          - question: "Design consensus-based distributed lock service"
            answer: "Use Raft (etcd, Consul) for distributed locks. API: AcquireLock(key, ttl) → lock_id, ReleaseLock(lock_id), RenewLock(lock_id). Implementation: 1) Client creates ephemeral key with TTL=10s + unique client_id, 2) If key doesn't exist → Lock acquired, 3) Client renews every 5s (heartbeat), 4) If client dies: TTL expires → Lock auto-released. Correctness: Fencing tokens (monotonic lock_id) prevent stale lock holder from writing after timeout. Scale: 100K locks = 100K keys = 10MB. Lock contention: Use queue (first-come-first-served). Cost: $2K/month. Alternative: Redis (single-node, not highly available). Use cases: Job scheduling, leader election, resource coordination."
          - question: "Build strongly consistent cache using consensus protocols"
            answer: "Use Raft + in-memory cache. Architecture: 1) Cache nodes form Raft cluster (5 nodes), 2) Writes go through Raft log (strong consistency), 3) Reads from local state machine (fast). Write: Client → Leader → Raft commit → Apply to cache (50ms). Read: Any node serves from local cache (1ms, linearizable with read quorum or leader lease). Eviction: LRU on each node (eventually consistent). Invalidation: Broadcast via Raft log. Scale: 100GB per node × 5 nodes = 500GB cache. Throughput: 10K writes/sec (leader limit), 1M reads/sec (all nodes). Cost: $10K/month. Use cases: Session store, feature flags, rate limiting. Alternative: Redis + Sentinel (weaker consistency, simpler)."

      resources:
        - title: "Raft Consensus Algorithm"
          url: "https://raft.github.io/"
          description: "Complete Raft specification with visualization and interactive demo"
        - title: "Lamport's Paxos Made Simple"
          url: "http://lamport.azurewebsites.net/pubs/paxos-simple.pdf"
          description: "Definitive paper on Paxos algorithm by its creator"
        - title: "PBFT: Practical Byzantine Fault Tolerance"
          url: "http://pmg.csail.mit.edu/papers/osdi99.pdf"
          description: "Foundational paper on practical Byzantine consensus"
        - title: "etcd Documentation"
          url: "https://etcd.io/docs/"
          description: "Production-grade consensus system powering Kubernetes"
        - title: "Jepsen: Distributed Systems Safety Research"
          url: "https://jepsen.io/"
          description: "Testing framework and analysis of consensus system failures"
        - title: "Designing Data-Intensive Applications - Chapter 9"
          url: "https://dataintensive.net/"
          description: "Consensus and atomic commit in distributed systems"
        - title: "MIT 6.824 Distributed Systems"
          url: "https://pdos.csail.mit.edu/6.824/"
          description: "Academic course with Raft lab implementation"
        - title: "Apache Zookeeper Internals"
          url: "https://zookeeper.apache.org/doc/current/zookeeperInternals.html"
          description: "ZAB consensus protocol and coordination service design"
      time_estimate: 135

    - day: 26
      topic: "System Integration & Architecture Patterns"
      activity: "Master comprehensive system architectures, integration patterns, and avoid common anti-patterns."
      detailed_content: |
        Core Architecture Patterns:
        - Monolithic: Single deployable unit, simpler initially but harder to scale
        - Layered: Separation of concerns (presentation, business, data layers)
        - Microservices: Service-oriented decomposition with independent deployment
        - Event-driven: Loose coupling through asynchronous event communication
        - Serverless/FaaS: Function-based execution, auto-scaling, pay-per-use
        - CQRS: Command Query Responsibility Segregation for read/write optimization
        - Hexagonal (Ports & Adapters): Business logic isolation from external concerns
        - Space-based: In-memory data grids for high scalability
        - Pipeline: Sequential data processing stages (ETL, stream processing)

        Integration Patterns:
        - Request-Response: Synchronous communication for immediate results
        - Fire-and-Forget: Asynchronous messaging without waiting for response
        - API Composition: Aggregate data from multiple services
        - Database per Service: Data ownership and independence
        - Saga Pattern: Distributed transaction management with compensation
        - Backend for Frontend (BFF): Service layer tailored for specific clients
        - Strangler Fig: Gradual legacy system migration
        - Circuit Breaker: Prevent cascade failures in service calls
        - Message Routing: Content-based and topic-based message distribution
        - Scatter-Gather: Parallel data collection and aggregation
        - Event Sourcing: Store events instead of current state
        - Bulkhead: Resource isolation to prevent resource exhaustion

        Anti-Patterns to Avoid:
        - Distributed Monolith: Microservices with tight coupling and shared databases
        - Chatty Interfaces: Excessive fine-grained API calls causing network overhead
        - Shared Database: Multiple services accessing the same database
        - Synchronous Communication Overuse: Creating tight coupling and cascade failures
        - God Service: Single service handling too many responsibilities
        - Data Inconsistency: Lack of eventual consistency handling
        - Distributed Transactions: Using 2PC in microservices (prefer saga pattern)

        Modern Architecture Concepts:
        - API Gateway: Single entry point for client requests with cross-cutting concerns
        - Service Mesh: Infrastructure layer for service-to-service communication
        - Sidecar Pattern: Deploy auxiliary services alongside main application
        - Ambassador Pattern: Proxy for external service communication
        - Container Orchestration: Kubernetes patterns for deployment and scaling
        - GitOps: Infrastructure and application deployment through Git workflows

        System Boundaries:
        - Domain-driven Design: Bounded contexts and ubiquitous language
        - Conway's Law: System structure mirrors organization communication patterns
        - Service Interfaces: Contract-first design with API versioning
        - Data Consistency Boundaries: Eventual consistency across service boundaries
        - Failure Domain Isolation: Contain failures within service boundaries

        Quality Attributes:
        - Scalability: Horizontal (scale out) vs vertical (scale up) approaches
        - Availability: System uptime and fault tolerance (different from reliability)
        - Reliability: System correctness and error handling
        - Elasticity: Dynamic scaling based on demand (auto-scaling)
        - Performance: Latency (response time) and throughput (requests per second)
        - Security: Authentication, authorization, encryption, defense in depth
        - Maintainability: Code organization, testing, documentation
        - Observability: Monitoring, logging, tracing for system understanding
        - Testability: Unit, integration, contract, chaos testing strategies
        - Deployability: CI/CD pipelines, blue-green, canary deployments

        Integration Trade-offs:
        - Synchronous vs Asynchronous: Latency vs complexity
        - Push vs Pull: Real-time updates vs resource usage
        - Choreography vs Orchestration: Decentralized vs centralized coordination
        - REST vs GraphQL vs gRPC: Different API paradigms and use cases
        - Event Streaming vs Message Queues: Kafka vs RabbitMQ/SQS patterns
      resources:
        - title: "Software Architecture Patterns"
          url: "https://www.oreilly.com/library/view/software-architecture-patterns/9781491971437/"
          description: "Common architecture patterns"
        - title: "Building Evolutionary Architectures"
          url: "https://www.thoughtworks.com/insights/books/building-evolutionary-architectures"
          description: "Adaptable system design"
        - title: "Microservices Anti-patterns"
          url: "https://microservices.io/patterns/microservices.html"
          description: "What to avoid in microservices design"
        - title: "Service Mesh Patterns"
          url: "https://www.oreilly.com/library/view/istio-up-and/9781492043775/"
          description: "Modern service communication patterns"
      practice_questions:
        estimation:
          - question: "A monolith serves 10K RPS. Split into 5 microservices with 2ms network overhead per call. If each request needs 3 service calls, what's the latency impact?"
            answer: "Monolith latency: Assume 50ms (in-memory calls, no network). Microservices: 3 service calls × 2ms network overhead = 6ms added latency. Total: 50ms + 6ms = 56ms (12% increase). But: Services now run in parallel (if independent). Parallel: max(service times) + network = 20ms + 6ms = 26ms (48% faster). Sequential (dependent calls): 50ms + 6ms = 56ms (12% slower). Real cost: Error rate (network failures: 0.1% per call × 3 = 0.3% vs 0% monolith). Throughput: Each service can scale independently = 10x throughput possible. Choose: Microservices for scale, monolith for latency."
          - question: "API Gateway handles 50K RPS with 5ms overhead. Backend services can handle 100K RPS. What's the bottleneck and how to scale?"
            answer: "Bottleneck: API Gateway (50K RPS < 100K backend capacity). Gateway overhead: 5ms processing + SSL termination + auth + routing. Scaling options: 1) Horizontal scale: Add 1 more gateway = 100K RPS (matches backend). Cost: $500/month per gateway × 2 = $1K. 2) Vertical scale: Bigger instance (4× CPU) = 80K RPS. Cost: $2K/month. 3) Optimize: Move auth to sidecar (removes 2ms) = 70K RPS. Choose: Horizontal (cheaper, better availability). New bottleneck: Backend at 100K RPS. Solution: Scale backends (add replicas)."
          - question: "Event-driven system processes 1M events/day. Each event triggers 3 downstream services. How many service calls per second at peak (assuming 10x daily average)?"
            answer: "Average: 1M events/day = 11.6 events/sec. Per event: 3 service calls = 11.6 × 3 = 34.8 calls/sec avg. Peak: 10× daily average = 116 events/sec × 3 = 348 calls/sec. With fan-out: If services process in parallel, 348 concurrent calls/sec. Service capacity: Each service needs 116 events/sec capacity. Failure handling: Dead letter queue for failed events (retry with backoff). Kafka: 3 topics (one per downstream service) = 348 messages/sec total = negligible load. Cost: $200/month for 3-node Kafka cluster."
        concepts:
          - question: "When would you choose microservices over monolith? What are the trade-offs?"
            answer: "Choose microservices when: 1) Scale (different services need different scaling: 100× for API, 1× for admin), 2) Team size (>20 engineers, need autonomy), 3) Polyglot (different tech stacks: Python ML, Go API), 4) Deployment frequency (deploy multiple times/day without full system restart). Monolith better when: Small team (<10), simple app, low traffic (<10K RPS), tight coupling (shared state). Trade-offs: Microservices = 3× complexity (distributed debugging, network failures, eventual consistency), 2× cost (overhead, duplication), but 10× scaling flexibility. Start monolith, migrate to microservices at scale inflection point."
          - question: "How do you handle distributed transactions without 2PC? Explain saga pattern."
            answer: "Saga pattern: Break transaction into local transactions + compensating actions. Example (hotel + flight booking): 1) Reserve hotel (local tx), 2) Book flight (local tx), 3) If flight fails: Cancel hotel (compensating tx). Types: Choreography (event-driven, services listen for events) vs Orchestration (central coordinator). Choreography: Hotel emits 'reserved' event → Flight service listens → Books flight. Pro: Decentralized. Con: Hard to trace. Orchestration: Saga coordinator calls Hotel → Flight → Payment. Pro: Easy to trace. Con: Single point of failure. Failure: Each step has compensating action (cancel, refund). Consistency: Eventual (not ACID). Use when: Distributed data, can't use 2PC."
          - question: "What's the difference between API Gateway and Service Mesh?"
            answer: "API Gateway: North-south traffic (external → internal). Functions: Auth, rate limiting, SSL termination, routing, caching. Sits at edge. Example: Kong, AWS API Gateway. Latency: 5-10ms overhead. Service Mesh: East-west traffic (service → service). Functions: Load balancing, retry, circuit breaking, tracing, mTLS. Runs as sidecar proxy (Envoy). Example: Istio, Linkerd. Latency: 1-2ms overhead per hop. Overlap: Both do routing, observability. Use both: Gateway for external API, mesh for internal services. Alternative: Gateway only (simpler, less overhead). Cost: Gateway $500/month, mesh $2K/month (more infrastructure)."
          - question: "How do you prevent cascade failures in a microservices architecture?"
            answer: "Cascade failure: Service A fails → Service B (depends on A) times out → Service C (depends on B) overloaded. Prevention: 1) Circuit breaker: After N failures (5), open circuit = fast fail (no timeout wait). Close after cooldown (30s). 2) Timeouts: Aggressive timeouts (1s) prevent thread exhaustion. 3) Bulkheads: Separate thread pools per dependency (Service A gets 10 threads, Service B gets 10). 4) Rate limiting: Limit requests to downstream (100 RPS max). 5) Graceful degradation: Return cached/default data when service unavailable. 6) Async: Use message queue (decouples services). Tools: Hystrix, Resilience4j. Result: Isolated failures, no cascade."
          - question: "Explain the difference between availability and reliability with examples."
            answer: "Availability: Uptime percentage (can system respond?). Example: 99.9% = 8.7h downtime/year. Measured: Total uptime / Total time. Reliability: Correctness over time (does system work correctly?). Example: Returns correct results 99.9% of time. Measured: MTBF (mean time between failures). Can have: High availability, low reliability (system responds but wrong data). Low availability, high reliability (system rarely available but correct when it is). Trade-off: Replication increases availability (more replicas = less downtime) but can decrease reliability (more replicas = more consistency issues). Goal: Both high (99.99% available + correct results)."
          - question: "When would you use CQRS and what are the complexities it introduces?"
            answer: "CQRS (Command Query Responsibility Segregation): Separate write model (commands) from read model (queries). Use when: 1) Read/write patterns differ (95% reads, 5% writes → optimize differently), 2) Complex queries (joins across services), 3) Different scaling needs (100K reads/sec, 1K writes/sec), 4) Event sourcing (replay events to build read models). Example: E-commerce (write to order DB, read from denormalized product catalog). Complexities: 1) Eventual consistency (write delay to read model: 100ms-1s), 2) Data duplication (2× storage), 3) Sync complexity (keep models in sync via events), 4) More infrastructure (2 databases). Cost: 2× database cost. Use only when read/write patterns significantly different."
          - question: "How do you handle data consistency in an event-driven architecture?"
            answer: "Event-driven = eventual consistency (events propagate with delay: 100ms-1s). Strategies: 1) Event ordering: Use partition key (user_id) → all user events in order. 2) Idempotency: Use event ID to deduplicate (handle duplicate deliveries). 3) Compensating events: If error, emit reverse event (OrderCancelled after OrderCreated). 4) Saga pattern: Coordinate multi-step workflows with events. 5) Event sourcing: Store events as source of truth, rebuild state by replay. 6) Versioning: Version events (OrderCreatedV1 → OrderCreatedV2) for schema evolution. Challenges: No ACID transactions across services. Debugging: Hard to trace (distributed events). Monitoring: Track event lag (<1s), replay capability (rebuild state from events)."
        tradeoffs:
          - question: "Compare synchronous vs asynchronous communication: when to use each?"
            answer: "Synchronous (REST/gRPC): Request → Wait → Response. Pros: Simple, immediate feedback (10ms). Cons: Tight coupling, caller blocks (thread held), cascade failures. Use for: User-facing APIs (need instant response), critical path (payment). Latency: 10-50ms. Asynchronous (Message Queue): Send message → Return immediately → Process later. Pros: Decoupled (sender continues), resilient (queue buffers), scalable (parallel processing). Cons: Complex, eventual consistency (1s delay), no immediate feedback. Use for: Background jobs (email, analytics), high volume (1M events/day), non-critical path. Latency: 100ms-1s. Hybrid: Sync for user requests (95%), async for background (5%). Result: Balance responsiveness + resilience."
          - question: "Choreography vs Orchestration for microservices coordination: pros and cons"
            answer: "Choreography: Decentralized, event-driven. Service A emits event → Service B listens → Service C listens. Pros: No single point of failure, loose coupling, scalable. Cons: Hard to trace (no central view), implicit dependencies, debugging difficult. Use for: Simple workflows (2-3 steps), high autonomy teams. Example: Order created → Notification sent + Inventory updated. Orchestration: Central coordinator (Saga orchestrator). Coordinator calls Service A → Service B → Service C. Pros: Easy to trace (central log), explicit workflow, timeouts/retries centralized. Cons: Coordinator is bottleneck + single point of failure, tighter coupling. Use for: Complex workflows (>3 steps), compensation logic. Example: Book flight → hotel → car (rollback if any fails). Choose: Orchestration for complex, choreography for simple."
          - question: "API Gateway vs Backend for Frontend (BFF): when to use which pattern?"
            answer: "API Gateway: Single entry point for all clients (web, mobile, IoT). Handles: Auth, rate limiting, routing, SSL. Generic aggregation (same for all clients). Pro: Centralized control, consistent security. Con: One-size-fits-all (mobile gets same data as web, over-fetching). Cost: $500/month. BFF: Separate backend per client type (web-bff, mobile-bff, iot-bff). Each BFF tailored to client needs (mobile-bff returns less data, web-bff aggregates more). Pro: Client-optimized APIs (mobile gets 1KB, web gets 10KB), independent deployment. Con: Code duplication, more services to manage. Cost: $1.5K/month (3 BFFs). Choose: Gateway for simple apps, BFF for multiple distinct clients (web + mobile with different needs)."
          - question: "Event Sourcing vs traditional CRUD: benefits and drawbacks"
            answer: "CRUD (traditional): Store current state only (UPDATE users SET balance=100). Pros: Simple, fast reads (10ms), small storage. Cons: Lose history (can't answer 'what was balance yesterday?'), hard to debug, no audit trail. Storage: 1GB. Event Sourcing: Store all events (BalanceSet(100), BalanceAdded(50)). Rebuild state by replaying events. Pros: Full audit trail (regulatory compliance), time travel (query historical state), debugging (replay events). Cons: Complex, slower reads (replay 1000 events = 100ms), more storage (10× events vs state). Storage: 10GB. Requires: Snapshots (rebuild from snapshot + recent events, not all history). Use for: Financial systems (audit trail), analytics (need history), undo/replay. Cost: 10× storage + CQRS for fast reads."
          - question: "Service Mesh vs API Gateway: overlapping concerns and complementary use"
            answer: "Overlap: Both do routing, load balancing, observability, retries, timeouts. Difference: Gateway (north-south, external → internal), Mesh (east-west, internal ↔ internal). Gateway features: Auth, rate limiting (external clients), SSL termination, API composition. Mesh features: mTLS (service-to-service encryption), fine-grained routing (per-service), distributed tracing (spans across services), zero-trust security. Use both: Gateway at edge (external traffic) + mesh for internal (service-to-service). Alternative: Gateway only (simpler, less overhead). Cost: Gateway $500/month, mesh $2K/month (sidecar per pod = 2× pods). Choose mesh when: Need service-to-service security (mTLS), advanced routing (canary per service), observability at scale. Otherwise: Gateway sufficient."
          - question: "Database per service vs shared database: data consistency challenges"
            answer: "Database per service: Each microservice has own DB. Pros: Independent scaling (scale order DB separately), loose coupling (schema changes isolated), polyglot (MySQL for orders, MongoDB for products). Cons: No ACID transactions across services, data duplication, eventual consistency. Consistency: Use Saga pattern (compensating transactions) or event-driven sync. Query: Can't JOIN across services → need API composition or CQRS. Cost: 5 services × $200 = $1K/month. Shared database: All services access same DB. Pros: ACID transactions (strong consistency), easy queries (JOIN works). Cons: Tight coupling (schema change breaks all), scaling bottleneck (single DB), deployment risk (shared state). Cost: $500/month. Choose: Per-service for true microservices (loose coupling), shared for transitional architecture (monolith → microservices)."
          - "REST vs GraphQL vs gRPC: when to choose each API style?"
        scenarios:
          - "Design integration between a legacy monolith and new microservices. How would you use Strangler Fig pattern?"
          - "Your microservices architecture has tight coupling. Identify anti-patterns and propose solutions."
          - "A service is overwhelmed with requests. Design a solution using Circuit Breaker and Bulkhead patterns."
          - "You need to migrate from synchronous to event-driven architecture. What are the challenges and steps?"
          - "Design a system where user actions trigger multiple downstream processes. How do you ensure consistency?"
      time_estimate: 60

    - day: 27
      topic: "Mock Interview 1 - Ride Sharing System (Uber/Lyft)"
      activity: "Complete mock system design interview for ride-sharing platform with real-time matching, dynamic pricing, and global scale considerations."
      detailed_content: |
        Interview Structure & Timing:
        - Requirements Clarification (8-10 minutes): Scope, users, scale, constraints
        - High-Level Architecture (15-18 minutes): Core services, data flow, APIs
        - Deep Dive Components (15-18 minutes): Critical systems detailed design
        - Scale & Trade-offs (5-7 minutes): Bottlenecks, optimization, monitoring
        - Q&A (3-5 minutes): Edge cases, failure scenarios, next steps

        Requirements Gathering Framework:
        - Functional Requirements:
          * User registration and profile management (drivers, riders)
          * Real-time location tracking and updates
          * Driver-rider matching and assignment
          * Trip lifecycle management (request, start, end, payment)
          * Dynamic pricing based on supply/demand
          * ETA calculation and route optimization
          * Payment processing and billing
          * Rating and feedback system
          * Trip history and analytics

        - Non-Functional Requirements:
          * Scale: 10M active riders, 1M active drivers daily
          * Geographic: Global service across 100+ cities
          * Availability: 99.9% uptime with graceful degradation
          * Latency: <500ms for matching, <100ms for location updates
          * Consistency: Strong for payments, eventual for location data
          * Real-time: Location updates every 2-4 seconds

        Core System Components:
        1. User Service: Authentication, profiles, preferences, driver verification
        2. Location Service: Real-time GPS tracking, geospatial indexing, location history
        3. Matching Service: Driver-rider pairing algorithms, supply-demand optimization
        4. Trip Service: State management, lifecycle tracking, route calculation
        5. Pricing Service: Dynamic fare calculation, surge pricing, promotions
        6. Payment Service: Transaction processing, billing, refunds, driver payouts
        7. Notification Service: Real-time updates, push notifications, SMS/email
        8. Analytics Service: Business intelligence, operational metrics, ML training data

        Detailed Design Deep Dives:

        Location Service Architecture:
        - Real-time Location Updates: WebSocket connections, Redis for caching
        - Geospatial Indexing: QuadTree or Geohash for efficient proximity searches
        - Location Storage: Time-series database for historical tracking
        - Data Pipeline: Kafka for streaming location events to analytics
        - Optimization: Location prediction, batch updates, data compression

        Matching Algorithm Design:
        - Proximity Search: Find drivers within 5km radius using geospatial index
        - Scoring System: Distance, driver rating, acceptance rate, trip direction
        - Assignment Strategy: Greedy vs optimal assignment with timeout
        - Fairness: Prevent driver starvation, rotation policies
        - Machine Learning: Demand prediction, driver positioning optimization

        Trip State Management:
        - State Machine: REQUESTED → ACCEPTED → STARTED → COMPLETED → PAID
        - Fault Tolerance: Distributed coordination, timeout handling, retries
        - Consistency: Event sourcing for trip audit trail
        - Real-time Updates: WebSocket for trip status to both parties
        - Compensation: Handle payment failures, trip cancellations

        Dynamic Pricing Engine:
        - Surge Pricing: Supply/demand ratio, geographic heat maps
        - ML Models: Demand forecasting, price elasticity, revenue optimization
        - Real-time Updates: Price recalculation every 30 seconds
        - Transparency: Clear pricing communication to users
        - Regulation Compliance: Price caps, fare regulations per city

        Scalability Considerations:
        - Geographic Partitioning: City-based sharding, regional data centers
        - Database Scaling: Read replicas, write sharding, caching strategies
        - Service Mesh: Circuit breakers, load balancing, service discovery
        - Auto-scaling: Kubernetes HPA based on location update volume
        - CDN: Static content, mobile app assets, configuration data

        Data Storage Strategy:
        - User Data: PostgreSQL with read replicas
        - Location Data: Redis for real-time, InfluxDB for historical
        - Trip Data: PostgreSQL with partitioning by time/geography
        - Analytics: Data lake (S3) + Spark for batch processing
        - Caching: Redis for hot data, application-level caching

        Monitoring & Observability:
        - Business Metrics: Successful matches/minute, average ETA accuracy
        - Technical Metrics: API latency, error rates, database performance
        - Real-time Dashboards: Geographic heat maps, system health
        - Alerting: SLA violations, spike in failed matches, payment issues
        - Distributed Tracing: End-to-end request flow across microservices

        Security & Compliance:
        - Authentication: OAuth 2.0, JWT tokens, multi-factor for drivers
        - Data Privacy: PII encryption, GDPR compliance, data retention policies
        - Payment Security: PCI DSS compliance, tokenization, fraud detection
        - Location Privacy: Data anonymization, user consent, location expiry
        - Driver Verification: Background checks, document verification, real-time monitoring

        Common Interview Questions & Answers:
        - "How do you handle driver going offline during trip?"
        - "What happens when payment fails after trip completion?"
        - "How do you prevent fake GPS locations from drivers?"
        - "How do you handle network partitions between regions?"
        - "What's your strategy for expanding to a new city?"

        Failure Scenarios & Recovery:
        - Location Service Down: Use cached locations, degrade to manual assignment
        - Matching Service Overload: Queue requests, increase timeout, circuit breaker
        - Payment Service Failure: Async retry, manual reconciliation, credit system
        - Database Partition: Regional failover, read-only mode, eventual consistency
        - Network Issues: Offline capability, request queuing, graceful degradation

        Performance Optimization:
        - Location Updates: Batch processing, compression, adaptive frequency
        - Matching Algorithm: Pre-computed driver pools, parallel processing
        - Database: Connection pooling, query optimization, proper indexing
        - Caching: Multi-level caching, cache warming, intelligent eviction
        - API Design: Pagination, filtering, async processing for heavy operations

      practice_questions:
        capacity_estimation:
          - question: "Calculate storage for 10M trips/day with location updates every 3 seconds"
            answer: "Trip data: 10M trips/day × 2KB/trip = 20GB/day. Location updates: 10M trips × 20 min avg duration × (60/3) updates/min × 100B/update = 10M × 20 × 20 × 100B = 400GB/day. Total: 20GB + 400GB = 420GB/day = 153TB/year. With compression (3×): 51TB/year. Hot storage (30 days): 12.6TB. Cold storage (11 months): 38.4TB. Database: Use time-series DB (InfluxDB) for locations = 10× compression. Cost: Hot $1.5K/month, Cold $500/month = $2K/month total."
          - question: "Estimate bandwidth for real-time location updates from 1M drivers"
            answer: "Active drivers: 1M concurrent. Update frequency: 3 seconds. Payload: 100B (lat, lon, heading, speed, timestamp). Upload: 1M × 100B / 3s = 33MB/sec upload. Broadcast to riders: 10M riders watching drivers × 100B / 3s = 333MB/sec download. Total: 33MB + 333MB = 366MB/sec = 2.9Gbps. With WebSocket compression (2×): 1.5Gbps. Peak (5× avg): 7.3Gbps. CDN for rider updates: $5K/month. Direct upload from drivers: $2K/month. Total: $7K/month bandwidth."
          - question: "Size matching service to handle 100K concurrent ride requests"
            answer: "Requests: 100K concurrent = peak load. Processing: 10ms per match (QuadTree search + scoring). Throughput: 1 core handles 100 req/sec. Cores needed: 100K / 100 = 1000 cores. Servers: 1000 cores / 32 cores per server = 32 servers. With 2× headroom: 64 servers. Memory: 10GB per server (cache driver locations) = 640GB total. Database: 1K queries/sec (read-heavy) = 5 read replicas. Cost: 64 servers × $500 = $32K/month. Alternative: Pre-compute driver pools per geohash = reduce to 5ms/match = 16 servers = $8K/month."
          - question: "Calculate database capacity for storing 1 year of trip history"
            answer: "Trips: 10M/day × 365 = 3.65B trips/year. Per trip: 2KB (user_id, driver_id, route, price, timestamps) = 7.3TB. Indexes (50%): 3.65TB. Total: 10.95TB ≈ 11TB. Sharding: 100GB per shard = 110 shards. Replication (3×): 33TB storage. Growth: 11TB/year, plan for 3 years = 33TB capacity. Cost: $3K/month for storage. Query pattern: 80% recent (30 days), 20% historical. Solution: Hot tier (1 month: 600GB SSD), Cold tier (rest: HDD). Cost: $1K/month."

        architecture_decisions:
          - question: "Why use QuadTree vs Geohash for location indexing?"
            answer: "QuadTree: Recursively divide space into 4 quadrants. Pros: Efficient radius search (O(log n) within cell), dynamic (adapts to density). Cons: In-memory only (hard to shard), complex updates. Use for: Real-time matching (drivers within 5km). Latency: 1-5ms. Geohash: Encode lat/lon as string (e.g., '9q5'). Pros: Simple, DB-friendly (range query), easy sharding (by prefix). Cons: Edge problem (nearby points may have different prefixes), fixed precision. Use for: Persistent storage, coarse filtering. Latency: 10ms. Hybrid: Geohash for initial filter (100 drivers) → QuadTree for final ranking. Result: Best of both (DB sharding + fast search)."
          - question: "SQL vs NoSQL for different data types in the system?"
            answer: "Use SQL for: 1) Users (PostgreSQL): ACID transactions (payment, trips), complex queries (analytics), structured. 2) Trips (PostgreSQL): Strong consistency (no double-billing), foreign keys, reporting. Use NoSQL for: 1) Locations (Cassandra): Write-heavy (1M drivers × 0.33 writes/sec = 330K writes/sec), time-series, partition by driver_id. 2) Driver pool (Redis): In-memory, fast reads (<1ms), geospatial index (GEORADIUS). 3) Notifications (DynamoDB): Event-driven, high availability. Polyglot persistence: 5 databases = complexity, but optimized per use case. Cost: SQL $2K/month, NoSQL $3K/month = $5K total."
          - question: "Microservices vs monolith for a startup building ride-sharing?"
            answer: "Start with modular monolith: Single deployment, but organized into modules (matching, payment, trips). Pros: Simple (1 codebase), fast iteration (no network overhead), easier debugging. Cons: Shared database, single scaling unit. When to split: 1) Team size >20 (coordination overhead), 2) Independent scaling (matching 100×, admin 1×), 3) Polyglot needs (Python ML, Go API). Timeline: Monolith for MVP (6 months), microservices at scale (1M users). Migration: Strangler Fig pattern (gradually extract services). Cost: Monolith $5K/month, microservices $15K/month. Choose: Monolith until pain points justify 3× cost increase."
          - question: "Event-driven vs request-response for trip state updates?"
            answer: "Request-response (sync): Client → API → Update DB → Return status. Pros: Immediate feedback (10ms), simple debugging. Cons: Tight coupling, blocks client. Use for: Critical path (rider requests trip, driver accepts). Latency: 50ms. Event-driven (async): Client → Emit event → Queue → Subscribers update. Pros: Decoupled (add notification without changing trip service), resilient (queue buffers), scalable. Cons: Eventual consistency (100ms lag), complex debugging. Use for: Non-critical updates (notifications, analytics, billing). Hybrid: Sync for critical (trip creation), async for side effects (notify rider, update analytics). Result: 80% event-driven, 20% sync."

        scalability_challenges:
          - question: "How do you scale matching service for 10x traffic growth?"
            answer: "Current: 100K concurrent requests. Target: 1M concurrent. Strategy: 1) Horizontal scale: 64 servers → 640 servers (10×). Cost: $320K/month. 2) Optimization: Pre-compute driver pools per geohash = 5× faster. New: 128 servers. Cost: $64K/month (5× cheaper). 3) Caching: Cache driver locations in Redis (1M drivers × 100B = 100MB, fits in memory). Update every 3s. Reduces DB load 10×. 4) Async processing: Queue ride requests, process in batches (100 req/batch) = 2× throughput. 5) Multi-region: 5 regions × 128 servers = 640 servers, but local processing (10ms vs 100ms cross-region). Final: 128 servers + Redis cluster = $70K/month."
          - question: "Design disaster recovery for multi-region deployment"
            answer: "Architecture: Active-active across 3 regions (US-West, US-East, EU). Per region: Full stack (API, matching, DB). Data: Users/Trips (sync replicated: strong consistency, 100ms lag), Locations (local only: eventual consistency). Failure: Region goes down → DNS/load balancer redirects to nearest region (30s failover). User impact: In-progress trips reassigned to backup region (drivers/riders in same region mostly). Trips in failed region: Use location history (last known) + manual assignment. RTO: 30s, RPO: 0 (sync replication). Cost: 3× infrastructure = $150K/month (vs $50K single region). Alternative: Active-passive (cheaper: 1.5× cost, but 5min RTO)."
          - question: "Handle hotspots during peak hours (concerts, airports)"
            answer: "Problem: 10K riders request at same location (concert) = 100× normal density. Solution: 1) Dynamic pricing (surge): Increase price 2-5× → attract more drivers, reduce demand. 2) Pre-positioning: ML predicts events → notify drivers in advance (incentives). 3) Geohash expansion: If no drivers in cell, search adjacent cells (expand radius 1km → 3km → 5km). 4) Request queuing: Queue requests, process FIFO (estimated wait time: 10 min). 5) Dedicated servers: Spin up extra matching servers for event (auto-scaling: 10× capacity for 2 hours). Cost: Surge pricing revenue offsets capacity cost. User experience: Transparent wait times + alternatives (carpool, shuttle)."
          - question: "Scale payment processing to handle Black Friday volumes"
            answer: "Normal: 10M trips/day = 116 payments/sec. Black Friday: 100M trips/day = 1160 payments/sec (10×). Strategy: 1) Async processing: Charge after trip (not during) = decouples user experience from payment latency. Queue in Kafka (1160 msg/sec = trivial). 2) Payment provider scaling: Use Stripe (handles millions TPS), pre-warm with test transactions. 3) Retry logic: Exponential backoff (1s, 2s, 4s) for failed payments. Dead letter queue for manual review. 4) Database: Shard by user_id (100 shards), each handles 11.6 payments/sec. 5) Idempotency: Use payment_id to prevent double-charge. Cost: Stripe fees (2.9% + 30¢) = $2.9M on $100M revenue. Capacity: No extra cost (Stripe auto-scales)."

        real_world_scenarios:
          - "Driver reports wrong trip completion - design resolution flow"
          - "Network partition isolates 20% of drivers - how to handle?"
          - "Competitor enters market with 50% lower prices - system adaptations?"
          - "Government requires real-time location sharing - compliance design"

      resources:
        - title: "Designing Uber - High Scalability"
          url: "http://highscalability.com/blog/2022/1/11/designing-uber.html"
          description: "Comprehensive system design breakdown with architecture patterns"
        - title: "Uber Engineering Blog - Real-time Architecture"
          url: "https://eng.uber.com/category/architecture/"
          description: "Production insights from Uber's engineering team"
        - title: "Geospatial Indexing at Scale"
          url: "https://medium.com/@buckhx/unwinding-uber-s-most-efficient-service-406413c5871d"
          description: "Deep dive into location services and geospatial algorithms"
        - title: "System Design Interview Guide"
          url: "https://github.com/donnemartin/system-design-primer"
          description: "Complete preparation framework for system design interviews"
        - title: "Grokking System Design - Uber Design"
          url: "https://www.educative.io/courses/grokking-system-design-fundamentals"
          description: "Step-by-step system design interview preparation"
        - title: "Distributed Systems Patterns"
          url: "https://martinfowler.com/articles/patterns-of-distributed-systems/"
          description: "Common patterns for building distributed systems like ride-sharing"
        - title: "Real-time Data Processing at Uber"
          url: "https://eng.uber.com/real-time-exactly-once-ad-event-processing/"
          description: "Stream processing architecture for real-time systems"

      time_estimate: 120

    - day: 28
      topic: "Mock Interview 2 - Video Streaming Platform (Netflix/YouTube)"
      activity: "Complete comprehensive capstone mock interview for video streaming platform demonstrating mastery of all Week 4 concepts with advanced scalability challenges."
      detailed_content: |
        Final Assessment Structure & Timing:
        - Requirements Clarification (8-10 minutes): Scope, scale, features, constraints
        - High-Level Architecture (15-18 minutes): Core services, data flow, global distribution
        - Deep Dive Components (20-22 minutes): Video pipeline, CDN, recommendations, storage
        - Advanced Challenges (8-10 minutes): Live streaming, ML, content protection
        - Scale & Optimization (5-7 minutes): Bottlenecks, cost optimization, future growth

        Requirements Gathering Framework:
        - Functional Requirements:
          * User registration, authentication, and profile management
          * Video upload, processing, and transcoding pipeline
          * Video streaming with adaptive bitrate (ABR)
          * Content discovery through search and browse
          * Personalized recommendation system
          * User interactions (like, comment, share, subscribe)
          * Content creator monetization and analytics
          * Offline downloads for mobile devices
          * Live streaming capabilities
          * Content moderation and copyright protection

        - Non-Functional Requirements:
          * Scale: 2B registered users, 1B daily active users
          * Content: 500M hours watched daily, 1M hours uploaded daily
          * Geographic: Global service across 190+ countries
          * Availability: 99.95% uptime with regional redundancy
          * Latency: <200ms for video start, <3 seconds buffering
          * Storage: Petabytes of video content with multiple formats
          * Bandwidth: Multi-Tbps global distribution capacity

        Core System Architecture:

        Video Ingestion & Processing Pipeline:
        - Upload Service: Chunked upload, resume capability, metadata extraction
        - Transcoding Service: Multiple resolutions (4K, 1080p, 720p, 480p, 360p)
        - Format Conversion: H.264, H.265/HEVC, VP9, AV1 for different devices
        - Thumbnail Generation: Multiple timestamps, A/B testing thumbnails
        - Quality Control: Automated content analysis, duplicate detection
        - Storage: Raw videos in blob storage, processed videos in distributed storage

        Content Delivery Network (CDN):
        - Global PoPs: 200+ edge locations worldwide
        - Cache Hierarchy: Origin → Regional → Edge caches
        - Intelligent Routing: Geographic and network-aware content delivery
        - Adaptive Streaming: DASH/HLS protocols for bitrate adaptation
        - Prefetching: ML-driven content pre-positioning based on predictions
        - Multi-CDN Strategy: Primary/secondary CDN providers for redundancy

        Recommendation Engine Architecture:
        - Data Collection: View history, engagement metrics, social signals
        - Feature Engineering: User profiles, content features, contextual data
        - ML Models: Collaborative filtering, deep neural networks, reinforcement learning
        - Real-time Serving: Sub-100ms recommendation generation
        - A/B Testing: Continuous experimentation on recommendation algorithms
        - Cold Start: New user/content recommendations without historical data

        User & Content Management:
        - User Service: Authentication, profiles, preferences, viewing history
        - Content Catalog: Metadata service with search indexing and faceting
        - Creator Studio: Upload interface, analytics dashboard, monetization tools
        - Comment System: Threaded discussions, moderation, spam detection
        - Subscription Service: Follow relationships, notification management

        Search & Discovery Service:
        - Search Index: Elasticsearch with video metadata, transcripts, tags
        - Query Processing: Auto-complete, spell correction, semantic search
        - Ranking Algorithm: Relevance, popularity, personalization, recency
        - Browse Categories: Trending, genre-based, personalized sections
        - Content Tagging: Automated tagging using computer vision and NLP

        Detailed Component Deep Dives:

        Video Processing Pipeline:
        - Distributed Transcoding: Kubernetes-based auto-scaling workers
        - Format Optimization: Device-specific encoding (mobile, TV, web)
        - Quality Metrics: PSNR, SSIM automated quality assessment
        - Watermarking: Content protection and tracking
        - Closed Captions: Auto-generation using speech recognition
        - Content Analysis: Violence, copyright, spam detection using ML

        Global Distribution Strategy:
        - Regional Data Centers: Primary content storage in 10+ regions
        - Edge Computing: Real-time processing at edge locations
        - Bandwidth Optimization: Compression, efficient protocols (QUIC)
        - Network Peering: Direct connections with ISPs for better performance
        - Cost Optimization: Intelligent cache eviction, traffic engineering

        Recommendation System Implementation:
        - Candidate Generation: Retrieve top-k videos from billion+ catalog
        - Ranking Model: Deep neural networks with embeddings
        - Diversity & Freshness: Balance popular and new content
        - Contextual Features: Time of day, device type, location
        - Real-time Updates: Stream processing for immediate feedback integration
        - Explainability: Transparent recommendations with reasoning

        Data Storage Architecture:
        - Video Storage: Distributed object storage (S3, GCS) with replication
        - Metadata Database: Sharded SQL databases for structured data
        - User Data: NoSQL for profiles, preferences, viewing history
        - Analytics: Data lake with time-series databases for metrics
        - Caching: Multi-level Redis/Memcached for hot data
        - Archive Storage: Cold storage for old/unpopular content

        Advanced Features & Challenges:

        Live Streaming Infrastructure:
        - RTMP Ingestion: Real-time video input from creators
        - Low-latency Streaming: WebRTC for interactive experiences
        - Transcoding: Real-time processing with minimal delay
        - Chat Integration: Real-time messaging with moderation
        - DVR Capability: Record live streams for later viewing
        - Scalability: Handle millions of concurrent viewers

        Machine Learning Integration:
        - Content Understanding: Automatic tagging, scene detection
        - Recommendation Models: Collaborative and content-based filtering
        - Abuse Detection: Spam, harassment, fake account identification
        - Thumbnail Selection: A/B testing optimal video thumbnails
        - Ad Targeting: Personalized advertisement placement
        - Creator Analytics: Audience insights and growth recommendations

        Content Protection & Moderation:
        - DRM: Digital rights management for premium content
        - Geo-blocking: Regional content restrictions and compliance
        - Copyright Detection: Content ID system for automated protection
        - Community Guidelines: ML-based content policy enforcement
        - Human Review: Escalation workflow for complex moderation cases
        - DMCA Compliance: Takedown request processing and appeals

        Monetization Systems:
        - Ad Serving: Real-time bidding, personalized ad insertion
        - Subscription Management: Premium tiers, billing, content access
        - Creator Revenue: Ad revenue sharing, super chat, memberships
        - Analytics: Revenue tracking, creator dashboards, audience insights
        - Payment Processing: Global payment methods, fraud detection

        Scalability & Performance:
        - Auto-scaling: Dynamic resource allocation based on traffic
        - Load Balancing: Geographic and algorithm-aware traffic distribution
        - Database Sharding: Horizontal partitioning strategies
        - Caching Strategy: Content popularity prediction and preloading
        - Monitoring: Real-time alerting on performance degradation
        - Capacity Planning: Predictive scaling for traffic growth

        Security & Privacy:
        - Authentication: OAuth 2.0, multi-factor authentication
        - Data Protection: Encryption at rest and in transit
        - Privacy Controls: User data management, GDPR compliance
        - Access Control: Role-based permissions for content and features
        - Security Monitoring: Anomaly detection, intrusion prevention
        - Child Safety: Age verification, restricted content filtering

        Operational Excellence:
        - Monitoring: Business and technical metrics dashboards
        - Alerting: SLA violations, system health, user experience
        - Incident Response: Runbooks, escalation procedures, post-mortems
        - Deployment: Blue-green deployments, feature flags, rollback procedures
        - Cost Management: Resource optimization, usage analytics
        - Compliance: Regional regulations, content standards, data governance

        Interview Deep Dive Questions:
        - "How do you handle a viral video that suddenly gets 100M views?"
        - "Design the recommendation system to avoid filter bubbles"
        - "How do you detect and prevent copyright infringement at scale?"
        - "What's your strategy for expanding to a country with poor internet?"
        - "How do you handle live streaming for 1M concurrent viewers?"

        Failure Scenarios & Mitigation:
        - CDN Failure: Multi-CDN failover, degraded quality serving
        - Recommendation System Down: Fallback to trending/popular content
        - Upload Service Overload: Queue management, priority processing
        - Database Partition: Read-only mode, eventual consistency handling
        - DDoS Attack: Rate limiting, traffic filtering, CDN protection

        Performance Optimization Strategies:
        - Video Compression: Next-gen codecs (AV1), quality-bitrate optimization
        - Predictive Caching: ML-driven content pre-positioning
        - Edge Computing: Process transcoding closer to users
        - Network Optimization: QUIC protocol, HTTP/3 adoption
        - Mobile Optimization: Adaptive streaming, offline capability

        Cost Optimization:
        - Storage Tiering: Hot, warm, cold storage based on access patterns
        - CDN Optimization: Traffic engineering, peering agreements
        - Transcoding Efficiency: GPU acceleration, smart encoding
        - Infrastructure: Spot instances, reserved capacity, auto-scaling
        - Content Lifecycle: Automated archival, duplicate detection

      practice_questions:
        capacity_estimation:
          - question: "Calculate storage needs for 1M hours uploaded daily with multiple formats"
            answer: "Uploads: 1M hours/day. Formats: 4K (25GB/h), 1080p (5GB/h), 720p (2GB/h), 480p (1GB/h), 360p (500MB/h). Per upload: Average 50 min × (25 + 5 + 2 + 1 + 0.5) = 50/60 × 33.5GB = 27.9GB per upload. Daily: 1M hours × 27.9GB = 27.9PB/day. With 2× (raw + processed): 55.8PB/day. Monthly: 1.67EB. Yearly: 20EB. Compression (HEVC 30% savings): 14EB/year. Storage tiers: Hot (30 days): 420PB, Warm (1 year): 14EB, Cold (archive): rest. Cost: Hot $42K/month, Warm $140K/month, Cold $20K/month = $202K/month."
          - question: "Estimate bandwidth for 1B users watching 500M hours daily"
            answer: "Watch time: 500M hours/day = 20.8M hours/hour = 5780 hours/sec. Average bitrate: 3Mbps (mix of qualities). Bandwidth: 5780 × 3Mbps = 17.3Tbps = 17,300Gbps. Peak (5× avg): 86.5Tbps. CDN cost: $0.02/GB. Daily transfer: 500M hours × 3Mbps × 3600s / 8 = 675PB/day. Monthly: 20.25EB. Cost: 20.25EB × $0.02/GB = 20,250TB × $20 = $405K/month. Optimization: Peer-to-peer (WebRTC) reduces CDN by 30% = $283K/month. Multi-CDN (Akamai + Cloudflare + Fastly): 3× redundancy, negotiate bulk discount (50% off) = $202K/month."
          - question: "Size recommendation system to serve 2B users with <100ms latency"
            answer: "Users: 2B. Requests: 10 req/day per user = 20B req/day = 231K req/sec. Peak: 5× = 1.15M req/sec. Latency budget: <100ms. Model serving: 10ms (TensorFlow Serving on GPU). Feature lookup: 20ms (Redis). Post-processing: 10ms (ranking). Total: 40ms (60ms headroom). Throughput: 1 GPU server handles 100 req/sec (10ms × 100 = 1s). Servers needed: 1.15M / 100 = 11,500 GPU servers. Cost: $2/hour × 11,500 = $23K/hour = $16.6M/month. Optimization: Pre-compute top 1000 recommendations per user (batch job overnight) → reduce real-time to 100 servers = $150K/month. Trade-off: Freshness (12h old) vs cost (100× cheaper)."
          - question: "Calculate transcoding capacity for real-time processing of uploads"
            answer: "Uploads: 1M hours/day = 41.6K hours/hour = 694 min/min = 694 concurrent transcodes. Per video: Transcode to 5 formats (4K, 1080p, 720p, 480p, 360p). Time: 1 hour source → 30 min transcode time (2× realtime). Parallel: 5 formats × 694 videos = 3470 concurrent transcode jobs. Hardware: 1 server (32 cores, 2 GPUs) handles 8 concurrent transcodes. Servers needed: 3470 / 8 = 434 servers. Cost: $500/month × 434 = $217K/month. Peak handling (5× avg): 2170 servers = $1.08M/month (use spot instances 70% off) = $324K/month. Alternative: AWS MediaConvert (pay-per-minute) = $250K/month."

        architecture_decisions:
          - question: "SQL vs NoSQL for different data types in video platform?"
            answer: "Use SQL for: 1) Users (PostgreSQL): ACID transactions (subscriptions, payments), complex queries (churn analysis). 2) Videos metadata (PostgreSQL): Structured (title, description, upload_date), foreign keys (user_id, channel_id), analytics. Use NoSQL for: 1) View events (Cassandra): Write-heavy (20B events/day), time-series, partition by user_id + day. 2) Recommendations (Redis): In-memory, fast reads (<1ms), precomputed lists. 3) Comments (MongoDB): Nested structure (replies), high write volume (100M/day). Polyglot persistence: 5 databases. Cost: SQL $10K/month, NoSQL $50K/month = $60K total. Alternative: All-SQL (simpler, but 10× cost for view events)."
          - question: "CDN vs regional data centers: cost and performance trade-offs"
            answer: "CDN (Cloudflare, Akamai): Pros: Global edge locations (200+), <50ms latency anywhere, DDoS protection, handles traffic spikes. Cons: Expensive ($0.02/GB), less control, vendor lock-in. Cost: 20EB/month × $0.02/GB = $400K/month. Regional data centers (self-managed): Pros: Cheaper (owned bandwidth $0.005/GB), full control, custom optimizations. Cons: Limited coverage (10 regions), higher latency (100-200ms), need to manage. Cost: $100K/month bandwidth + $200K infra = $300K/month. Hybrid: CDN for popular content (80% of views, 20% of catalog) = $80K, data centers for long-tail = $300K. Total: $380K (5% savings + better control)."
          - question: "Microservices vs monolith for video processing pipeline?"
            answer: "Video processing: Upload → Transcode → Thumbnail → Metadata → Publish. Monolith: Single service handles all steps. Pros: Simple deployment, easy debugging, low latency (in-memory). Cons: Single point of failure, hard to scale (transcoding needs 100× more resources than thumbnail). Microservices: Separate services (Upload, Transcode, Thumbnail, Metadata, Publish). Pros: Independent scaling (transcode 100× thumbnail), fault isolation (thumbnail fails, video still published), polyglot (Python ML, Go API). Cons: Complex (5 services), network overhead (10ms per hop), distributed tracing needed. Choose: Microservices (pipeline stages have vastly different resource needs). Cost: Monolith $50K/month, microservices $150K/month (3×, but necessary for scale)."
          - question: "Real-time vs batch processing for recommendation updates?"
            answer: "Real-time: Update recommendations after each view (within seconds). Pros: Personalized instantly (user watches cat video → immediately see more cats), higher engagement. Cons: Expensive (1.15M req/sec × 10ms = 11,500 GPU servers = $16.6M/month), complex (streaming ML). Batch: Update once per day (overnight job). Pros: Cheap (100 servers = $150K/month), simple (batch Spark job). Cons: Stale (12h old recommendations), lower engagement. Hybrid: Batch for base recommendations (nightly) + real-time for signals (trending, new uploads). Result: 90% batch, 10% real-time = $2M/month (vs $16.6M pure real-time). Trade-off: 95% of accuracy at 8× lower cost."

        scalability_challenges:
          - question: "Design disaster recovery for multi-region video platform"
            answer: "Architecture: 5 regions (US-West, US-East, EU, Asia, South America), active-active. Data: Videos (replicated to all regions via CDN), Metadata (PostgreSQL with cross-region replication: async, <1s lag), View events (Cassandra multi-DC: async, eventual consistency). Failure: Region down → CDN redirects to nearest region (10s DNS update). Impact: Users in failed region see 50-100ms higher latency, all content available. User uploads: S3 cross-region replication (async, <15 min). RTO: 10s (CDN failover), RPO: 15 min (upload replication lag). Cost: 5× infrastructure = $1M/month (vs $200K single region). Monitoring: Health checks every 10s, auto-failover. Alternative: Active-passive = 2× cost, 5 min RTO."
          - question: "Handle 10x traffic growth during major live event"
            answer: "Normal: 500M hours/day = 5780 hours/sec = 17.3Tbps. Live event: 10× = 57,800 hours/sec = 173Tbps. Preparation: 1) Pre-scale CDN (notify providers 24h advance, pre-warm caches). 2) Spin up 10× transcoding servers (spot instances, 2h before event). 3) Add read replicas (5× DB replicas for metadata). 4) Rate limiting (non-live content: degrade to 720p max). 5) Graceful degradation (turn off recommendations, use cached trending). 6) Multi-CDN (Akamai + Cloudflare + Fastly = 3× capacity). Cost: Normal $400K/month, event day $4M (10× for 24h). Revenue: Ad revenue 5× higher during event offsets cost. Post-event: Scale down within 1h."
          - question: "Scale recommendation system for emerging markets with poor connectivity"
            answer: "Problem: Poor bandwidth (1-5Mbps), high latency (200-500ms), expensive data. Solution: 1) Lightweight models (10MB model vs 1GB, deploy to client). 2) Offline-first (preload 50 videos when on WiFi). 3) P2P sharing (WebRTC between nearby users, reduce CDN 50%). 4) Lower bitrate (480p max, 1Mbps). 5) Simplified UI (reduce API calls by 5×). 6) Regional data centers (closer to users, <100ms latency). 7) Partnerships (ISP caching, zero-rating deals). Cost: Additional regional DCs $50K/month, P2P infrastructure $20K/month = $70K investment. User growth: 10× in emerging markets. Revenue: Lower ARPU ($2/month vs $10), but volume compensates."
          - question: "Optimize video delivery for mobile-first regions"
            answer: "Mobile constraints: Small screen (6 inch), limited data (2GB/month), variable network (3G/4G/WiFi). Optimizations: 1) Adaptive bitrate: Start 360p (500KB/min), upgrade to 720p if bandwidth allows. 2) Data saver mode: 480p max, skip video previews = 50% data savings. 3) Smart preloading: Next 2 videos when on WiFi (ML predicts what user will watch). 4) Thumbnail efficiency: WebP format (30% smaller than JPEG). 5) API optimization: Reduce JSON payload (send only mobile-needed fields) = 10× smaller. 6) Edge computing: Process recommendations at edge (reduce latency 200ms → 50ms). 7) Progressive download: Watch while downloading. Result: 50% less data usage, 4× faster load, 2× engagement. Cost: Edge servers $100K/month."

        advanced_scenarios:
          - question: "Competitor launches with AI-generated personalized content - response strategy"
            answer: "Competitor: AI generates unique videos per user (infinite content, perfect personalization). Challenges: We have static content (1B videos, same for all users). Response: 1) AI content generation (OpenAI Sora, Runway): Generate summaries, clips, mashups from existing videos. 2) Hyper-personalization: AI-generated thumbnails/titles per user (A/B test 100 variants, pick best). 3) Interactive videos: Let users choose storyline (branching videos). 4) Creator tools: Give creators AI tools (auto-edit, captions, effects) = 10× more content. 5) AI dubbing: Translate to 100 languages with voice cloning. Timeline: MVP in 6 months, full rollout in 18 months. Cost: $50M/year (GPUs, engineering). Revenue impact: 20% engagement increase = $10B/year. ROI: 200×."
          - question: "Government requires local data storage - compliance architecture"
            answer: "Requirement: User data (profiles, view history) must stay in-country (GDPR, China data laws). Architecture: 1) Data residency: Store user data in regional PostgreSQL (EU users → EU DB, China users → China DB). 2) Video content: Global CDN (content not user data, allowed). 3) Cross-border queries: Not allowed (EU user can't query US data). 4) Data export: Manual approval (user requests data, gov't signs off). 5) Encryption: At-rest + in-transit (AES-256). Implementation: Shard by country (200 countries = 200 DB shards). Cost: 10× more infrastructure (vs single global DB) = $2M/month. Alternative: Exit market (China, Russia = too complex). Revenue: Keep 80% of global market."
          - question: "Major content creator threatens platform switch - retention features"
            answer: "Problem: Creator (100M subscribers) threatens to move to competitor (better revenue share 70/30 vs our 55/45). Cost of losing: $100M/year ad revenue. Retention strategy: 1) Custom revenue deal (negotiate 70/30 for top creators). 2) Exclusive features (early access to new tools, priority support). 3) Audience lock-in (subscribers can't easily follow to new platform). 4) Analytics (better creator analytics, A/B testing tools). 5) Monetization tools (memberships, super chat, merch integration). 6) Multi-platform (let them post elsewhere, but premiere on our platform). Cost: $10M/year (custom deal + features). Alternative: Let them go, focus on 1000 mid-tier creators (diversification). Risk: Domino effect (other creators follow)."
          - question: "New codec offers 50% compression - migration strategy"
            answer: "New codec: AV2 (50% smaller than H.265/HEVC), but slower encode (5× CPU). Benefits: Halve storage (20EB → 10EB = $100K/month savings), halve bandwidth (20EB/month → 10EB = $200K/month savings). Total: $300K/month = $3.6M/year. Migration: 1) Pilot: Re-encode top 1% of videos (most-watched) in AV2 (10TB, 1 week). 2) Measure: Verify 50% savings, check playback compatibility (95% of devices support AV2). 3) Gradual rollout: Re-encode 1%/month (entire catalog in 8 years, high-value content in 6 months). 4) New uploads: Encode in AV2 immediately. Cost: Re-encoding $1M (5× slower = 5× more servers × 6 months). ROI: Payback in 4 months. Challenge: Old devices (5% users) can't play AV2 → dual-encode (AV2 + H.265) for 2 years."

      resources:
        - title: "Netflix Technology Blog"
          url: "https://netflixtechblog.com/"
          description: "Production insights from Netflix engineering on streaming architecture"
        - title: "YouTube Engineering Blog"
          url: "https://blog.youtube/inside-youtube/engineering/"
          description: "Technical deep dives from YouTube's engineering team"
        - title: "Video Streaming at Scale"
          url: "https://www.infoq.com/presentations/netflix-streaming-scale/"
          description: "Netflix's approach to global video distribution"
        - title: "Building Recommendation Systems"
          url: "https://developers.google.com/machine-learning/recommendation"
          description: "Google's guide to scalable recommendation systems"
        - title: "CDN Architecture Deep Dive"
          url: "https://blog.cloudflare.com/how-we-built-spectrum/"
          description: "Modern CDN architecture and optimization strategies"
        - title: "Live Streaming Architecture"
          url: "https://aws.amazon.com/solutions/implementations/live-streaming-on-aws/"
          description: "Building scalable live streaming infrastructure"
        - title: "Video Compression and Encoding"
          url: "https://netflixtechblog.com/high-quality-video-encoding-at-scale-d159db052746"
          description: "Advanced video processing and compression techniques"
        - title: "System Design Interview - Volume 2"
          url: "https://www.amazon.com/System-Design-Interview-Insiders-Guide/dp/1736049119"
          description: "Comprehensive guide including video streaming system design"

      time_estimate: 135