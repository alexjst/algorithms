track: system_design
description: 28-day system design interview preparation with spaced repetition
weeks:
  week1:
  - day: 1
    topic: Scalability Fundamentals
    activity: Master scaling strategies, capacity planning, performance optimization,
      and bottleneck identification.
    detailed_content: "Scalability Concepts:\n- Horizontal scaling (Scale-out): Add\
      \ more servers/instances\n  * Pros: No single point of failure, cost-effective,\
      \ unlimited scaling potential\n  * Cons: Complex coordination, network overhead,\
      \ data consistency challenges\n  * Examples: Web servers, microservices, NoSQL\
      \ databases\n- Vertical scaling (Scale-up): Increase power of existing servers\
      \ (CPU, RAM, storage)\n  * Pros: Simple implementation, no application changes,\
      \ strong consistency\n  * Cons: Hardware limits, single point of failure, expensive\
      \ at high end\n  * Examples: Traditional databases, legacy applications, compute-intensive\
      \ tasks\n\nScalability Dimensions:\n- Load scalability: Handle more concurrent\
      \ users/requests\n- Data scalability: Store and process larger datasets\n- Geographic\
      \ scalability: Serve users across multiple regions\n- Administrative scalability:\
      \ Manage larger systems with same effort\n\nPerformance Metrics:\n- Latency:\
      \ Response time for single request (p50, p95, p99 percentiles)\n- Throughput:\
      \ Requests processed per unit time (QPS, TPS)\n- Availability: System uptime\
      \ percentage (99.9% = 8.77 hours downtime/year)\n- Consistency: Data accuracy\
      \ across distributed components\n- Reliability: System's ability to perform\
      \ correctly during failures\n\nCapacity Planning Process:\n1. Baseline measurement:\
      \ Current traffic patterns and resource usage\n2. Growth projection: Expected\
      \ user growth and feature expansion\n3. Load modeling: Peak traffic scenarios\
      \ and seasonal variations\n4. Resource estimation: CPU, memory, storage, network\
      \ requirements\n5. Testing validation: Load testing to verify assumptions\n\
      6. Monitoring setup: Track actual vs predicted performance\n\nCommon Bottlenecks:\n\
      - CPU bound: Complex calculations, encryption, compression\n- Memory bound:\
      \ Large datasets, caching, in-memory processing\n- I/O bound: Database queries,\
      \ file operations, network calls\n- Network bound: Data transfer, API calls,\
      \ distributed operations\n- Database bound: Complex queries, lock contention,\
      \ slow storage\n\nScalability Patterns:\n- Stateless design: No server-side\
      \ session state for easy horizontal scaling\n- Asynchronous processing: Decouple\
      \ time-consuming operations\n- Caching: Reduce expensive operations through\
      \ data reuse\n- Data partitioning: Distribute data across multiple storage systems\n\
      - Load balancing: Distribute traffic across multiple servers\n- Auto-scaling:\
      \ Dynamically adjust resources based on demand\n\nDesign Principles:\n- Design\
      \ for failure: Assume components will fail and plan accordingly\n- Decouple\
      \ components: Reduce dependencies between system parts\n- Scale horizontally:\
      \ Prefer adding servers over upgrading hardware\n- Cache aggressively: Store\
      \ frequently accessed data closer to users\n- Optimize for bottlenecks: Focus\
      \ efforts on limiting resources\n- Monitor everything: Track metrics to identify\
      \ issues early\n\nLoad Estimation Calculations:\n- Daily Active Users (DAU)\
      \ to QPS: DAU × daily requests ÷ 86,400 seconds\n- Peak traffic estimation:\
      \ Average QPS × 2-10 (depends on usage patterns)\n- Storage estimation: Records\
      \ × record size × growth factor × replication factor\n- Bandwidth estimation:\
      \ QPS × average response size + growth buffer\n- Memory estimation: Active dataset\
      \ size + buffer for operations\n\nReal-world Examples:\n- Netflix: Horizontal\
      \ scaling with microservices, CDN for content delivery\n- Facebook: Massive\
      \ horizontal scaling, custom database sharding\n- Amazon: Service-oriented architecture,\
      \ auto-scaling infrastructure\n- Google: Distributed systems design, MapReduce\
      \ for data processing\n"
    resources:
    - title: 'ByteByteGo: How to Scale to Millions of Users'
      url: https://www.youtube.com/@ByteByteGo
      description: Visual explanation of scaling strategies and performance optimization
    - title: 'ByteByteGo: Top 20 System Design Concepts'
      url: https://blog.bytebytego.com/p/ep160-top-20-system-design-concepts
      description: Overview of key system design concepts with diagrams
    - title: 'Hello Interview: Core Concepts - System Design in a Hurry'
      url: https://www.hellointerview.com/learn/system-design/in-a-hurry/core-concepts
      description: FAANG-focused guide to scalability fundamentals
    - title: Designing Data-Intensive Applications - Chapter 1
      url: https://dataintensive.net/
      description: Fundamental concepts of scalable systems
    - title: High Scalability Blog
      url: https://highscalability.com/
      description: Real-world scalability case studies
    practice_questions:
      estimation:
      - question: Calculate QPS for 10M DAU with 50 requests per user per day. What's
          peak QPS assuming 5x multiplier?
        answer: 'Average QPS: (10M × 50) / 86,400s = 5,787 QPS. Peak QPS: 5,787 ×
          5 = 28,935 QPS. Need servers handling ~30K QPS. If each server handles 1K
          QPS, need 30 servers with load balancing.'
      - question: 'Social media app: 1M users, 10 posts/day average, 2KB per post.
          Calculate daily storage and annual growth.'
        answer: 'Daily: 1M × 10 × 2KB = 20GB/day. Annual: 20GB × 365 = 7.3TB/year.
          With 3x replication: 22TB/year. Plan for 30-40TB storage with buffer. Cost:
          ~$600/month on cloud storage.'
      - question: 'Video streaming: 100K concurrent streams at 1080p (5Mbps). Calculate
          total bandwidth and CDN requirements.'
        answer: 'Total bandwidth: 100K × 5Mbps = 500Gbps = 62.5GB/s. CDN egress: ~$0.08/GB
          = $5/sec = $432K/day. Need multi-tier CDN with regional POPs to reduce costs
          70-80%.'
      - question: 'E-commerce site: 1M products, 100 bytes metadata each, 1000 QPS
          total traffic. With 10x read/write ratio and 95% cache hit rate, estimate
          database IOPS.'
        answer: 'Storage: 1M × 100B = 100MB (tiny). Traffic split: 1000 QPS = 900
          reads + 100 writes. With 95% cache hit rate: 900 × 5% = 45 cache misses.
          Database IOPS: 45 reads + 100 writes = ~145 IOPS. Use Redis cache + standard
          SSD. Cost: ~$30/month RDS + $20/month Redis.'
      - question: 'Chat app: 50M users, average 100 messages/day, 200 bytes per message.
          Calculate storage for 1 year.'
        answer: 'Daily: 50M × 100 × 200B = 1TB/day. Annual: 365TB. With 3x replication:
          1.1PB. Archive old messages to cheap storage after 90 days to save 70% costs.
          Active storage: ~300TB.'
      concepts:
      - question: A startup expects 100x user growth in 6 months. Should they optimize
          current code or plan for horizontal scaling?
        answer: Plan for horizontal scaling NOW. Optimization buys 2-5x improvement,
          but 100x needs architecture change. Design stateless services, decouple
          components, use message queues, implement caching. Vertical scaling has
          limits; horizontal scales infinitely.
      - question: Your database CPU hits 90% during traffic spikes. Compare vertical
          scaling vs read replicas vs sharding solutions.
        answer: 'Read replicas: Best if read-heavy (80%+ reads), fast to deploy, $500/month
          per replica. Vertical scaling: Quick fix, works for 6-12 months, hits hardware
          limits ($5K/month for large instance). Sharding: Complex but scales indefinitely,
          needed for write-heavy or >5TB data.'
      - question: When would you choose vertical scaling over horizontal scaling?
          Give 3 specific scenarios.
        answer: 1) Legacy monolith with tight coupling (refactoring cost > hardware),
          2) Strong ACID requirements on single database, 3) Small to medium scale
          (<10K QPS) where simplicity matters. Vertical is simpler but has ceiling
          at ~96 cores/$20K month.
      - question: What's the difference between scalability and performance? Can you
          have one without the other?
        answer: 'Performance: Speed for one user (latency). Scalability: Maintaining
          performance as users increase (throughput). Yes, can have one without other:
          Fast but doesn''t scale (single-threaded), or scales but slow (inefficient
          algorithm). Need both for production systems.'
      - question: How do you identify if your bottleneck is CPU, memory, I/O, or network
          bound?
        answer: 'CPU: High CPU%, low I/O wait. Memory: Swapping, OOM errors, high
          cache misses. I/O: High disk queue, >10ms latencies. Network: High bandwidth
          usage, packet loss. Use: top, iostat, sar, netstat. Profile in production
          with <1% overhead tools.'
      - question: Explain the trade-offs between consistency and scalability in distributed
          systems.
        answer: 'CAP theorem: Can''t have all of Consistency, Availability, Partition-tolerance.
          Strong consistency (ACID) limits scalability, requires coordination (2PC,
          Raft). Eventual consistency scales better but has stale reads. Choose based
          on use case: banking=strong, social feeds=eventual.'
      tradeoffs:
      - question: 'Compare costs: 10 small servers vs 1 large server with same total
          capacity. Consider failure scenarios.'
        answer: '10 small: $100/mo each = $1K total, lose 10% capacity on failure,
          easier updates. 1 large: $900/mo, lose 100% on failure, simpler ops. Choose
          small for: High availability needs. Choose large for: Simple workloads,
          strong consistency needs.'
      - question: 'Horizontal scaling with microservices vs vertical scaling with
          monolith: when to choose each?'
        answer: 'Horizontal+microservices: Large teams (>50 eng), high scale (>100K
          QPS), independent deployments needed. Complex, 2-3x ops cost. Vertical+monolith:
          Small teams (<10), moderate scale (<10K QPS), rapid development. Simple,
          faster to market.'
      - question: 'Auto-scaling vs over-provisioning: cost and performance trade-offs'
        answer: 'Auto-scaling: Saves 40-60% cost in variable traffic, 2-5 min scale-up
          lag, complex setup. Over-provisioning: Always fast, simple, wastes 40-60%
          capacity during off-peak. Use auto-scaling for: Predictable patterns with
          >3x variance.'
      - question: 'Strong consistency vs eventual consistency: impact on scalability'
        answer: 'Strong consistency: Single-region only, <10K writes/sec typical,
          synchronous coordination slows writes. Eventual: Multi-region, 100K+ writes/sec,
          async replication, stale reads for seconds. Strong for: Financial, inventory.
          Eventual for: Social media, analytics.'
      - question: 'Synchronous vs asynchronous processing: latency and scalability
          implications'
        answer: 'Synchronous: User waits, predictable, <200ms critical for UX, limits
          throughput to connection pool size. Asynchronous: Queue+workers, high throughput
          (100K+ jobs/sec), unpredictable timing. Use async for: Email, reports, batch
          processing. Sync for: Payments, bookings.'
      scenarios:
      - Your API response time degrades from 100ms to 2s as traffic doubles. Diagnose
        potential causes and solutions.
      - Design a system for a viral social media feature that could get 100x normal
        traffic in 1 hour.
      - A legacy monolith serves 1000 QPS. Design a migration strategy to handle 10,000
        QPS in 6 months.
      - Your database is at 80% capacity. Plan a scaling strategy that maintains <
        100ms query latency.
      - Traffic varies 10x between peak and off-peak hours. Design a cost-effective
        auto-scaling strategy.
    time_estimate: 60
  - day: 2
    topic: Load Balancing
    activity: Master load balancing architecture, algorithms, health checks, and advanced
      routing strategies.
    detailed_content: "Load Balancer Types:\n- L4 (Transport/Network Layer): Routes\
      \ based on IP address and port\n  * Pros: High performance, low latency, protocol\
      \ agnostic\n  * Cons: Limited routing intelligence, no content-based decisions\n\
      \  * Use cases: TCP/UDP traffic, high-throughput scenarios, database connections\n\
      - L7 (Application Layer): Routes based on content (HTTP headers, URLs, cookies)\n\
      \  * Pros: Advanced routing logic, SSL termination, content manipulation\n \
      \ * Cons: Higher latency, resource intensive, protocol specific\n  * Use cases:\
      \ Web applications, microservices, API gateways\n\nLoad Balancing Algorithms:\n\
      - Round Robin: Sequential distribution to each server\n  * Pros: Simple, fair\
      \ distribution\n  * Cons: Ignores server capacity and current load\n  * Best\
      \ for: Homogeneous servers with similar response times\n- Weighted Round Robin:\
      \ Distribution based on assigned server weights\n  * Pros: Accounts for different\
      \ server capacities\n  * Cons: Static weights don't adapt to real-time load\n\
      \  * Best for: Heterogeneous server capacities\n- Least Connections: Route to\
      \ server with fewest active connections\n  * Pros: Considers current server\
      \ load\n  * Cons: Overhead of tracking connections\n  * Best for: Long-lived\
      \ connections, varying request processing times\n- Weighted Least Connections:\
      \ Combines weights with connection count\n  * Pros: Best of both weighted and\
      \ least connections\n  * Cons: More complex implementation\n  * Best for: Mixed\
      \ server capacities with varying load patterns\n- IP Hash: Consistent routing\
      \ based on client IP hash\n  * Pros: Session affinity without server-side state\n\
      \  * Cons: Uneven distribution if clients cluster by IP\n  * Best for: Applications\
      \ requiring session persistence\n- Least Response Time: Route to server with\
      \ fastest response\n  * Pros: Optimizes for performance\n  * Cons: Complex monitoring\
      \ required\n  * Best for: Performance-critical applications\n- Resource Based:\
      \ Route based on server resource utilization\n  * Pros: Dynamic adaptation to\
      \ server health\n  * Cons: Requires continuous monitoring\n  * Best for: Cloud\
      \ environments with auto-scaling\n\nHealth Check Strategies:\n- Active Health\
      \ Checks:\n  * HTTP/HTTPS: GET requests to health endpoints\n  * TCP: Connection\
      \ establishment tests\n  * Custom: Application-specific health scripts\n- Passive\
      \ Health Checks: Monitor actual traffic for failures\n- Health Check Parameters:\n\
      \  * Interval: Frequency of health checks (e.g., every 30 seconds)\n  * Timeout:\
      \ Max time to wait for response (e.g., 5 seconds)\n  * Threshold: Consecutive\
      \ failures before marking unhealthy (e.g., 3)\n  * Recovery: Consecutive successes\
      \ before marking healthy (e.g., 2)\n\nAdvanced Features:\n- Session Affinity\
      \ (Sticky Sessions):\n  * Cookie-based: Use application cookies for routing\n\
      \  * IP-based: Route based on client IP\n  * Pros: Maintains session state\n\
      \  * Cons: Uneven load distribution, scaling challenges\n- SSL Termination:\n\
      \  * Offload SSL processing from backend servers\n  * Centralized certificate\
      \ management\n  * Reduces backend server load\n- SSL Pass-through: Forward encrypted\
      \ traffic without decryption\n- Connection Multiplexing: Reuse backend connections\n\
      - Rate Limiting: Control request rates per client/IP\n- Geographic Routing:\
      \ Route based on client location\n\nLoad Balancer Placement:\n- Internet-facing:\
      \ External traffic distribution\n- Internal: Service-to-service communication\n\
      - Multi-tier: Load balancers at multiple application layers\n- Cross-zone: Distribution\
      \ across availability zones\n- Global: DNS-based geographic distribution\n\n\
      High Availability Patterns:\n- Active-Passive: Primary LB with standby failover\n\
      - Active-Active: Multiple LBs sharing load\n- Cross-region: Load balancers in\
      \ multiple regions\n- Anycast: Single IP announced from multiple locations\n\
      \nPerformance Considerations:\n- Throughput: Requests per second capacity\n\
      - Latency: Additional delay introduced\n- Connection Limits: Maximum concurrent\
      \ connections\n- CPU/Memory: Resource usage for different algorithms\n- Network\
      \ Bandwidth: Throughput requirements\n\nFailure Scenarios:\n- Server failures:\
      \ Automatic traffic redirection\n- Load balancer failures: Failover to secondary\
      \ LB\n- Network partitions: Split-brain prevention\n- Cascading failures: Circuit\
      \ breaker integration\n- Overload protection: Backpressure and throttling\n\n\
      Implementation Types:\n- Hardware Load Balancers:\n  * Pros: High performance,\
      \ dedicated hardware, vendor support\n  * Cons: Expensive, vendor lock-in, limited\
      \ flexibility\n  * Examples: F5 Big-IP, Citrix NetScaler\n- Software Load Balancers:\n\
      \  * Pros: Cost-effective, flexible, programmable\n  * Cons: Shared resources,\
      \ performance limits\n  * Examples: HAProxy, NGINX, Apache HTTPd\n- Cloud Load\
      \ Balancers:\n  * Pros: Managed service, auto-scaling, integrated monitoring\n\
      \  * Cons: Vendor lock-in, limited customization\n  * Examples: AWS ALB/NLB,\
      \ Google Cloud Load Balancing, Azure Load Balancer\n\nGlobal Load Balancing:\n\
      - DNS-based: Use DNS to route to different regions\n- GeoDNS: Route based on\
      \ client geographic location\n- Anycast: Announce same IP from multiple locations\n\
      - CDN integration: Combine with content delivery networks\n"
    resources:
    - title: AWS Application Load Balancer Guide
      url: https://docs.aws.amazon.com/elasticloadbalancing/latest/application/
      description: Practical load balancing concepts
    - title: NGINX Load Balancing
      url: https://nginx.org/en/docs/http/load_balancing.html
      description: Software load balancing implementation
    - title: HAProxy Configuration Guide
      url: https://www.haproxy.org/download/1.7/doc/configuration.txt
      description: Advanced load balancing configuration
    - title: Load Balancing Strategies
      url: https://kemptechnologies.com/load-balancer/load-balancing-algorithms-techniques/
      description: Comprehensive algorithm comparison
    practice_questions:
      estimation:
      - question: System serves 50K RPS. Each LB handles 10K RPS. How many LBs needed?
          Consider N+1 redundancy.
        answer: 'Need: 50K / 10K = 5 LBs at capacity. With N+1 redundancy: 6 LBs total.
          During failure, 5 LBs handle 50K = 10K RPS each (at capacity). Use DNS round-robin
          or L4 LB in front. Cost: ~$300/month for 6 cloud LBs.'
      - question: LB adds 2ms latency. Backend processing takes 50ms. What's total
          response time and % overhead?
        answer: 'Total: 2ms + 50ms = 52ms. LB overhead: (2/52) × 100 = 3.8%. If targeting
          <100ms p99, this is acceptable. For <10ms systems, 2ms is 20% overhead -
          consider L4 or hardware LB (~0.5ms).'
      - question: 'Global system: 100K RPS total, 3 regions (40%, 35%, 25% split).
          Calculate regional LB requirements.'
        answer: 'US: 40K RPS / 10K = 4 LBs (+1 redundancy) = 5. EU: 35K / 10K = 4.
          APAC: 25K / 10K = 3. Total: 12 LBs. Use GeoDNS for routing. Cost: ~$600/month
          + GeoDNS $50/month.'
      - question: 'Health checks: 100 servers, check every 30s, 5ms per check. What''s
          monitoring overhead on LB?'
        answer: 'Checks/sec: 100 / 30 = 3.33 checks/sec. CPU time: 3.33 × 5ms = 16.7ms/sec
          = 1.67% CPU. Minimal overhead. Network: 3.33 × 1KB = 3.33KB/sec. Health
          check overhead is negligible.'
      concepts:
      - question: E-commerce site needs session persistence for shopping carts. Compare
          sticky sessions vs stateless design.
        answer: 'Sticky sessions: Simple, ties user to server, problematic on failures,
          limits scaling. Stateless: Store cart in Redis/DB, any server handles request,
          scales better, 2-5ms cache lookup cost. Recommendation: Stateless with Redis
          for e-commerce flexibility.'
      - question: 'Microservices architecture: when to use L4 vs L7 load balancing
          for service-to-service communication?'
        answer: 'L4: High-throughput internal services (>10K RPS), uniform traffic,
          low latency needs (<1ms). L7: API gateway, path-based routing, canary deployments,
          need metrics/tracing. Service mesh (Envoy) provides L7 features with <5ms
          overhead.'
      - question: Your servers have different capacities (2-core, 4-core, 8-core).
          Which algorithm maximizes throughput?
        answer: 'Weighted Least Connections. Assign weights: 2-core=1, 4-core=2, 8-core=4.
          Considers both capacity AND current load. Achieves ~30% better utilization
          than round-robin on heterogeneous servers. Alternative: Resource-based (CPU/memory)
          for dynamic adaptation.'
      - question: How do you prevent a single slow server from affecting the entire
          system?
        answer: '1) Timeout: Set 2-3x median response time, 2) Circuit breaker: Auto-remove
          after 3 consecutive failures, 3) Least Response Time algorithm, 4) Active
          health checks (every 5-10s), 5) Outlier detection: Remove servers at p99
          >2x p50. Monitor with percentile metrics.'
      - question: Explain the difference between load balancing and service discovery.
        answer: 'Service discovery: Finding available services (Consul, etcd, DNS).
          Load balancing: Distributing requests among found services. Pattern: Service
          discovery provides IPs → LB distributes traffic. Modern: Service mesh combines
          both (client-side LB with central registry).'
      - question: How do you handle SSL termination in a microservices environment?
        answer: 'Option 1: Edge LB terminates SSL, internal HTTP (simple, central
          cert management). Option 2: End-to-end TLS with service mesh (secure but
          complex). Option 3: Hybrid - terminate at edge + re-encrypt for sensitive
          services. Edge termination saves CPU on backend servers.'
      tradeoffs:
      - question: 'Round Robin vs Least Connections: when does each perform better?
          Consider connection patterns.'
        answer: 'Round Robin: Short uniform requests (REST APIs <50ms), stateless
          services. 5-10% better throughput for predictable workloads. Least Connections:
          Long-lived connections (WebSockets, DB connections), variable request times.
          Prevents overload on slow servers.'
      - question: 'Hardware vs Software vs Cloud load balancers: cost, performance,
          and operational trade-offs'
        answer: 'Hardware: $10K-50K, 1M+ RPS, complex ops, 99.999% uptime. Software:
          $0 (NGINX), 100K RPS, flexible, requires management. Cloud: $20/month, 50K
          RPS, auto-scale, managed. Choose: Cloud for startups, Hardware for high-perf,
          Software for customization.'
      - question: 'Active-Active vs Active-Passive LB setup: availability vs complexity
          analysis'
        answer: 'Active-Active: 100% capacity used, no failover delay, even load distribution,
          more complex health checks. Active-Passive: 50% capacity idle, failover
          in 30-60s, simple, wastes resources. Use Active-Active for: High availability
          needs, variable traffic patterns.'
      - question: 'Session affinity vs stateless design: scalability and complexity
          implications'
        answer: 'Session affinity: Simple app code, limits scaling, lost sessions
          on failure, uneven load. Stateless: Scales infinitely, survives failures,
          needs shared state (Redis), 2-5ms overhead. Modern systems: Stateless with
          sticky cookies for performance (best effort affinity).'
      - question: 'L4 vs L7 load balancing: performance vs features comparison'
        answer: 'L4: 1M+ RPS, <0.5ms latency, IP/port only, no SSL insights, simple.
          L7: 100K RPS, 2-5ms latency, path/header routing, WAF, metrics, SSL termination.
          Use L4 for: Internal services, throughput critical. L7 for: External APIs,
          microservices, security needs.'
      scenarios:
      - Design load balancing for a video streaming service with global users and
        varying content popularity.
      - Your LB becomes a bottleneck at 50K RPS. Design a scaling strategy without
        service interruption.
      - Backend servers randomly fail during traffic spikes. Design health checking
        to minimize impact.
      - 'Multi-region deployment: design global load balancing with automatic failover
        between regions.'
      - 'Real-time gaming application: design load balancing to minimize latency and
        maintain session continuity.'
    time_estimate: 60
    video_resources:
    - title: 'ByteByteGo: What is a CDN?'
      url: https://www.youtube.com/@ByteByteGo
      duration: 8 min
      description: How content delivery networks work
      priority: high
    - title: 'ByteByteGo: How DNS Works'
      url: https://www.youtube.com/@ByteByteGo
      duration: 10 min
      description: Domain name system explained
      priority: high
  - day: 3
    topic: Caching Strategies
    activity: Learn caching patterns, cache levels, eviction policies, and invalidation
      strategies for system performance.
    detailed_content: 'Cache Patterns:

      - Cache-aside (Lazy Loading): Application manages cache, read miss triggers
      DB read + cache write

      - Write-through: Write to cache and DB simultaneously (synchronous, strong consistency)

      - Write-behind (Write-back): Write to cache first, DB later (asynchronous, better
      performance)

      - Write-around: Write directly to DB, bypass cache (prevents cache pollution
      from infrequent writes)

      - Refresh-ahead: Proactively refresh before expiration


      Cache Levels:

      - Browser cache: Client-side storage (HTML5 localStorage, sessionStorage)

      - CDN/Edge cache: Geographic distribution for static content

      - Reverse proxy cache: Server-side cache (Nginx, Varnish)

      - Application cache: In-memory cache within application (Redis, Memcached)

      - Database cache: Query result cache, buffer pools


      Eviction Policies:

      - LRU (Least Recently Used): Best for temporal locality

      - LFU (Least Frequently Used): Best for frequency-based access

      - FIFO (First In, First Out): Simple but less effective

      - TTL (Time To Live): Time-based expiration

      - Random: Simple, surprisingly effective for large caches


      Cache Invalidation Strategies:

      - TTL-based expiration: Automatic expiration after fixed time

      - Manual invalidation: Explicit cache deletion on data updates

      - Tag-based invalidation: Group related cache entries for bulk invalidation

      - Event-driven invalidation: Invalidate based on system events

      - Write-through invalidation: Remove stale data during writes


      Memcached vs Redis:

      Memcached:

      - Simple key-value store with string keys and binary values

      - Supports complex data structures only through client-side serialization

      - Multi-threaded, faster for simple operations

      - No persistence, pure in-memory cache

      - Simpler deployment and operation


      Redis:

      - Rich data types: strings, lists, sets, sorted sets, hashes, bitmaps, streams

      - Server-side operations on data structures (LPUSH, SADD, HINCRBY)

      - Single-threaded (6.0+ has I/O threading)

      - Optional persistence (RDB snapshots, AOF logs)

      - Advanced features: pub/sub, Lua scripting, clustering, replication


      Cache Warming and Cold Start:

      - Cache warming: Pre-populate cache with likely-to-be-accessed data

      - Lazy loading vs eager loading trade-offs

      - Cache priming strategies during deployment

      - Handling thundering herd problem during cold starts

      '
    resources:
    - title: Redis Caching Patterns
      url: https://redis.io/docs/manual/patterns/
      description: Common caching implementation patterns
    - title: Memcached vs Redis
      url: https://aws.amazon.com/elasticache/redis-vs-memcached/
      description: Choosing the right cache technology
    - title: Cache Invalidation Strategies
      url: https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Invalidation.html
      description: Practical cache invalidation techniques
    practice_questions:
      estimation:
      - question: Cache hit ratio is 80% with 10ms cache lookup and 100ms DB query.
          What's average response time?
        answer: 'Average time = (0.8 × 10ms) + (0.2 × 100ms) = 8ms + 20ms = 28ms.
          Without cache: 100ms. Speedup: 100/28 = 3.57x. To reach <20ms avg, need
          90% hit ratio: (0.9 × 10) + (0.1 × 100) = 19ms.'
      - question: 1M cache entries, 1KB each. Using LRU, how much memory needed? What
          if 20% are hot data?
        answer: 'Total: 1M × 1KB = 1GB + overhead (~200MB for LRU metadata) = 1.2GB.
          If 20% hot data: 200K entries × 1KB = 200MB base + 40MB overhead = 240MB.
          Can serve 80% of traffic with 240MB cache if using LRU. Cost: ~$30/month
          for 1.2GB Redis.'
      - question: API serves 1000 RPS. Cache TTL is 60s. How many cache misses per
          minute for new data?
        answer: 'Requests/min: 1000 × 60 = 60K. With 60s TTL, each unique key misses
          once per minute. If 1000 unique keys: 1000 misses/min = 16.7 misses/sec
          = 1.67% miss rate. DB load: 16.7 QPS vs 1000 QPS without cache (60x reduction).'
      concepts:
      - question: When would you choose write-through vs write-behind caching?
        answer: 'Write-through: When data consistency critical (financial, inventory).
          Writes are 2x slower (cache + DB), but always consistent, survives cache
          failures. Write-behind: High write throughput needs (social media, analytics),
          accepts eventual consistency, 10x faster writes, risk of data loss if cache
          fails before DB write.'
      - question: How do you handle cache invalidation in a distributed system?
        answer: '1) Pub/Sub: Publish invalidation events to all cache nodes (Redis
          Pub/Sub, Kafka). 2) TTL: Short expiration (30-60s) for simple cases. 3)
          Cache tags: Group related entries, invalidate by tag. 4) Version keys: Append
          version to cache key, increment on update. Use Pub/Sub for low latency (<100ms)
          or TTL for simplicity.'
      - question: What's the thundering herd problem and how do you solve it?
        answer: 'Problem: Popular cache key expires, 1000s of requests hit DB simultaneously.
          Solutions: 1) Mutex lock: First request rebuilds cache, others wait (adds
          50-100ms latency). 2) Probabilistic early expiration: Refresh before expiry
          with random jitter. 3) Background refresh: Separate job refreshes popular
          keys. Use probabilistic for <10ms overhead.'
      - question: When would you use Redis over Memcached?
        answer: 'Redis: Need data structures (lists, sets, sorted sets), persistence,
          pub/sub, atomic operations, single large values (512MB). Memcached: Simple
          key-value, multi-threaded performance (2x faster for simple GET/SET), no
          persistence needed, distributed environment. Use Redis for complex caching,
          Memcached for pure speed.'
      - question: How do you prevent cache stampede during high traffic?
        answer: '1) Request coalescing: Deduplicate concurrent requests for same key
          (reduces DB load 100x). 2) Negative caching: Cache ''not found'' for 30s.
          3) Stale-while-revalidate: Serve stale data while refreshing. 4) Rate limiting:
          Limit DB queries per key. 5) Circuit breaker: Stop queries after failures.
          Combine for 99.9% stampede prevention.'
      tradeoffs:
      - question: Compare cache-aside vs write-through patterns for a read-heavy vs
          write-heavy system
        answer: 'Cache-aside (read-heavy): App controls caching, lazy loading, 90%
          hit rate typical, 2x DB load on misses. Best for: 10:1 read:write ratio.
          Write-through (write-heavy): Always consistent, every write updates cache+DB
          (2x write latency), no stale data. Best for: Strong consistency needs, 1:1
          read:write ratio.'
      - question: 'TTL-based vs event-driven cache invalidation: pros and cons'
        answer: 'TTL: Simple, no coordination, works across regions, but serves stale
          data (0-60s). Event-driven: Immediate consistency (<100ms), complex infrastructure
          (Kafka/SNS), coordination overhead. Use TTL for: Analytics, dashboards (stale
          OK). Event-driven for: Inventory, pricing (stale not OK).'
      - question: Multi-level caching benefits vs complexity overhead
        answer: 'Benefits: Browser (0ms) → CDN (20ms) → App cache (2ms) → DB (100ms).
          Hit ratios multiply: 50% × 80% × 90% = 0.4% DB load. Complexity: 3 invalidation
          layers, consistency challenges, 5x ops overhead. Use for: >1M DAU, global
          users. Skip for: <100K DAU.'
      - question: 'Redis persistence options: RDB vs AOF trade-offs'
        answer: 'RDB (snapshots): Fast recovery, compact, periodic (every 5min = up
          to 5min data loss), lower disk I/O. AOF (append log): Every write logged,
          <1s data loss, larger files, slower recovery. Hybrid: RDB hourly + AOF =
          best of both. Use RDB-only for: Recreatable data. AOF for: Critical data.'
      - question: 'Distributed cache vs local cache: when to use each?'
        answer: 'Distributed (Redis): Shared across servers, 2-5ms latency, 100GB+
          capacity, complex ops, $500/month. Local (in-process): 0.1ms latency, limited
          to server RAM (4-8GB), no network, free. Pattern: Local for hot data (10%),
          distributed for long-tail (90%). Achieve 0.5ms avg latency vs 2ms distributed-only.'
    time_estimate: 45
    video_resources:
    - title: 'ByteByteGo: Database Replication Explained'
      url: https://www.youtube.com/@ByteByteGo
      duration: 12 min
      description: Master-slave and master-master replication
      priority: high
    - title: 'ByteByteGo: SQL vs NoSQL'
      url: https://www.youtube.com/@ByteByteGo
      duration: 10 min
      description: When to use each database type
      priority: high
  - day: 4
    topic: Database Fundamentals
    activity: Master RDBMS vs NoSQL selection, ACID properties, transactions, and
      performance optimization.
    detailed_content: "RDBMS (Relational Database) Characteristics:\n- ACID Properties:\n\
      \  * Atomicity: All operations in transaction succeed or fail together\n  *\
      \ Consistency: Database remains in valid state after transactions\n  * Isolation:\
      \ Concurrent transactions don't interfere with each other\n  * Durability: Committed\
      \ changes survive system failures\n- Strong consistency: All nodes see same\
      \ data simultaneously\n- JOINS: Complex queries across multiple tables\n- Schema\
      \ enforcement: Predefined structure with data types\n- Mature ecosystem: Extensive\
      \ tooling and expertise\n- Examples: PostgreSQL, MySQL, Oracle, SQL Server\n\
      \nSQL Transaction Isolation Levels:\n- Read Uncommitted: Lowest isolation, dirty\
      \ reads possible\n- Read Committed: Prevents dirty reads, allows non-repeatable\
      \ reads\n- Repeatable Read: Prevents dirty and non-repeatable reads\n- Serializable:\
      \ Highest isolation, prevents all phenomena\n- Trade-off: Higher isolation =\
      \ lower concurrency\n\nDatabase Normalization:\n- 1NF: Eliminate repeating groups,\
      \ atomic values\n- 2NF: Remove partial dependencies on composite keys\n- 3NF:\
      \ Remove transitive dependencies\n- Benefits: Reduces redundancy, prevents anomalies\n\
      - Trade-offs: More JOINs, complex queries, potential performance impact\n\n\
      NoSQL Database Types:\n- Document Stores (MongoDB, CouchDB):\n  * Flexible JSON-like\
      \ documents\n  * Schema evolution without migration\n  * Rich queries and indexing\n\
      \  * Use cases: Content management, catalogs, user profiles\n- Key-Value Stores\
      \ (Redis, DynamoDB, Riak):\n  * Simple get/put operations\n  * High performance\
      \ and scalability\n  * Limited querying capabilities\n  * Use cases: Caching,\
      \ session storage, real-time recommendations\n- Column-Family (Cassandra, HBase):\n\
      \  * Wide column storage model\n  * Excellent write performance\n  * Time-series\
      \ data optimization\n  * Use cases: IoT data, logging, analytics\n- Graph Databases\
      \ (Neo4j, Amazon Neptune):\n  * Relationships as first-class citizens\n  * Complex\
      \ relationship queries\n  * ACID transactions on graphs\n  * Use cases: Social\
      \ networks, fraud detection, recommendation engines\n- Time-Series Databases\
      \ (InfluxDB, TimescaleDB, Prometheus):\n  * Optimized for time-stamped data\n\
      \  * Automatic data retention and rollups\n  * Time-range queries and aggregations\n\
      \  * Use cases: Monitoring, IoT sensors, financial data\n\nDatabase Selection\
      \ Criteria:\n- Data Structure:\n  * Structured, relational data → RDBMS\n  *\
      \ Semi-structured, evolving schema → Document stores\n  * Simple key-value access\
      \ → Key-value stores\n  * Time-series data → Time-series databases\n  * Complex\
      \ relationships → Graph databases\n- Scalability Requirements:\n  * Vertical\
      \ scaling acceptable → RDBMS\n  * Massive horizontal scaling needed → NoSQL\n\
      - Consistency Requirements:\n  * Strong consistency critical → RDBMS\n  * Eventual\
      \ consistency acceptable → NoSQL\n- Query Complexity:\n  * Complex JOINs and\
      \ analytics → RDBMS\n  * Simple lookups and aggregations → NoSQL\n- Team Expertise:\n\
      \  * SQL expertise → RDBMS\n  * NoSQL experience → NoSQL\n\nIndexing Strategies:\n\
      - Primary Index: Unique identifier for fast lookups\n- Secondary Index: Additional\
      \ access paths for queries\n- Composite Index: Multiple columns for complex\
      \ queries\n- Partial Index: Index subset of rows meeting conditions\n- Full-text\
      \ Index: Search within text content\n- Trade-offs: Faster reads vs slower writes,\
      \ storage overhead\n\nPerformance Optimization:\n- Query Optimization:\n  *\
      \ Use EXPLAIN plans to analyze query execution\n  * Avoid SELECT *, use specific\
      \ columns\n  * Optimize WHERE clauses and JOINs\n  * Use appropriate indexes\n\
      - Connection Pooling: Reuse database connections\n- Read Replicas: Distribute\
      \ read traffic\n- Caching: Cache frequently accessed data\n- Partitioning: Distribute\
      \ data across multiple nodes\n\nConsistency Models:\n- Strong Consistency: All\
      \ nodes return same data\n- Eventual Consistency: Nodes converge to same state\
      \ over time\n- Causal Consistency: Preserves order of causally related operations\n\
      - Session Consistency: Consistency within user session\n- Monotonic Consistency:\
      \ Never return older versions\n\nDatabase Trade-offs:\n- RDBMS vs NoSQL:\n \
      \ * Consistency vs Availability (CAP theorem)\n  * Query flexibility vs Performance\n\
      \  * Schema rigidity vs Flexibility\n  * Vertical vs Horizontal scaling\n  *\
      \ Mature tooling vs Modern features\n- SQL vs NoSQL Decision Matrix:\n  * Complex\
      \ queries → SQL\n  * Simple operations → NoSQL\n  * ACID requirements → SQL\n\
      \  * High availability → NoSQL\n  * Rapid development → NoSQL\n  * Data integrity\
      \ critical → SQL\n\nCommon Anti-patterns:\n- Using NoSQL for complex relational\
      \ data\n- Using RDBMS for simple key-value access\n- Over-normalization leading\
      \ to complex JOINs\n- Under-indexing causing slow queries\n- Single database\
      \ for all use cases\n"
    resources:
    - title: MongoDB vs PostgreSQL
      url: https://www.mongodb.com/compare/mongodb-postgresql
      description: Document vs relational database comparison
    - title: NoSQL Database Types
      url: https://aws.amazon.com/nosql/
      description: Overview of NoSQL database categories
    - title: Database Design Fundamentals
      url: https://www.postgresql.org/docs/current/tutorial.html
      description: SQL database design principles
    - title: CAP Theorem Explained
      url: https://en.wikipedia.org/wiki/CAP_theorem
      description: Understanding consistency, availability, partition tolerance
    practice_questions:
      estimation:
      - question: 'E-commerce site: 1M products, 10M orders/year. Estimate storage
          for RDBMS vs document store approach.'
        answer: 'RDBMS: Products (1M × 5KB = 5GB) + Orders (10M × 2KB = 20GB) + OrderItems
          (30M × 500B = 15GB) + Indexes (30%) = 52GB. Document store: Embed order
          items in order doc (10M × 5KB = 50GB) + Products (5GB) = 55GB. Similar storage,
          but document approach reduces JOINs, 3x faster reads, harder analytics.'
      - question: 'Social network: 100M users, average 500 friends each. Calculate
          storage for graph vs relational representation.'
        answer: 'Relational: Users (100M × 1KB = 100GB) + Friendships (100M × 500
          × 16B = 800GB for edges) + Indexes (30%) = 1.17TB. Graph DB: Nodes (100GB)
          + Edges (800GB, but compressed) = ~700GB, 3x faster relationship queries.
          Graph wins for <3 hops traversal, SQL better for analytics.'
      - question: 'Time-series data: 1K metrics, 10K data points/minute. Storage and
          query performance comparison across database types.'
        answer: 'Data rate: 1K × 10K × 60 = 600M points/hour × 50B = 30GB/hour = 720GB/day.
          PostgreSQL: 720GB/day, slow aggregations (>10s). InfluxDB: 100GB/day (10x
          compression), <100ms aggregations. Cassandra: 500GB/day, fastest writes
          (1M/sec). Use InfluxDB for time-series workload.'
      - question: 'Banking system: 1M accounts, 100M transactions/day. Estimate IOPS
          and storage requirements.'
        answer: 'Storage: 1M × 2KB accounts = 2GB + 100M × 500B txns/day × 365 = 18TB/year.
          IOPS: 100M txns/day = 1,157 TPS × 10 IOPS/txn (reads/writes/logs) = 11,570
          IOPS. Use SSD (10K IOPS each), need 2 SSDs + replication = 4 total. Cost:
          ~$2K/month for storage + IOPS.'
      concepts:
      - question: When would you choose MongoDB over PostgreSQL for a new project?
          Consider 3 scenarios.
        answer: 'Choose MongoDB for: 1) Rapid prototyping with evolving schema (e.g.,
          startup MVP, can add fields without migrations), 2) Document-heavy apps
          (CMS, catalogs where data naturally nested), 3) High write throughput (>10K
          writes/sec, horizontal sharding easier). Choose PostgreSQL for: Complex
          JOINs, strong consistency, mature ecosystem, data integrity critical.'
      - question: Your application needs both complex analytics and real-time lookups.
          Design a polyglot persistence strategy.
        answer: 'Pattern: PostgreSQL for OLTP (transactional data, strong consistency)
          → Stream changes via CDC (Debezium) → Kafka → Load into: 1) Redis for real-time
          lookups (<1ms), 2) ClickHouse for analytics (columnar, fast aggregations).
          Keep PostgreSQL as source of truth. Cost: ~$1K/month for 3-system stack
          vs $300 single DB.'
      - question: Explain ACID properties with concrete examples. Why might you sacrifice
          some for NoSQL benefits?
        answer: 'ACID: Atomicity (transfer $100: debit+credit both succeed/fail),
          Consistency (balances sum unchanged), Isolation (concurrent transfers don''t
          interfere), Durability (committed = survives crash). Sacrifice for: 10x
          write throughput (eventual consistency), horizontal scaling (partition tolerance),
          sub-10ms latency (no distributed transactions). Use for: Social media, not
          banking.'
      - question: How do database isolation levels affect application concurrency
          and consistency?
        answer: 'Read Uncommitted: Dirty reads possible, highest concurrency (1000
          TPS), use for: analytics on replicas. Read Committed: Prevents dirty reads,
          good concurrency (800 TPS), default in PostgreSQL. Repeatable Read: Prevents
          non-repeatable reads, medium concurrency (500 TPS). Serializable: Full isolation,
          lowest concurrency (200 TPS), use for: financial transactions. Higher isolation
          = lower throughput.'
      - question: Design indexing strategy for an e-commerce product catalog with
          various search patterns.
        answer: '1) Primary: product_id (B-tree, unique), 2) category_id + price (composite,
          for browsing), 3) name, description (full-text GIN index for search), 4)
          created_at (B-tree for new arrivals), 5) Partial: WHERE in_stock=true (skip
          out-of-stock). Index overhead: 30% storage, 20% slower writes. Monitor query
          patterns, add indexes for slow queries (>100ms).'
      - question: When would you denormalize a relational database? What are the trade-offs?
        answer: 'Denormalize when: 1) Read:Write ratio >10:1 (dashboards, reports),
          2) Complex JOINs slow (>5 tables, >500ms), 3) Read replicas insufficient.
          Example: Embed user info in posts table. Trade-offs: 3x faster reads, 2x
          slower writes, data redundancy (larger storage), update anomalies. Use materialized
          views for query-time denormalization without storage penalty.'
      tradeoffs:
      - question: 'Strong consistency vs high availability: analyze trade-offs for
          different application types'
        answer: 'Strong consistency (CP): Banking, inventory (cannot oversell), single-region
          writes, 99.9% availability, higher latency (50-100ms). High availability
          (AP): Social media, analytics, multi-region writes, 99.99% availability,
          eventual consistency (1-5s lag). Use CP for: Money, reservations. AP for:
          Posts, likes, views. Hybrid: CRDT for AP with convergence guarantees.'
      - question: 'Single large database vs multiple specialized databases: operational
          and performance implications'
        answer: 'Single DB: Simpler ops (1 team, 1 backup strategy), ACID transactions,
          but limited scalability, one-size-fits-all compromises. Multi-DB: Optimized
          per workload (2x faster), independent scaling, but 3x ops complexity, eventual
          consistency between DBs, no cross-DB transactions. Use multi-DB when: >1M
          QPS, diverse workloads (OLTP + analytics + cache).'
      - question: 'Normalized vs denormalized data models: query performance vs data
          integrity'
        answer: 'Normalized (3NF): No redundancy, easier updates, data integrity,
          but 5-table JOINs slow (>500ms), complex queries. Denormalized: 3x faster
          reads (no JOINs), simpler queries, but 2x storage, update anomalies, data
          drift risk. Pattern: Normalize for writes (OLTP), denormalize for reads
          (OLAP). Use materialized views for best of both.'
      - question: 'SQL vs NoSQL for a startup: development speed vs long-term maintainability'
        answer: 'SQL (PostgreSQL): Battle-tested, predictable performance, strong
          consistency, but schema migrations slow down iteration. NoSQL (MongoDB):
          Faster initial development (no migrations), flexible schema, easier horizontal
          scaling, but eventually need schema validation, harder analytics, less mature
          tooling. Recommendation: Start with PostgreSQL (JSON columns give flexibility),
          migrate to NoSQL only if scaling issues.'
      - question: 'Read replicas vs caching: cost, complexity, and consistency trade-offs'
        answer: 'Read replicas: Full data copy (100GB replica = 100GB), 1-5s lag,
          SQL queries work, $200/month per replica. Cache (Redis): Hot data only (10GB
          = 90% hit rate), 0-60s staleness (TTL), limited queries, $50/month. Pattern:
          Use cache for hot data (landing pages), replicas for complex queries (reports).
          Replicas for: Strong consistency needs. Cache for: Performance (<10ms).'
      scenarios:
      - Design data storage for a multi-tenant SaaS application. Consider isolation,
        scalability, and compliance.
      - Your RDBMS reaches scaling limits. Plan migration strategy to NoSQL while
        maintaining data consistency.
      - 'Choose database technology for: (a) Financial trading system (b) Content
        management (c) IoT sensor data (d) Social media feed.'
      - Application needs real-time analytics on transactional data. Design architecture
        avoiding impact on OLTP performance.
      - Global application needs low-latency reads worldwide. Design distributed database
        strategy.
    time_estimate: 60
    video_resources:
    - title: 'ByteByteGo: How Caching Works'
      url: https://www.youtube.com/@ByteByteGo
      duration: 15 min
      description: Cache strategies and invalidation
      priority: high
    - title: 'ByteByteGo: Redis Explained'
      url: https://www.youtube.com/@ByteByteGo
      duration: 8 min
      description: In-memory data store use cases
      priority: high
  - day: 5
    topic: Consistency Models
    activity: Master consistency levels, implementation techniques, and trade-offs
      in distributed systems.
    detailed_content: "Consistency Spectrum (Strongest to Weakest):\n- Linearizability\
      \ (Strong Consistency):\n  * All operations appear instantaneous at some point\
      \ between start and finish\n  * Global ordering of all operations\n  * Most\
      \ expensive to implement\n  * Example: Distributed consensus algorithms (Raft,\
      \ Paxos)\n- Sequential Consistency:\n  * All nodes see operations in same order\n\
      \  * Operations can be reordered but maintain program order per process\n  *\
      \ Easier than linearizability but still strong\n- Causal Consistency:\n  * Causally\
      \ related operations appear in same order to all nodes\n  * Concurrent operations\
      \ can be seen in different orders\n  * Preserves intuitive ordering relationships\n\
      - Eventual Consistency:\n  * All nodes converge to same state eventually\n \
      \ * No timing guarantees for convergence\n  * Common in AP systems (Amazon DynamoDB,\
      \ Cassandra)\n- Weak Consistency:\n  * No guarantees about when or if nodes\
      \ converge\n  * Best effort synchronization\n  * Used in systems prioritizing\
      \ performance over consistency\n\nClient-Centric Consistency Models:\n- Read-After-Write\
      \ Consistency:\n  * User sees their own writes immediately\n  * Implementation:\
      \ Route reads to write node or use timestamps\n  * Example: User updates profile,\
      \ immediately sees changes\n- Session Consistency:\n  * Consistency guarantees\
      \ within single session/connection\n  * Different sessions may see different\
      \ views\n  * Implementation: Sticky sessions or session tokens\n- Monotonic\
      \ Read Consistency:\n  * User never sees older versions after seeing newer ones\n\
      \  * Implementation: Read from same replica or use version vectors\n  * Example:\
      \ Reading email thread doesn't go backwards in time\n- Monotonic Write Consistency:\n\
      \  * User's writes are applied in order they were issued\n  * Implementation:\
      \ Serialize writes per user\n  * Example: Bank transfers processed in chronological\
      \ order\n\nImplementation Techniques:\n- Quorum-Based Systems:\n  * R + W >\
      \ N ensures strong consistency (R=reads, W=writes, N=replicas)\n  * R=1, W=N:\
      \ Fast reads, slow writes\n  * R=N, W=1: Fast writes, slow reads\n  * R=W=(N+1)/2:\
      \ Balanced approach\n- Vector Clocks:\n  * Track causal relationships between\
      \ events\n  * Each node maintains counter for itself and others\n  * Detect\
      \ concurrent vs sequential operations\n  * Used in Amazon DynamoDB, Riak\n-\
      \ Logical Timestamps (Lamport):\n  * Total ordering of events in distributed\
      \ system\n  * Simpler than vector clocks but less precise\n- Multi-Version Concurrency\
      \ Control (MVCC):\n  * Store multiple versions of data with timestamps\n  *\
      \ Readers don't block writers, writers don't block readers\n  * Used in PostgreSQL,\
      \ Oracle, MongoDB\n\nConflict Resolution Strategies:\n- Last-Write-Wins (LWW):\n\
      \  * Use timestamp to determine winning value\n  * Simple but can lose data\n\
      \  * Good for commutative operations\n- Application-Level Resolution:\n  * Application\
      \ logic decides how to merge conflicts\n  * Most flexible but requires custom\
      \ logic\n  * Example: Shopping cart merges items from both versions\n- Convergent\
      \ Replicated Data Types (CRDTs):\n  * Data structures that automatically converge\n\
      \  * No coordination needed for conflict resolution\n  * Examples: G-Counter,\
      \ PN-Counter, OR-Set\n- Operational Transform:\n  * Transform operations to\
      \ maintain consistency\n  * Used in collaborative editing (Google Docs)\n\n\
      Database Consistency Examples:\n- ACID Databases (PostgreSQL, MySQL):\n  * Strong\
      \ consistency within single database\n  * ACID transactions guarantee consistency\n\
      \  * Challenges in distributed deployments\n- NoSQL Eventual Consistency (Cassandra,\
      \ DynamoDB):\n  * Eventual consistency by default\n  * Tunable consistency levels\n\
      \  * Trade consistency for availability and performance\n- MongoDB:\n  * Strong\
      \ consistency for single document operations\n  * Configurable read/write concerns\n\
      \  * Eventual consistency for replica sets\n\nConsistency in Microservices:\n\
      - Saga Pattern:\n  * Manage consistency across service boundaries\n  * Compensating\
      \ actions for failure handling\n  * Eventual consistency with business logic\
      \ coordination\n- Event Sourcing:\n  * Store events instead of current state\n\
      \  * Replay events to rebuild state\n  * Ensures consistency through event ordering\n\
      - Two-Phase Commit (2PC):\n  * Strong consistency across distributed services\n\
      \  * High latency and availability issues\n  * Generally avoided in microservices\n\
      \nPerformance vs Consistency Trade-offs:\n- Strong Consistency:\n  * Pros: Simple\
      \ application logic, data integrity\n  * Cons: Higher latency, reduced availability,\
      \ scalability limits\n  * Use cases: Financial systems, inventory management\n\
      - Eventual Consistency:\n  * Pros: High performance, availability, scalability\n\
      \  * Cons: Complex application logic, temporary inconsistencies\n  * Use cases:\
      \ Social media feeds, content delivery, analytics\n\nMonitoring Consistency:\n\
      - Replication Lag: Time difference between primary and replicas\n- Consistency\
      \ Violations: Detection of out-of-order operations\n- Convergence Time: How\
      \ long for eventual consistency\n- Split-Brain Detection: Identifying network\
      \ partition scenarios\n"
    resources:
    - title: Consistency Models Explained
      url: https://jepsen.io/consistency
      description: Detailed explanation of consistency models
    - title: DynamoDB Consistency
      url: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadConsistency.html
      description: Practical consistency implementation
    - title: Designing Data-Intensive Applications - Chapter 9
      url: https://dataintensive.net/
      description: Consistency and consensus in depth
    - title: Vector Clocks Explained
      url: https://en.wikipedia.org/wiki/Vector_clock
      description: Understanding causality tracking
    practice_questions:
      estimation:
      - question: 'DynamoDB: 5 replicas, R=2, W=3. What consistency guarantees? Calculate
          availability if 2 nodes fail.'
        answer: 'R+W=5 > N=5, guarantees strong consistency (reads always see latest
          write). With 2 nodes failed (3 remaining): W=3 still possible (write succeeds),
          R=2 still possible (read succeeds). Availability: 100% for up to 2 node
          failures. For 3 nodes failed: Can''t achieve W=3, system becomes unavailable
          for writes (chooses C over A in CP).'
      - question: 'Cassandra cluster: 1000 writes/sec, replication factor 3. Estimate
          convergence time with 10ms network latency.'
        answer: 'Writes: 1000/sec × RF=3 = 3000 replicas/sec. With async replication
          at 10ms latency: convergence in 10-50ms (depends on hinted handoff, gossip
          protocol). Under partition: Can take seconds to minutes. For consistency:
          Use QUORUM (R=2,W=2) sacrificing 10ms write latency for immediate consistency
          vs 50ms eventual consistency.'
      - question: 'Vector clock overhead: 100 nodes, 1KB messages. Calculate metadata
          overhead percentage.'
        answer: 'Vector clock size: 100 nodes × 8B (counter) = 800B per message. Overhead:
          800B / 1KB = 80%. For large messages (100KB): 800B/100KB = 0.8%. High overhead
          for small messages. Optimization: Prune vector clocks (keep only recent/active
          nodes), reduce to 10 nodes = 80B = 8% overhead.'
      concepts:
      - question: 'Social media app: users post updates seen by followers. Design
          consistency model for timeline generation.'
        answer: 'Use eventual consistency with causal consistency for replies. Pattern:
          Post → Write to user''s timeline (async fan-out to followers'' timelines).
          Convergence time: 1-5s acceptable. For replies: Preserve causality (show
          post before reply). Use CRDT for likes/counts. Trade-off: Users see posts
          at slightly different times, but system scales to millions of posts/sec
          vs strong consistency limiting to 10K/sec.'
      - question: 'E-commerce inventory: prevent overselling while maintaining performance.
          Choose consistency approach.'
        answer: 'Use optimistic locking with version numbers. On purchase: Read inventory
          + version → Validate available → Write with version check. If conflict:
          Retry. Achieves strong consistency for inventory (no overselling) with 50ms
          latency vs 10ms eventual consistency. For hot items: Use distributed lock
          (Redis) or reserved inventory pool. Alternative: Saga pattern for multi-item
          carts.'
      - question: 'Banking system: transfer money between accounts. Why is strong
          consistency required? Design implementation.'
        answer: 'Strong consistency prevents: 1) Double-spending, 2) Negative balances,
          3) Lost money in transfers. Implementation: Use 2PC or Saga pattern. 2PC:
          Prepare phase (lock both accounts) → Commit phase (transfer). Latency: 50-100ms.
          Alternative: Single DB with ACID transactions (simpler, 20ms latency). Event
          sourcing for audit trail. Never use eventual consistency for money.'
      - question: 'Global content management: editors in multiple regions. Design
          conflict resolution for concurrent edits.'
        answer: 'Use Operational Transform (OT) or CRDT. OT: Transform concurrent
          edits to maintain intent (used in Google Docs), complex algorithm, 10-50ms
          latency. CRDT: Automatically merge edits (simpler), may need manual conflict
          UI for complex cases. Implementation: Last-write-wins for metadata, OT for
          content. Lock documents during edit with 5min timeout for exclusive editing.'
      - question: Explain why read-after-write consistency is easier to implement
          than monotonic read consistency.
        answer: 'Read-after-write: Route user''s reads to same node that handled write,
          or use session token with timestamp. Simple: Track single write per user.
          Monotonic read: Must track ALL reads across all nodes, ensure never return
          older version. Harder: Needs version vectors, sticky sessions, or always
          read from primary. Monotonic read requires global coordination vs read-after-write
          is per-user local.'
      - question: When would you choose causal consistency over eventual consistency?
          Give specific scenarios.
        answer: 'Choose causal when: 1) Comments/replies (reply shouldn''t appear
          before post), 2) Chat apps (messages in conversation order), 3) Collaborative
          editing (see others'' edits in logical order). Cost: 2x metadata overhead
          (vector clocks), 20-50ms latency vs 10ms eventual. Use eventual for: Independent
          events (likes, views, analytics) where order doesn''t matter. Causal gives
          intuitive UX at moderate cost.'
      tradeoffs:
      - question: 'Strong vs eventual consistency for shopping cart: analyze user
          experience vs performance impact'
        answer: 'Strong consistency: User always sees correct cart, 50ms add-to-cart
          latency, can''t scale past 10K writes/sec, survives browser close. Eventual:
          <10ms latency, scales to 100K+ writes/sec, but temporary inconsistencies
          (added item appears after 1-2s), works across devices. Recommendation: Use
          eventual with read-after-write (user sees own changes immediately), eventual
          across devices. Best UX/performance balance.'
      - question: 'Quorum configurations: R=1,W=3 vs R=3,W=1 vs R=2,W=2. Compare latency,
          consistency, availability.'
        answer: 'R=1,W=3 (read-optimized): 10ms reads, 30ms writes, eventual consistency,
          survives 2 node failures. R=3,W=1 (write-optimized): 30ms reads, 10ms writes,
          may read stale, survives 2 failures. R=2,W=2 (balanced): 20ms both, strong
          consistency (R+W>N), survives 1 failure. Use R=1,W=3 for: Read-heavy apps.
          R=3,W=1 for: Write-heavy logging. R=2,W=2 for: Balanced workloads needing
          consistency.'
      - question: 'Vector clocks vs Lamport timestamps: accuracy vs overhead analysis'
        answer: 'Vector clocks: Detect all causality (concurrent vs sequential), O(N)
          space per node, 100 nodes = 800B overhead. Lamport: Total order only (can''t
          detect concurrent), O(1) space = 8B overhead, simpler. Use vector clocks
          for: Conflict detection (Riak, Dynamo). Lamport for: Event ordering (logs,
          audit trails). Trade-off: 100x space overhead for perfect causality detection.'
      - question: 'CRDT vs application-level conflict resolution: development complexity
          vs runtime overhead'
        answer: 'CRDT: Zero conflict resolution code (automatic convergence), 2-5x
          storage overhead (tombstones, metadata), limited operations (counters, sets,
          registers). App-level: Custom merge logic (complex dev), minimal overhead,
          supports any operation. Use CRDT for: Counters, flags, sets (90% of conflicts).
          App-level for: Complex business logic (shopping cart merge, document reconciliation).
          CRDT saves dev time, app-level saves runtime costs.'
      - question: 'Session consistency vs global consistency: implementation complexity
          vs user experience'
        answer: 'Session: Simple (sticky sessions or session token), per-user consistent,
          cheap (no global coordination), but different users see different views.
          Global: Complex (distributed consensus), all users see same data, expensive
          (2PC, Paxos), 3-5x higher latency. Use session for: Social media, dashboards
          (users don''t compare views). Global for: Auctions, inventory (users compete
          for resources). Session gives 90% of UX at 10% of cost.'
      scenarios:
      - Design consistency model for collaborative document editor with real-time
        collaboration.
      - 'Chat application: users in different regions send messages. Ensure message
        ordering per conversation.'
      - 'Online auction system: handle concurrent bids while preventing race conditions.'
      - 'Distributed counter (likes, views): design for high write volume with acceptable
        read consistency.'
      - 'Multi-tenant SaaS: ensure tenant data isolation while optimizing for performance.'
    time_estimate: 60
    video_resources:
    - title: 'ByteByteGo: Message Queue Explained'
      url: https://www.youtube.com/@ByteByteGo
      duration: 12 min
      description: Async communication patterns
      priority: high
    - title: 'Hello Interview: Queue vs Topic'
      url: https://www.youtube.com/@HelloInterview
      duration: 10 min
      description: Different messaging patterns
      priority: medium
  - day: 6
    topic: CAP Theorem
    activity: Master CAP theorem trade-offs, PACELC extensions, and practical system
      design decisions.
    detailed_content: "CAP Theorem Fundamentals:\nYou can only guarantee 2 of 3 properties\
      \ simultaneously:\n- Consistency (C): All nodes return the same data at the\
      \ same time\n  * Every read receives the most recent write or an error\n  *\
      \ Also called \"linearizability\" or \"atomic consistency\"\n  * Requires coordination\
      \ between nodes\n- Availability (A): System remains operational and responsive\n\
      \  * Every request receives a response (success or failure)\n  * System continues\
      \ to function despite node failures\n  * No guarantees about data freshness\n\
      - Partition Tolerance (P): System continues despite network failures\n  * System\
      \ functions when network connections fail between nodes\n  * Essential for distributed\
      \ systems (network partitions are inevitable)\n  * Must be included in any distributed\
      \ system design\n\nCAP Trade-off Scenarios:\n- CP (Consistency + Partition Tolerance):\n\
      \  * Choose consistency over availability during network partitions\n  * System\
      \ becomes unavailable to maintain data consistency\n  * Examples: Traditional\
      \ RDBMS with synchronous replication, MongoDB, HBase\n  * Use cases: Financial\
      \ systems, inventory management, configuration stores\n- AP (Availability +\
      \ Partition Tolerance):\n  * Choose availability over consistency during network\
      \ partitions\n  * System remains available but may return stale data\n  * Examples:\
      \ Cassandra, DynamoDB, CouchDB, DNS\n  * Use cases: Social media feeds, content\
      \ delivery, analytics\n- CA (Consistency + Availability):\n  * Only possible\
      \ in absence of network partitions\n  * Not realistic for distributed systems\
      \ at scale\n  * Examples: Single-node databases, systems within same datacenter\n\
      \  * Use cases: Simple web applications, development environments\n\nPACELC\
      \ Theorem Extension:\n\"If Partition, choose between Availability and Consistency,\n\
      \ Else (no partition), choose between Latency and Consistency\"\n- PA/EL: Prioritize\
      \ availability during partitions, latency during normal operation\n  * Examples:\
      \ Cassandra, DynamoDB\n- PA/EC: Prioritize availability during partitions, consistency\
      \ during normal operation\n  * Examples: MongoDB with eventual consistency settings\n\
      - PC/EL: Prioritize consistency during partitions, latency during normal operation\n\
      \  * Examples: BigTable, HBase\n- PC/EC: Prioritize consistency during partitions\
      \ and normal operation\n  * Examples: Traditional RDBMS, strongly consistent\
      \ systems\n\nReal-world System Examples:\n- Amazon DynamoDB (AP/EL):\n  * Eventually\
      \ consistent by default\n  * Remains available during partitions\n  * Optimizes\
      \ for low latency\n  * Offers strongly consistent reads at higher latency\n\
      - Google Spanner (CP/EC):\n  * Strongly consistent across global deployment\n\
      \  * May become unavailable during certain partition scenarios\n  * Uses synchronized\
      \ clocks for global consistency\n- Apache Cassandra (AP/EL):\n  * Eventual consistency\
      \ with tunable consistency levels\n  * Always available for reads and writes\n\
      \  * Optimizes for low latency and high throughput\n- MongoDB (CP/EL):\n  *\
      \ Strong consistency within replica sets\n  * Primary becomes unavailable during\
      \ partition\n  * Chooses consistency over availability\n- Redis Cluster (AP/EL):\n\
      \  * Eventually consistent between clusters\n  * Individual nodes prioritize\
      \ availability\n  * Fast in-memory operations\n\nImplementation Strategies:\n\
      - CP Systems Implementation:\n  * Quorum-based writes: W > N/2\n  * Master-slave\
      \ with failover delays\n  * Consensus protocols (Raft, Paxos)\n  * Strong consistency\
      \ at cost of availability\n- AP Systems Implementation:\n  * Multi-master replication\n\
      \  * Conflict resolution mechanisms\n  * Read-repair and anti-entropy\n  * Vector\
      \ clocks for causality tracking\n- Tunable Consistency:\n  * Allow applications\
      \ to choose per-operation\n  * Examples: Cassandra's consistency levels, DynamoDB's\
      \ read consistency\n\nPractical Design Decisions:\n- Analyze Business Requirements:\n\
      \  * Can business tolerate stale data? → AP\n  * Must data always be accurate?\
      \ → CP\n  * Are network partitions common? → Must choose P\n- System Characteristics:\n\
      \  * High read/write ratio → Consider AP with read-heavy optimization\n  * Critical\
      \ transactions → CP for data integrity\n  * Global distribution → AP for latency\n\
      - Hybrid Approaches:\n  * Different components can make different CAP choices\n\
      \  * Core data: CP (user accounts, transactions)\n  * Derived data: AP (recommendations,\
      \ analytics)\n  * Caching layer: AP for performance\n\nCommon Misconceptions:\n\
      - \"CAP forces binary choice\": Can tune consistency levels\n- \"CA systems\
      \ are viable\": Network partitions are inevitable\n- \"AP means no consistency\"\
      : Eventual consistency still provides guarantees\n- \"Choose once globally\"\
      : Different parts can make different choices\n\nBeyond CAP - Additional Considerations:\n\
      - Performance: Latency and throughput requirements\n- Operational Complexity:\
      \ Monitoring, debugging, maintenance\n- Data Model Fit: Relational vs document\
      \ vs key-value\n- Ecosystem Maturity: Tools, expertise, community support\n\
      - Cost: Infrastructure, development, operational costs\n\nCAP in Modern Architecture:\n\
      - Microservices: Each service can make independent CAP choices\n- Event-Driven:\
      \ Often AP with eventual consistency\n- CQRS: Write side CP, read side AP\n\
      - Multi-region: Geographic regions may have different CAP profiles\n"
    resources:
    - title: CAP Theorem Explained
      url: https://www.ibm.com/topics/cap-theorem
      description: Comprehensive CAP theorem guide
    - title: PACELC Theorem
      url: https://en.wikipedia.org/wiki/PACELC_theorem
      description: Extension of CAP theorem with latency
    - title: CAP Twelve Years Later
      url: https://www.infoq.com/articles/cap-twelve-years-later-how-the-rules-have-changed/
      description: Eric Brewer's updated perspective on CAP
    - title: Jepsen Testing
      url: https://jepsen.io/
      description: Real-world consistency testing of distributed systems
    practice_questions:
      estimation:
      - question: 'Banking system: 99.99% availability requirement. Calculate downtime
          budget and analyze CAP implications.'
        answer: '99.99% uptime = 52.56 min downtime/year = 4.38 min/month. For banking:
          Must choose CP (consistency critical for money). During partition: System
          unavailable but maintains data integrity. Alternative: Multi-region with
          async replication (99.99% per region, fail over in <1min). Cost: $10K/month
          for multi-region CP system vs $3K single region.'
      - question: 'Global CDN: 100ms latency target, 99.9% availability. Design CAP
          strategy for content delivery.'
        answer: 'CDN is classic AP system: Availability critical (serve stale content
          better than errors), eventual consistency acceptable (content changes infrequent).
          Edge nodes: 20-50ms regional latency, 1-5min cache staleness. On partition:
          Serve from cache indefinitely. TTL: 5min for dynamic, 24h for static. Cost:
          $500/month for 1TB/month globally distributed.'
      - question: 'E-commerce: Black Friday traffic 100x normal. Choose CAP properties
          for different system components.'
        answer: 'Product catalog: AP (stale prices OK for 1-5s, 100K QPS). Inventory:
          CP with reserved pool (prevent overselling, 10K TPS). Shopping cart: AP
          with session consistency (user sees own cart, 50K writes/sec). Checkout:
          CP (payment critical, 5K TPS). Analytics: AP (eventual, 1M events/sec).
          Trade-off: 95% AP for scale, 5% CP for money.'
      concepts:
      - question: 'Social media platform: design CAP strategy for user posts vs friend
          relationships vs activity feeds.'
        answer: 'Posts: AP (eventual consistency, 1-5s propagation acceptable, scales
          to 1M posts/sec). Friend relationships: CP (prevent double-adds, mutual
          consistency, 10K/sec sufficient). Activity feeds: AP (timelines can be eventually
          consistent, use fan-out on write). Use separate DBs: Posts in Cassandra
          (AP), Friendships in PostgreSQL (CP), Feeds in Redis (AP cache).'
      - question: 'Online gaming: real-time leaderboard vs game state vs user profiles.
          Assign CAP choices per component.'
        answer: 'Game state: CP (prevent cheating, authoritative server, 10K updates/sec,
          50ms latency acceptable). Leaderboard: AP (eventual consistency OK, 1-5s
          lag, CRDT counters, 100K updates/sec). User profiles: CP (consistent inventory,
          prevent dupes, 1K updates/sec). Pattern: Critical game logic CP, auxiliary
          features AP for scale.'
      - question: 'IoT sensor network: 10K sensors, occasional network issues. Design
          data collection strategy.'
        answer: 'Use AP: Sensors write to local buffer → Async to cloud when connected.
          Accept: 1) Duplicate data (dedupe server-side by timestamp+sensor_id), 2)
          Out-of-order arrival (sort by timestamp), 3) Temporary data loss (sensor
          buffer 1h = 3600 readings). Benefits: 100% uptime, 10K sensors × 1 reading/sec
          = 10K writes/sec to time-series DB (InfluxDB). CP would fail during network
          issues.'
      - question: Why are CA systems not viable in distributed environments? Provide
          concrete scenarios.
        answer: 'Network partitions are inevitable: 1) Switch failure splits datacenter
          (happens annually), 2) Regional internet outage (AWS us-east-1 2021), 3)
          Software bugs cause split-brain. CA assumes no partitions = single point
          of failure. Example: 2-node CA system with network partition must: Stop
          serving (lose A) or risk split-brain (lose C). Reality: All distributed
          systems must choose CP or AP.'
      - question: Explain how Cassandra achieves AP while still providing some consistency
          guarantees.
        answer: 'Cassandra is AP but offers tunable consistency: 1) Write with QUORUM
          (W=2/3): Majority write for consistency, 2) Read with QUORUM: Majority read
          ensures latest value, 3) Read-repair: Fix inconsistencies on read, 4) Hinted
          handoff: Queue writes during node failure. Trade-off: QUORUM consistency
          = 2x latency (20ms vs 10ms) but still available (tolerates 1/3 node failures).'
      - question: How does Google Spanner achieve strong consistency across global
          deployment?
        answer: 'Spanner uses TrueTime API: Atomic clocks + GPS provide time uncertainty
          bounds (<7ms). For writes: Wait out uncertainty before commit = global order.
          Paxos consensus groups per region. Read from closest replica with version
          guarantee. Cost: 5-10ms write latency (2PC + TrueTime wait). Chooses CP
          (unavailable during partition) with optimizations for global scale. Alternative:
          CockroachDB uses hybrid logical clocks (cheaper, no atomic clocks).'
      tradeoffs:
      - question: 'Financial trading system vs social media feed: compare CAP choices
          and justify decisions'
        answer: 'Trading: CP mandatory (prevent double-trades, accurate order books,
          regulatory compliance). Cost: 50-100ms latency, single-region master, 99.9%
          availability acceptable. Social feed: AP optimal (stale posts OK, scale
          to 1M posts/sec, 99.99% availability). Cost: 1-5s eventual consistency.
          Trading loses $1M/min during downtime, justifies CP. Social loses engagement
          but survives outages, justifies AP.'
      - question: 'Strong consistency vs eventual consistency: quantify business impact
          for e-commerce inventory'
        answer: 'Strong: No overselling (0 customer complaints), 50ms checkout latency,
          10K TPS limit, complex ops. Eventual: 0.1% oversell rate = 100 complaints
          per 100K orders = $10K refunds/day, 10ms latency, 100K TPS, simple ops.
          Decision: Use strong for high-value items (>$1000), eventual for cheap items
          (<$20). Middle ground: Reserved inventory pool (95% strong, 5% eventual
          flexibility).'
      - question: 'Single-region CP vs multi-region AP: availability, latency, and
          cost analysis'
        answer: 'Single-region CP: 99.9% availability (regional outage = total failure),
          20-50ms latency (within region), $3K/month. Multi-region AP: 99.99% availability
          (one region fails = others work), 20ms local + 200ms cross-region writes,
          $15K/month (5x cost). Use multi-region for: Global apps (>10M users), revenue
          >$1M/month (downtime cost justifies 5x infrastructure cost).'
      - question: 'PACELC analysis: MongoDB vs Cassandra for content management system'
        answer: 'MongoDB (PC/EC): Chooses consistency over availability during partition,
          strong consistency normally. CMS needs: Version control (strong consistency),
          moderate writes (1K/sec), complex queries. Cassandra (PA/EL): Always available,
          eventual consistency, 100K writes/sec. CMS benefit from MongoDB: ACID for
          multi-document updates, easier queries. Use Cassandra for: High-write CMS
          (news sites, 10K articles/hour).'
      - question: 'Hybrid approach: different CAP choices for microservices vs monolithic
          design complexity'
        answer: 'Hybrid microservices: User service (CP, account consistency), Posts
          service (AP, high throughput), Feed service (AP, eventual aggregation).
          Complexity: 3 databases, eventual consistency between services, saga patterns
          needed, 3x ops cost. Monolith: Single CP database, ACID transactions, simpler,
          but limited scale (10K QPS). Use hybrid when: >100K QPS, need independent
          scaling. Use monolith until: Scale bottleneck.'
      scenarios:
      - Design CAP strategy for multi-tenant SaaS with customers having different
        consistency requirements.
      - Network partition splits your cluster 60-40. Design behavior for CP and AP
        system variants.
      - Migrate from CP system (MySQL) to AP system (Cassandra). Plan transition strategy
        and challenges.
      - 'Global messaging app: ensure message delivery while optimizing for local
        latency.'
      - 'Real-time analytics platform: balance between data freshness and query performance.'
    time_estimate: 60
    video_resources:
    - title: 'ByteByteGo: API Gateway Pattern'
      url: https://www.youtube.com/@ByteByteGo
      duration: 10 min
      description: Single entry point for microservices
      priority: high
    - title: 'ByteByteGo: REST vs GraphQL vs gRPC'
      url: https://www.youtube.com/@ByteByteGo
      duration: 12 min
      description: API communication protocols compared
      priority: high
  - day: 7
    topic: Week 1 Integration - URL Shortener Design
    activity: Apply Week 1 concepts to design a complete URL shortener system integrating
      scalability, caching, databases, and consistency.
    detailed_content: "System Requirements Analysis:\n- Functional Requirements:\n\
      \  * Shorten long URLs to short aliases (e.g., bit.ly/abc123)\n  * Redirect\
      \ short URLs to original URLs\n  * Custom aliases for premium users\n  * URL\
      \ expiration and deletion\n  * Analytics (click tracking, geographic data)\n\
      \  * User accounts and URL management\n- Non-functional Requirements:\n  * Scale:\
      \ 100M new URLs/day, 10B redirects/day (100:1 read/write ratio)\n  * Latency:\
      \ <100ms for redirects, <500ms for shortening\n  * Availability: 99.9% uptime\
      \ (8.77 hours downtime/year)\n  * Durability: URLs must not be lost\n  * Security:\
      \ Prevent malicious URLs, abuse prevention\n\nCapacity Estimation:\n- Traffic\
      \ Analysis:\n  * Write QPS: 100M URLs/day = 1,157 URLs/sec (avg), 2,300/sec\
      \ (peak 2x)\n  * Read QPS: 10B redirects/day = 115,740/sec (avg), 231,480/sec\
      \ (peak)\n  * Total QPS: ~233K at peak (heavily read-dominated)\n- Storage Requirements:\n\
      \  * URL record: original_url (2KB avg) + short_url (7 bytes) + metadata (100\
      \ bytes) = ~2.1KB\n  * Daily storage: 100M × 2.1KB = 210GB/day\n  * 5-year storage:\
      \ 210GB × 365 × 5 = 383TB\n  * With replication (3x): ~1.15PB total\n- Bandwidth:\n\
      \  * Incoming: 100M × 2KB = 200GB/day writes\n  * Outgoing: 10B × 300 bytes\
      \ (redirect response) = 3TB/day reads\n  * Peak bandwidth: 3TB/86400s × 2 =\
      \ ~70MB/s\n- Memory (Cache):\n  * Cache 20% of daily reads (80/20 rule): 2B\
      \ URLs × 300 bytes = 600GB\n  * Distributed across cache nodes: 600GB / 10 nodes\
      \ = 60GB per node\n\nShort URL Generation Strategies:\n- Base62 Encoding:\n\
      \  * Characters: [a-z, A-Z, 0-9] = 62 characters\n  * URL length 7: 62^7 = 3.5\
      \ trillion unique URLs\n  * Counter-based: Encode auto-incrementing ID\n  *\
      \ Pros: Simple, predictable length\n  * Cons: Predictable sequence, scalability\
      \ bottleneck\n- Hash-based Generation:\n  * Hash original URL (MD5/SHA1) + take\
      \ first 7 characters\n  * Pros: Distributed generation, no coordination\n  *\
      \ Cons: Collision handling needed, longer to avoid collisions\n- Random Generation:\n\
      \  * Generate random 7-character strings\n  * Check for collisions in database\n\
      \  * Pros: Unpredictable, distributed\n  * Cons: Collision probability increases\
      \ over time\n- Hybrid Approach:\n  * Combine timestamp + node_id + counter for\
      \ uniqueness\n  * Base62 encode the result\n  * Pros: Guaranteed uniqueness,\
      \ distributed, time-ordered\n  * Cons: Slightly more complex\n\nDatabase Design:\n\
      - URL Mapping Table:\n  ```sql\n  CREATE TABLE url_mappings (\n    short_url\
      \ VARCHAR(7) PRIMARY KEY,\n    original_url TEXT NOT NULL,\n    user_id BIGINT,\n\
      \    created_at TIMESTAMP,\n    expires_at TIMESTAMP,\n    click_count BIGINT\
      \ DEFAULT 0,\n    is_active BOOLEAN DEFAULT true\n  );\n  ```\n- Sharding Strategy:\n\
      \  * Shard by short_url hash: consistent distribution\n  * Range-based: A-G,\
      \ H-N, O-T, U-Z, 0-9 (uneven distribution)\n  * Hash-based sharding recommended\
      \ for uniform distribution\n- Database Selection:\n  * Read-heavy workload:\
      \ Consider read replicas\n  * NoSQL option: DynamoDB/Cassandra for horizontal\
      \ scaling\n  * SQL option: PostgreSQL with sharding for ACID properties\n\n\
      Caching Architecture:\n- Cache Layers:\n  * Browser cache: Cache 302 redirects\
      \ (1 hour TTL)\n  * CDN cache: Cache redirects geographically (1 hour TTL)\n\
      \  * Application cache: Redis cluster for hot URLs (6 hour TTL)\n  * Database\
      \ cache: Query result caching\n- Cache Strategy:\n  * Write-around: Don't cache\
      \ on write (avoid cold URLs in cache)\n  * Cache-aside: Load popular URLs into\
      \ cache on read miss\n  * LRU eviction: Remove least recently used URLs\n- Cache\
      \ Sizing:\n  * Monitor hit ratio: Target >90% for popular URLs\n  * 20% of URLs\
      \ generate 80% of traffic (Pareto principle)\n\nSystem Architecture:\n- Load\
      \ Balancer Layer:\n  * Global load balancer: DNS-based geographic routing\n\
      \  * Regional load balancers: L7 for path-based routing\n  * Algorithm: Least\
      \ connections for backend services\n- Application Services:\n  * URL Shortening\
      \ Service: Handle URL creation\n  * Redirect Service: Handle URL lookups and\
      \ redirects\n  * Analytics Service: Process click events asynchronously\n  *\
      \ User Management Service: Handle accounts and authentication\n- Data Storage:\n\
      \  * Primary Database: Sharded SQL/NoSQL for URL mappings\n  * Analytics Database:\
      \ Time-series DB for click analytics\n  * Cache: Redis cluster for hot URL lookups\n\
      \nConsistency and Availability Trade-offs:\n- CAP Analysis:\n  * Choose AP (Availability\
      \ + Partition tolerance)\n  * Eventual consistency acceptable for URL mappings\n\
      \  * Strong consistency for user accounts and custom aliases\n- Conflict Resolution:\n\
      \  * Custom aliases: Strong consistency to prevent duplicates\n  * Auto-generated\
      \ URLs: Last-write-wins for metadata updates\n  * Analytics: Eventual consistency,\
      \ aggregate asynchronously\n\nAdvanced Features:\n- Custom Aliases:\n  * Check\
      \ availability in real-time\n  * Reserve popular words for premium users\n \
      \ * Implement with distributed locking or consensus\n- URL Expiration:\n  *\
      \ Background job to mark expired URLs as inactive\n  * Grace period before actual\
      \ deletion\n  * Cleanup process for storage reclamation\n- Analytics Implementation:\n\
      \  * Asynchronous event processing for click tracking\n  * Real-time counters\
      \ vs batch processing trade-offs\n  * Geographic data collection and privacy\
      \ considerations\n- Security Measures:\n  * URL validation: Check for malicious\
      \ content\n  * Rate limiting: Prevent abuse and spam\n  * Blacklist management:\
      \ Block harmful domains\n  * CAPTCHA for suspicious activity\n\nScalability\
      \ Optimizations:\n- Read Scaling:\n  * CDN for global distribution\n  * Read\
      \ replicas for database scaling\n  * Multi-level caching strategy\n- Write Scaling:\n\
      \  * Database sharding for URL creation\n  * Asynchronous processing for analytics\n\
      \  * Batch processing for non-critical operations\n- Auto-scaling:\n  * Horizontal\
      \ scaling based on QPS metrics\n  * Cache warm-up during scale events\n  * Connection\
      \ pooling for database efficiency\n\nMonitoring and Operations:\n- Key Metrics:\n\
      \  * Latency: p95, p99 for redirects and URL creation\n  * Throughput: QPS for\
      \ reads and writes\n  * Cache hit ratio: Monitor cache effectiveness\n  * Error\
      \ rates: 4xx/5xx response tracking\n- Alerting:\n  * High latency alerts (>100ms\
      \ redirects)\n  * Cache hit ratio drops (<90%)\n  * Database connection pool\
      \ exhaustion\n  * Disk space usage for URL storage\n- Logging:\n  * Access logs\
      \ for debugging and analytics\n  * Error logs for system health monitoring\n\
      \  * Audit logs for security and compliance\n\nAlternative Designs:\n- Serverless\
      \ Architecture:\n  * AWS Lambda for URL shortening and redirects\n  * DynamoDB\
      \ for storage with auto-scaling\n  * CloudFront CDN for global distribution\n\
      \  * Pros: No server management, automatic scaling\n  * Cons: Cold start latency,\
      \ vendor lock-in\n- Microservices vs Monolith:\n  * Monolith: Simple deployment,\
      \ shared cache\n  * Microservices: Independent scaling, service isolation\n\
      \  * Trade-off: Operational complexity vs scalability\n\nMigration and Deployment:\n\
      - Blue-Green Deployment:\n  * Zero-downtime deployment for critical service\n\
      \  * Database migration strategies\n  * Cache warming for new deployments\n\
      - Data Migration:\n  * Online migration for minimal downtime\n  * Checksum validation\
      \ for data integrity\n  * Rollback procedures for failed migrations\n"
    resources:
    - title: 'ByteByteGo: Design a URL Shortener'
      url: https://www.youtube.com/@ByteByteGo
      description: Visual walkthrough of URL shortener architecture with diagrams
    - title: 'Hello Interview: Design a URL Shortener'
      url: https://www.hellointerview.com/learn/system-design/problem-breakdowns/overview
      description: FAANG-level breakdown with level-specific expectations
    - title: 'System Design: URL Shortener'
      url: https://www.educative.io/courses/grokking-system-design-fundamentals/url-shortener
      description: Complete URL shortener design walkthrough
    - title: Designing TinyURL
      url: https://github.com/donnemartin/system-design-primer/tree/master/solutions/system_design/pastebin
      description: Detailed system design with code examples
    practice_questions:
      estimation:
      - 'Calculate database size for 10 years: 500M URLs/day, avg 2KB per URL, 3x
        replication factor.'
      - 'Design cache size for 95% hit ratio: 1B daily redirects, 80/20 rule, 300
        bytes per cached entry.'
      - 'Network bandwidth: Peak 500K QPS, avg response 500 bytes, calculate total
        bandwidth needs.'
      - 'Shard calculation: 100B total URLs, 10TB per shard limit, how many shards
        needed over 10 years?'
      concepts:
      - 'Compare Base62 encoding vs hash-based generation: collision probability,
        scalability, predictability.'
      - Design URL shortener for 10x higher write traffic (1B URLs/day). What changes
        to architecture?
      - How would you handle custom aliases at scale while preventing collisions across
        regions?
      - Design analytics system to track clicks in real-time vs batch processing.
        Compare trade-offs.
      - 'Implement URL expiration: design background cleanup without affecting read
        performance.'
      - How do you ensure shortened URLs are evenly distributed across database shards?
      tradeoffs:
      - 'SQL vs NoSQL for URL storage: consistency, scalability, query flexibility
        comparison'
      - 'CDN caching vs application caching: cost, latency, cache invalidation complexity'
      - 'Synchronous vs asynchronous analytics: real-time accuracy vs system performance'
      - 'Monolithic vs microservices architecture: deployment simplicity vs independent
        scaling'
      - 'Strong vs eventual consistency for custom aliases: user experience vs system
        complexity'
      scenarios:
      - Viral URL gets 1M clicks in 1 hour (100x normal traffic). Design caching and
        scaling strategy.
      - Database shard becomes unavailable. Design failover strategy maintaining service
        availability.
      - 'Implement geographical URL shortening: different short domains per region
        with global analytics.'
      - 'Add URL preview feature: fetch page title/description without affecting redirect
        performance.'
      - 'Design A/B testing for redirect pages: route 10% traffic to experimental
        landing pages.'
      - 'Handle malicious URL detection: integrate security scanning without blocking
        legitimate URLs.'
    time_estimate: 120
    video_resources:
    - title: 'ByteByteGo: System Design Fundamentals'
      url: https://www.youtube.com/@ByteByteGo
      duration: 20 min
      description: Review of Week 1 concepts
      priority: high
  week2:
  - day: 8
    topic: Message Queues & Async Communication
    activity: Master message queuing patterns, delivery guarantees, system selection,
      and async architecture design.
    detailed_content: "Asynchronous Communication Benefits:\n- Decoupling: Services\
      \ don't need to know about each other directly\n- Scalability: Handle traffic\
      \ spikes by queuing requests\n- Reliability: Messages persist even if consumers\
      \ are temporarily down\n- Performance: Non-blocking operations improve response\
      \ times\n- Flexibility: Easy to add new consumers without changing producers\n\
      \nMessage Queue Patterns:\n- Point-to-Point (Queue Model):\n  * One producer\
      \ sends messages to one consumer\n  * Message consumed by exactly one consumer\n\
      \  * Load balancing: Multiple consumers compete for messages\n  * Use cases:\
      \ Task processing, job queues, work distribution\n  * Example: E-commerce order\
      \ processing pipeline\n- Publish/Subscribe (Topic Model):\n  * One producer\
      \ publishes to multiple subscribers\n  * Message delivered to all interested\
      \ consumers\n  * Topic-based routing: Consumers subscribe to specific topics\n\
      \  * Use cases: Event notifications, real-time updates, logging\n  * Example:\
      \ User activity events distributed to analytics, recommendations, notifications\n\
      - Request/Reply Pattern:\n  * Asynchronous version of synchronous request-response\n\
      \  * Producer sends request and continues processing\n  * Consumer processes\
      \ and sends reply to callback queue\n  * Correlation ID links requests to responses\n\
      \  * Use cases: Long-running computations, external API calls\n\nMessage Routing\
      \ Strategies:\n- Direct Routing:\n  * Messages routed based on exact routing\
      \ key match\n  * Simple and efficient for known message types\n  * Example:\
      \ Order messages routed to order-processing queue\n- Topic-based Routing:\n\
      \  * Hierarchical routing using wildcards (user.*, user.created)\n  * Flexible\
      \ subscription patterns\n  * Example: user.created, user.updated, user.deleted\
      \ events\n- Content-based Routing:\n  * Route based on message content/headers\n\
      \  * Most flexible but computationally expensive\n  * Example: Route based on\
      \ geographic region, customer tier\n- Fanout:\n  * Broadcast messages to all\
      \ bound queues\n  * No routing logic needed\n  * Use cases: Broadcasting announcements,\
      \ cache invalidation\n\nDelivery Guarantees:\n- At-Most-Once:\n  * Messages\
      \ delivered zero or one time (never duplicated)\n  * Some messages may be lost\
      \ during failures\n  * Lowest overhead and highest performance\n  * Use cases:\
      \ Metrics, logs, non-critical notifications\n- At-Least-Once:\n  * Messages\
      \ delivered one or more times (may be duplicated)\n  * No message loss but requires\
      \ idempotent consumers\n  * Most common approach in practice\n  * Use cases:\
      \ Payment processing, inventory updates\n- Exactly-Once:\n  * Messages delivered\
      \ exactly once (no loss, no duplication)\n  * Highest complexity and overhead\n\
      \  * Achieved through distributed transactions or deduplication\n  * Use cases:\
      \ Financial transactions, critical state changes\n\nMessage Ordering Guarantees:\n\
      - No Ordering: Messages may arrive in any order\n- Partition Ordering: Messages\
      \ within same partition are ordered\n- Global Ordering: All messages across\
      \ partitions are ordered\n- Trade-off: Stronger ordering = lower throughput\
      \ and availability\n\nPopular Message Queue Systems:\n- Apache Kafka:\n  * Distributed\
      \ streaming platform with high throughput\n  * Pull-based consumption model\n\
      \  * Partition-based scaling with ordering guarantees\n  * Excellent for event\
      \ streaming and data pipelines\n  * Pros: High throughput, durable, scalable,\
      \ replay capability\n  * Cons: Complex setup, no built-in routing, Java-centric\n\
      - RabbitMQ:\n  * Feature-rich message broker with flexible routing\n  * Push-based\
      \ delivery with acknowledgments\n  * Exchange types: direct, topic, fanout,\
      \ headers\n  * Pros: Flexible routing, easy setup, multiple protocols\n  * Cons:\
      \ Lower throughput than Kafka, single point of failure\n- Amazon SQS:\n  * Fully\
      \ managed queue service with auto-scaling\n  * Standard queues (at-least-once)\
      \ and FIFO queues (exactly-once)\n  * Dead letter queues for failed message\
      \ handling\n  * Pros: Managed service, auto-scaling, integration with AWS\n\
      \  * Cons: Vendor lock-in, limited message size, eventual consistency\n- Apache\
      \ Pulsar:\n  * Multi-tenant messaging with geo-replication\n  * Tiered storage\
      \ architecture (hot/cold data)\n  * Both queuing and streaming in single system\n\
      \  * Pros: Multi-tenancy, geo-replication, unified model\n  * Cons: Newer ecosystem,\
      \ complex architecture\n- Redis Pub/Sub:\n  * In-memory pub/sub with very low\
      \ latency\n  * Fire-and-forget delivery (no persistence)\n  * Pros: Extremely\
      \ fast, simple setup\n  * Cons: No durability, limited scaling\n\nMessage Durability\
      \ and Persistence:\n- In-Memory: Fastest but messages lost on restart\n- Disk-based:\
      \ Persistent but slower performance\n- Replicated: Multiple copies for high\
      \ availability\n- Tiered Storage: Hot data in memory, cold data on disk\n\n\
      Consumer Patterns:\n- Competing Consumer:\n  * Multiple consumers compete for\
      \ messages from same queue\n  * Provides load balancing and fault tolerance\n\
      \  * Messages processed by only one consumer\n- Message Dispatcher:\n  * Single\
      \ consumer distributes work to worker threads\n  * Centralized control but potential\
      \ bottleneck\n- Event-driven Consumer:\n  * React to specific events or message\
      \ types\n  * Stateless processing of individual messages\n- Batch Consumer:\n\
      \  * Process multiple messages together\n  * Higher throughput but higher latency\n\
      \nError Handling and Resilience:\n- Dead Letter Queues (DLQ):\n  * Storage for\
      \ messages that failed processing repeatedly\n  * Prevents poison messages from\
      \ blocking queue\n  * Manual investigation and reprocessing capability\n- Retry\
      \ Logic:\n  * Exponential backoff for temporary failures\n  * Maximum retry\
      \ limits to prevent infinite loops\n  * Circuit breaker pattern for downstream\
      \ failures\n- Message TTL (Time-to-Live):\n  * Automatic expiration of old messages\n\
      \  * Prevents queue buildup from slow consumers\n- Poison Message Handling:\n\
      \  * Detection and isolation of malformed messages\n  * Prevents single bad\
      \ message from stopping processing\n\nPerformance Considerations:\n- Throughput\
      \ Optimization:\n  * Batch message publishing and consuming\n  * Asynchronous\
      \ acknowledgments\n  * Partition/shard messages for parallel processing\n- Latency\
      \ Optimization:\n  * In-memory queues for low latency\n  * Minimize message\
      \ serialization overhead\n  * Co-locate producers and consumers\n- Capacity\
      \ Planning:\n  * Monitor queue depth and consumer lag\n  * Auto-scaling based\
      \ on queue metrics\n  * Provision for traffic spikes\n\nMessage Serialization:\n\
      - JSON: Human-readable, widely supported, larger size\n- Protocol Buffers: Compact,\
      \ fast, schema evolution\n- Avro: Schema evolution, data validation, Kafka integration\n\
      - MessagePack: Compact binary format, cross-language support\n\nMonitoring and\
      \ Observability:\n- Key Metrics:\n  * Message throughput (messages/second)\n\
      \  * Consumer lag (time behind latest message)\n  * Queue depth (messages waiting\
      \ to be processed)\n  * Error rates and failed message counts\n- Distributed\
      \ Tracing:\n  * Track message flow across services\n  * Identify bottlenecks\
      \ and failures\n  * Correlation IDs for request tracking\n- Alerting:\n  * High\
      \ consumer lag indicating processing issues\n  * Queue depth growing faster\
      \ than consumption\n  * Error rate spikes indicating system problems\n\nArchitecture\
      \ Patterns:\n- Event-Driven Architecture:\n  * Services communicate through\
      \ events\n  * Loose coupling and high scalability\n  * Example: E-commerce with\
      \ order, payment, inventory services\n- Saga Pattern:\n  * Manage distributed\
      \ transactions through events\n  * Choreography vs orchestration approaches\n\
      \  * Compensation events for rollback scenarios\n- CQRS with Event Sourcing:\n\
      \  * Separate read and write models\n  * Event store as source of truth\n  *\
      \ Replay events to rebuild state\n- Lambda Architecture:\n  * Batch and stream\
      \ processing layers\n  * Speed layer for real-time processing\n  * Batch layer\
      \ for comprehensive analysis\n\nAnti-patterns to Avoid:\n- Synchronous Chain:\
      \ Calling services synchronously in sequence\n- Shared Database: Multiple services\
      \ sharing same database\n- Chatty Messaging: Too many small messages instead\
      \ of batching\n- Message Ordering Dependency: Requiring global message ordering\n\
      - Large Message Payloads: Sending large data through messages\n\nSelection Criteria:\n\
      - Throughput Requirements: Kafka for high throughput, RabbitMQ for moderate\n\
      - Ordering Requirements: Kafka for partition ordering, SQS FIFO for strict ordering\n\
      - Durability Needs: Persistent queues for critical data, in-memory for performance\n\
      - Operational Complexity: Managed services (SQS) vs self-hosted (Kafka)\n- Integration\
      \ Ecosystem: Consider existing technology stack\n"
    resources:
    - title: Kafka vs RabbitMQ
      url: https://aws.amazon.com/compare/the-difference-between-rabbitmq-and-kafka/
      description: Message queue technology comparison
    - title: Message Queue Patterns
      url: https://www.enterpriseintegrationpatterns.com/patterns/messaging/
      description: Enterprise messaging patterns
    - title: Designing Event-Driven Systems
      url: https://www.confluent.io/designing-event-driven-systems/
      description: Event-driven architecture with Kafka
    - title: AWS SQS Best Practices
      url: https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-best-practices.html
      description: Managed queue service optimization
    practice_questions:
      estimation:
      - question: 'E-commerce system: 1M orders/day, each order triggers 5 events.
          Calculate Kafka partition strategy and throughput requirements.'
        answer: 'Events: 1M × 5 = 5M events/day = 58 events/sec avg, 300 peak (5x).
          Kafka throughput: 1KB/event × 300 = 300KB/sec = 0.3MB/sec. Partitions: 10
          partitions (each handles 30 events/sec), enables 10 parallel consumers.
          Replication factor 3 = 0.9MB/sec write. Storage: 5M × 1KB × 365 = 1.8TB/year.
          Cost: ~$200/month for 3-node Kafka cluster.'
      - question: 'Real-time analytics: 100K events/sec, 1KB per message. Estimate
          memory and storage for 7-day retention.'
        answer: 'Throughput: 100K × 1KB = 100MB/sec = 360GB/hour = 8.6TB/day. 7-day
          retention: 8.6TB × 7 = 60TB. Memory (hot data, 1 hour): 360GB across 10
          Kafka brokers = 36GB each. Compression (5:1): 60TB → 12TB storage. With
          3x replication: 36TB total. Cost: ~$5K/month for storage + compute.'
      - question: 'Message processing: 1000 msg/sec rate, 100ms processing time per
          message. How many consumers needed for real-time processing?'
        answer: 'Processing capacity per consumer: 1000ms / 100ms = 10 msg/sec. Consumers
          needed: 1000 / 10 = 100 consumers. With headroom (80% utilization): 125
          consumers. Cost: If 10 consumers per instance (1 core each), need 13 instances.
          Alternative: Reduce processing to 50ms (optimization) = 63 consumers needed.'
      - question: 'Queue capacity: 10K msg/sec peak, 2-hour processing lag tolerance.
          Calculate required queue capacity.'
        answer: 'Max messages in queue: 10K/sec × 2 hours × 3600s = 72M messages.
          At 1KB/message: 72GB. With overhead (30%): 94GB. Use disk-backed queue (SSD).
          Normal operation: 1K/sec × 5min lag = 300K messages = 300MB (memory-based).
          Design: Redis for normal (<5min lag), fall back to Kafka for spikes (>5min
          lag).'
      concepts:
      - question: 'Design message flow for ride-sharing app: trip requests, driver
          matching, payment processing, trip completion.'
        answer: 'Flow: 1) User → trip.requested event → Kafka, 2) Matching service
          consumes → finds driver → trip.matched event, 3) Payment service → charges
          card → payment.completed, 4) Trip service → trip.completed. Use: At-least-once
          delivery with idempotency keys. Ordering: Partition by trip_id (per-trip
          ordering). Latency: 100ms total. Failure: DLQ for payment failures, manual
          review.'
      - question: Compare at-least-once vs exactly-once delivery for inventory updates.
          Design idempotent consumers.
        answer: 'At-least-once: Simpler, 10ms latency, may duplicate (e.g., decrement
          inventory twice). Exactly-once: Complex (2PC), 50ms latency, guarantees
          no duplicates. Idempotent design: Store processed message IDs in DB, check
          before processing. Redis SET with 24h TTL for deduplication. Cost: 1ms lookup
          overhead. Recommendation: Use at-least-once + idempotency (best of both
          worlds).'
      - question: 'Implement saga pattern for hotel booking: reserve room, charge
          payment, confirm booking with rollback capability.'
        answer: 'Choreography approach: 1) booking.initiated → Room service reserves
          (30s timeout) → room.reserved, 2) Payment service charges → payment.completed,
          3) Confirmation service → booking.confirmed. Rollback: payment.failed →
          room.release-reservation. Use compensating transactions. Store saga state
          in DB. Timeout: Cancel after 2min total. Alternative: Orchestrator pattern
          with centralized workflow (simpler debugging, single point of failure).'
      - question: 'Design event-driven microservices for social media: user posts,
          notifications, timeline updates, analytics.'
        answer: 'Events: post.created → Kafka (4 partitions by user_id). Consumers:
          1) Notification service (sends push, 50ms), 2) Timeline service (fan-out
          to followers, 200ms), 3) Analytics service (batch processing, 5min lag).
          Use fanout exchange (RabbitMQ) or topic (Kafka). Throughput: 1000 posts/sec.
          Each event copied to 3 services = 3K msg/sec consumption. Cost: Kafka cluster
          $500/month.'
      - question: 'Handle message ordering in distributed system: per-user ordering
          vs global ordering trade-offs.'
        answer: 'Per-user: Partition by user_id, 100K users × 10 msg/sec = 1M msg/sec
          total across partitions. Parallelism: 100 partitions = 100 consumers. Latency:
          10ms. Global ordering: Single partition = 10K msg/sec limit (bottleneck),
          single consumer, 50ms latency. Use per-user for: Chat apps, user feeds.
          Global for: Banking ledger, audit logs (less common). Per-user gives 100x
          throughput.'
      - question: 'Design dead letter queue strategy: retry logic, poison message
          detection, manual recovery process.'
        answer: 'Strategy: 1) Initial failure → Retry queue (exponential backoff:
          1s, 5s, 30s), 2) After 3 retries → DLQ with error reason, 3) Monitor DLQ
          depth, alert if >100 messages, 4) Manual replay: Fix issue → Move messages
          back to main queue. Poison messages: Detect by error pattern (e.g., JSON
          parse error) → Auto-skip, log for investigation. Metrics: DLQ size, retry
          success rate.'
      tradeoffs:
      - question: 'Kafka vs RabbitMQ for real-time chat application: latency, throughput,
          complexity comparison'
        answer: 'Kafka: 10-20ms latency, 100K msg/sec throughput, complex setup (ZooKeeper/KRaft),
          replay messages, partition ordering. RabbitMQ: 5-10ms latency, 20K msg/sec
          throughput, easier setup, flexible routing, no replay. For chat: Use RabbitMQ
          (lower latency matters more than ultra-high throughput, 20K msg/sec sufficient
          for 100K users). Use Kafka for: Chat history/analytics pipeline.'
      - question: 'Push vs pull consumption models: resource usage, flow control,
          scalability analysis'
        answer: 'Push (RabbitMQ): Broker pushes to consumers, low latency (5ms), consumers
          may be overwhelmed (need flow control), broker tracks consumer state. Pull
          (Kafka): Consumers pull messages, slightly higher latency (10ms), consumers
          control rate (natural backpressure), consumers track offset. Use push for:
          Low-latency real-time. Pull for: High throughput, batch processing, independent
          consumer rates.'
      - question: 'Synchronous vs asynchronous API calls: consistency, performance,
          error handling trade-offs'
        answer: 'Synchronous: Immediate response, strong consistency, 50-200ms latency,
          caller waits, simple error handling (HTTP status), tight coupling. Async:
          Immediate accept (202), eventual consistency, 10ms response + background
          processing, caller continues, complex error handling (callbacks/polling),
          loose coupling. Use sync for: User-facing APIs (<200ms). Async for: Long
          operations (reports, batch), decoupled systems.'
      - question: 'Message durability vs performance: in-memory vs persistent queues
          for different use cases'
        answer: 'In-memory (Redis Pub/Sub): 1ms latency, 100K msg/sec, no durability
          (lost on restart), $50/month. Persistent (Kafka): 10ms latency, 50K msg/sec,
          durable (survives restart), $500/month. Use in-memory for: Real-time notifications,
          ephemeral data. Persistent for: Financial data, audit logs, critical events.
          Hybrid: In-memory primary + persistent backup for best performance + durability.'
      - question: 'Single large message vs multiple small messages: network overhead,
          processing complexity'
        answer: 'Large message (1MB): 1 network call, 50ms transfer, harder to process
          (parse all), retry entire message on failure. Small messages (100 × 10KB):
          100 network calls (overhead), 50ms total (parallel), easier to process (stream),
          retry only failed messages. Use large for: Batch uploads, reports. Small
          for: Real-time events, granular processing. Threshold: Messages >100KB should
          be split or use reference (store in S3, send URL).'
      scenarios:
      - 'Design messaging for flash sale: handle 100x traffic spike, ensure inventory
        consistency, prevent overselling.'
      - 'Implement event sourcing for banking system: account transactions, balance
        calculations, audit requirements.'
      - 'Design notification system: email, SMS, push notifications with user preferences
        and delivery guarantees.'
      - 'Handle queue consumer failures: automatic failover, message reprocessing,
        duplicate detection.'
      - 'Design cross-region message replication: data consistency, network partitions,
        conflict resolution.'
      - 'Implement circuit breaker for message processing: detect downstream failures,
        graceful degradation.'
    time_estimate: 60
    video_resources:
    - title: 'Hello Interview: Design TinyURL'
      url: https://www.youtube.com/@HelloInterview
      duration: 25 min
      description: URL shortener system design walkthrough
      priority: high
    - title: 'ByteByteGo: URL Shortening System'
      url: https://www.youtube.com/@ByteByteGo
      duration: 15 min
      description: Complete URL shortener design
      priority: high
  - day: 9
    topic: API Design Best Practices
    activity: Master comprehensive API design including REST/GraphQL, security, performance,
      monitoring, and developer experience optimization.
    detailed_content: "RESTful API Design Principles:\nREST (Representational State\
      \ Transfer) Core Constraints:\n- Client-Server: Separation of concerns between\
      \ UI and data storage\n- Stateless: Each request contains all necessary information\n\
      - Cacheable: Responses explicitly indicate cacheability\n- Uniform Interface:\
      \ Consistent resource identification and manipulation\n- Layered System: Architecture\
      \ composed of hierarchical layers\n- Code on Demand (optional): Server can extend\
      \ client functionality\n\nHTTP Methods and Semantics:\n- GET: Retrieve resource\
      \ representation (idempotent, safe)\n- POST: Create new resource or non-idempotent\
      \ operations\n- PUT: Replace entire resource (idempotent)\n- PATCH: Partial\
      \ resource update (may not be idempotent)\n- DELETE: Remove resource (idempotent)\n\
      - HEAD: GET without response body (metadata only)\n- OPTIONS: Discover allowed\
      \ methods for resource\n\nResource Design and URL Structure:\n- Use nouns for\
      \ resources, not verbs: /users/123/orders (not /getUserOrders)\n- Hierarchical\
      \ relationships: /users/123/orders/456/items\n- Collection vs singleton: /users\
      \ (collection), /users/123 (singleton)\n- Query parameters for filtering: /users?status=active&role=admin\n\
      - Avoid deep nesting (max 2-3 levels): /users/123/orders/456\n- Consistent naming\
      \ conventions: snake_case or camelCase\n\nHTTP Status Codes Strategy:\nSuccess\
      \ Codes:\n- 200 OK: Standard successful response\n- 201 Created: Resource successfully\
      \ created\n- 202 Accepted: Request accepted for processing\n- 204 No Content:\
      \ Successful with no response body\n\nClient Error Codes:\n- 400 Bad Request:\
      \ Invalid request format/parameters\n- 401 Unauthorized: Authentication required\n\
      - 403 Forbidden: Access denied (authenticated but not authorized)\n- 404 Not\
      \ Found: Resource doesn't exist\n- 409 Conflict: Resource conflict (duplicate\
      \ creation)\n- 422 Unprocessable Entity: Valid format but semantic errors\n\
      - 429 Too Many Requests: Rate limit exceeded\n\nServer Error Codes:\n- 500 Internal\
      \ Server Error: Unexpected server condition\n- 502 Bad Gateway: Invalid response\
      \ from upstream\n- 503 Service Unavailable: Temporary service overload\n- 504\
      \ Gateway Timeout: Upstream timeout\n\nGraphQL vs REST Comparison:\nREST Advantages:\n\
      - Simple and widely understood\n- Leverages HTTP caching effectively\n- Better\
      \ tooling and infrastructure support\n- Easier to cache at CDN/proxy level\n\
      - Stateless and scalable by default\n\nGraphQL Advantages:\n- Single endpoint\
      \ for all operations\n- Client specifies exact data requirements\n- Strongly\
      \ typed schema with introspection\n- Real-time subscriptions support\n- Eliminates\
      \ over-fetching and under-fetching\n- Better for mobile applications (bandwidth\
      \ efficiency)\n\nWhen to Choose Each:\n- REST: Public APIs, simple CRUD operations,\
      \ heavy caching requirements\n- GraphQL: Complex data relationships, mobile\
      \ apps, rapid client development\n\nAPI Versioning Strategies:\n1. URL Path\
      \ Versioning:\n   - Format: /v1/users, /v2/users\n   - Pros: Clear, explicit,\
      \ cacheable\n   - Cons: URL proliferation, tight coupling\n\n2. Header Versioning:\n\
      \   - Format: Accept: application/vnd.api+json;version=2\n   - Pros: Clean URLs,\
      \ flexible content negotiation\n   - Cons: Less visible, harder to test manually\n\
      \n3. Query Parameter Versioning:\n   - Format: /users?version=2\n   - Pros:\
      \ Simple, backward compatible\n   - Cons: Easy to forget, query parameter pollution\n\
      \n4. Custom Header Versioning:\n   - Format: API-Version: 2.0\n   - Pros: Explicit,\
      \ doesn't affect URL structure\n   - Cons: Additional complexity, tooling requirements\n\
      \nVersioning Best Practices:\n- Semantic versioning: Major.Minor.Patch (2.1.3)\n\
      - Deprecation timeline and communication\n- Sunset HTTP header for deprecated\
      \ versions\n- Maintain backward compatibility within major versions\n- Version\
      \ breaking changes only, not additions\n\nAuthentication and Authorization:\n\
      Authentication Methods:\n- API Keys: Simple but limited (no user context)\n\
      - OAuth 2.0: Industry standard, supports various flows\n- JWT (JSON Web Tokens):\
      \ Stateless, self-contained\n- Basic Authentication: Simple but requires HTTPS\n\
      - Bearer Tokens: Flexible token-based approach\n\nOAuth 2.0 Flows:\n- Authorization\
      \ Code: For web applications with backend\n- Client Credentials: For service-to-service\
      \ communication\n- Resource Owner Password: Legacy apps (not recommended)\n\
      - Implicit Flow: Deprecated for security reasons\n- PKCE: Enhanced security\
      \ for mobile/SPA applications\n\nJWT Implementation:\n- Header: Algorithm and\
      \ token type\n- Payload: Claims (user data, expiration)\n- Signature: Verification\
      \ of token integrity\n- Stateless: No server-side session storage required\n\
      - Security: Use strong secrets, short expiration times\n\nAuthorization Strategies:\n\
      - Role-Based Access Control (RBAC): Users have roles with permissions\n- Attribute-Based\
      \ Access Control (ABAC): Fine-grained attribute evaluation\n- Scope-based: OAuth\
      \ scopes define access boundaries\n- Resource-based: Per-resource permission\
      \ checking\n\nRate Limiting and Throttling:\nRate Limiting Algorithms:\n1. Token\
      \ Bucket:\n   - Tokens added at fixed rate to bucket (max capacity)\n   - Requests\
      \ consume tokens\n   - Allows burst traffic up to bucket capacity\n   - Implementation:\
      \ Redis with INCR and EXPIRE\n\n2. Leaky Bucket:\n   - Requests enter bucket,\
      \ processed at fixed rate\n   - Smooth, consistent request rate\n   - Drops\
      \ requests when bucket overflows\n   - Good for traffic shaping\n\n3. Fixed\
      \ Window Counter:\n   - Count requests in fixed time windows\n   - Simple to\
      \ implement and understand\n   - Thundering herd problem at window boundaries\n\
      \   - Implementation: Redis with time-based keys\n\n4. Sliding Window Log:\n\
      \   - Track timestamps of all requests\n   - Accurate but memory intensive\n\
      \   - Good for strict rate limiting\n   - Implementation: Redis sorted sets\n\
      \n5. Sliding Window Counter:\n   - Hybrid approach combining fixed window efficiency\n\
      \   - Approximate sliding window accuracy\n   - Memory efficient\n   - Weighted\
      \ calculation across windows\n\nDistributed Rate Limiting:\n- Centralized: Redis/database\
      \ for global limits\n- Local + Sync: Local counters with periodic sync\n- Consistent\
      \ Hashing: Distribute limits across nodes\n- Leader Election: Designated node\
      \ tracks global state\n\nAPI Security Best Practices:\nInput Validation and\
      \ Sanitization:\n- Validate all input parameters and headers\n- Use allow-lists\
      \ over deny-lists\n- Implement proper data type checking\n- Sanitize outputs\
      \ to prevent XSS\n- SQL injection prevention with parameterized queries\n\n\
      Transport Security:\n- Always use HTTPS (TLS 1.2+ minimum)\n- HTTP Strict Transport\
      \ Security (HSTS) headers\n- Certificate pinning for mobile applications\n-\
      \ Secure cookie flags (HttpOnly, Secure, SameSite)\n\nCommon Vulnerabilities:\n\
      - Injection attacks (SQL, NoSQL, Command injection)\n- Broken authentication\
      \ and session management\n- Insecure direct object references\n- Cross-Site\
      \ Request Forgery (CSRF)\n- Server-Side Request Forgery (SSRF)\n- Mass assignment\
      \ vulnerabilities\n\nSecurity Headers:\n- Content-Security-Policy: Prevent XSS\
      \ attacks\n- X-Frame-Options: Prevent clickjacking\n- X-Content-Type-Options:\
      \ Prevent MIME sniffing\n- X-XSS-Protection: Enable browser XSS filters\n\n\
      Error Handling and Response Design:\nError Response Structure:\n- Consistent\
      \ error format across all endpoints\n- Machine-readable error codes\n- Human-readable\
      \ error messages\n- Additional context for debugging (in development)\n- Correlation\
      \ IDs for request tracing\n\nExample Error Response:\n```json\n{\n  \"error\"\
      : {\n    \"code\": \"VALIDATION_FAILED\",\n    \"message\": \"Request validation\
      \ failed\",\n    \"details\": [\n      {\n        \"field\": \"email\",\n  \
      \      \"error\": \"Invalid email format\"\n      }\n    ],\n    \"trace_id\"\
      : \"abc123xyz\"\n  }\n}\n```\n\nPagination Strategies:\n1. Offset-based (LIMIT/OFFSET):\n\
      \   - Simple but inefficient for large datasets\n   - Consistent with SQL databases\n\
      \   - Problem: data shifts during pagination\n\n2. Cursor-based (keyset pagination):\n\
      \   - Efficient for large datasets\n   - Stable pagination with data changes\n\
      \   - Uses unique, ordered field as cursor\n\n3. Token-based:\n   - Opaque pagination\
      \ tokens\n   - Flexible implementation\n   - Enables advanced pagination features\n\
      \nResponse Filtering and Field Selection:\n- Sparse fieldsets: ?fields=id,name,email\n\
      - Resource expansion: ?expand=profile,orders\n- Filtering: ?filter[status]=active\n\
      - Sorting: ?sort=-created_at,name\n\nAPI Documentation and Developer Experience:\n\
      OpenAPI/Swagger Specification:\n- Machine-readable API definitions\n- Automatic\
      \ documentation generation\n- Code generation for clients and servers\n- Interactive\
      \ API exploration\n- Contract-first development approach\n\nDocumentation Best\
      \ Practices:\n- Complete request/response examples\n- Error scenarios and handling\n\
      - Authentication requirements\n- Rate limiting information\n- SDK availability\
      \ and code samples\n- Changelog and migration guides\n\nDeveloper Onboarding:\n\
      - Quick start guides and tutorials\n- Sandbox environment for testing\n- Postman\
      \ collections or similar tools\n- Community forums and support channels\n- Clear\
      \ pricing and usage tiers\n\nAPI Monitoring and Observability:\nKey Metrics\
      \ to Track:\n- Request latency (P50, P95, P99 percentiles)\n- Throughput (requests\
      \ per second)\n- Error rates by status code\n- Authentication failure rates\n\
      - Rate limiting trigger frequency\n- Payload sizes and response times\n\nLogging\
      \ Best Practices:\n- Structured logging (JSON format)\n- Correlation IDs for\
      \ request tracing\n- Log levels: ERROR, WARN, INFO, DEBUG\n- Sensitive data\
      \ scrubbing\n- Centralized log aggregation\n\nDistributed Tracing:\n- Request\
      \ flow across microservices\n- Performance bottleneck identification\n- Error\
      \ propagation tracking\n- Tools: Jaeger, Zipkin, DataDog APM\n\nHealth Checks\
      \ and Status Pages:\n- Health endpoint (/health, /status)\n- Dependency health\
      \ checking\n- Circuit breaker status\n- Public status page for incidents\n\n\
      Performance Optimization:\nCaching Strategies:\n- HTTP caching headers (Cache-Control,\
      \ ETag)\n- CDN integration for static responses\n- Application-level caching\
      \ (Redis, Memcached)\n- Database query optimization\n\nResponse Optimization:\n\
      - Compression (gzip, Brotli)\n- Minification of response payloads\n- Efficient\
      \ serialization formats\n- Connection pooling and keep-alive\n\nDatabase Performance:\n\
      - Query optimization and indexing\n- Connection pooling\n- Read replicas for\
      \ query distribution\n- Caching frequently accessed data\n\nAPI Gateway Patterns:\n\
      Gateway Responsibilities:\n- Request routing and load balancing\n- Authentication\
      \ and authorization\n- Rate limiting and throttling\n- Request/response transformation\n\
      - API versioning and deprecation\n- Monitoring and analytics\n- Security policy\
      \ enforcement\n\nPopular API Gateway Solutions:\n- Kong: Plugin-based, high\
      \ performance\n- Ambassador: Kubernetes-native, Envoy-based\n- AWS API Gateway:\
      \ Serverless, fully managed\n- Zuul: Netflix OSS, Java-based\n- Istio: Service\
      \ mesh with gateway capabilities\n\nGateway vs Service Mesh:\n- API Gateway:\
      \ North-south traffic (client-to-service)\n- Service Mesh: East-west traffic\
      \ (service-to-service)\n- Complementary patterns for complete traffic management\n\
      \nAPI Testing Strategies:\nUnit Testing:\n- Controller/handler testing\n- Business\
      \ logic validation\n- Mock external dependencies\n- Test data validation and\
      \ serialization\n\nIntegration Testing:\n- End-to-end API workflow testing\n\
      - Database integration validation\n- External service integration\n- Authentication\
      \ and authorization flows\n\nContract Testing:\n- Provider contracts (API specification\
      \ adherence)\n- Consumer contracts (client expectation validation)\n- Tools:\
      \ Pact, Spring Cloud Contract\n\nPerformance Testing:\n- Load testing under\
      \ normal conditions\n- Stress testing at capacity limits\n- Spike testing with\
      \ sudden traffic increases\n- Endurance testing for memory leaks\n\nReal-World\
      \ Examples:\nE-commerce API Design:\n- Product catalog with search and filtering\n\
      - Shopping cart management (session vs persistent)\n- Order processing workflow\n\
      - Payment integration and webhooks\n- Inventory management and concurrency\n\
      \nSocial Media API:\n- User profile and relationship management\n- Content publishing\
      \ and feed generation\n- Real-time notifications\n- Image/video upload handling\n\
      - Privacy controls and content moderation\n\nFinancial Services API:\n- Account\
      \ and transaction management\n- Payment processing and settlements\n- Regulatory\
      \ compliance (PCI DSS, SOX)\n- Fraud detection integration\n- High availability\
      \ and consistency requirements\n"
    resources:
    - title: REST API Tutorial
      url: https://restfulapi.net/
      description: Comprehensive REST API design guide and best practices
    - title: OpenAPI Specification
      url: https://swagger.io/specification/
      description: Standard for REST API documentation and tooling
    - title: GraphQL Best Practices
      url: https://graphql.org/learn/best-practices/
      description: Official GraphQL implementation guidance
    - title: OAuth 2.0 Security Best Practices
      url: https://tools.ietf.org/html/draft-ietf-oauth-security-topics
      description: Security considerations for OAuth implementations
    - title: API Rate Limiting Patterns
      url: https://stripe.com/blog/rate-limiters
      description: Stripe's approach to distributed rate limiting
    - title: HTTP Status Code Guide
      url: https://httpstatuses.com/
      description: Complete reference for HTTP status codes
    practice_questions:
      estimation:
      - question: API serves 10M requests/day. With 1KB average response, what's daily
          bandwidth? How would you cache this?
        answer: 'Bandwidth: 10M × 1KB = 10GB/day = 115KB/sec avg, 575KB/sec peak (5x).
          Caching: 80/20 rule → cache 20% of endpoints = 2M requests × 1KB = 2GB cache
          (Redis). Hit rate: 80% → bandwidth reduced to 2GB/day (8x reduction). CDN
          for static assets: Additional 50% reduction = 1GB/day total. Cost: Redis
          $50/month + CDN $100/month.'
      - question: Social feed API has 100K users, each following 500 people. How do
          you design pagination for timeline queries?
        answer: 'Options: 1) Offset-based: Skip N, take 20 (simple but slow for large
          offsets, O(N) scan). 2) Cursor-based: Use last_seen_id, WHERE id < last_id
          LIMIT 20 (O(1) lookup with index). Recommendation: Cursor-based. Storage:
          100K users × 500 follows = 50M edges × 16B = 800MB graph data. Timeline
          generation: Fan-out on write (pre-compute timelines), 1KB × 20 items = 20KB
          per page.'
      - question: Payment API requires 99.99% availability. Design rate limiting for
          1000 TPS with burst capability.
        answer: 'Rate limiting: Token bucket with 1000 tokens/sec refill, bucket size
          5000 (5s burst). Implementation: Redis INCR with TTL. Availability: Multi-region
          deployment (3 regions) with failover. Each region handles 333 TPS normally,
          can handle 1000 TPS during failover. Monitoring: Alert if TPS >800 (80%
          capacity). Cost: $5K/month for 3-region setup + $100 for Redis rate limiter.'
      - question: File upload API handles 1GB files. How do you design chunked upload
          with resume capability?
        answer: 'Design: 1) Client splits into 10MB chunks, 2) POST /uploads → returns
          upload_id, 3) PUT /uploads/{id}/chunks/{chunk_num} for each chunk, 4) POST
          /uploads/{id}/complete to finalize. Resume: Store chunk metadata in Redis
          (uploaded_chunks bitmap). Timeout: 24h expiry for incomplete uploads. Storage:
          S3 multipart upload (min chunk 5MB). Bandwidth: 1GB/100s = 10MB/sec per
          upload. Support 100 concurrent uploads = 1GB/sec bandwidth needed.'
      concepts:
      - question: When would you choose GraphQL over REST for a mobile application?
        answer: 'Choose GraphQL when: 1) Mobile needs flexible queries (fetch exactly
          what''s needed, save bandwidth), 2) Complex nested data (user + posts +
          comments in one query vs 3 REST calls), 3) Rapid iteration (mobile doesn''t
          need new API endpoints). REST when: Simple CRUD, caching important (GraphQL
          queries harder to cache), team unfamiliar with GraphQL. Mobile benefit:
          50% less data transfer, 3x fewer API calls.'
      - question: How do you implement idempotent operations for payment processing?
        answer: 'Use idempotency keys: 1) Client generates UUID for each payment request,
          2) API checks if key exists in Redis/DB (TTL 24h), 3) If exists: Return
          cached response, 4) If new: Process payment + store result with key. Payment
          provider idempotency: Stripe accepts idempotency_key header. Prevents: Double
          charges from retries. Store: Redis SET with 24h expiry, or DB unique constraint.
          Cost: 1ms lookup overhead per request.'
      - question: What's the difference between authentication and authorization in
          API design?
        answer: 'Authentication: Who are you? Verify identity (login with password,
          OAuth, JWT token). Authorization: What can you do? Check permissions (role-based:
          admin/user, resource-based: own posts only). Implementation: Auth → JWT
          contains user_id + roles, 50ms. Authz → Check user permissions for resource,
          10ms DB query or 1ms cache. Example: User authenticated (valid JWT) but
          unauthorized (can''t access admin endpoint).'
      - question: How do you handle API versioning when breaking changes are required?
        answer: 'Strategy: 1) URL versioning: /v1/users, /v2/users (clear, cacheable,
          multiple versions deployed), 2) Deprecation timeline: Announce → 6 months
          warning → Shut down v1. Migration: Run v1 and v2 in parallel (6-12 months),
          monitor v1 usage, contact active users. Breaking changes: New fields OK
          in v1, changed field types need v2. Cost: 2x infrastructure during migration.
          Alternative: Header versioning (Accept: application/vnd.api+json; version=2).'
      - question: What are the security implications of using JWT vs session-based
          authentication?
        answer: 'JWT: Stateless (no DB lookup), scales horizontally, can''t revoke
          (until expiry), token size 1KB (sent in every request), short expiry (15min)
          + refresh token (7 days). Session: Stateful (Redis/DB lookup 1ms), easy
          revocation, small cookie (32B session ID), can track concurrent sessions.
          JWT risks: Stolen token valid until expiry, mitigate with short expiry +
          HTTPS only. Session risks: Session fixation, mitigate with regeneration
          on login.'
      - question: How do you design APIs for eventual consistency in microservices?
        answer: 'Pattern: 1) API returns 202 Accepted (not 200 OK), 2) Provide status
          endpoint: GET /operations/{id}/status, 3) Webhook callback when complete.
          Example: POST /orders → 202 + order_id → Async processing (inventory, payment)
          → Webhook to client when done. Client polling: Every 1s for 30s. Alternative:
          WebSocket for real-time updates. Trade-off: Complex client vs simple API.
          Consistency: 1-5s lag acceptable for non-critical operations.'
      tradeoffs:
      - question: 'URL versioning vs header versioning: pros and cons for public APIs'
        answer: 'URL (/v1/users): Clear, cacheable (different URLs), visible in logs,
          clutters URL space, easy testing (just change URL). Header (Accept: version=2):
          Clean URLs, same endpoint, harder caching (Vary header), harder testing
          (need custom headers), invisible in browser. Public API recommendation:
          URL versioning (easier for API consumers, Stripe/Twilio use this). Internal:
          Header versioning acceptable.'
      - question: Synchronous vs asynchronous API design for order processing
        answer: 'Sync: User waits for order confirmation (200 OK), 500ms-2s latency,
          simple client, tight coupling, scales to 1K orders/sec. Async: Immediate
          202 Accepted, <50ms response, complex client (polling/webhooks), loose coupling,
          scales to 100K orders/sec. Use sync for: Simple orders (<500ms total). Async
          for: Complex orders (inventory check + payment + shipping = >2s), high throughput
          needed.'
      - question: 'JWT vs session tokens: security and scalability trade-offs'
        answer: 'JWT: Stateless, no DB lookup (0ms), scales to 1M RPS, can''t revoke
          (security risk), 1KB token size (bandwidth), horizontal scaling easy. Session:
          1ms Redis lookup, limited to 100K RPS per Redis cluster, instant revocation
          (logout works immediately), 32B cookie (small), vertical scaling needed.
          Use JWT for: Microservices, stateless APIs. Session for: Traditional web
          apps, need instant logout.'
      - question: REST vs GraphQL for complex data relationships
        answer: 'REST: Over-fetching (get full user object when need just name), under-fetching
          (N+1 queries: users → posts for each user), predictable, easy caching. GraphQL:
          Fetch exactly what''s needed, single query for nested data, flexible, but
          harder caching, N+1 problem on server (use DataLoader). Use GraphQL for:
          Mobile apps (minimize bandwidth), complex UIs (many variations). REST for:
          Simple CRUD, public APIs, caching critical.'
      - question: 'API Gateway vs direct service access: performance vs functionality'
        answer: 'API Gateway: Single entry point, auth/rate limiting centralized,
          5-10ms latency overhead, single point of failure (mitigate with HA), rich
          features (CORS, logging). Direct access: No latency overhead, simpler, but
          auth duplicated across services, harder rate limiting. Use Gateway for:
          External APIs (public, mobile), >10 services. Direct for: Internal service-to-service
          (service mesh instead), performance critical (<5ms).'
      - question: Optimistic vs pessimistic locking in concurrent API operations
        answer: 'Optimistic: Assume no conflicts, use version numbers, retry on conflict,
          10ms latency (no locks), 95% success rate at low contention, scales to 10K
          TPS. Pessimistic: Lock resource (SELECT FOR UPDATE), 50ms latency (lock
          wait), 100% success, scales to 1K TPS. Use optimistic for: Rare conflicts
          (inventory with 1000 units), high concurrency. Pessimistic for: Frequent
          conflicts (last ticket), financial transactions.'
      scenarios:
      - Design a rate limiting system for a multi-tenant SaaS API with different pricing
        tiers
      - How would you handle API deprecation for a public API with 1000+ integrations?
      - Design error handling for a payment API that interacts with multiple bank
        APIs
      - Create an API authentication strategy for a mobile app with offline capabilities
      - Design a file upload API that supports resumable uploads and virus scanning
      - How would you implement real-time notifications alongside a REST API?
      - Design API monitoring to detect and respond to DDoS attacks
      - Create a webhook system for event notifications with guaranteed delivery
    time_estimate: 90
    video_resources:
    - title: 'Hello Interview: Design Instagram'
      url: https://www.youtube.com/@HelloInterview
      duration: 30 min
      description: Photo sharing platform design
      priority: high
    - title: 'ByteByteGo: Design Instagram Feed'
      url: https://www.youtube.com/@ByteByteGo
      duration: 12 min
      description: News feed generation strategies
      priority: medium
  - day: 10
    topic: Microservices Architecture
    activity: Master comprehensive microservices design including decomposition strategies,
      communication patterns, data management, and operational concerns.
    detailed_content: "Microservices Architecture Fundamentals:\nMicroservices Definition\
      \ and Characteristics:\n- Independent deployability: Each service can be deployed\
      \ separately\n- Business capability alignment: Services organized around business\
      \ functions\n- Decentralized governance: Teams own their services end-to-end\n\
      - Technology diversity: Different services can use different tech stacks\n-\
      \ Failure isolation: Service failures don't cascade across the system\n- Evolutionary\
      \ design: Services can evolve independently\n\nMonolith vs Microservices Trade-offs:\n\
      Monolith Advantages:\n- Simpler development and testing initially\n- Easier\
      \ debugging and monitoring\n- Better performance (in-process calls)\n- ACID\
      \ transactions across entire application\n- Simpler deployment and operations\n\
      \nMicroservices Advantages:\n- Technology diversity and innovation\n- Independent\
      \ scaling of components\n- Team autonomy and ownership\n- Fault isolation and\
      \ resilience\n- Easier to understand individual services\n- Parallel development\
      \ by multiple teams\n\nWhen to Choose Microservices:\n- Large, complex applications\
      \ with multiple business domains\n- Multiple teams working on different features\n\
      - Need for independent deployment and scaling\n- Different technology requirements\
      \ per domain\n- Organizations with Conway's Law considerations\n\nService Decomposition\
      \ Strategies:\nDomain-Driven Design (DDD) Approach:\n1. Identify Business Domains:\n\
      \   - Core domains: Primary business value\n   - Supporting domains: Necessary\
      \ but not core\n   - Generic domains: Common functionality\n\n2. Bounded Contexts:\n\
      \   - Define clear boundaries where domain models apply\n   - Each bounded context\
      \ becomes a potential microservice\n   - Avoid sharing data models across contexts\n\
      \   - Context mapping between domains\n\n3. Aggregate Design:\n   - Identify\
      \ business entities and their relationships\n   - Group tightly coupled entities\
      \ into aggregates\n   - Each aggregate typically maps to one microservice\n\
      \   - Ensure consistency within aggregates\n\nDecomposition Patterns:\n1. Decompose\
      \ by Business Capability:\n   - Organize services around what the business does\n\
      \   - Examples: User Management, Order Processing, Payment\n   - Services should\
      \ be cohesive and loosely coupled\n\n2. Decompose by Sub-domain:\n   - Based\
      \ on DDD sub-domains\n   - Each sub-domain has its own service\n   - Clear boundaries\
      \ and responsibilities\n\n3. Decompose by Transaction Boundaries:\n   - Services\
      \ handle complete business transactions\n   - Avoid distributed transactions\
      \ where possible\n   - Design for eventual consistency\n\n4. Strangler Fig Pattern:\n\
      \   - Gradually replace monolith components\n   - Route new features to microservices\n\
      \   - Incrementally migrate existing functionality\n\nService Sizing Guidelines:\n\
      - Single Responsibility: One service, one business capability\n- Team Size:\
      \ Service should be owned by one team (2-pizza rule)\n- Data Consistency: Service\
      \ owns its data and business rules\n- Deployment Independence: Service can be\
      \ deployed without affecting others\n- Technology Boundaries: Different technology\
      \ needs may justify separation\n\nInter-Service Communication Patterns:\nSynchronous\
      \ Communication:\n1. HTTP/REST APIs:\n   - Simple and widely understood\n  \
      \ - Good for request-response patterns\n   - Easy to test and debug\n   - Challenges:\
      \ Tight coupling, cascading failures\n\n2. gRPC:\n   - High-performance RPC\
      \ framework\n   - Protocol buffers for serialization\n   - Streaming support\
      \ and bi-directional communication\n   - Strong typing and code generation\n\
      \   - Better performance than REST for high-frequency calls\n\n3. GraphQL:\n\
      \   - Client specifies exact data requirements\n   - Single endpoint for multiple\
      \ services\n   - Good for mobile and frontend applications\n   - Challenges:\
      \ Query complexity, caching\n\nAsynchronous Communication:\n1. Message Queues:\n\
      \   - Point-to-point communication\n   - Guaranteed delivery and ordering\n\
      \   - Good for command processing\n   - Examples: RabbitMQ, ActiveMQ\n\n2. Event\
      \ Streaming:\n   - Publish-subscribe pattern\n   - Event sourcing and replay\
      \ capabilities\n   - Good for real-time data processing\n   - Examples: Apache\
      \ Kafka, Amazon Kinesis\n\n3. Event-Driven Architecture:\n   - Services publish\
      \ domain events\n   - Other services subscribe to relevant events\n   - Loose\
      \ coupling and scalability\n   - Eventual consistency model\n\nCommunication\
      \ Best Practices:\n- Prefer asynchronous communication for better resilience\n\
      - Use synchronous calls only when immediate response needed\n- Implement circuit\
      \ breakers for fault tolerance\n- Design for network failures and timeouts\n\
      - Use correlation IDs for request tracing\n\nData Management in Microservices:\n\
      Database Per Service Pattern:\nBenefits:\n- Service independence and autonomy\n\
      - Technology diversity (SQL, NoSQL, etc.)\n- Independent scaling of data layer\n\
      - Clear ownership and responsibility\n\nChallenges:\n- No cross-service transactions\n\
      - Data consistency across services\n- Complex queries spanning services\n- Data\
      \ synchronization requirements\n\nImplementation Strategies:\n- Private database\
      \ per service\n- Shared database anti-pattern (avoid)\n- Database interface\
      \ per service\n- API-first approach to data access\n\nDistributed Transaction\
      \ Patterns:\n1. Saga Pattern:\n   - Sequence of local transactions\n   - Compensating\
      \ transactions for rollback\n   - Two types: Choreography and Orchestration\n\
      \nChoreography-based Saga:\n- Services communicate through events\n- Each service\
      \ listens and reacts to events\n- Decentralized coordination\n- Good for simple\
      \ workflows\n\nOrchestration-based Saga:\n- Central coordinator manages the\
      \ saga\n- Explicit workflow definition\n- Better for complex business processes\n\
      - Easier debugging and monitoring\n\n2. Two-Phase Commit (2PC):\n   - Traditional\
      \ distributed transaction approach\n   - Coordinator manages prepare/commit\
      \ phases\n   - Problems: Blocking, single point of failure\n   - Generally avoided\
      \ in microservices\n\nEvent Sourcing Pattern:\n- Store events instead of current\
      \ state\n- Replay events to reconstruct state\n- Complete audit trail of changes\n\
      - Supports temporal queries\n- Challenges: Event schema evolution, complexity\n\
      \nCQRS (Command Query Responsibility Segregation):\n- Separate read and write\
      \ models\n- Optimize read and write operations independently\n- Often combined\
      \ with event sourcing\n- Improved performance and scalability\n- Challenges:\
      \ Eventual consistency, complexity\n\nData Consistency Patterns:\n1. Eventual\
      \ Consistency:\n   - Accept temporary inconsistency\n   - System converges to\
      \ consistent state\n   - Better availability and performance\n   - Requires\
      \ careful design\n\n2. Strong Consistency:\n   - Immediate consistency across\
      \ services\n   - May impact availability and performance\n   - Use only when\
      \ business requires it\n\nService Discovery and Registration:\nService Discovery\
      \ Patterns:\n1. Client-Side Discovery:\n   - Client queries service registry\
      \ directly\n   - Client handles load balancing\n   - Examples: Netflix Eureka,\
      \ Consul\n\n2. Server-Side Discovery:\n   - Load balancer queries service registry\n\
      \   - Client unaware of service locations\n   - Examples: AWS ALB, Kubernetes\
      \ Services\n\n3. Service Registry:\n   - Central database of service instances\n\
      \   - Health checking and monitoring\n   - Examples: Consul, etcd, Zookeeper\n\
      \nService Registration Patterns:\n- Self-registration: Services register themselves\n\
      - Third-party registration: Platform registers services\n- Health check integration\n\
      - Graceful shutdown handling\n\nAPI Gateway Pattern:\nAPI Gateway Responsibilities:\n\
      - Request routing to appropriate services\n- Authentication and authorization\n\
      - Rate limiting and throttling\n- Request/response transformation\n- Protocol\
      \ translation (HTTP to gRPC)\n- Monitoring and analytics\n- Circuit breaking\
      \ and retries\n\nAPI Gateway Benefits:\n- Single entry point for clients\n-\
      \ Cross-cutting concerns centralization\n- Protocol abstraction\n- Service composition\n\
      \nAPI Gateway Challenges:\n- Potential bottleneck\n- Single point of failure\n\
      - Additional complexity\n- Versioning and deployment coordination\n\nService\
      \ Mesh Architecture:\nService Mesh Components:\n1. Data Plane:\n   - Sidecar\
      \ proxies (Envoy, Linkerd)\n   - Handle service-to-service communication\n \
      \  - Traffic management and security\n\n2. Control Plane:\n   - Configuration\
      \ and policy management\n   - Service discovery integration\n   - Security certificate\
      \ management\n\nService Mesh Benefits:\n- Traffic management (load balancing,\
      \ routing)\n- Security (mTLS, authentication)\n- Observability (metrics, tracing,\
      \ logging)\n- Policy enforcement\n- Language-agnostic features\n\nPopular Service\
      \ Mesh Solutions:\n- Istio: Feature-rich, complex setup\n- Linkerd: Lightweight,\
      \ easy to use\n- Consul Connect: HashiCorp ecosystem\n- AWS App Mesh: Managed\
      \ service mesh\n\nResilience Patterns:\n1. Circuit Breaker:\n   - Prevent cascading\
      \ failures\n   - Fast failure when downstream unhealthy\n   - Automatic recovery\
      \ attempts\n   - States: Closed, Open, Half-Open\n\n2. Retry Pattern:\n   -\
      \ Retry failed requests with backoff\n   - Handle transient failures\n   - Avoid\
      \ retry storms\n   - Exponential backoff and jitter\n\n3. Timeout Pattern:\n\
      \   - Set maximum wait time for responses\n   - Prevent resource exhaustion\n\
      \   - Fail fast approach\n   - Different timeouts for different operations\n\
      \n4. Bulkhead Pattern:\n   - Isolate critical resources\n   - Separate thread\
      \ pools or queues\n   - Prevent one failure affecting others\n   - Resource\
      \ isolation\n\n5. Rate Limiting:\n   - Control request rate to services\n  \
      \ - Prevent service overload\n   - Different limits for different clients\n\
      \   - Graceful degradation\n\nMonitoring and Observability:\nThree Pillars of\
      \ Observability:\n1. Metrics:\n   - Business metrics (orders, revenue)\n   -\
      \ Application metrics (latency, throughput)\n   - Infrastructure metrics (CPU,\
      \ memory)\n   - Tools: Prometheus, InfluxDB\n\n2. Logging:\n   - Structured\
      \ logging (JSON format)\n   - Correlation IDs for request tracing\n   - Centralized\
      \ log aggregation\n   - Tools: ELK Stack, Splunk\n\n3. Distributed Tracing:\n\
      \   - Request flow across services\n   - Performance bottleneck identification\n\
      \   - Error propagation tracking\n   - Tools: Jaeger, Zipkin\n\nHealth Monitoring:\n\
      - Health check endpoints (/health)\n- Liveness vs readiness probes\n- Dependency\
      \ health checking\n- Circuit breaker integration\n\nSecurity in Microservices:\n\
      Authentication and Authorization:\n- OAuth 2.0 and OpenID Connect\n- JWT tokens\
      \ for stateless authentication\n- API keys for service-to-service\n- Service\
      \ identity and mTLS\n\nNetwork Security:\n- Private networks and VPCs\n- Service\
      \ mesh security features\n- Zero-trust network model\n- Encryption in transit\
      \ and at rest\n\nSecret Management:\n- Centralized secret storage\n- Secret\
      \ rotation and lifecycle\n- Environment-specific secrets\n- Tools: HashiCorp\
      \ Vault, AWS Secrets Manager\n\nDeployment and DevOps:\nDeployment Patterns:\n\
      1. Blue-Green Deployment:\n   - Two identical production environments\n   -\
      \ Switch traffic between environments\n   - Zero-downtime deployment\n   - Easy\
      \ rollback capability\n\n2. Canary Deployment:\n   - Gradual rollout to subset\
      \ of users\n   - Monitor metrics and rollback if needed\n   - Risk mitigation\
      \ for new releases\n   - A/B testing capabilities\n\n3. Rolling Deployment:\n\
      \   - Update instances one by one\n   - Maintain service availability\n   -\
      \ Slower than blue-green\n   - Resource efficient\n\nContainer Orchestration:\n\
      - Docker containers for packaging\n- Kubernetes for orchestration\n- Service\
      \ discovery and load balancing\n- Auto-scaling and self-healing\n\nCI/CD Pipeline:\n\
      - Independent service pipelines\n- Automated testing and deployment\n- Service\
      \ contract testing\n- Database migration handling\n\nConfiguration Management:\n\
      - Externalized configuration\n- Environment-specific configs\n- Dynamic configuration\
      \ updates\n- Tools: Spring Cloud Config, Consul\n\nPerformance and Scalability:\n\
      Scaling Patterns:\n1. Horizontal Scaling:\n   - Add more service instances\n\
      \   - Load balancing across instances\n   - Stateless service design\n   - Auto-scaling\
      \ based on metrics\n\n2. Vertical Scaling:\n   - Increase resources per instance\n\
      \   - Simpler but limited scalability\n   - Temporary solution\n\n3. Data Partitioning:\n\
      \   - Shard data across services\n   - Service-specific data stores\n   - Geographic\
      \ distribution\n\nCaching Strategies:\n- Service-level caching\n- Distributed\
      \ caching\n- Cache invalidation coordination\n- CDN for static content\n\nMigration\
      \ Strategies:\nMonolith to Microservices Migration:\n1. Strangler Fig Pattern:\n\
      \   - Gradually replace monolith components\n   - Route new features to microservices\n\
      \   - Extract services incrementally\n\n2. Database Decomposition:\n   - Start\
      \ with shared database\n   - Gradually separate data stores\n   - Handle data\
      \ migration carefully\n\n3. Big Bang Migration:\n   - Complete rewrite approach\n\
      \   - High risk but clean architecture\n   - Suitable for small applications\n\
      \nMigration Best Practices:\n- Start with least risky services\n- Maintain backward\
      \ compatibility\n- Monitor and measure progress\n- Team training and skills\
      \ development\n\nTesting Strategies:\nTesting Pyramid for Microservices:\n1.\
      \ Unit Tests:\n   - Test individual service components\n   - Fast feedback and\
      \ high coverage\n   - Mock external dependencies\n\n2. Integration Tests:\n\
      \   - Test service interactions\n   - Database and external service integration\n\
      \   - Test contracts and APIs\n\n3. Contract Tests:\n   - Provider and consumer\
      \ contracts\n   - Prevent breaking changes\n   - Tools: Pact, Spring Cloud Contract\n\
      \n4. End-to-End Tests:\n   - Test complete user journeys\n   - Expensive and\
      \ slow\n   - Focus on critical business flows\n\n5. Chaos Engineering:\n   -\
      \ Test system resilience\n   - Inject failures deliberately\n   - Tools: Chaos\
      \ Monkey, Gremlin\n\nReal-World Implementation Examples:\nE-commerce Microservices:\n\
      - User Service: Authentication and profiles\n- Product Catalog: Product information\
      \ and search\n- Inventory Service: Stock management\n- Order Service: Order\
      \ processing and workflow\n- Payment Service: Payment processing\n- Shipping\
      \ Service: Logistics and tracking\n- Notification Service: Email and SMS\n-\
      \ Analytics Service: Business intelligence\n\nFinancial Services Architecture:\n\
      - Account Service: Account management\n- Transaction Service: Payment processing\n\
      - Risk Service: Fraud detection\n- Compliance Service: Regulatory requirements\n\
      - Reporting Service: Financial reporting\n- Integration Service: External bank\
      \ APIs\n\nCommon Anti-Patterns:\n1. Distributed Monolith:\n   - Services tightly\
      \ coupled\n   - Cannot deploy independently\n   - Shared database and schemas\n\
      \n2. Chatty Interfaces:\n   - Too many service-to-service calls\n   - Performance\
      \ degradation\n   - Network latency issues\n\n3. Shared Database:\n   - Multiple\
      \ services access same database\n   - Tight coupling through data schema\n \
      \  - Difficult to evolve independently\n\n4. God Service:\n   - Service with\
      \ too many responsibilities\n   - Violates single responsibility principle\n\
      \   - Difficult to maintain and scale\n\n5. Data Inconsistency:\n   - Ignoring\
      \ eventual consistency\n   - Poor error handling\n   - Lack of compensation\
      \ mechanisms\n"
    resources:
    - title: Microservices Patterns
      url: https://microservices.io/patterns/microservices.html
      description: Comprehensive catalog of microservice patterns and solutions
    - title: Building Microservices (2nd Edition)
      url: https://samnewman.io/books/building_microservices_2nd_edition/
      description: Authoritative guide to microservices architecture
    - title: Domain-Driven Design
      url: https://domainlanguage.com/ddd/
      description: Eric Evans' foundational work on DDD principles
    - title: Microservices on AWS
      url: https://aws.amazon.com/microservices/
      description: AWS guide to implementing microservices
    - title: Martin Fowler on Microservices
      url: https://martinfowler.com/articles/microservices.html
      description: Foundational article defining microservices architecture
    - title: Service Mesh Comparison
      url: https://servicemesh.es/
      description: Comparison of service mesh technologies
    practice_questions:
      estimation:
      - question: E-commerce system with 1M users needs microservices decomposition.
          Estimate service count and communication patterns.
        answer: 'Services: User (auth, profile), Product (catalog, search), Order
          (checkout, fulfillment), Payment, Inventory, Notification, Analytics = 7
          core services. Communication: Order → Payment (sync, 50ms), Order → Inventory
          (sync, 30ms), Order → Notification (async, event). Traffic: 10K orders/day
          = 116 orders/sec peak × 5 service calls = 580 inter-service calls/sec. Cost:
          $2K/month for infrastructure.'
      - question: Order processing system handles 10K orders/hour. Design service
          boundaries and calculate inter-service call volume.
        answer: 'Order service receives order → calls Inventory (check stock, 20ms)
          → calls Payment (charge, 100ms) → publishes order.completed event → Shipping
          service subscribes (async). Calls/hour: 10K orders × 2 sync calls = 20K
          sync calls + 10K async events. Peak: 3 orders/sec × 2 = 6 sync RPS. Services
          handle 100 RPS each, plenty of headroom.'
      - question: Social media platform with 100M users. Design microservices for
          feed generation with scalability estimates.
        answer: 'Services: Post (create/store, 10K posts/sec), Timeline (fan-out,
          50K updates/sec), Feed (read, 100K reads/sec), User Graph (follows, 1K updates/sec).
          Feed generation: Fan-out on write (popular users) + fan-out on read (normal
          users). Storage: 100M users × 1KB timeline = 100GB Redis. Cost: $5K/month
          for infrastructure.'
      - question: Banking system with 24/7 availability requirement. Design service
          resilience and estimate downtime costs.
        answer: '99.99% uptime = 52 min downtime/year. Multi-region (3 regions), each
          handles 33% traffic, can handle 100% during failover. Circuit breakers:
          5 consecutive failures → open (30s timeout). Downtime cost: $10K/min (transactions
          blocked). Investment: $50K/month infrastructure to prevent $520K annual
          downtime. ROI: 10x.'
      concepts:
      - question: How do you handle distributed transactions in microservices without
          2PC?
        answer: 'Use Saga pattern: Choreography (event-driven, services react to events,
          decentralized) or Orchestration (central coordinator, easier debugging).
          Example: Order saga → Reserve inventory → Charge payment → Confirm order.
          Rollback: Compensating transactions (refund payment, release inventory).
          Store saga state in DB. Timeout: 2min max. Alternative: Event sourcing for
          audit trail.'
      - question: What's the difference between API Gateway and Service Mesh?
        answer: 'API Gateway: North-south traffic (external → internal), routing,
          auth, rate limiting, single entry point, 5-10ms latency. Service Mesh: East-west
          traffic (service → service), mTLS, observability, circuit breakers, distributed
          across sidecars, 1-3ms latency. Use both: Gateway for external APIs, mesh
          for internal. Examples: Kong Gateway + Istio mesh.'
      - question: How do you ensure data consistency across microservices?
        answer: 'Eventual consistency with patterns: 1) Saga for transactions, 2)
          Event sourcing for audit, 3) CQRS (separate read/write models), 4) Idempotency
          for retries, 5) Outbox pattern (publish events reliably). Strong consistency:
          Only within service boundary. Cross-service: Accept 1-5s lag. Monitor: Track
          event delivery, saga completion rates.'
      - question: When would you choose synchronous vs asynchronous communication?
        answer: 'Sync (REST/gRPC): User-facing workflows (<500ms response needed),
          immediate feedback required, simple error handling, low latency (50ms).
          Async (Kafka/RabbitMQ): Long-running operations (>500ms), high throughput
          (>10K/sec), decoupling needed, eventual consistency OK, complex error handling.
          Example: Checkout uses sync (payment), order fulfillment uses async (shipping).'
      - question: How do you handle service discovery in a dynamic environment?
        answer: 'Use service registry (Consul, Eureka) with health checks. Services
          register on startup (IP + port), deregister on shutdown. Clients query registry
          every 30s (cache). Health check: HTTP GET /health every 10s, 3 failures
          → mark unhealthy. Auto-scaling: New instances register automatically. DNS
          as fallback (60s TTL). Kubernetes: Native service discovery via DNS.'
      - question: What are the trade-offs of database per service pattern?
        answer: 'Pros: Independent scaling, technology choice (Postgres + Mongo +
          Redis), clear ownership, failure isolation. Cons: No ACID across services,
          cross-service queries hard (need aggregation service), data duplication
          (eventual consistency), 3x ops complexity. Use when: >5 services, different
          data models needed. Avoid when: Complex transactions, team <10 developers.'
      tradeoffs:
      - question: 'Monolith vs microservices for a startup: pros and cons'
        answer: 'Monolith: Faster initial development (1 codebase), easier debugging,
          ACID transactions, 1 deployment, team <10 works well. Microservices: Complex
          from day 1, distributed debugging, eventual consistency, 10+ deployments,
          need team >20. Startup recommendation: Start monolith, extract services
          when: Scaling bottlenecks, team >20, clear boundaries emerge. Netflix extracted
          500+ services over 5 years.'
      - question: 'Event-driven architecture vs request-response: when to use each'
        answer: 'Event-driven: Loose coupling, 1-N communication (1 event → many consumers),
          eventual consistency, harder debugging, scales to 100K events/sec. Request-response:
          Tight coupling, 1-1 communication, strong consistency, easier debugging,
          scales to 10K RPS. Use events for: Notifications, analytics, audit logs.
          Request-response for: User-facing queries, transactional operations.'
      - question: Service mesh vs API gateway for cross-cutting concerns
        answer: 'Service mesh (Istio): Service-to-service, distributed (sidecar per
          pod), mTLS, observability, 1-3ms overhead, complex setup. API gateway (Kong):
          Client-to-service, centralized, API key/OAuth, rate limiting, 5-10ms overhead,
          simple setup. Use mesh when: >20 services, need mTLS everywhere. Gateway
          when: External APIs, simpler architecture. Can use both.'
      - question: Strong consistency vs eventual consistency in distributed data
        answer: 'Strong: All reads see latest write (CP), 50-100ms latency (consensus),
          limited scale (10K TPS), use for: Money, inventory. Eventual: Reads may
          be stale (AP), <10ms latency, unlimited scale (1M TPS), use for: Social
          feeds, analytics. Middle ground: Session consistency (user sees own writes).
          Cost: Strong consistency = 5x infrastructure for same throughput.'
      - question: Client-side vs server-side service discovery
        answer: 'Client-side: Client queries registry directly (Consul, Eureka), faster
          (1 hop less), client complexity, language-specific libraries. Server-side:
          Load balancer queries registry, simpler client (just HTTP), extra hop (+5ms),
          centralized logic. Use client-side for: Internal services, performance critical.
          Server-side for: External clients, polyglot environments. Kubernetes: Native
          server-side.'
      - question: Choreography vs orchestration for saga pattern
        answer: 'Choreography: Services listen for events, react independently, decentralized,
          scales better (no bottleneck), harder to debug (distributed state). Orchestration:
          Central coordinator, explicit workflow, easier debugging (single state machine),
          coordinator = bottleneck. Use choreography for: Simple workflows (<5 steps),
          high throughput. Orchestration for: Complex workflows (>5 steps), need visibility.'
      scenarios:
      - Design microservices decomposition for a ride-sharing platform
      - Handle payment processing across multiple services with failure scenarios
      - Migrate a monolithic e-commerce system to microservices incrementally
      - Design service communication for real-time chat application
      - Implement distributed logging and tracing across 20+ services
      - Design data synchronization between microservices for inventory management
      - Handle service deployment with zero downtime across interconnected services
      - Design authentication and authorization for microservices architecture
    time_estimate: 120
    video_resources:
    - title: 'Hello Interview: Design Twitter'
      url: https://www.youtube.com/@HelloInterview
      duration: 35 min
      description: Social media system design
      priority: high
    - title: 'ByteByteGo: Design Twitter Timeline'
      url: https://www.youtube.com/@ByteByteGo
      duration: 15 min
      description: Fan-out strategies for timeline
      priority: high
  - day: 11
    topic: Service Discovery & Health Monitoring
    activity: Master comprehensive service discovery mechanisms, health monitoring
      strategies, circuit breaker patterns, and failure detection systems.
    detailed_content: "Service Discovery Fundamentals:\nWhy Service Discovery is Needed:\n\
      - Dynamic service environments: Services start, stop, scale up/down\n- Network\
      \ address changes: IP addresses and ports not static\n- Load balancing requirements:\
      \ Distribute traffic across instances\n- Failure handling: Remove unhealthy\
      \ instances from rotation\n- Service composition: Services need to find dependencies\n\
      - Cross-region deployment: Services in different data centers\n\nService Discovery\
      \ Challenges:\n- Service registry consistency and availability\n- Network partitions\
      \ and split-brain scenarios\n- Service registration and deregistration timing\n\
      - Load balancing strategy integration\n- Health check accuracy and frequency\n\
      - Configuration propagation delays\n\nService Discovery Architectural Patterns:\n\
      1. Client-Side Discovery:\n   How it Works:\n   - Client queries service registry\
      \ directly\n   - Client obtains list of available service instances\n   - Client\
      \ implements load balancing logic\n   - Client handles failure detection and\
      \ retries\n\n   Advantages:\n   - Client has full control over load balancing\n\
      \   - No additional network hop for discovery\n   - Rich load balancing algorithms\
      \ possible\n   - Better performance (cached service locations)\n\n   Disadvantages:\n\
      \   - Service registry coupling in client code\n   - Language-specific client\
      \ libraries required\n   - Complex client implementation\n   - Service registry\
      \ becomes critical dependency\n\n   Examples: Netflix Eureka, Apache Zookeeper\n\
      \n2. Server-Side Discovery:\n   How it Works:\n   - Client makes request to\
      \ load balancer/proxy\n   - Load balancer queries service registry\n   - Load\
      \ balancer routes request to healthy instance\n   - Client unaware of individual\
      \ service instances\n\n   Advantages:\n   - Simpler client implementation\n\
      \   - Language-agnostic approach\n   - Centralized load balancing logic\n  \
      \ - Better security (internal topology hidden)\n\n   Disadvantages:\n   - Additional\
      \ network hop overhead\n   - Load balancer becomes potential bottleneck\n  \
      \ - Limited load balancing algorithm options\n   - Load balancer needs high\
      \ availability\n\n   Examples: AWS ALB, NGINX Plus, HAProxy\n\n3. Service Mesh\
      \ Discovery:\n   How it Works:\n   - Sidecar proxy handles service discovery\n\
      \   - Control plane manages service registry\n   - Data plane (proxies) implement\
      \ load balancing\n   - Transparent to application code\n\n   Advantages:\n \
      \  - Language-agnostic solution\n   - Advanced traffic management features\n\
      \   - Security features (mTLS, authentication)\n   - Observability built-in\n\
      \n   Disadvantages:\n   - Additional infrastructure complexity\n   - Resource\
      \ overhead (sidecar proxies)\n   - Learning curve and operational overhead\n\
      \   - Vendor lock-in potential\n\nService Registry Technologies:\n1. HashiCorp\
      \ Consul:\n   Features:\n   - Distributed service registry with consensus\n\
      \   - Built-in health checking capabilities\n   - DNS interface for service\
      \ discovery\n   - Key-value store for configuration\n   - Multi-datacenter support\
      \ with WAN gossip\n   - Service mesh capabilities (Consul Connect)\n\n   Architecture:\n\
      \   - Raft consensus protocol for consistency\n   - Gossip protocol for failure\
      \ detection\n   - HTTP and DNS APIs for service access\n   - Agent deployment\
      \ on each node\n\n   Use Cases:\n   - Multi-cloud deployments\n   - Traditional\
      \ and containerized workloads\n   - Service mesh implementations\n   - Configuration\
      \ management\n\n2. Netflix Eureka:\n   Features:\n   - REST-based service registry\n\
      \   - Client-side load balancing integration\n   - Self-preservation mode during\
      \ network partitions\n   - Zone-aware service discovery\n   - Integration with\
      \ Netflix OSS stack\n\n   Architecture:\n   - Eventually consistent replication\
      \ model\n   - Client-side heartbeat mechanism\n   - Peer-to-peer replication\
      \ between servers\n   - Lease-based service registration\n\n   Use Cases:\n\
      \   - AWS cloud deployments\n   - Spring Boot applications\n   - Microservices\
      \ in Java ecosystem\n\n3. Apache Zookeeper:\n   Features:\n   - Highly available\
      \ coordination service\n   - Strong consistency guarantees\n   - Hierarchical\
      \ namespace organization\n   - Watch mechanism for configuration changes\n \
      \  - Leader election capabilities\n\n   Architecture:\n   - ZAB (Zookeeper Atomic\
      \ Broadcast) protocol\n   - Ensemble of servers for high availability\n   -\
      \ Sequential consistency model\n   - Session-based client connections\n\n  \
      \ Use Cases:\n   - Configuration management\n   - Distributed coordination\n\
      \   - Service discovery for Kafka, Hadoop\n   - Legacy system integration\n\n\
      4. etcd:\n   Features:\n   - Distributed key-value store\n   - Raft consensus\
      \ algorithm\n   - Watch API for real-time updates\n   - TTL (Time-To-Live) for\
      \ automatic cleanup\n   - Multi-version concurrency control\n\n   Architecture:\n\
      \   - Leader-follower replication model\n   - gRPC API for client communication\n\
      \   - Cluster membership management\n   - Consistent and partition-tolerant\n\
      \n   Use Cases:\n   - Kubernetes cluster coordination\n   - Configuration storage\n\
      \   - Service discovery in container environments\n   - Distributed locking\n\
      \n5. Kubernetes Services:\n   Features:\n   - Native service discovery in Kubernetes\n\
      \   - DNS-based service resolution\n   - Service types (ClusterIP, NodePort,\
      \ LoadBalancer)\n   - Endpoint management and health checking\n   - Integration\
      \ with ingress controllers\n\n   Architecture:\n   - kube-proxy for traffic\
      \ routing\n   - CoreDNS for service name resolution\n   - Endpoint controllers\
      \ for instance tracking\n   - Service mesh integration (Istio, Linkerd)\n\n\
      Service Registration Patterns:\n1. Self-Registration:\n   How it Works:\n  \
      \ - Service instances register themselves\n   - Service sends heartbeats to\
      \ maintain registration\n   - Service deregisters on shutdown\n   - Health checks\
      \ performed by service registry\n\n   Implementation Steps:\n   - Service startup:\
      \ Register with service registry\n   - Periodic heartbeats: Maintain active\
      \ status\n   - Health endpoint: Expose health check endpoint\n   - Graceful\
      \ shutdown: Deregister before stopping\n\n   Advantages:\n   - Simple and direct\
      \ approach\n   - Service controls its own lifecycle\n   - No external dependencies\
      \ for registration\n   - Fast registration and deregistration\n\n   Disadvantages:\n\
      \   - Service registry coupling in application code\n   - Need for registration\
      \ logic in every service\n   - Potential for failed deregistration\n   - Language-specific\
      \ implementation required\n\n2. Third-Party Registration:\n   How it Works:\n\
      \   - External component handles registration\n   - Service deployment platform\
      \ manages lifecycle\n   - Health checks performed by external agent\n   - No\
      \ application code changes required\n\n   Implementation Examples:\n   - Kubernetes:\
      \ Service and endpoint controllers\n   - Docker Swarm: Built-in service discovery\n\
      \   - AWS ECS: Service discovery integration\n   - Consul: Registrator or consul-template\n\
      \n   Advantages:\n   - No application code changes needed\n   - Consistent registration\
      \ across services\n   - Platform handles failure scenarios\n   - Language-agnostic\
      \ approach\n\n   Disadvantages:\n   - Additional infrastructure complexity\n\
      \   - Potential delays in registration updates\n   - Platform-specific implementation\n\
      \   - Less control over registration process\n\nDNS-Based Service Discovery:\n\
      DNS Service Discovery Implementation:\n- SRV records: Service location information\n\
      - A/AAAA records: IP address resolution\n- TXT records: Additional service metadata\n\
      - PTR records: Reverse DNS lookup support\n\nSRV Record Format:\n```\n_service._protocol.domain\
      \ TTL class SRV priority weight port target\n_http._tcp.example.com. 60 IN SRV\
      \ 10 20 8080 server1.example.com.\n```\n\nAdvantages:\n- Universal DNS support\
      \ across platforms\n- No additional client libraries required\n- Built-in caching\
      \ at multiple levels\n- Standard protocol with wide tooling support\n\nDisadvantages:\n\
      - Limited metadata support\n- DNS caching can delay updates\n- No built-in health\
      \ checking\n- Security considerations (DNS spoofing)\n\nHealth Monitoring Strategies:\n\
      Health Check Types and Implementation:\n1. Shallow Health Checks:\n   - Basic\
      \ connectivity and responsiveness\n   - HTTP endpoint returning 200 OK\n   -\
      \ Minimal resource consumption\n   - Fast execution (< 1 second)\n\n   Example\
      \ Implementation:\n   ```\n   GET /health\n   Response: 200 OK\n   Body: {\"\
      status\": \"UP\"}\n   ```\n\n2. Deep Health Checks:\n   - Verify critical dependencies\n\
      \   - Database connectivity checks\n   - External service availability\n   -\
      \ Resource utilization validation\n\n   Example Implementation:\n   ```\n  \
      \ GET /health/detailed\n   Response: 200 OK\n   Body: {\n     \"status\": \"\
      UP\",\n     \"components\": {\n       \"database\": {\"status\": \"UP\", \"\
      responseTime\": \"45ms\"},\n       \"cache\": {\"status\": \"UP\", \"hitRatio\"\
      : 0.85},\n       \"externalAPI\": {\"status\": \"DOWN\", \"error\": \"timeout\"\
      }\n     }\n   }\n   ```\n\n3. Readiness vs Liveness Probes:\n   Liveness Probe:\n\
      \   - Determines if service should be restarted\n   - Detects deadlocks and\
      \ unrecoverable errors\n   - Should not check external dependencies\n   - Failure\
      \ results in service restart\n\n   Readiness Probe:\n   - Determines if service\
      \ can accept traffic\n   - Checks if service is ready to serve requests\n  \
      \ - May include dependency checks\n   - Failure removes from load balancer\n\
      \n4. Custom Health Indicators:\n   - Business-specific health metrics\n   -\
      \ Performance threshold monitoring\n   - Resource availability checks\n   -\
      \ Circuit breaker state monitoring\n\nHealth Check Best Practices:\n- Timeout\
      \ configuration: 5-30 seconds typically\n- Check frequency: Every 10-30 seconds\n\
      - Failure threshold: 3-5 consecutive failures\n- Success threshold: 1-2 consecutive\
      \ successes\n- Graceful degradation: Partial functionality indication\n\nCircuit\
      \ Breaker Pattern Deep Dive:\nCircuit Breaker States:\n1. Closed State:\n  \
      \ - Normal operation mode\n   - All requests passed through\n   - Failure count\
      \ tracking\n   - Transition to Open on threshold breach\n\n2. Open State:\n\
      \   - Fast-fail mode activated\n   - All requests immediately rejected\n   -\
      \ Timeout period before testing recovery\n   - Prevents cascading failures\n\
      \n3. Half-Open State:\n   - Testing recovery mode\n   - Limited number of test\
      \ requests allowed\n   - Success transitions to Closed\n   - Failure returns\
      \ to Open\n\nCircuit Breaker Configuration:\n- Failure threshold: Number of\
      \ failures to open circuit\n- Timeout period: Time to wait before testing recovery\n\
      - Success threshold: Successes needed to close circuit\n- Request timeout: Maximum\
      \ wait time for responses\n- Sliding window: Time period for failure counting\n\
      \nCircuit Breaker Implementation Patterns:\n1. Request-Based Circuit Breaker:\n\
      \   - Tracks success/failure rate over request count\n   - Good for high-traffic\
      \ services\n   - Responsive to sudden failure spikes\n\n2. Time-Based Circuit\
      \ Breaker:\n   - Tracks success/failure rate over time windows\n   - Good for\
      \ varying traffic patterns\n   - Smoother response to intermittent failures\n\
      \n3. Hybrid Circuit Breaker:\n   - Combines request and time-based approaches\n\
      \   - More sophisticated failure detection\n   - Better suited for complex scenarios\n\
      \nAdvanced Health Monitoring:\nDependency Health Aggregation:\n- Service health\
      \ depends on dependency health\n- Weighted dependency importance\n- Partial\
      \ functionality with degraded dependencies\n- Cascade failure prevention strategies\n\
      \nHealth Check Optimization:\n- Parallel dependency checking\n- Cached health\
      \ status with TTL\n- Asynchronous health reporting\n- Health check result aggregation\n\
      \nMonitoring and Alerting:\nKey Metrics to Monitor:\n1. Service Discovery Metrics:\n\
      \   - Service registration/deregistration rate\n   - Service registry query\
      \ latency\n   - Service registry availability\n   - Stale service instance detection\n\
      \n2. Health Check Metrics:\n   - Health check success rate\n   - Health check\
      \ response time\n   - Failed health check reasons\n   - Health state transition\
      \ frequency\n\n3. Circuit Breaker Metrics:\n   - Circuit breaker state changes\n\
      \   - Request success/failure rates\n   - Circuit breaker trigger frequency\n\
      \   - Recovery time measurements\n\nAlerting Strategies:\n- Service unavailability\
      \ alerts\n- High failure rate notifications\n- Service registry partition warnings\n\
      - Circuit breaker state change alerts\n- Health check timeout notifications\n\
      \nFailure Detection and Recovery:\nFailure Modes and Detection:\n1. Service\
      \ Instance Failure:\n   - Process crashes or becomes unresponsive\n   - Health\
      \ check failures indicate problems\n   - Automatic deregistration from service\
      \ registry\n   - Load balancer removes from rotation\n\n2. Network Partition:\n\
      \   - Service registry becomes unreachable\n   - Services continue with cached\
      \ information\n   - Split-brain scenarios in distributed registries\n   - Partition\
      \ tolerance strategies required\n\n3. Service Registry Failure:\n   - Central\
      \ registry becomes unavailable\n   - Client-side caching provides resilience\n\
      \   - Fallback to static configuration\n   - Peer-to-peer discovery mechanisms\n\
      \nRecovery Strategies:\n- Exponential backoff for retry attempts\n- Jitter to\
      \ prevent thundering herd\n- Circuit breaker integration\n- Graceful degradation\
      \ mechanisms\n\nConfiguration Management Integration:\nDynamic Configuration:\n\
      - Feature flags for service behavior control\n- A/B testing configuration management\n\
      - Runtime configuration updates\n- Configuration validation and rollback\n\n\
      Configuration Sources:\n- Environment variables for deployment-specific settings\n\
      - Configuration files for static settings\n- Service registry for dynamic configuration\n\
      - External configuration services (Spring Cloud Config)\n\nReal-World Implementation\
      \ Examples:\nE-commerce Platform Service Discovery:\nServices:\n- Product Catalog\
      \ Service: 10 instances across 3 AZs\n- User Service: 5 instances with database\
      \ dependency\n- Order Service: 8 instances with multiple dependencies\n- Payment\
      \ Service: 3 instances with external API dependency\n\nDiscovery Implementation:\n\
      - Consul for service registry with multi-DC setup\n- Health checks every 15\
      \ seconds with 3-failure threshold\n- Circuit breakers for external payment\
      \ gateway\n- DNS interface for legacy service integration\n\nMicrofinance Application:\n\
      Services:\n- Account Service: Critical with 99.9% availability requirement\n\
      - Transaction Service: High throughput with rate limiting\n- Risk Assessment\
      \ Service: ML-based with variable response times\n- Notification Service: Non-critical\
      \ with graceful degradation\n\nDiscovery Implementation:\n- Kubernetes native\
      \ service discovery\n- Istio service mesh for advanced traffic management\n\
      - Custom health checks for ML model availability\n- Circuit breakers for third-party\
      \ credit checks\n\nCommon Anti-Patterns and Solutions:\n1. Health Check Dependency\
      \ Cascade:\n   Problem: Deep health checks create dependency chains\n   Solution:\
      \ Shallow checks for liveness, deep for readiness\n\n2. Chatty Health Checks:\n\
      \   Problem: Too frequent health checks consume resources\n   Solution: Optimize\
      \ check frequency and implement caching\n\n3. Ignored Circuit Breaker States:\n\
      \   Problem: Applications don't respect circuit breaker decisions\n   Solution:\
      \ Proper integration and fallback mechanisms\n\n4. Static Service Configuration:\n\
      \   Problem: Hardcoded service endpoints resist change\n   Solution: Dynamic\
      \ service discovery with configuration management\n\nTroubleshooting and Debugging:\n\
      Common Issues:\n- Service registration delays causing 404 errors\n- Health check\
      \ false positives during startup\n- Circuit breaker premature opening\n- Service\
      \ registry split-brain scenarios\n- DNS caching preventing service updates\n\
      \nDebugging Tools:\n- Service registry health dashboards\n- Circuit breaker\
      \ state monitoring\n- Health check trace logging\n- Network connectivity testing\n\
      - Configuration validation tools\n"
    resources:
    - title: Service Discovery Patterns
      url: https://microservices.io/patterns/service-registry.html
      description: Comprehensive guide to service discovery implementation patterns
    - title: Circuit Breaker Pattern
      url: https://martinfowler.com/bliki/CircuitBreaker.html
      description: Martin Fowler's explanation of circuit breaker fault tolerance
        pattern
    - title: Consul Service Discovery
      url: https://www.consul.io/docs/discovery
      description: HashiCorp Consul service discovery documentation and best practices
    - title: Kubernetes Service Discovery
      url: https://kubernetes.io/docs/concepts/services-networking/service/
      description: Native Kubernetes service discovery and networking concepts
    - title: Netflix Eureka Guide
      url: https://github.com/Netflix/eureka/wiki
      description: Netflix Eureka service registry implementation guide
    - title: Health Check Patterns
      url: https://microservices.io/patterns/observability/health-check-api.html
      description: Health check API implementation patterns and best practices
    practice_questions:
      estimation:
      - question: E-commerce platform with 50 microservices needs service discovery.
          Estimate registry query load and design capacity planning.
        answer: 'Each service queries registry every 30s: 50 services × 2 QPS (per
          service) = 100 QPS to registry. Health checks: 50 services × 1 check/10s
          = 5 checks/sec. Registry: Consul cluster (3 nodes, CP model), handles 10K
          QPS easily. Storage: 50 services × 1KB metadata = 50KB total. Failover:
          3-node cluster survives 1 node failure. Cost: ~$300/month for registry cluster.'
      - question: Financial system requiring 99.99% availability. Design health monitoring
          strategy with failure detection times.
        answer: 'Health checks: Shallow (every 5s, <10ms response) + deep (every 60s,
          <100ms, checks DB connections). Failure detection: 3 consecutive shallow
          failures = 15s to mark unhealthy. Alert immediately, auto-failover in 30s.
          Circuit breaker: Open after 5 failures, half-open after 30s, close after
          3 successes. Downtime: 52 min/year max. Cost: $10K/month for multi-region
          HA.'
      - question: Global application with 5 regions. Design service discovery architecture
          with cross-region failover.
        answer: 'Per-region registry (Consul cluster), replicate metadata globally
          (async, 1-5s lag). Services query local registry (10ms latency). Cross-region
          failover: If regional registry down, fall back to nearest region (adds 50-200ms
          latency). Traffic: 50 services/region × 5 regions = 250 services total.
          Registry storage: 250 × 1KB = 250KB. Cost: ~$1.5K/month for 5 regional clusters.'
      - question: High-traffic API gateway serving 100K RPS. Design circuit breaker
          configuration and estimate failure scenarios.
        answer: 'Circuit breaker: Per-service circuit, 10% error rate threshold, 5s
          evaluation window, 30s timeout. At 100K RPS: If service fails, circuit opens,
          10K requests/sec to that service fail fast (<1ms) instead of timing out
          (1s), saves 10K connections. Recovery: Half-open tries 10 requests, if successful,
          close circuit. Failure scenario: 5% of requests fail = 5K errors/sec handled
          gracefully.'
      concepts:
      - question: What's the difference between liveness and readiness probes in health
          monitoring?
        answer: 'Liveness: Is service alive? Check process running. If fails: Restart
          container. Use for: Deadlock detection. Readiness: Is service ready to serve
          traffic? Check dependencies (DB, cache). If fails: Remove from load balancer
          (don''t restart). Use for: Gradual rollout, startup checks. Example: Service
          alive but DB down = liveness passes, readiness fails, no traffic sent.'
      - question: How do you handle service discovery during network partitions?
        answer: 'CP registry (Consul): Minority partition becomes unavailable (prefers
          consistency), services cache last known state, continue with stale data
          (degraded mode). AP registry (Eureka): Both partitions accept registrations
          (availability over consistency), may route to unreachable services, eventual
          consistency on heal. Recommendation: CP for critical services (banking),
          AP for resilient services (web apps). Cache TTL: 60s for graceful degradation.'
      - question: When would you choose client-side vs server-side service discovery?
        answer: 'Client-side (Consul, Eureka): Service queries registry, gets IP list,
          load balances client-side. Pros: Faster (no proxy), flexible LB. Cons: Client
          complexity, language libraries needed. Server-side (Kubernetes, AWS ELB):
          DNS lookup or LB. Pros: Simple client (just HTTP), language-agnostic. Cons:
          Extra hop (+5ms), LB = single point. Use client-side for: Performance, internal
          services. Server-side for: External, polyglot.'
      - question: How do circuit breakers prevent cascading failures in microservices?
        answer: 'Service A calls Service B. B slows down (DB overload). Without breaker:
          A waits 1s per call, threads pile up, A becomes unresponsive, cascades to
          callers. With breaker: After 5 failures, circuit opens, A fails fast (<1ms),
          threads freed, A stays healthy. B gets time to recover. Half-open state
          tests recovery. Prevents: Thread exhaustion, resource starvation, cascading
          timeouts.'
      - question: What are the trade-offs between DNS-based and registry-based service
          discovery?
        answer: 'DNS: Simple (no extra infra), works everywhere, but slow updates
          (60s TTL), no health checks, no metadata. Registry (Consul): Real-time updates
          (<1s), health-aware, rich metadata (version, tags), but needs extra infra
          ($300/month), client integration. Use DNS for: Simple, static environments.
          Registry for: Dynamic, cloud-native, frequent changes. Kubernetes: DNS for
          simplicity + registry for advanced features.'
      - question: How do you implement graceful degradation with health monitoring?
        answer: 'Health check returns: Healthy (100%), Degraded (50% capacity, DB
          slow), Unhealthy (0%, DB down). Load balancer adjusts: Send less traffic
          to degraded instances. Service implementation: Degraded mode = disable non-critical
          features (recommendations OFF, core functions ON). Example: E-commerce checkout
          works, recommendations disabled. Monitoring: Alert on degraded, page on
          unhealthy. Prevents: Complete outage, maintains core functionality.'
      tradeoffs:
      - question: 'Consul vs Eureka for service registry: consistency vs availability
          trade-offs'
        answer: 'Consul (CP): Raft consensus, strong consistency, minority partition
          = unavailable, 99.9% availability, financial/critical systems. Eureka (AP):
          Peer replication, eventual consistency (1-30s lag), always available (even
          during partition), 99.99% availability, web applications. Consul: Accurate
          routing, slower writes (50ms). Eureka: Fast writes (10ms), may route to
          dead instances. Use Consul for: Consistency matters. Eureka for: Availability
          matters.'
      - question: 'Shallow vs deep health checks: performance vs accuracy'
        answer: 'Shallow (ping/HTTP 200): 1ms response, every 5s, CPU check only,
          fast but inaccurate (service up but DB down). Deep (check DB/cache): 50-100ms
          response, every 60s, validates dependencies, accurate but expensive (DB
          load). Pattern: Shallow for liveness (restart container), deep for readiness
          (remove from LB). At 50 services: Shallow = 10 checks/sec, Deep = 0.8 checks/sec.
          Use both for comprehensive health.'
      - question: Self-registration vs third-party registration patterns
        answer: 'Self-registration: Service registers itself on startup, simple, needs
          service awareness of registry, tightly coupled. Third-party: Sidecar/agent
          registers service (Kubernetes service mesh), service unaware of registry,
          loosely coupled, extra component. Self: Good for simple apps, direct control.
          Third-party: Good for polyglot, legacy apps, consistent registration. Kubernetes:
          Uses third-party (kubelet registers pods).'
      - question: Circuit breaker vs retry patterns for fault tolerance
        answer: 'Retry: Transient failures (network blip), exponential backoff (1s,
          2s, 4s), max 3 attempts, fixes temporary issues, but amplifies load on struggling
          service. Circuit breaker: Persistent failures (service down), fast fail
          after threshold, gives service time to recover, prevents cascade. Use both:
          Retry for transient errors (5XX with retry-able codes), circuit breaker
          for repeated failures. Together: 3 retries → if keeps failing → open circuit.'
      - question: DNS caching vs real-time service discovery updates
        answer: 'DNS caching: 60s TTL, simple, works everywhere, but stale for 60s
          after change, no health awareness, eventual consistency. Real-time (Consul):
          <1s updates, health-aware, accurate, but needs client integration, extra
          infra. Use DNS for: Stable services (databases, rarely change). Real-time
          for: Dynamic services (auto-scaling, frequent deployments). Hybrid: DNS
          SRV records with short TTL (10s) for compromise.'
      - question: Centralized vs distributed health monitoring approaches
        answer: 'Centralized (single health checker): Simple, consistent view, single
          point of failure, scales to ~1000 services, 50ms latency (network to central).
          Distributed (each LB checks): No SPOF, scales infinitely, independent decisions,
          but inconsistent view (split-brain risk), <5ms latency (local). Use centralized
          for: Small deployments (<100 services), need consistency. Distributed for:
          Large scale (>1000 services), regional deployments.'
      scenarios:
      - Design service discovery for a multi-cloud deployment spanning AWS and Azure
      - Implement health monitoring for a payment service with external bank API dependencies
      - Handle service discovery during a rolling deployment with zero downtime
      - Design circuit breaker strategy for a recommendation service with ML models
      - Create health monitoring for a microservices system with 100+ services
      - Implement service discovery for a hybrid environment with VMs and containers
      - Design failure detection for services with variable response times
      - Handle service discovery in a Kubernetes cluster with auto-scaling
    time_estimate: 100
    video_resources:
    - title: 'Hello Interview: Design YouTube'
      url: https://www.youtube.com/@HelloInterview
      duration: 30 min
      description: Video streaming platform design
      priority: high
    - title: 'ByteByteGo: Video Streaming Architecture'
      url: https://www.youtube.com/@ByteByteGo
      duration: 12 min
      description: Adaptive bitrate streaming
      priority: high
  - day: 12
    topic: Data Partitioning Strategies
    activity: Master comprehensive data partitioning including horizontal/vertical
      sharding, consistent hashing, rebalancing strategies, and cross-shard operations.
    detailed_content: "Data Partitioning Fundamentals:\nWhy Data Partitioning is Needed:\n\
      - Scalability: Single database performance limits\n- Storage capacity: Data\
      \ grows beyond single machine capacity\n- Performance: Distribute load across\
      \ multiple machines\n- Availability: Isolate failures to specific partitions\n\
      - Geographic distribution: Data locality for global applications\n- Cost optimization:\
      \ Use appropriate hardware for different data types\n\nWhen to Consider Partitioning:\n\
      - Database size exceeding single server capacity (>1TB)\n- Query performance\
      \ degrading despite optimization\n- Write throughput hitting database limits\n\
      - Need for geographic data distribution\n- Different data access patterns requiring\
      \ optimization\n- Regulatory requirements for data sovereignty\n\nPartitioning\
      \ vs Scaling Alternatives:\n- Vertical scaling: Increase hardware resources\
      \ (temporary solution)\n- Read replicas: Scale read operations only\n- Caching:\
      \ Reduce database load but doesn't solve storage\n- Partitioning: Scale both\
      \ storage and throughput horizontally\n\nTypes of Data Partitioning:\n1. Horizontal\
      \ Partitioning (Sharding):\n   Definition: Split rows across multiple database\
      \ instances\n\n   How it Works:\n   - Each shard contains subset of total data\n\
      \   - Same schema across all shards\n   - Application routes queries to appropriate\
      \ shard\n   - Data distribution based on partitioning key\n\n   Benefits:\n\
      \   - Linear scalability for both reads and writes\n   - Improved query performance\
      \ (smaller datasets)\n   - Failure isolation (one shard down doesn't affect\
      \ others)\n   - Geographic distribution possible\n\n   Challenges:\n   - Complex\
      \ application logic for shard routing\n   - Cross-shard queries and transactions\n\
      \   - Uneven data distribution (hotspots)\n   - Schema changes across multiple\
      \ shards\n\n2. Vertical Partitioning:\n   Definition: Split columns across different\
      \ systems\n\n   Column-Level Vertical Partitioning:\n   - Frequently accessed\
      \ columns on fast storage\n   - Infrequently accessed columns on cheaper storage\n\
      \   - Large BLOB/text fields separated from main table\n   - Different access\
      \ patterns optimized separately\n\n   Example:\n   ```\n   User table split\
      \ into:\n   - user_core: id, username, email (frequent access)\n   - user_profile:\
      \ bio, preferences, settings (occasional access)\n   - user_media: profile_picture,\
      \ documents (rare access)\n   ```\n\n   Service-Level Vertical Partitioning:\n\
      \   - Different services own different data domains\n   - Microservices architecture\
      \ pattern\n   - Domain-driven design principles\n   - Independent scaling and\
      \ development\n\n   Benefits:\n   - Optimized storage and performance per data\
      \ type\n   - Reduced I/O for frequently accessed data\n   - Better cache utilization\n\
      \   - Technology choice flexibility\n\n   Challenges:\n   - Joins across partitions\
      \ become complex\n   - Application complexity increases\n   - Data consistency\
      \ across partitions\n   - Additional network calls\n\n3. Functional Partitioning:\n\
      \   Definition: Split data by feature or business domain\n\n   Implementation\
      \ Approaches:\n   - Feature-based: Orders, Users, Products in separate databases\n\
      \   - Geographic: US customers, EU customers, APAC customers\n   - Temporal:\
      \ Current data, archived data separation\n   - Tenant-based: Multi-tenant application\
      \ isolation\n\n   Benefits:\n   - Clear ownership and responsibility boundaries\n\
      \   - Independent scaling per domain\n   - Technology choice per domain requirements\n\
      \   - Easier compliance and data governance\n\n   Challenges:\n   - Cross-domain\
      \ operations complexity\n   - Data synchronization requirements\n   - Reporting\
      \ across domains\n   - Maintaining referential integrity\n\nHorizontal Sharding\
      \ Strategies:\n1. Range-Based Sharding:\n   How it Works:\n   - Partition data\
      \ based on value ranges\n   - Example: Users A-M on shard1, N-Z on shard2\n\
      \   - Date ranges: 2023 data on shard1, 2024 on shard2\n\n   Implementation:\n\
      \   ```\n   def get_shard(user_id):\n       if user_id < 1000000:\n        \
      \   return \"shard1\"\n       elif user_id < 2000000:\n           return \"\
      shard2\"\n       else:\n           return \"shard3\"\n   ```\n\n   Advantages:\n\
      \   - Simple to understand and implement\n   - Range queries efficiently handled\n\
      \   - Good for time-series data\n   - Sequential data stays together\n\n   Disadvantages:\n\
      \   - Uneven data distribution possible\n   - Hotspots on active ranges\n  \
      \ - Difficult to rebalance\n   - Requires knowledge of data distribution\n\n\
      2. Hash-Based Sharding:\n   How it Works:\n   - Apply hash function to partitioning\
      \ key\n   - Use hash result to determine shard\n   - Uniform distribution across\
      \ shards\n\n   Implementation:\n   ```\n   def get_shard(user_id, num_shards):\n\
      \       return hash(user_id) % num_shards\n   ```\n\n   Advantages:\n   - Even\
      \ data distribution\n   - Simple implementation\n   - No hotspot issues\n  \
      \ - Works well for unknown data patterns\n\n   Disadvantages:\n   - Range queries\
      \ require checking all shards\n   - Difficult to rebalance (all data reshuffled)\n\
      \   - Loss of data locality\n   - Fixed number of shards\n\n3. Directory-Based\
      \ Sharding:\n   How it Works:\n   - Maintain lookup service for shard locations\n\
      \   - Directory service maps keys to shard locations\n   - Application queries\
      \ directory before data access\n\n   Architecture:\n   ```\n   Client -> Directory\
      \ Service -> Appropriate Shard\n   Directory Service maintains:\n   - Key ranges\
      \ -> Shard mappings\n   - Shard health status\n   - Rebalancing operations\n\
      \   ```\n\n   Advantages:\n   - Flexible sharding strategies\n   - Dynamic rebalancing\
      \ possible\n   - Complex routing logic centralized\n   - Can combine multiple\
      \ strategies\n\n   Disadvantages:\n   - Additional network hop (latency)\n \
      \  - Directory service becomes critical dependency\n   - Consistency challenges\
      \ in directory\n   - More complex architecture\n\n4. Consistent Hashing:\n \
      \  How it Works:\n   - Hash both data keys and shard identifiers\n   - Map both\
      \ to points on a hash ring\n   - Data assigned to next shard clockwise on ring\n\
      \   - Adding/removing shards only affects neighbors\n\n   Virtual Nodes:\n \
      \  - Each physical shard has multiple virtual nodes\n   - Better load distribution\n\
      \   - Reduces impact of shard additions/removals\n\n   Implementation Concepts:\n\
      \   ```\n   Hash Ring: 0 -------- 2^32\n   Virtual Nodes: Each shard gets multiple\
      \ positions\n   Data Placement: Key hashed, assigned to next shard\n   Rebalancing:\
      \ Only adjacent virtual nodes affected\n   ```\n\n   Advantages:\n   - Minimal\
      \ data movement during rebalancing\n   - Excellent for distributed systems\n\
      \   - Handles node failures gracefully\n   - Scales elastically\n\n   Disadvantages:\n\
      \   - Complex implementation\n   - Potential for uneven distribution\n   - Requires\
      \ virtual nodes for balance\n   - Range queries still problematic\n\nSharding\
      \ Key Design:\nCharacteristics of Good Sharding Keys:\n1. High Cardinality:\n\
      \   - Many possible values for even distribution\n   - Avoid low-cardinality\
      \ keys (gender, status)\n   - Ensure sufficient values for future growth\n\n\
      2. Even Distribution:\n   - Avoid skewed access patterns\n   - Consider data\
      \ growth patterns\n   - Monitor and adjust over time\n\n3. Query Pattern Alignment:\n\
      \   - Most queries should target single shard\n   - Avoid frequent cross-shard\
      \ operations\n   - Balance between reads and writes\n\n4. Stability:\n   - Key\
      \ values shouldn't change frequently\n   - Avoid keys that can be updated\n\
      \   - Consider immutable identifiers\n\nCommon Sharding Key Patterns:\n1. User\
      \ ID-Based:\n   - Works well for user-centric applications\n   - Good for social\
      \ media, e-commerce\n   - Potential celebrity user hotspots\n\n2. Tenant ID-Based:\n\
      \   - Perfect for multi-tenant applications\n   - Clear isolation boundaries\n\
      \   - Uneven tenant sizes can cause imbalance\n\n3. Geographic-Based:\n   -\
      \ Good for location-aware applications\n   - Reduces latency for regional users\n\
      \   - Population density affects distribution\n\n4. Time-Based:\n   - Excellent\
      \ for time-series data\n   - Archive old shards easily\n   - Recent data hotspots\
      \ common\n\n5. Hash of Natural Key:\n   - Even distribution guaranteed\n   -\
      \ Lost semantic meaning\n   - Range queries become expensive\n\nAnti-Patterns\
      \ in Sharding Key Selection:\n- Monotonically increasing keys (creates hotspots)\n\
      - Low cardinality keys (uneven distribution)\n- Frequently updated keys (data\
      \ movement)\n- Keys requiring joins across shards\n- Business-critical fields\
      \ that might change\n\nCross-Shard Operations:\nDistributed Queries:\n1. Scatter-Gather\
      \ Pattern:\n   - Send query to all relevant shards\n   - Gather and merge results\n\
      \   - Application-level aggregation\n\n   Implementation Challenges:\n   - Partial\
      \ failures handling\n   - Result set pagination\n   - Performance optimization\n\
      \   - Memory management for large results\n\n2. Two-Phase Queries:\n   - Phase\
      \ 1: Identify relevant shards\n   - Phase 2: Execute targeted queries\n   -\
      \ Optimization for selective queries\n\n3. Aggregation Strategies:\n   - Pre-computed\
      \ aggregates per shard\n   - Real-time aggregation across shards\n   - Approximate\
      \ algorithms (HyperLogLog, Bloom filters)\n\nDistributed Transactions:\n1. Two-Phase\
      \ Commit (2PC):\n   - Coordinator manages commit protocol\n   - All shards must\
      \ agree to commit\n   - Blocking protocol with availability issues\n\n2. Saga\
      \ Pattern:\n   - Sequence of local transactions\n   - Compensating actions for\
      \ rollback\n   - Better availability but eventual consistency\n\n3. Distributed\
      \ Locks:\n   - Coordinate access across shards\n   - Prevent concurrent modifications\n\
      \   - Deadlock detection and resolution\n\nData Rebalancing Strategies:\nWhen\
      \ Rebalancing is Needed:\n- Uneven data distribution detected\n- Shard capacity\
      \ limits reached\n- Performance degradation observed\n- New shards added to\
      \ cluster\n- Failed shards need redistribution\n\nRebalancing Approaches:\n\
      1. Stop-and-Copy:\n   - Stop all writes to affected shards\n   - Copy data to\
      \ new distribution\n   - Update routing configuration\n   - Resume operations\n\
      \n   Advantages: Simple, ensures consistency\n   Disadvantages: Downtime required\n\
      \n2. Live Migration:\n   - Gradually move data while serving requests\n   -\
      \ Double-write during migration period\n   - Switch routing after migration\
      \ complete\n\n   Phases:\n   - Phase 1: Copy existing data\n   - Phase 2: Sync\
      \ incremental changes\n   - Phase 3: Switch traffic routing\n   - Phase 4: Cleanup\
      \ old data\n\n3. Virtual Node Redistribution:\n   - Move virtual nodes between\
      \ physical shards\n   - Gradual rebalancing process\n   - Minimal data movement\
      \ required\n\nRebalancing Best Practices:\n- Monitor shard metrics continuously\n\
      - Automate rebalancing triggers\n- Implement circuit breakers during migration\n\
      - Test rebalancing procedures regularly\n- Plan for rollback scenarios\n\nDatabase-Specific\
      \ Partitioning:\nSQL Database Partitioning:\n1. Built-in Partitioning:\n   -\
      \ PostgreSQL: Table partitioning (range, hash, list)\n   - MySQL: Partition\
      \ by range, hash, key, list\n   - SQL Server: Horizontal, vertical, functional\
      \ partitioning\n\n2. Application-Level Sharding:\n   - Multiple database instances\n\
      \   - Application routing logic\n   - Connection pooling per shard\n\nNoSQL\
      \ Database Partitioning:\n1. MongoDB:\n   - Automatic sharding with shard keys\n\
      \   - Range and hash-based strategies\n   - Balancer for automatic rebalancing\n\
      \n2. Cassandra:\n   - Consistent hashing by default\n   - Partition key determines\
      \ data placement\n   - Virtual nodes for load distribution\n\n3. DynamoDB:\n\
      \   - Automatic partitioning by partition key\n   - Adaptive capacity for hot\
      \ partitions\n   - Global secondary indexes for different access patterns\n\n\
      Monitoring and Observability:\nKey Metrics to Monitor:\n1. Shard-Level Metrics:\n\
      \   - Data size per shard\n   - Query latency per shard\n   - CPU and memory\
      \ utilization\n   - Disk I/O patterns\n\n2. Distribution Metrics:\n   - Data\
      \ distribution skew\n   - Query distribution across shards\n   - Hotspot detection\n\
      \   - Rebalancing progress\n\n3. Cross-Shard Operation Metrics:\n   - Cross-shard\
      \ query frequency\n   - Distributed transaction success rates\n   - Aggregate\
      \ query performance\n   - Data consistency lag\n\nAlerting Strategies:\n- Shard\
      \ capacity thresholds\n- Performance degradation alerts\n- Rebalancing operation\
      \ status\n- Cross-shard operation failures\n\nReal-World Implementation Examples:\n\
      Instagram Photo Storage Sharding:\n- Sharding key: Photo ID (time-based + shard\
      \ ID)\n- Strategy: Custom ID generation ensuring even distribution\n- Challenge:\
      \ Handling celebrity accounts with high activity\n- Solution: Additional hash\
      \ layer for viral content\n\nUber Trip Data Partitioning:\n- Primary sharding:\
      \ Geographic (city-based)\n- Secondary partitioning: Time-based for historical\
      \ data\n- Cross-shard operations: Global analytics and ML models\n- Real-time\
      \ routing: GPS-based shard selection\n\nSlack Message Storage:\n- Sharding key:\
      \ Team ID\n- Strategy: Hash-based distribution\n- Challenge: Large teams creating\
      \ hotspots\n- Solution: Sub-sharding for large teams by channel\n\nNetflix Viewing\
      \ History:\n- Sharding key: User ID\n- Strategy: Consistent hashing with virtual\
      \ nodes\n- Time-based archival: Old data moved to different storage\n- Cross-shard\
      \ analytics: Recommendation engine queries\n\nCommon Pitfalls and Solutions:\n\
      1. Celebrity Problem:\n   Problem: Popular entities create hotspots\n   Solutions:\n\
      \   - Additional hash layer for viral content\n   - Caching layer for popular\
      \ data\n   - Read replicas for high-traffic shards\n\n2. Thundering Herd During\
      \ Rebalancing:\n   Problem: All clients hit same shard during migration\n  \
      \ Solutions:\n   - Gradual traffic shifting\n   - Circuit breakers and rate\
      \ limiting\n   - Client-side backoff and retry\n\n3. Cross-Shard Join Explosion:\n\
      \   Problem: Complex queries require multiple shard hits\n   Solutions:\n  \
      \ - Denormalization and data duplication\n   - Separate analytical database\n\
      \   - Pre-computed result caching\n\n4. Shard Key Changes:\n   Problem: Business\
      \ requirements change sharding strategy\n   Solutions:\n   - Design for multiple\
      \ sharding strategies\n   - Gradual migration to new key\n   - Hybrid routing\
      \ during transition\n\nAdvanced Partitioning Patterns:\n1. Multi-Level Partitioning:\n\
      \   - Geographic -> Tenant -> Time-based\n   - Each level optimizes for different\
      \ access patterns\n   - Increased complexity but better performance\n\n2. Partition\
      \ Pruning:\n   - Query optimizer eliminates irrelevant shards\n   - Metadata-driven\
      \ shard selection\n   - Significant performance improvements\n\n3. Dynamic Repartitioning:\n\
      \   - Automatic shard splitting based on load\n   - Machine learning for partition\
      \ prediction\n   - Self-healing partition strategies\n\n4. Federated Partitioning:\n\
      \   - Different data types use different strategies\n   - Optimized partitioning\
      \ per data access pattern\n   - Complex but highly optimized approach\n"
    resources:
    - title: Database Sharding Explained
      url: https://aws.amazon.com/what-is/database-sharding/
      description: AWS comprehensive guide to database sharding strategies
    - title: Consistent Hashing
      url: https://en.wikipedia.org/wiki/Consistent_hashing
      description: Detailed explanation of consistent hashing algorithm
    - title: PostgreSQL Partitioning
      url: https://www.postgresql.org/docs/current/ddl-partitioning.html
      description: PostgreSQL built-in partitioning features and best practices
    - title: MongoDB Sharding
      url: https://docs.mongodb.com/manual/sharding/
      description: MongoDB automatic sharding implementation guide
    - title: Cassandra Data Modeling
      url: https://cassandra.apache.org/doc/latest/data_modeling/
      description: Cassandra partitioning and data modeling best practices
    - title: Sharding Pinterest
      url: https://medium.com/pinterest-engineering/sharding-pinterest-how-we-scaled-our-mysql-fleet-3f341e96ca6f
      description: Pinterest's real-world MySQL sharding experience
    practice_questions:
      estimation:
      - question: Social media platform with 1B users needs data partitioning. Design
          sharding strategy and estimate shard count.
        answer: 'Shard by user_id (hash-based). Data per user: 1KB profile + 10KB
          posts = 11KB. Total: 1B × 11KB = 11TB. Shard size: 100GB per shard = 110
          shards. Use consistent hashing with 1000 virtual nodes per server to enable
          smooth rebalancing. Servers: 55 physical servers (2 shards each). Cross-shard
          queries: User graph stored in separate graph DB. Cost: $20K/month for storage
          + compute.'
      - question: E-commerce database growing 100GB/month. When to shard and estimate
          timeline for implementation?
        answer: 'Shard when: 1) Single DB >1TB (query performance degrades), 2) Write
          throughput >10K TPS, 3) Read replicas insufficient. Current: 100GB/month
          = 1TB in 10 months. Sharding timeline: 3 months planning + 2 months migration
          = 5 months total. Start at 6-month mark (600GB). Strategy: Shard by customer_id,
          10 shards initially, room to scale to 100 shards. Cost: $5K migration +
          $2K/month extra operational.'
      - question: Global chat application needs geographic partitioning. Design strategy
          for 5 regions with data sovereignty.
        answer: 'Partition by region (US, EU, APAC, LATAM, Africa). Users assigned
          to region on signup (immutable). Cross-region chats: Replicate messages
          to both regions (eventual consistency, 50-200ms lag). Storage per region:
          200M users × 5KB = 1TB. Compliance: EU data stays in EU (GDPR). Latency:
          20ms within region, 200ms cross-region. Cost: $1K/month per region = $5K
          total.'
      - question: Time-series database storing 1M events/second. Design partitioning
          for efficient queries and archival.
        answer: 'Partition by time (daily partitions): 1M events/sec × 86,400s × 100B
          = 8.6TB/day. Query pattern: Recent data (last 7 days) = hot, old data =
          cold. Hot partitions: SSD, 7 × 8.6TB = 60TB. Cold: S3, compressed 10:1 =
          300TB/year → 30TB storage. Query: WHERE time > ''2025-10-03'' (partition
          pruning, scans 1 day = 8.6TB instead of full dataset). Cost: $5K/month hot
          + $1K/month cold storage.'
      concepts:
      - question: When would you choose range-based over hash-based sharding?
        answer: 'Range (by date, ID ranges): Enables range queries (WHERE date BETWEEN),
          sequential access, but prone to hotspots (current month hot). Use for: Time-series,
          audit logs, incremental IDs. Hash: Uniform distribution, prevents hotspots,
          but no range queries (must query all shards). Use for: User data, random
          access patterns. Hybrid: Hash for users, range for their time-series data
          (orders by date within user shard).'
      - question: How do you handle cross-shard transactions in a partitioned system?
        answer: 'Avoid cross-shard transactions (design around them). If unavoidable:
          1) Two-phase commit (2PC): Slow (100-200ms), blocks resources, use rarely.
          2) Saga pattern: Eventual consistency, compensating transactions, 50ms.
          3) Denormalize: Duplicate data to avoid cross-shard joins. Example: Order
          service stores customer name (denormalized) to avoid joining customer shard.
          90% of transactions should be single-shard.'
      - question: What are the trade-offs between consistent hashing and directory-based
          sharding?
        answer: 'Consistent hashing: Automatic rebalancing (add/remove nodes), minimal
          data movement (1/N keys), no central directory, but complex implementation,
          not range-query friendly. Directory: Central mapping (key → shard), simple,
          supports range queries, but manual rebalancing, directory = SPOF (mitigate
          with replication). Use consistent for: Dynamic environments (cloud, auto-scaling).
          Directory for: Static, range queries important.'
      - question: How do you detect and resolve hotspots in a sharded database?
        answer: 'Detection: Monitor per-shard metrics (QPS, latency, CPU). Hotspot:
          One shard >2x average load. Causes: Celebrity user (millions of followers),
          seasonal data (current month hot), poor shard key. Resolution: 1) Split
          hot shard (if range-based), 2) Add read replicas for hot shard, 3) Cache
          hot data (Redis), 4) Rebalance with better key (add salt to celebrity IDs).
          Monitor every 5min, auto-alert if >80% capacity.'
      - question: What factors determine good vs bad sharding keys?
        answer: 'Good key: 1) High cardinality (millions of values, not just 10 countries),
          2) Uniform distribution (no hotspots), 3) Stable (doesn''t change), 4) Query-aligned
          (most queries filter by this key). Examples: user_id, order_id. Bad key:
          1) Low cardinality (country = 200 values = uneven shards), 2) Temporal (created_date
          = current month hot), 3) Mutable (status changes). Worst: Boolean (active/inactive
          = only 2 shards).'
      - question: How do you implement zero-downtime data rebalancing?
        answer: 'Dual-write approach: 1) Write to both old and new shards, 2) Background
          copy existing data (batch 1000 rows/sec), 3) Verify consistency, 4) Switch
          reads to new shard, 5) Stop writes to old shard, 6) Drop old shard. Duration:
          1TB at 1GB/hour = 1000 hours = 6 weeks. Use consistent hashing to minimize
          data movement (only affected keys). Monitor: Replication lag, query latency,
          error rates. Rollback plan: Keep old shard for 1 week.'
      tradeoffs:
      - question: Horizontal vs vertical partitioning for analytics workloads
        answer: 'Horizontal (row-based): Split by user_id, scales writes, enables
          parallel queries, but cross-shard aggregations expensive. Vertical (column-based):
          Split by column groups (user profile vs user activity), reduces I/O for
          columnar queries, optimizes for specific query patterns, but complex joins.
          Analytics: Use vertical (columnar storage like Redshift), 10x faster scans.
          OLTP: Use horizontal, scales transactions. Hybrid: Horizontal for users
          + vertical for analytics (separate OLAP DB).'
      - question: Single vs multi-level partitioning strategies
        answer: 'Single (by user_id): Simple, one routing decision, but limited to
          one dimension. Multi-level (by region then user_id): Supports multiple query
          patterns (by region, by user), more flexible, but 2x routing complexity,
          harder rebalancing. Use single for: Uniform access patterns. Multi-level
          for: Geographic data (region/country/user), hierarchical data (tenant/user/document).
          Cost: Multi-level = 2x operational complexity.'
      - question: Automatic vs manual sharding rebalancing
        answer: 'Automatic (Cassandra, MongoDB): Detects imbalance, triggers rebalance,
          hands-off, but unpredictable timing (may happen during peak), less control,
          potential disruption. Manual: Planned during low-traffic (3 AM), controlled,
          tested, but requires monitoring and intervention. Use automatic for: Dev/test,
          self-service teams (<10 people). Manual for: Production, critical systems,
          enterprise (>100M users). Automatic risks: Rebalancing during Black Friday.'
      - question: Application-level vs database-level partitioning
        answer: 'Application-level: App routes queries to correct shard, full control,
          works with any DB, but complexity in app code, must update all clients.
          Database-level (Citus, Vitess): DB handles routing, transparent to app,
          simpler app code, but DB lock-in, less flexibility. Use app-level for: Polyglot,
          custom logic, multiple DB types. DB-level for: Single DB product, simpler
          operations, prefer managed solutions.'
      - question: Strong vs eventual consistency in cross-shard operations
        answer: 'Strong (2PC): Cross-shard transactions ACID, 100-200ms latency, locks
          held across shards, limited to 1K TPS. Eventual (saga): 50ms latency, scales
          to 100K TPS, but temporary inconsistencies (1-5s), complex error handling.
          Use strong for: Money, inventory (can''t have inconsistency). Eventual for:
          Social features, analytics, denormalized data. Cost: Strong = 5x infrastructure
          for same throughput.'
      - question: Complex sharding vs read replicas for scaling reads
        answer: 'Read replicas: Simple (1 master + N replicas), scales reads to 10x,
          1-5s replication lag, SPOF = master write bottleneck. Sharding: Complex
          (10-100 shards), scales both reads and writes to 100x, no replication lag
          (eventual per shard), no SPOF, but operational complexity. Use replicas
          when: Read:write = 10:1, <1TB data, <10K writes/sec. Shard when: Write-heavy,
          >1TB, need horizontal write scaling.'
      scenarios:
      - Design partitioning strategy for a multi-tenant SaaS with varying tenant sizes
      - Handle data rebalancing for a gaming platform with seasonal traffic spikes
      - Implement cross-shard analytics for a financial trading platform
      - Design sharding for a content management system with global distribution
      - Handle partition key changes for an evolving e-commerce platform
      - Implement efficient cross-shard search for a social media platform
      - Design temporal partitioning for regulatory compliance in healthcare
      - Handle celebrity user problem in a social networking application
    time_estimate: 110
    video_resources:
    - title: 'ByteByteGo: Design Web Crawler'
      url: https://www.youtube.com/@ByteByteGo
      duration: 18 min
      description: Scalable web crawling system
      priority: high
    - title: 'Hello Interview: Design Google Search'
      url: https://www.youtube.com/@HelloInterview
      duration: 25 min
      description: Search engine architecture
      priority: medium
  - day: 13
    topic: Database Replication
    activity: Master comprehensive database replication including replication topologies,
      consistency models, conflict resolution strategies, and failover mechanisms.
    detailed_content: "Database Replication Fundamentals:\nWhy Database Replication\
      \ is Needed:\n- High availability: Survive individual server failures\n- Read\
      \ scalability: Distribute read load across replicas\n- Disaster recovery: Geographically\
      \ distributed backups\n- Data locality: Reduce latency for global applications\n\
      - Offline access: Local copies for disconnected operations\n- Load distribution:\
      \ Separate read and write workloads\n\nWhen to Implement Replication:\n- Single\
      \ point of failure concerns\n- Read traffic exceeding single server capacity\n\
      - Geographic distribution requirements\n- Business continuity and disaster recovery\
      \ needs\n- Regulatory compliance for data availability\n- Development/testing\
      \ environment isolation\n\nReplication vs Other Scaling Solutions:\n- Vertical\
      \ scaling: Limited by hardware constraints\n- Horizontal partitioning: Complex\
      \ for non-partitionable data\n- Caching: Temporary solution, doesn't solve availability\n\
      - Replication: Provides both availability and read scalability\n\nDatabase Replication\
      \ Topologies:\n1. Master-Slave (Primary-Replica) Replication:\n   Architecture:\n\
      \   - Single master node handles all writes\n   - Multiple read-only slave nodes\
      \ receive updates\n   - Slaves replicate master's transaction log\n   - Read\
      \ requests distributed across slaves\n\n   Data Flow:\n   ```\n   Write Request\
      \ -> Master -> Slave 1\n                          -> Slave 2\n             \
      \             -> Slave N\n   Read Request -> Any Slave (load balanced)\n   ```\n\
      \n   Advantages:\n   - Simple to understand and implement\n   - Strong consistency\
      \ for writes (single source)\n   - Read scalability through multiple replicas\n\
      \   - Clear separation of read and write workloads\n\n   Disadvantages:\n  \
      \ - Master is single point of failure for writes\n   - Write scalability limited\
      \ to master capacity\n   - Replica lag can cause read inconsistency\n   - Manual\
      \ intervention required for master failure\n\n   Use Cases:\n   - Read-heavy\
      \ applications (blogs, news sites)\n   - Reporting and analytics workloads\n\
      \   - Development/testing environment separation\n   - Geographic read distribution\n\
      \n2. Master-Master (Multi-Master) Replication:\n   Architecture:\n   - Multiple\
      \ nodes accept write operations\n   - Bidirectional replication between masters\n\
      \   - Each master maintains full copy of data\n   - Conflict resolution mechanisms\
      \ required\n\n   Data Flow:\n   ```\n   Master 1 <-> Master 2\n   Master 1 <->\
      \ Master 3\n   Master 2 <-> Master 3\n   (Full mesh or ring topology)\n   ```\n\
      \n   Advantages:\n   - No single point of failure for writes\n   - Write scalability\
      \ across multiple nodes\n   - Geographic write distribution possible\n   - Better\
      \ availability during node failures\n\n   Disadvantages:\n   - Complex conflict\
      \ resolution required\n   - Potential for inconsistent data states\n   - Higher\
      \ network overhead\n   - More difficult to debug and monitor\n\n   Use Cases:\n\
      \   - Geographically distributed applications\n   - High-availability write\
      \ requirements\n   - Collaborative editing systems\n   - Multi-region deployments\n\
      \n3. Cascade Replication:\n   Architecture:\n   - Master replicates to primary\
      \ slaves\n   - Primary slaves replicate to secondary slaves\n   - Hierarchical\
      \ replication topology\n   - Reduces load on master for many replicas\n\n  \
      \ Benefits:\n   - Reduced network load on master\n   - Scalable for large numbers\
      \ of replicas\n   - Geographic distribution optimization\n   - Bandwidth usage\
      \ optimization\n\n   Challenges:\n   - Increased replication lag for deeper\
      \ levels\n   - Complexity in failure handling\n   - Dependency chains create\
      \ fragility\n\n4. Ring Replication:\n   Architecture:\n   - Nodes arranged in\
      \ circular topology\n   - Each node replicates to next node in ring\n   - Eventually\
      \ consistent across all nodes\n   - Good for distributed systems\n\n   Benefits:\n\
      \   - Balanced load distribution\n   - Fault tolerance (multiple paths)\n  \
      \ - Scalable architecture\n   - Geographic distribution support\n\n   Challenges:\n\
      \   - Longer convergence time\n   - Complex failure detection\n   - Potential\
      \ for split rings\n\nReplication Methods and Consistency:\n1. Synchronous Replication:\n\
      \   How it Works:\n   - Master waits for acknowledgment from all replicas\n\
      \   - Transaction commits only after replica confirmation\n   - Strong consistency\
      \ guarantee\n   - Higher latency due to network coordination\n\n   Implementation:\n\
      \   ```\n   1. Client sends write to master\n   2. Master forwards to all replicas\n\
      \   3. Replicas acknowledge write completion\n   4. Master commits transaction\n\
      \   5. Master responds to client\n   ```\n\n   Advantages:\n   - Strong consistency\
      \ across all nodes\n   - No data loss during failures\n   - Immediate consistency\
      \ for read operations\n   - Simplified application logic\n\n   Disadvantages:\n\
      \   - Higher write latency\n   - Reduced availability (all nodes must be up)\n\
      \   - Network partition sensitivity\n   - Slower performance under load\n\n\
      \   Use Cases:\n   - Financial systems requiring accuracy\n   - Critical business\
      \ data\n   - Compliance-sensitive applications\n   - Small numbers of replicas\n\
      \n2. Asynchronous Replication:\n   How it Works:\n   - Master commits locally\
      \ without waiting for replicas\n   - Replication happens in background\n   -\
      \ Better performance but eventual consistency\n   - Potential for data loss\
      \ during failures\n\n   Implementation:\n   ```\n   1. Client sends write to\
      \ master\n   2. Master commits transaction immediately\n   3. Master responds\
      \ to client\n   4. Master asynchronously replicates to slaves\n   ```\n\n  \
      \ Advantages:\n   - Low write latency\n   - High availability (master independent)\n\
      \   - Better performance under load\n   - Network partition tolerance\n\n  \
      \ Disadvantages:\n   - Eventual consistency only\n   - Potential data loss during\
      \ master failure\n   - Replica lag complexity\n   - Read-after-write consistency\
      \ issues\n\n   Use Cases:\n   - High-performance applications\n   - Social media\
      \ platforms\n   - Content management systems\n   - Non-critical data scenarios\n\
      \n3. Semi-Synchronous Replication:\n   How it Works:\n   - Configurable number\
      \ of replicas must acknowledge\n   - Hybrid approach balancing consistency and\
      \ performance\n   - Timeout mechanisms for unresponsive replicas\n   - Flexible\
      \ consistency guarantees\n\n   Configuration Options:\n   - Minimum replica\
      \ acknowledgments required\n   - Timeout for replica responses\n   - Fallback\
      \ to asynchronous on timeout\n   - Priority-based replica selection\n\n   Advantages:\n\
      \   - Balanced consistency and performance\n   - Configurable based on requirements\n\
      \   - Better availability than full synchronous\n   - Reduced data loss risk\n\
      \n   Disadvantages:\n   - More complex configuration\n   - Potential for partial\
      \ consistency\n   - Monitoring complexity\n   - Tuning requirements\n\nReplication\
      \ Lag and Monitoring:\nCauses of Replication Lag:\n- Network latency between\
      \ nodes\n- High write volume on master\n- Slow replica hardware or configuration\n\
      - Long-running transactions\n- Lock contention on replicas\n\nMeasuring Replication\
      \ Lag:\n- Log sequence number (LSN) differences\n- Timestamp-based lag calculation\n\
      - Row count verification\n- Checksum comparison\n\nHandling Replication Lag:\n\
      - Read preference configuration\n- Lag-aware load balancing\n- Master fallback\
      \ for critical reads\n- Cached results for tolerance\n\nConflict Resolution\
      \ Strategies:\n1. Last-Write-Wins (LWW):\n   How it Works:\n   - Use timestamp\
      \ to determine latest write\n   - Simple but may lose concurrent updates\n \
      \  - Clock synchronization critical\n   - Often combined with vector clocks\n\
      \n   Advantages:\n   - Simple to implement and understand\n   - Deterministic\
      \ resolution\n   - No human intervention required\n   - Scales well\n\n   Disadvantages:\n\
      \   - Data loss for concurrent writes\n   - Clock synchronization dependency\n\
      \   - May not reflect business intent\n   - Poor for collaborative scenarios\n\
      \n2. Vector Clocks:\n   How it Works:\n   - Track causality between updates\n\
      \   - Each node maintains version vector\n   - Detect concurrent vs sequential\
      \ updates\n   - Preserve all concurrent versions\n\n   Implementation:\n   ```\n\
      \   Vector Clock: [Node1: 3, Node2: 1, Node3: 2]\n   Update increments local\
      \ node counter\n   Merge vectors on conflict detection\n   ```\n\n   Advantages:\n\
      \   - Preserves causality information\n   - No data loss for concurrent updates\n\
      \   - Distributed system friendly\n   - Language/platform agnostic\n\n   Disadvantages:\n\
      \   - Complex implementation\n   - Storage overhead grows\n   - Requires conflict\
      \ resolution UI\n   - Vector size management needed\n\n3. Operational Transformation\
      \ (OT):\n   How it Works:\n   - Transform operations based on context\n   -\
      \ Resolve conflicts at operation level\n   - Maintain operation semantics\n\
      \   - Common in collaborative editing\n\n   Example:\n   ```\n   Initial: \"\
      Hello\"\n   Op1: Insert \"X\" at position 2 -> \"HeXllo\"\n   Op2: Insert \"\
      Y\" at position 3 -> \"HelYlo\"\n   Transform: Adjust positions for both operations\n\
      \   ```\n\n   Advantages:\n   - Preserves user intent\n   - Real-time collaboration\
      \ support\n   - Automatic conflict resolution\n   - User-friendly resolution\n\
      \n   Disadvantages:\n   - Complex algorithm implementation\n   - Operation-specific\
      \ logic required\n   - Performance overhead\n   - Limited to certain data types\n\
      \n4. Custom Business Logic Resolution:\n   How it Works:\n   - Application-specific\
      \ conflict resolution\n   - Business rules determine winner\n   - May require\
      \ human intervention\n   - Context-aware resolution strategies\n\n   Examples:\n\
      \   - Financial: Always prefer larger amount\n   - CRM: Merge customer information\n\
      \   - Inventory: Sum quantities from all sources\n   - User preferences: Most\
      \ recent wins\n\n   Advantages:\n   - Business logic alignment\n   - Context-aware\
      \ decisions\n   - Flexible resolution strategies\n   - Optimal user experience\n\
      \n   Disadvantages:\n   - Complex implementation\n   - Requires domain expertise\n\
      \   - May need manual intervention\n   - Testing complexity\n\nFailover Strategies\
      \ and High Availability:\nAutomatic Failover:\nComponents:\n- Health monitoring\
      \ and detection\n- Replica promotion algorithms\n- Client connection redirection\n\
      - Data consistency verification\n\nHealth Monitoring:\n- Heartbeat mechanisms\n\
      - Query response monitoring\n- Resource utilization tracking\n- Network connectivity\
      \ checks\n\nPromotion Process:\n```\n1. Detect master failure\n2. Select best\
      \ replica candidate\n3. Promote replica to master\n4. Update DNS/load balancer\
      \ configuration\n5. Redirect write traffic\n6. Monitor new master health\n```\n\
      \nReplica Selection Criteria:\n- Most up-to-date data (lowest lag)\n- Best hardware\
      \ resources\n- Network proximity to clients\n- Administrative preferences\n\n\
      Manual Failover:\nWhen Manual is Preferred:\n- Critical business decisions\n\
      - Data integrity concerns\n- Complex conflict situations\n- Planned maintenance\
      \ scenarios\n\nManual Process:\n- Administrator verification\n- Data consistency\
      \ checks\n- Controlled traffic redirection\n- Rollback preparation\n\nSplit-Brain\
      \ Prevention:\nProblem: Multiple nodes believe they are master\nSolutions:\n\
      - Quorum-based decisions\n- Witness nodes for tie-breaking\n- Fencing mechanisms\n\
      - Coordinator-based systems\n\nQuorum Implementation:\n```\nNodes: 3 total\n\
      Quorum: 2 nodes minimum\nNetwork partition: [Node1] vs [Node2, Node3]\nResult:\
      \ Node2-Node3 partition becomes active master\n```\n\nDatabase-Specific Replication:\n\
      MySQL Replication:\n- Binary log-based replication\n- Statement vs row-based\
      \ formats\n- GTID (Global Transaction ID) support\n- Multi-source replication\
      \ capabilities\n\nPostgreSQL Replication:\n- WAL (Write-Ahead Log) streaming\n\
      - Hot standby replicas\n- Logical replication for partial data\n- Built-in failover\
      \ mechanisms\n\nMongoDB Replica Sets:\n- Automatic failover with elections\n\
      - Oplog-based replication\n- Read preference configuration\n- Arbiter nodes\
      \ for odd numbers\n\nRedis Replication:\n- Master-slave with Sentinel\n- Redis\
      \ Cluster for sharding\n- Pub/sub replication patterns\n- Memory-optimized replication\n\
      \nCloud Database Replication:\nAWS RDS:\n- Read replicas across regions\n- Multi-AZ\
      \ deployments\n- Automated backup and recovery\n- Cross-region disaster recovery\n\
      \nGoogle Cloud SQL:\n- Regional persistent disks\n- Read replicas and high availability\n\
      - Point-in-time recovery\n- Cross-region replication\n\nAzure Database:\n- Geo-replication\
      \ capabilities\n- Active-passive configurations\n- Automatic failover groups\n\
      - Zone redundancy options\n\nMonitoring and Observability:\nKey Metrics to Monitor:\n\
      1. Replication Health:\n   - Replication lag measurements\n   - Replica connectivity\
      \ status\n   - Error rates and retry counts\n   - Throughput and latency metrics\n\
      \n2. Data Consistency:\n   - Checksum verification results\n   - Row count comparisons\n\
      \   - Timestamp drift measurements\n   - Conflict resolution frequency\n\n3.\
      \ Performance Metrics:\n   - Query response times per replica\n   - Resource\
      \ utilization (CPU, memory, disk)\n   - Network bandwidth usage\n   - Connection\
      \ pool status\n\nAlerting Strategies:\n- Replica lag threshold violations\n\
      - Failed replica connections\n- Master-replica data divergence\n- Failover event\
      \ notifications\n- Performance degradation alerts\n\nReal-World Implementation\
      \ Examples:\nNetflix Database Replication:\n- Global master-slave topology\n\
      - Cross-region read replicas\n- Automatic failover with Hystrix\n- Custom lag\
      \ monitoring and alerting\n\nInstagram Photo Metadata:\n- Master-slave with\
      \ geographic distribution\n- Async replication for performance\n- Custom sharding\
      \ with replication\n- Cassandra multi-datacenter setup\n\nAirbnb Booking System:\n\
      - Multi-master for global writes\n- Conflict resolution for concurrent bookings\n\
      - Regional read replicas for performance\n- Custom failover orchestration\n\n\
      WhatsApp Message Storage:\n- Master-slave with real-time replication\n- Custom\
      \ conflict resolution for message ordering\n- Erlang-based cluster management\n\
      - Geographic message distribution\n\nBest Practices and Common Pitfalls:\nReplication\
      \ Best Practices:\n- Monitor replication lag continuously\n- Test failover procedures\
      \ regularly\n- Plan for network partition scenarios\n- Implement proper backup\
      \ strategies\n- Use connection pooling efficiently\n\nCommon Pitfalls:\n1. Ignoring\
      \ Replication Lag:\n   Problem: Stale data served to users\n   Solution: Lag-aware\
      \ routing and monitoring\n\n2. Inadequate Failover Testing:\n   Problem: Failures\
      \ during actual outages\n   Solution: Regular disaster recovery drills\n\n3.\
      \ Poor Conflict Resolution:\n   Problem: Data corruption or loss\n   Solution:\
      \ Comprehensive conflict testing\n\n4. Insufficient Monitoring:\n   Problem:\
      \ Undetected replication issues\n   Solution: Comprehensive metrics and alerting\n\
      \n5. Over-Synchronization:\n   Problem: Performance degradation\n   Solution:\
      \ Balance consistency needs with performance\n\nAdvanced Replication Patterns:\n\
      1. Multi-Tier Replication:\n   - Production -> Staging -> Development\n   -\
      \ Data masking at each tier\n   - Different lag tolerances\n   - Security boundary\
      \ enforcement\n\n2. Selective Replication:\n   - Replicate only specific tables/columns\n\
      \   - Filter based on business rules\n   - Reduce bandwidth and storage\n  \
      \ - Privacy and compliance benefits\n\n3. Bidirectional Replication:\n   - Two-way\
      \ sync between regions\n   - Complex conflict resolution\n   - Active-active\
      \ configurations\n   - Geographic load distribution\n\n4. Event-Driven Replication:\n\
      \   - Trigger-based replication\n   - Business event sourcing\n   - Real-time\
      \ analytics feeds\n   - Audit trail maintenance\n\nSecurity Considerations:\n\
      Replication Security:\n- Encrypted connections between nodes\n- Certificate-based\
      \ authentication\n- Role-based access control\n- Data masking for non-production\
      \ replicas\n\nCompliance and Governance:\n- Data residency requirements\n- GDPR\
      \ compliance for EU data\n- Audit logging for data access\n- Retention policy\
      \ enforcement\n"
    resources:
    - title: Database Replication Patterns
      url: https://en.wikipedia.org/wiki/Replication_(database)
      description: Comprehensive overview of database replication concepts and patterns
    - title: MySQL Replication Guide
      url: https://dev.mysql.com/doc/refman/8.0/en/replication.html
      description: MySQL replication implementation and best practices
    - title: PostgreSQL High Availability
      url: https://www.postgresql.org/docs/current/high-availability.html
      description: PostgreSQL replication and failover strategies
    - title: MongoDB Replica Sets
      url: https://docs.mongodb.com/manual/replication/
      description: MongoDB replica set configuration and management
    - title: Redis Replication
      url: https://redis.io/topics/replication
      description: Redis master-slave replication and high availability
    - title: Conflict-Free Replicated Data Types
      url: https://crdt.tech/
      description: Advanced conflict resolution techniques for distributed systems
    practice_questions:
      estimation:
      - question: E-commerce platform needs 99.9% availability across 3 regions. Design
          replication strategy and estimate RTO/RPO.
        answer: 'Use master-slave with async replication per region + cross-region
          async replication. RTO: 1-2 minutes (auto-failover), RPO: 5-10 seconds (replication
          lag). Deploy master + 2 replicas per region (9 nodes total). Cost: ~$5K/month
          for 3 regions with monitoring.'
      - question: Social media app with 100M users requires read scalability. How
          many read replicas needed for 10K RPS?
        answer: 'Assume 80% reads, 20% writes. Read load: 8K RPS. If each replica
          handles 1K RPS, need 8 read replicas + 1 master. With safety margin (2x),
          deploy 16 read replicas across regions. Cost: ~$2K/month.'
      - question: Financial system needs zero data loss replication. Design synchronous
          replication architecture.
        answer: Use synchronous replication with 2-phase commit. Master + 2 sync replicas
          (quorum=2). Write latency increases ~50ms. Use nearby data centers (<10ms
          network latency). Requires strong failover mechanism with fencing to prevent
          split-brain.
      - question: Global chat application requires <100ms replication lag. Design
          network topology and estimate costs.
        answer: 'Deploy master-slave in each region (US, EU, APAC). Use dedicated
          network links (AWS Direct Connect / GCP Interconnect). Async replication
          between regions with binlog streaming. Network cost: ~$1K/month per region
          link. Monitor lag with alerts at 50ms threshold.'
      concepts:
      - question: When would you choose synchronous vs asynchronous replication?
        answer: 'Synchronous: When data loss is unacceptable (financial transactions,
          critical data). Trade-off: Higher latency, reduced availability. Asynchronous:
          When performance matters more than consistency (social media, analytics).
          Trade-off: Possible data loss, better performance.'
      - question: How do you handle the split-brain problem in master-master replication?
        answer: Use consensus algorithm (Raft/Paxos) or quorum-based writes. Implement
          fencing (STONITH) to isolate failed master. Use external coordination service
          (ZooKeeper, etcd) for leader election. Prevent writes to minority partition.
      - question: What are the trade-offs between statement-based and row-based replication?
        answer: 'Statement-based: Compact, efficient for bulk operations. Issues:
          Non-deterministic functions (NOW(), RAND()), trigger differences. Row-based:
          Deterministic, exact data replication. Issues: Large binary logs for bulk
          updates, more network bandwidth.'
      - question: How do you implement conflict resolution for concurrent writes?
        answer: 'Strategies: 1) Last-write-wins (timestamp-based, simple but loses
          data), 2) Vector clocks (track causality, complex), 3) Application-level
          merge (business logic), 4) CRDT (conflict-free data types). Choose based
          on data semantics and consistency requirements.'
      - question: What factors affect replication lag and how do you minimize it?
        answer: 'Factors: Network latency, write volume, replica load, large transactions.
          Solutions: Parallel replication threads, dedicated network links, partition
          data, batching, SSD storage, monitor and alert on lag spikes. Use read-your-writes
          pattern for consistency.'
      - question: How do you ensure data consistency during failover scenarios?
        answer: 1) Wait for replica to catch up before promoting (increases downtime),
          2) Use semi-sync replication (at least one replica in sync), 3) Implement
          fencing to prevent old master from accepting writes, 4) Use GTID (Global
          Transaction ID) to track replication position accurately.
      tradeoffs:
      - question: Master-slave vs master-master replication for global applications
        answer: 'Master-slave: Simpler, consistent writes, single source of truth.
          Cons: Write bottleneck, higher latency for distant users. Master-master:
          Local writes, better availability, lower latency. Cons: Conflict resolution
          complexity, eventual consistency. Choose master-master for write-heavy global
          apps.'
      - question: 'Synchronous vs asynchronous replication: consistency vs performance'
        answer: 'Synchronous: Strong consistency, no data loss, higher latency (~2x),
          reduced availability (all replicas must respond). Asynchronous: Better performance,
          higher availability, eventual consistency, possible data loss during failures.
          Use semi-sync as middle ground.'
      - question: Read replicas vs caching for read scalability
        answer: 'Read replicas: Fresh data (seconds lag), complex queries supported,
          SQL interface. Higher cost per query. Caching: Very fast (<1ms), cheaper,
          stale data, limited query complexity. Use both: Cache for hot data, replicas
          for complex queries.'
      - question: Automatic vs manual failover strategies
        answer: 'Automatic: Fast recovery (1-2 min), reduces downtime, risk of split-brain
          if misconfigured. Manual: Safer, human verification, slower (10-30 min),
          requires on-call. Use automatic with careful monitoring and fencing. Manual
          for critical financial systems.'
      - question: Last-write-wins vs vector clocks for conflict resolution
        answer: 'LWW: Simple, works with timestamps, loses concurrent writes. Use
          for: Non-critical data, shopping carts. Vector clocks: Preserves causality,
          detects conflicts, complex to implement. Use for: Collaborative editing,
          distributed databases (Riak, Cassandra).'
      - question: Single-master vs multi-master for high availability
        answer: 'Single-master: Simple, consistent, write bottleneck, master failure
          stops writes. Multi-master: No single point of failure, geographic distribution,
          complex conflict resolution. Choose multi-master when: Write availability
          > consistency, geographic distribution required.'
      scenarios:
      - Design replication strategy for a banking system with regulatory compliance
      - Handle failover for an e-commerce platform during Black Friday traffic
      - Implement cross-region replication for a social media platform
      - Design conflict resolution for a collaborative document editing system
      - Handle replication lag in a real-time analytics dashboard
      - Implement disaster recovery for a mission-critical healthcare system
      - Design replication for a multi-tenant SaaS with data isolation requirements
      - Handle master failure during peak traffic with zero downtime
    time_estimate: 105
    video_resources:
    - title: 'ByteByteGo: Design Notification System'
      url: https://www.youtube.com/@ByteByteGo
      duration: 15 min
      description: Push, SMS, email notifications at scale
      priority: high
  - day: 14
    topic: Week 2 Integration - Chat System Design
    activity: Design a comprehensive real-time chat system integrating microservices
      architecture, message queues, API design, service discovery, data partitioning,
      and replication strategies.
    detailed_content: "System Requirements Analysis:\nFunctional Requirements:\n-\
      \ 1-on-1 messaging: Real-time text message exchange\n- Group messaging: Support\
      \ up to 256 members per group\n- Online presence: Real-time user status (online/offline/away)\n\
      - Message history: Persistent storage and retrieval\n- Message delivery status:\
      \ Sent, delivered, read receipts\n- File sharing: Images, documents, and media\
      \ files\n- Push notifications: Offline message delivery\n- Search functionality:\
      \ Message and user search\n- User profiles: Basic user information management\n\
      \nNon-Functional Requirements:\n- Scale: 500M daily active users (DAU)\n- Volume:\
      \ 100B messages per day\n- Latency: <100ms message delivery\n- Availability:\
      \ 99.99% uptime (52 minutes downtime/year)\n- Consistency: Strong consistency\
      \ for message ordering\n- Security: End-to-end encryption support\n- Global:\
      \ Multi-region deployment support\n\nCapacity Estimation:\nMessage Volume Calculations:\n\
      - 500M DAU × 40 messages/user/day = 20B messages/day\n- Peak traffic: 3x average\
      \ = 60B messages/day\n- Messages per second: 60B / (24 × 3600) = ~694K messages/second\n\
      - Peak messages per second: 2M messages/second\n\nStorage Requirements:\n- Average\
      \ message size: 100 bytes (text) + metadata\n- Daily storage: 20B × 100 bytes\
      \ = 2TB/day\n- Annual storage: 2TB × 365 = 730TB/year\n- With replication (3x):\
      \ 2.2PB/year\n\nNetwork Bandwidth:\n- Per message: 100 bytes\n- Peak bandwidth:\
      \ 2M messages/second × 100 bytes = 200MB/second\n- With protocol overhead: ~400MB/second\n\
      \nWebSocket Connections:\n- Concurrent users: 500M DAU × 10% = 50M concurrent\n\
      - Connections per server: 10K connections\n- Servers needed: 50M / 10K = 5,000\
      \ WebSocket servers\n\nHigh-Level Architecture:\nClient Applications:\n- Mobile\
      \ apps (iOS, Android): Primary user interface\n- Web applications: Browser-based\
      \ access\n- Desktop applications: Native desktop clients\n\nAPI Gateway Layer:\n\
      - Load balancing across microservices\n- Authentication and authorization\n\
      - Rate limiting and throttling\n- Protocol translation (HTTP/WebSocket)\n- Request\
      \ routing and service discovery\n\nMicroservices Architecture:\nService Decomposition\
      \ Strategy:\n1. User Service:\n   - User registration and authentication\n \
      \  - Profile management\n   - Contact list management\n   - User preferences\
      \ and settings\n\n2. Chat Service:\n   - Message processing and routing\n  \
      \ - Chat room management\n   - Message persistence\n   - Message ordering and\
      \ deduplication\n\n3. Presence Service:\n   - Online/offline status tracking\n\
      \   - Last seen timestamps\n   - Activity status updates\n   - Real-time presence\
      \ broadcasting\n\n4. Notification Service:\n   - Push notification delivery\n\
      \   - Offline message queuing\n   - Notification preferences\n   - Multi-platform\
      \ notification support\n\n5. Media Service:\n   - File upload and download\n\
      \   - Image and video processing\n   - Content moderation\n   - CDN integration\n\
      \n6. Search Service:\n   - Message content indexing\n   - User and group search\n\
      \   - Full-text search capabilities\n   - Search result ranking\n\n7. Analytics\
      \ Service:\n   - Usage metrics collection\n   - Performance monitoring\n   -\
      \ Business intelligence\n   - Real-time dashboards\n\nReal-Time Communication\
      \ Design:\nWebSocket vs Alternatives Comparison:\nWebSocket Advantages:\n- Full-duplex\
      \ communication\n- Low latency (<50ms)\n- Efficient for high-frequency messaging\n\
      - Native browser support\n- Reduced server overhead\n\nHTTP Long Polling:\n\
      - Simpler implementation\n- Better for low-frequency updates\n- Higher latency\
      \ (100-500ms)\n- More server resources required\n\nServer-Sent Events (SSE):\n\
      - One-way communication only\n- Good for notifications\n- Limited browser support\n\
      - Not suitable for chat applications\n\nSelected Approach: WebSocket with fallback\
      \ to long polling\n\nWebSocket Connection Management:\nConnection Pool Architecture:\n\
      ```\nClient -> Load Balancer -> WebSocket Gateway\n                        ->\
      \ Connection Manager\n                        -> Message Router\n```\n\nConnection\
      \ Lifecycle:\n1. Authentication: JWT token validation\n2. Connection establishment:\
      \ WebSocket handshake\n3. Session management: User session tracking\n4. Heartbeat:\
      \ Keep-alive mechanism\n5. Graceful disconnect: Cleanup resources\n\nConnection\
      \ Distribution:\n- Consistent hashing for user assignment\n- Session affinity\
      \ for WebSocket connections\n- Connection failover handling\n- Load balancing\
      \ across WebSocket servers\n\nMessage Flow Architecture:\nMessage Processing\
      \ Pipeline:\n1. Message Ingestion:\n   - Client sends message via WebSocket\n\
      \   - API Gateway validates and authenticates\n   - Message queued for processing\n\
      \n2. Message Processing:\n   - Content validation and filtering\n   - Spam and\
      \ abuse detection\n   - Message encryption\n   - Metadata enrichment\n\n3. Message\
      \ Routing:\n   - Determine recipients (1-on-1 or group)\n   - Fan-out for group\
      \ messages\n   - Real-time delivery via WebSocket\n   - Offline user handling\n\
      \n4. Message Persistence:\n   - Store in partitioned message database\n   -\
      \ Update delivery status\n   - Index for search functionality\n\nMessage Queue\
      \ Integration:\nApache Kafka Implementation:\n- Topic per chat type (1-on-1,\
      \ group)\n- Partitioning by chat_id for ordering\n- Consumer groups for horizontal\
      \ scaling\n- Message retention for replay capability\n\nQueue Architecture:\n\
      ```\nMessage Producer -> Kafka Topic -> Consumer Groups\n                  \
      \            -> WebSocket Delivery\n                              -> Database\
      \ Persistence\n                              -> Push Notifications\n```\n\n\
      Message Ordering Guarantees:\n- Per-chat ordering using Kafka partitions\n-\
      \ Vector clocks for conflict resolution\n- Idempotency tokens for deduplication\n\
      - Sequence numbers for message ordering\n\nData Storage and Partitioning:\n\
      Database Partitioning Strategy:\n1. User Data Partitioning:\n   - Partition\
      \ key: hash(user_id)\n   - Consistent hashing for distribution\n   - User profile,\
      \ contacts, preferences\n   - Cross-partition joins minimized\n\n2. Message\
      \ Data Partitioning:\n   - Primary partition: hash(chat_id)\n   - Secondary\
      \ partition: timestamp ranges\n   - Hot data in SSD, cold data in HDD\n   -\
      \ Time-based archival strategy\n\n3. Presence Data:\n   - Redis cluster for\
      \ real-time data\n   - TTL-based expiration\n   - Geographic distribution\n\
      \   - Backup to persistent storage\n\nDatabase Schema Design:\nMessages Table\
      \ Partitioning:\n```sql\nCREATE TABLE messages (\n    message_id UUID PRIMARY\
      \ KEY,\n    chat_id VARCHAR(64) NOT NULL,\n    sender_id VARCHAR(64) NOT NULL,\n\
      \    content TEXT,\n    message_type ENUM('text', 'image', 'file'),\n    timestamp\
      \ TIMESTAMP NOT NULL,\n    delivery_status ENUM('sent', 'delivered', 'read')\n\
      ) PARTITION BY HASH(chat_id);\n```\n\nChat Metadata Table:\n```sql\nCREATE TABLE\
      \ chats (\n    chat_id VARCHAR(64) PRIMARY KEY,\n    chat_type ENUM('direct',\
      \ 'group'),\n    participants JSON,\n    created_at TIMESTAMP,\n    updated_at\
      \ TIMESTAMP\n) PARTITION BY HASH(chat_id);\n```\n\nDatabase Replication Strategy:\n\
      Master-Slave Replication:\n- Write operations to master nodes\n- Read operations\
      \ distributed across replicas\n- Geographic distribution for latency\n- Automatic\
      \ failover for high availability\n\nReplication Configuration:\n- Synchronous\
      \ replication for critical data\n- Asynchronous replication for analytics\n\
      - Cross-region replication for disaster recovery\n- Read preference routing\
      \ by geography\n\nService Discovery and Health Monitoring:\nService Registry\
      \ Implementation:\n- Consul for service discovery\n- Health checks every 10\
      \ seconds\n- Automatic service deregistration\n- DNS-based service resolution\n\
      \nHealth Monitoring Strategy:\n- Service health endpoints (/health)\n- Dependency\
      \ health aggregation\n- Circuit breakers for external services\n- Real-time\
      \ alerting for failures\n\nCircuit Breaker Configuration:\n- Failure threshold:\
      \ 5 consecutive failures\n- Timeout period: 30 seconds\n- Success threshold:\
      \ 3 successful calls\n- Fallback mechanisms for degraded service\n\nAPI Design\
      \ and Versioning:\nRESTful API Design:\n```\nPOST /api/v1/chats/{chat_id}/messages\n\
      GET /api/v1/chats/{chat_id}/messages?page=1&limit=50\nPUT /api/v1/users/{user_id}/presence\n\
      GET /api/v1/users/{user_id}/profile\n```\n\nWebSocket API:\n```json\n{\n  \"\
      type\": \"message\",\n  \"payload\": {\n    \"chat_id\": \"chat_123\",\n   \
      \ \"content\": \"Hello World\",\n    \"timestamp\": \"2024-01-01T12:00:00Z\"\
      \n  }\n}\n```\n\nAPI Versioning Strategy:\n- URL path versioning: /api/v1/,\
      \ /api/v2/\n- Backward compatibility maintenance\n- Deprecation timeline communication\n\
      - Client SDK version management\n\nGroup Chat Scalability:\nFan-Out Strategies:\n\
      1. Push Model (Fan-Out on Write):\n   - Message copied to each recipient's queue\n\
      \   - Fast read operations\n   - High write overhead for large groups\n   -\
      \ Storage duplication\n\n2. Pull Model (Fan-Out on Read):\n   - Single message\
      \ storage\n   - Recipients fetch on demand\n   - Efficient storage usage\n \
      \  - Slower read operations\n\nHybrid Approach:\n- Push model for small groups\
      \ (<50 members)\n- Pull model for large groups (>50 members)\n- Configurable\
      \ threshold based on activity\n- Dynamic switching based on group size\n\nLarge\
      \ Group Optimization:\n- Message sampling for inactive users\n- Lazy loading\
      \ for message history\n- Compression for bulk operations\n- Hierarchical fan-out\
      \ for massive groups\n\nOnline Presence System:\nPresence State Management:\n\
      States: online, away, busy, offline\nImplementation:\n- Redis for real-time\
      \ presence data\n- TTL-based automatic offline detection\n- Heartbeat mechanism\
      \ for online users\n- Last seen timestamp tracking\n\nPresence Broadcasting:\n\
      - Contact list filtering\n- Presence change notifications\n- Batch updates for\
      \ efficiency\n- Rate limiting for presence updates\n\nPresence Data Structure:\n\
      ```json\n{\n  \"user_id\": \"user_123\",\n  \"status\": \"online\",\n  \"last_seen\"\
      : \"2024-01-01T12:00:00Z\",\n  \"device_type\": \"mobile\",\n  \"location\"\
      : \"us-east-1\"\n}\n```\n\nMessage Delivery Guarantees:\nDelivery Status Tracking:\n\
      1. Sent: Message stored in sender's outbox\n2. Delivered: Message reached recipient's\
      \ device\n3. Read: Message opened by recipient\n\nImplementation Strategy:\n\
      - Acknowledgment system for each stage\n- Retry mechanism for failed deliveries\n\
      - Exponential backoff for retries\n- Dead letter queues for failed messages\n\
      \nPush Notification System:\nOffline Message Handling:\n- Message queuing for\
      \ offline users\n- Push notification triggers\n- Notification batching and summarization\n\
      - Platform-specific notification formatting\n\nNotification Service Architecture:\n\
      ```\nMessage Queue -> Notification Service -> FCM/APNS\n                   \
      \                  -> Email/SMS\n                                     -> In-app\
      \ Notifications\n```\n\nSecurity and Privacy:\nEnd-to-End Encryption:\n- Signal\
      \ Protocol implementation\n- Key exchange mechanisms\n- Forward secrecy guarantees\n\
      - Message content encryption\n\nAuthentication and Authorization:\n- JWT token-based\
      \ authentication\n- OAuth 2.0 for third-party integration\n- Role-based access\
      \ control\n- Session management and timeout\n\nData Privacy:\n- GDPR compliance\
      \ for EU users\n- Data retention policies\n- User data deletion capabilities\n\
      - Audit logging for compliance\n\nMonitoring and Observability:\nKey Metrics:\n\
      1. Performance Metrics:\n   - Message delivery latency (P95, P99)\n   - WebSocket\
      \ connection stability\n   - API response times\n   - Database query performance\n\
      \n2. Business Metrics:\n   - Daily/monthly active users\n   - Message volume\
      \ and growth\n   - User retention rates\n   - Feature adoption rates\n\n3. System\
      \ Health:\n   - Service availability\n   - Error rates by service\n   - Resource\
      \ utilization\n   - Circuit breaker state\n\nAlerting Strategy:\n- Critical:\
      \ Service downtime, data loss\n- Warning: High latency, error rate spikes\n\
      - Info: Capacity thresholds, deployment events\n\nScaling and Performance Optimization:\n\
      Horizontal Scaling Strategies:\n- Auto-scaling based on connection count\n-\
      \ Load balancing across regions\n- Database read replica scaling\n- CDN for\
      \ static content delivery\n\nCaching Strategy:\n- Redis for frequently accessed\
      \ data\n- Application-level caching\n- CDN caching for media files\n- Browser\
      \ caching for static assets\n\nPerformance Optimizations:\n- Message compression\
      \ for large groups\n- Lazy loading for chat history\n- Connection pooling for\
      \ databases\n- Batch operations for efficiency\n\nDisaster Recovery and Backup:\n\
      Backup Strategy:\n- Daily incremental backups\n- Weekly full backups\n- Cross-region\
      \ backup replication\n- Point-in-time recovery capability\n\nDisaster Recovery:\n\
      - Multi-region active-passive setup\n- Automated failover procedures\n- RTO:\
      \ 5 minutes, RPO: 1 minute\n- Regular disaster recovery testing\n\nCost Optimization:\n\
      Infrastructure Cost Management:\n- Reserved instances for predictable workloads\n\
      - Spot instances for batch processing\n- Storage tiering for message archives\n\
      - Cost monitoring and alerting\n\nReal-World Implementation Examples:\nWhatsApp\
      \ Architecture Insights:\n- Erlang for high concurrency\n- FreeBSD for network\
      \ optimization\n- XMPP protocol customization\n- Message routing optimization\n\
      \nDiscord Scalability:\n- Elixir for fault tolerance\n- Cassandra for message\
      \ storage\n- Redis for presence and caching\n- Custom voice chat infrastructure\n\
      \nSlack's Approach:\n- MySQL for message storage\n- Redis for real-time features\n\
      - Elasticsearch for search\n- WebSocket connection management\n\nDeployment\
      \ and DevOps:\nContainerization:\n- Docker containers for all services\n- Kubernetes\
      \ for orchestration\n- Helm charts for deployment\n- Service mesh for communication\n\
      \nCI/CD Pipeline:\n- Automated testing for all services\n- Blue-green deployment\
      \ strategy\n- Feature flags for gradual rollouts\n- Automated rollback procedures\n\
      \nTesting Strategy:\n1. Unit Testing:\n   - Service-level testing\n   - Mock\
      \ external dependencies\n   - Test coverage >90%\n\n2. Integration Testing:\n\
      \   - API contract testing\n   - Database integration tests\n   - End-to-end\
      \ user flows\n\n3. Load Testing:\n   - WebSocket connection testing\n   - Message\
      \ throughput testing\n   - Database performance testing\n   - Chaos engineering\
      \ for resilience\n\nCommon Challenges and Solutions:\n1. Message Ordering:\n\
      \   Challenge: Ensuring global message ordering\n   Solution: Kafka partitioning\
      \ + vector clocks\n\n2. Connection Management:\n   Challenge: Handling millions\
      \ of WebSocket connections\n   Solution: Connection pooling + horizontal scaling\n\
      \n3. Group Chat Performance:\n   Challenge: Fan-out performance for large groups\n\
      \   Solution: Hybrid push/pull model\n\n4. Presence Accuracy:\n   Challenge:\
      \ Accurate online/offline detection\n   Solution: Heartbeat + TTL + graceful\
      \ disconnect\n\n5. Search Scalability:\n   Challenge: Searching across billions\
      \ of messages\n   Solution: Elasticsearch sharding + caching\n\nImplementation\
      \ Checklist:\nPhase 1 - MVP (Months 1-3):\n- [ ] Basic 1-on-1 messaging\n- [\
      \ ] User authentication and profiles\n- [ ] Real-time message delivery\n- [\
      \ ] Basic mobile and web clients\n\nPhase 2 - Growth (Months 4-6):\n- [ ] Group\
      \ messaging functionality\n- [ ] File and image sharing\n- [ ] Push notifications\n\
      - [ ] Message search capabilities\n\nPhase 3 - Scale (Months 7-12):\n- [ ] Multi-region\
      \ deployment\n- [ ] Advanced security features\n- [ ] Analytics and monitoring\n\
      - [ ] Performance optimizations\n\nDesign Trade-offs Summary:\n1. Consistency\
      \ vs Availability:\n   - Chose eventual consistency for better availability\n\
      \   - Strong consistency for message ordering within chats\n\n2. Storage vs\
      \ Performance:\n   - Duplicated data for faster reads\n   - SSD for hot data,\
      \ HDD for archives\n\n3. Complexity vs Scalability:\n   - Microservices for\
      \ independent scaling\n   - Increased operational complexity\n\n4. Cost vs Performance:\n\
      \   - Premium instances for real-time services\n   - Cost optimization for batch\
      \ processing\n"
    resources:
    - title: 'ByteByteGo: Design WhatsApp'
      url: https://www.youtube.com/@ByteByteGo
      description: Visual breakdown of real-time messaging architecture with WebSockets
        and Kafka
    - title: 'ByteByteGo: WebSocket vs Long Polling'
      url: https://www.youtube.com/@ByteByteGo
      description: Comparison of push notification strategies for chat systems
    - title: 'Hello Interview: Design a Chat System'
      url: https://www.hellointerview.com/practice/overview
      description: Interactive practice with FAANG-level expectations for chat system
        design
    - title: WhatsApp System Design Analysis
      url: https://www.educative.io/courses/grokking-system-design-fundamentals/whatsapp
      description: Comprehensive analysis of WhatsApp's architecture and scaling strategies
    - title: Discord Engineering Blog
      url: https://discord.com/category/engineering
      description: Real-world insights into Discord's chat system architecture
    - title: Apache Kafka for Messaging
      url: https://kafka.apache.org/documentation/
      description: Best practices for WebSocket implementation and scaling
    - title: Signal Protocol Documentation
      url: https://signal.org/docs/
      description: End-to-end encryption implementation for secure messaging
    practice_questions:
      estimation:
      - question: Chat system needs to handle 1B messages/day. Calculate required
          Kafka partitions and consumer groups.
        answer: 'Messages: 1B/day = 11.6K msg/sec avg, 58K peak (5x). Kafka throughput:
          1KB/msg × 58K = 58MB/sec. Partitions: 60 partitions (each handles 1K msg/sec),
          enables 60 parallel consumers. Consumer groups: 3 groups (persistence, notification,
          analytics) × 60 consumers = 180 total. Replication factor 3 = 174MB/sec
          write. Storage: 1B × 1KB × 30 days = 30TB. Cost: $3K/month for 10-node Kafka
          cluster.'
      - question: Real-time chat with 100M concurrent users. Estimate WebSocket server
          requirements and connection distribution.
        answer: 'Connections: 100M users. Per server: 10K connections (memory limit).
          Servers needed: 100M / 10K = 10,000 WebSocket servers. Each server: 8GB
          RAM (80KB/connection) + 2 vCPU. Distribution: Use consistent hashing by
          user_id. Load balancer: 100 L7 load balancers (1M connections each). Bandwidth:
          100B/msg × 10 msg/min × 100M = 1.67GB/sec = 13.3Gbps. Cost: $300K/month
          for infrastructure.'
      - question: Group chat with 10K members needs message delivery. Compare fan-out
          strategies and calculate costs.
        answer: 'Write fan-out: Write 10K copies per message to user inboxes. Cost:
          10K writes × $0.001 = $10 per message. Read latency: 10ms (direct read).
          Read fan-out: Write 1 copy to group feed. Cost: 1 write × $0.001 = $0.001
          per message. Read: Each user reads from group feed. Read latency: 50ms (need
          to filter). Hybrid: Write fan-out for <100 members, read fan-out for >100.
          Balance: Cost vs latency. For 10K members: Use read fan-out (1000x cheaper),
          cache recent messages.'
      - question: Global chat application across 5 regions. Design network architecture
          and estimate cross-region latency.
        answer: 'Regions: US-East, US-West, EU, Asia, Brazil. Architecture: Active-active,
          each region has full stack (WebSocket, Kafka, DB). Cross-region: Message
          replication via Kafka MirrorMaker 2.0. Latency: US-East to EU = 80ms, US-East
          to Asia = 200ms, Asia to Brazil = 250ms. Strategy: Send to nearest region
          (5ms), async replicate globally. User sees message in 5ms, other regions
          in 50-250ms. Cost: 5 regions × $100K/month = $500K. Bandwidth: 100GB/day
          cross-region = $3K/month.'
      concepts:
      - question: How do you ensure message ordering in a distributed chat system?
        answer: 'Use Kafka partitioning by conversation_id: All messages in same conversation
          go to same partition = total order. Within partition: Kafka guarantees order.
          Problem: Multiple consumers. Solution: Assign partition to single consumer.
          For 1:1 chat: Use hash(min(user1_id, user2_id)) for consistent partition.
          For group: Use group_id. Alternative: Lamport timestamps + vector clocks
          for causal ordering. Cost: Single consumer per partition = 60 consumers
          for 60 partitions.'
      - question: What are the trade-offs between push and pull models for group chat?
        answer: 'Push (WebSocket): Pros: Real-time (5ms), no polling. Cons: Stateful
          connections (10K/server), expensive at scale ($300K/month). Use for: 1:1
          chat, small groups (<100). Pull (HTTP polling): Pros: Stateless, cheaper
          ($30K/month). Cons: Latency (30s-1min), higher client battery usage. Use
          for: Large groups (>1000), notification channels. Hybrid: Push for active
          users, pull for inactive. Switch at 5min idle. Result: 80% push, 20% pull
          = $250K/month.'
      - question: How do you implement presence detection with high accuracy and low
          latency?
        answer: 'Heartbeat: Client sends heartbeat every 30s via WebSocket. Server:
          Redis TTL = 60s (2× heartbeat). If TTL expires = offline. Pub/Sub: Broadcast
          presence changes to friends. Latency: <100ms. Scalability: 100M users ×
          1 heartbeat/30s = 3.3M/sec. Redis cluster: 10 nodes (each 300K ops/sec).
          Cost: $5K/month. Optimization: Batch updates every 5s, reduce Redis writes
          by 10x. Alternative: Last-seen timestamp (eventual consistency, cheaper).'
      - question: What strategies ensure reliable message delivery in mobile networks?
        answer: 'Use at-least-once delivery: 1) Client sends message with UUID, 2)
          Server ACKs with msg_id, 3) Client retries if no ACK (exponential backoff:
          1s, 2s, 4s), 4) Server deduplicates by UUID. Offline: Queue messages in
          local SQLite. When online: Sync with server. Network switch: Use TCP keepalive
          + WebSocket ping/pong (30s). Connection lost: Reconnect with last_seen_msg_id,
          server sends missed messages. Cost: 5% retry rate × 1B msg/day = 50M retries/day.'
      - question: How do you handle WebSocket connection failover and session management?
        answer: 'Stateless servers: Store session in Redis (user_id, server_id, last_msg_id).
          On disconnect: Client reconnects to any server via load balancer. New server:
          Reads session from Redis, resumes from last_msg_id. Failover time: <5s.
          Kafka consumer rebalancing: 10s. Total: 15s downtime. Health check: Load
          balancer pings every 10s. If server fails: Remove from pool in 30s. Session
          TTL: 24h. Cost: Redis cluster $3K/month for 100M sessions.'
      - question: What are the security considerations for end-to-end encrypted messaging?
        answer: 'Use Signal Protocol: Double Ratchet (forward secrecy + break-in recovery).
          Key exchange: X3DH (Extended Triple Diffie-Hellman). Server: Never sees
          plaintext, only encrypted messages. Metadata: Server knows who talks to
          whom, not content. Multi-device: Each device has own identity key, messages
          encrypted per device. Group chat: Sender Key protocol (MLS for >100 members).
          Backup: Encrypted backup with user passphrase (PBKDF2, 100K rounds). Cost:
          10ms encryption overhead per message.'
      tradeoffs:
      - question: WebSocket vs HTTP long polling for real-time messaging
        answer: 'WebSocket: Pros: Real-time (5ms), bidirectional, less overhead (no
          HTTP headers). Cons: Stateful (10K connections/server), harder to load balance,
          firewall issues. Cost: $300K/month for 100M users. Long polling: Pros: Stateless,
          works everywhere, simpler. Cons: Higher latency (500ms-1s), more bandwidth
          (HTTP headers), server load. Cost: $100K/month. Hybrid: WebSocket for active
          users (80%), long polling for fallback (20%) = $250K/month.'
      - question: Microservices vs monolith for chat system architecture
        answer: 'Microservices: Services: Gateway, Chat, User, Group, Notification,
          Media, Presence. Pros: Independent scaling (chat 100x, user 10x), team autonomy.
          Cons: Complex (7 services), inter-service latency (10-50ms), distributed
          tracing needed. Cost: $500K/month (10 services × $50K). Monolith: Pros:
          Simple, low latency (in-memory), easier debugging. Cons: Single scaling
          unit, deployment risk. Cost: $200K/month. Start: Monolith. Scale: Microservices
          at 10M users.'
      - question: SQL vs NoSQL for message storage and retrieval
        answer: 'SQL (PostgreSQL): Pros: ACID, complex queries (search), indexing.
          Cons: Scaling (sharding complex), 10K TPS/shard. Use for: Audit logs, analytics.
          NoSQL (Cassandra): Pros: Horizontal scaling (1M TPS), partition by conversation_id.
          Cons: No joins, eventual consistency. Use for: Message history. Hybrid:
          Write to Cassandra (low latency), async sync to PostgreSQL (analytics).
          Storage: 1B msg/day × 1KB × 365 = 365TB. Cassandra: $10K/month, PostgreSQL:
          $30K/month.'
      - question: Push vs pull model for large group message distribution
        answer: 'Push (fan-out on write): Write to all 10K member inboxes. Pros: Fast
          reads (10ms). Cons: Slow writes (10s), expensive ($10/message). Use for:
          <100 members. Pull (fan-out on read): Write once to group feed. Pros: Fast
          writes (10ms), cheap ($0.001/message). Cons: Slow reads (50ms), need filtering.
          Use for: >1000 members. Hybrid: <100 = push, 100-1000 = push to online +
          pull for offline, >1000 = pull. Result: 95% of groups use push, but 95%
          of messages use pull (due to large groups).'
      - question: Strong vs eventual consistency for message ordering
        answer: 'Strong consistency: Use Kafka single partition per conversation.
          Pros: Total order guaranteed. Cons: Single consumer bottleneck (10K msg/sec
          max), higher latency (50ms). Use for: Banking, legal chat. Cost: 60 partitions
          = 60 parallel conversations max. Eventual consistency: Multiple partitions,
          Lamport timestamps. Pros: Higher throughput (100K msg/sec), lower latency
          (10ms). Cons: Out-of-order delivery (1%), need client-side reordering. Use
          for: Social chat. 99% of apps choose eventual (fast > perfect order).'
      - question: Horizontal vs vertical scaling for WebSocket connections
        answer: 'Horizontal: Add more servers (10K connections each). Pros: Linear
          scaling (add server = +10K users), fault tolerance. Cons: Complex load balancing,
          session affinity. Cost: $30/server/month × 10K servers = $300K. Vertical:
          Bigger servers (100K connections each). Pros: Simpler, less overhead. Cons:
          Hardware limits (128GB RAM max = 100K connections), single point of failure.
          Cost: $3K/server/month × 1K servers = $3M. Choose: Horizontal (10x cheaper,
          better reliability).'
      scenarios:
      - question: Design message delivery guarantees for a banking chat application
        answer: 'Requirements: No message loss, total ordering, audit trail. Architecture:
          1) Client sends with idempotency key (UUID), 2) Kafka with acks=all (all
          replicas confirm), 3) Store in PostgreSQL with transaction, 4) ACK to client.
          Ordering: Single Kafka partition per conversation. Audit: Immutable append-only
          log with signatures. Encryption: TLS 1.3 + E2E encryption. Delivery: At-least-once
          (retry until ACK). Latency: 100ms (vs 10ms social chat). Compliance: GDPR
          (right to delete = soft delete with hash). Cost: $50K/month for 1M messages/day.'
      - question: Handle a viral message in a group with 1M members efficiently
        answer: 'Problem: Fan-out to 1M users = $1K per message (write fan-out). Solution:
          Use read fan-out + caching. 1) Write message once to group feed, 2) CDN
          cache message for 1h (99% hit rate), 3) Users pull from CDN (1ms), 4) Push
          notification to online users (100K) = $100. Total cost: $100 vs $1K (10x
          savings). Scalability: CDN handles 1M concurrent reads. Alternative: Tree-based
          propagation (each user forwards to 10 peers) = decentralized, harder to
          implement.'
      - question: Implement offline message synchronization when user comes back online
        answer: 'Client stores last_seen_msg_id in local SQLite. On reconnect: 1)
          Send last_seen_msg_id to server, 2) Server queries messages WHERE id > last_seen_msg_id
          ORDER BY id LIMIT 1000, 3) Send batch to client, 4) Client ACKs, requests
          next batch. Pagination: 1000 messages/batch. For 10K missed: 10 batches
          × 100ms = 1s. Optimization: Compress batch (gzip 5x) = 200KB/batch. If >7
          days offline: Reset sync (download all conversations). Cost: 1% of users
          offline >1 day = 1M users × 10KB = 10GB sync/day.'
      - question: Design cross-platform real-time typing indicators
        answer: 'Protocol: 1) User types → send typing event every 3s (debounce),
          2) Server broadcasts to conversation members via WebSocket, 3) Display ''User
          is typing...'' for 5s (timeout). Scalability: 100K typing events/sec. Don''t
          persist: In-memory only (Redis Pub/Sub). Multi-device: Include device_id
          in event. Mobile optimization: Only send if >3 chars typed (reduce events
          80%). Cost: Ephemeral (no storage), but uses WebSocket bandwidth. Alternative:
          Coalesce typing events (batch updates every 1s) = reduce load 3x.'
      - question: Handle message encryption/decryption in a multi-device environment
        answer: 'Use Signal Protocol with multi-device support: 1) Each device has
          identity key pair, 2) Sender encrypts message per device (N encryptions
          for N devices), 3) Store encrypted copies in server, 4) Each device decrypts
          with own key. Key management: X3DH for initial key exchange, Double Ratchet
          for forward secrecy. New device: Uses QR code + verify existing device.
          Removed device: Rotate group keys. Cost: N× encryption overhead (10ms ×
          3 devices = 30ms). Storage: 3× messages (one per device). Alternative: Use
          envelope encryption (encrypt message once with data key, encrypt data key
          per device) = faster.'
      - question: Implement message search across billions of encrypted messages
        answer: 'Problem: Server can''t search encrypted content. Solutions: 1) Client-side
          search: Download all messages to device, index locally (SQLite FTS5). Pros:
          Privacy. Cons: Slow, limited to device storage. 2) Encrypted search (searchable
          encryption): Client encrypts search keywords with deterministic encryption,
          server searches encrypted index. Pros: Fast (100ms). Cons: Keyword leakage
          to server. 3) Hybrid: Store message hashes for exact match, client-side
          for full-text. Cost: 1B messages × 100B/hash = 100GB index. Use: Elasticsearch
          with encrypted fields. 99% apps: Client-side search only (privacy first).'
      - question: Design disaster recovery for chat system during datacenter failure
        answer: 'Multi-region active-active: 3 regions (US, EU, Asia). Each region:
          Full stack (WebSocket, Kafka, Cassandra). Replication: Kafka MirrorMaker
          2.0 (async, <1s lag), Cassandra multi-DC (quorum writes). Failure: Region
          failure → DNS/load balancer redirects to nearest region in 30s. Users: Reconnect
          to new region, resume from last_msg_id. Data loss: Max 1s of messages (async
          replication lag). RTO: 30s, RPO: 1s. Cost: 3 regions × $200K = $600K/month
          (2× single region for 3× availability). Alternative: Active-passive (cheaper,
          5min RTO).'
      - question: Handle gradual deployment of new chat features without downtime
        answer: 'Use feature flags + canary deployment: 1) Deploy new version to 1%
          of servers, 2) Monitor metrics (latency, error rate) for 1h, 3) If stable:
          Increase to 10%, 50%, 100% over 24h. Feature flags: Enable new feature for
          1% of users (A/B test). Backward compatibility: Old clients must work with
          new servers (version negotiation). Database migrations: Dual-write pattern
          (write to old + new schema), migrate background, then switch reads. Rollback:
          Instant (disable feature flag). Cost: No downtime, 10% extra capacity during
          deployment. Alternative: Blue-green deployment (requires 2× infrastructure).'
    time_estimate: 150
    video_resources:
    - title: 'ByteByteGo: System Design Interview Tips'
      url: https://www.youtube.com/@ByteByteGo
      duration: 20 min
      description: Review Week 2 patterns and strategies
      priority: high
  week3:
  - day: 15
    topic: Advanced Database Sharding
    activity: Master advanced sharding architectures, resharding strategies, cross-shard
      operations, and production-grade sharding solutions for massive scale.
    detailed_content: "Advanced Sharding Architectures:\n\n1. Sharding Strategies\
      \ Deep Dive:\n- Hash-based Sharding:\n  * Consistent hashing with virtual nodes\n\
      \  * Handling hash collisions and distribution\n  * Pros: Simple, good distribution\n\
      \  * Cons: Hard to range queries, resharding complexity\n- Range-based Sharding:\n\
      \  * Ordered data distribution across shards\n  * Hot spot management for sequential\
      \ keys\n  * Pros: Range queries, ordered scans\n  * Cons: Uneven distribution,\
      \ hot shards\n- Directory-based Sharding:\n  * Centralized shard mapping service\n\
      \  * Dynamic shard assignment and routing\n  * Pros: Flexible, easy resharding\n\
      \  * Cons: Single point of failure, lookup overhead\n- Hybrid Approaches:\n\
      \  * Combining multiple strategies\n  * Geographic + hash-based sharding\n \
      \ * Time-based + range sharding\n\n2. Shard Key Design and Evolution:\n- Shard\
      \ Key Selection Criteria:\n  * High cardinality for even distribution\n  * Query\
      \ pattern alignment\n  * Avoiding hot spots and monotonic keys\n  * Immutability\
      \ considerations\n- Composite Shard Keys:\n  * Multi-column sharding strategies\n\
      \  * Hierarchical key structures\n  * Balancing distribution vs query efficiency\n\
      - Shard Key Migration:\n  * Gradual key transformation\n  * Backward compatibility\
      \ during migration\n  * Dual-key support periods\n\n3. Hot Shard Detection and\
      \ Mitigation:\n- Detection Mechanisms:\n  * Real-time metrics monitoring\n \
      \ * Query pattern analysis\n  * Resource utilization tracking\n  * Automated\
      \ alerting systems\n- Mitigation Strategies:\n  * Shard splitting techniques\n\
      \  * Read replica creation\n  * Data redistribution algorithms\n  * Application-level\
      \ load balancing\n- Prevention Techniques:\n  * Better shard key design\n  *\
      \ Pre-splitting strategies\n  * Randomization techniques\n  * Temporal data\
      \ distribution\n\n4. Resharding and Data Migration:\n- Live Migration Strategies:\n\
      \  * Online resharding without downtime\n  * Incremental data movement\n  *\
      \ Consistency during migration\n  * Rollback mechanisms\n- Double-write Pattern:\n\
      \  * Writing to both old and new shards\n  * Synchronization challenges\n  *\
      \ Conflict resolution\n  * Performance implications\n- Pre-splitting Techniques:\n\
      \  * Creating more shards than needed\n  * Virtual shard management\n  * Elastic\
      \ shard allocation\n  * Capacity planning integration\n\n5. Cross-shard Operations:\n\
      - Distributed Transactions:\n  * Two-phase commit (2PC) protocol\n  * Three-phase\
      \ commit improvements\n  * Saga pattern for long transactions\n  * Compensation-based\
      \ transactions\n- Cross-shard Queries:\n  * Scatter-gather query execution\n\
      \  * Query planning and optimization\n  * Result aggregation strategies\n  *\
      \ Performance considerations\n- Global Secondary Indexes:\n  * Cross-shard index\
      \ maintenance\n  * Consistency guarantees\n  * Update propagation\n  * Query\
      \ routing optimization\n\n6. Consistency and ACID Properties:\n- Shard-level\
      \ ACID:\n  * Maintaining ACID within shards\n  * Cross-shard consistency challenges\n\
      \  * Eventual consistency models\n  * Conflict resolution strategies\n- Distributed\
      \ Locking:\n  * Cross-shard lock management\n  * Deadlock detection and resolution\n\
      \  * Lock timeout handling\n  * Performance optimization\n- Consensus Protocols:\n\
      \  * Raft for shard leadership\n  * Paxos for distributed decisions\n  * Byzantine\
      \ fault tolerance\n  * Leader election mechanisms\n\nReal-world Implementation\
      \ Examples:\n\n1. YouTube Vitess:\n- MySQL sharding at massive scale\n- Transparent\
      \ query routing\n- Online schema migrations\n- Automatic failover and recovery\n\
      - Horizontal and vertical sharding\n\n2. Pinterest Sharding:\n- UUID-based shard\
      \ keys\n- Consistent hashing implementation\n- Hot shard detection system\n\
      - Gradual migration strategies\n- Cross-shard analytics challenges\n\n3. Instagram\
      \ Sharding:\n- Photo and user data sharding\n- Geographic distribution strategies\n\
      - Timeline generation across shards\n- Follower graph partitioning\n- Real-time\
      \ notification challenges\n\n4. Discord Sharding:\n- Message history partitioning\n\
      - Guild-based sharding strategies\n- Real-time message routing\n- Voice chat\
      \ data distribution\n- Cross-guild operations\n\nMonitoring and Operations:\n\
      \n1. Performance Metrics:\n- Query latency per shard\n- Throughput distribution\n\
      - Resource utilization tracking\n- Hot spot identification\n- Connection pool\
      \ monitoring\n\n2. Data Distribution Analysis:\n- Shard size monitoring\n- Key\
      \ distribution analysis\n- Growth pattern tracking\n- Rebalancing triggers\n\
      - Capacity planning metrics\n\n3. Operational Tools:\n- Automated resharding\
      \ systems\n- Migration progress tracking\n- Health check frameworks\n- Alerting\
      \ and notification\n- Performance optimization tools\n\nAdvanced Patterns and\
      \ Techniques:\n\n1. Federated Queries:\n- Cross-database querying\n- Query federation\
      \ engines\n- Performance optimization\n- Result caching strategies\n- Security\
      \ and access control\n\n2. Materialized Views:\n- Cross-shard view maintenance\n\
      - Incremental view updates\n- Consistency guarantees\n- Query acceleration\n\
      - Storage optimization\n\n3. Event Sourcing with Sharding:\n- Event stream partitioning\n\
      - Aggregate root boundaries\n- Event replay across shards\n- Snapshot management\n\
      - Temporal querying\n\n4. CQRS with Sharding:\n- Command and query separation\n\
      - Write model sharding\n- Read model denormalization\n- Projection management\n\
      - Eventual consistency handling\n\nProduction Considerations:\n\n1. Deployment\
      \ Strategies:\n- Blue-green deployments\n- Canary releases\n- Rolling updates\n\
      - Feature flags integration\n- Rollback procedures\n\n2. Disaster Recovery:\n\
      - Cross-region replication\n- Backup and restore strategies\n- Point-in-time\
      \ recovery\n- Failover procedures\n- Data integrity verification\n\n3. Security\
      \ Considerations:\n- Shard-level access control\n- Encryption at rest and transit\n\
      - Audit logging\n- Compliance requirements\n- Attack vector mitigation\n\n4.\
      \ Cost Optimization:\n- Resource allocation strategies\n- Auto-scaling policies\n\
      - Storage optimization\n- Network cost management\n- Performance vs cost trade-offs\n\
      \nPractice Questions:\n\nCapacity Estimation:\n1. \"Design sharding for a social\
      \ media platform with 1B users, 100B posts. Calculate optimal shard count, considering\
      \ 10TB per shard limit and 50% growth yearly.\"\n2. \"Estimate resharding timeline\
      \ for migrating 500TB database from 50 to 500 shards with 100MB/s network, including\
      \ downtime windows.\"\n3. \"Calculate cross-shard query performance impact:\
      \ 1000 QPS queries hitting average 5 shards with 10ms per-shard latency plus\
      \ 2ms aggregation overhead.\"\n\nConceptual Understanding:\n1. \"Compare consistent\
      \ hashing vs range-based sharding for time-series data. Analyze query patterns,\
      \ hot spots, and resharding complexity.\"\n2. \"Explain trade-offs between shard-level\
      \ ACID vs cross-shard eventual consistency. When would you choose each approach?\"\
      \n3. \"Design global secondary indexes for sharded e-commerce inventory. Handle\
      \ real-time updates, query routing, and consistency challenges.\"\n\nTrade-off\
      \ Analysis:\n1. \"Analyze pre-splitting strategy: 10x more shards than needed\
      \ vs just-in-time splitting. Consider resource overhead, complexity, and performance.\"\
      \n2. \"Compare federation vs materialized views for cross-shard analytics. Evaluate\
      \ latency, consistency, storage, and maintenance costs.\"\n3. \"Design shard\
      \ key evolution strategy balancing query performance, distribution, and migration\
      \ complexity for user behavior analytics.\"\n\nScenario-based Design:\n1. \"\
      Design resharding strategy for chat application during 10x user growth spike.\
      \ Handle active conversations, message history, and real-time delivery.\"\n\
      2. \"Handle celebrity user causing hot shard in social media platform. Design\
      \ immediate mitigation, long-term prevention, and monitoring systems.\"\n3.\
      \ \"Implement cross-shard transaction for transferring money between users in\
      \ different shards. Ensure ACID properties and handle failures.\"\n4. \"Design\
      \ sharding strategy for IoT sensor data: 1M devices, 1 datapoint/second each.\
      \ Optimize for time-range queries and real-time aggregations.\"\n"
    resources:
    - title: 'Vitess: Production MySQL Sharding'
      url: https://vitess.io/docs/overview/
      description: Complete sharding framework by YouTube
    - title: 'Sharding Pinterest: Lessons from Scale'
      url: https://medium.com/@Pinterest_Engineering/sharding-pinterest-how-we-scaled-our-mysql-fleet-3f341e96ca6f
      description: Real-world sharding implementation case study
    - title: Instagram Sharding Architecture
      url: https://instagram-engineering.com/sharding-ids-at-instagram-1cf5a71e5a5c
      description: ID sharding and distributed systems design
    - title: 'Discord: Storing Billions of Messages'
      url: https://discord.com/blog/how-discord-stores-billions-of-messages
      description: Message sharding and Cassandra at scale
    - title: Consistent Hashing in Practice
      url: https://medium.com/@sent0hil/consistent-hashing-a-guide-go-implementation-fe3421ac3e8f
      description: Implementation details and best practices
    - title: Two-Phase Commit Protocol
      url: https://en.wikipedia.org/wiki/Two-phase_commit_protocol
      description: Distributed transaction coordination
    - title: Saga Pattern for Distributed Transactions
      url: https://microservices.io/patterns/data/saga.html
      description: Alternative to distributed transactions
    - title: 'Spanner: Google''s Globally Distributed Database'
      url: https://research.google/pubs/pub39966/
      description: Advanced sharding with global consistency
    time_estimate: 120
    video_resources:
    - title: 'ByteByteGo: Design Rate Limiter'
      url: https://www.youtube.com/@ByteByteGo
      duration: 12 min
      description: Token bucket and leaky bucket algorithms
      priority: high
    - title: 'Hello Interview: Rate Limiting Strategies'
      url: https://www.youtube.com/@HelloInterview
      duration: 15 min
      description: Different rate limiting approaches
      priority: medium
  - day: 16
    topic: Content Delivery Networks (CDN)
    activity: Master CDN architecture, global content distribution, edge computing,
      and performance optimization for modern web applications at scale.
    detailed_content: "CDN Architecture Fundamentals:\n\n1. Network Topology and Infrastructure:\n\
      - Edge Locations (POPs):\n  * Geographic distribution strategies\n  * Proximity-based\
      \ routing algorithms\n  * Edge server capacity planning\n  * Network interconnection\
      \ and peering\n- Origin Servers:\n  * Primary and secondary origins\n  * Origin\
      \ shielding strategies\n  * Load balancing between origins\n  * Failover and\
      \ disaster recovery\n- Hierarchical Cache Structure:\n  * Edge → Regional →\
      \ Origin hierarchy\n  * Multi-tier caching strategies\n  * Cache miss handling\
      \ and forwarding\n  * Bandwidth optimization between tiers\n\n2. Routing and\
      \ Request Direction:\n- DNS-based Routing:\n  * GeoDNS for geographic routing\n\
      \  * Weighted round-robin algorithms\n  * Health check integration\n  * Latency-based\
      \ routing decisions\n- Anycast Routing:\n  * IP anycast implementation\n  *\
      \ BGP route optimization\n  * Network topology considerations\n  * Failover\
      \ and traffic shifting\n- HTTP Redirects:\n  * 302 redirect mechanisms\n  *\
      \ Location header optimization\n  * Client-side routing logic\n  * Performance\
      \ implications\n\n3. Advanced Caching Strategies:\n- Cache Hierarchies:\n  *\
      \ L1 (Edge) → L2 (Regional) → L3 (Origin)\n  * Cache replacement algorithms\
      \ (LRU, LFU, FIFO)\n  * Cache warming and pre-population\n  * Hit ratio optimization\
      \ strategies\n- Content Classification:\n  * Static Assets: Images, CSS, JS,\
      \ fonts\n  * Semi-dynamic: User-generated content, thumbnails\n  * Dynamic Content:\
      \ API responses, personalized data\n  * Streaming Media: Video chunks, live\
      \ streams\n- Cache Key Design:\n  * URL normalization strategies\n  * Query\
      \ parameter handling\n  * Header-based cache variations\n  * User-specific cache\
      \ segmentation\n\n4. Cache Invalidation and Purging:\n- Time-based Expiration:\n\
      \  * Cache-Control and Expires headers\n  * TTL strategy optimization\n  * Stale-while-revalidate\
      \ patterns\n  * Grace period handling\n- Event-driven Invalidation:\n  * Tag-based\
      \ purging systems\n  * Real-time invalidation APIs\n  * Batch invalidation operations\n\
      \  * Selective cache clearing\n- Version-based Strategies:\n  * Cache busting\
      \ with versioned URLs\n  * Immutable content patterns\n  * Deployment-triggered\
      \ invalidation\n  * Rollback considerations\n\n5. Content Optimization Techniques:\n\
      - Compression and Encoding:\n  * Gzip vs Brotli compression algorithms\n  *\
      \ Real-time compression at edge\n  * Pre-compressed content delivery\n  * Compression\
      \ ratio optimization\n- Image and Media Optimization:\n  * Format selection\
      \ (WebP, AVIF, JPEG XL)\n  * Responsive image serving\n  * Real-time image resizing\n\
      \  * Progressive loading strategies\n- HTTP Protocol Optimization:\n  * HTTP/2\
      \ multiplexing and server push\n  * HTTP/3 QUIC protocol benefits\n  * Connection\
      \ reuse and pooling\n  * Header compression techniques\n\nEdge Computing and\
      \ Processing:\n\n1. Edge Functions and Serverless:\n- Edge Workers/Functions:\n\
      \  * JavaScript execution at edge locations\n  * Request/response manipulation\n\
      \  * A/B testing and feature flags\n  * Security and authentication\n- Edge-side\
      \ Includes (ESI):\n  * Fragment-based caching\n  * Dynamic content assembly\n\
      \  * Personalization at edge\n  * Template processing\n- WebAssembly at Edge:\n\
      \  * High-performance edge computing\n  * Custom logic execution\n  * Language-agnostic\
      \ processing\n  * Security sandboxing\n\n2. Real-time Processing:\n- Stream\
      \ Processing:\n  * Live video transcoding\n  * Real-time analytics\n  * Log\
      \ processing and aggregation\n  * Event stream manipulation\n- Edge Analytics:\n\
      \  * Real-time metrics collection\n  * User behavior tracking\n  * Performance\
      \ monitoring\n  * Security threat detection\n\nSecurity and Protection:\n\n\
      1. DDoS Protection:\n- Traffic Analysis:\n  * Baseline traffic pattern establishment\n\
      \  * Anomaly detection algorithms\n  * Attack signature recognition\n  * Rate\
      \ limiting mechanisms\n- Mitigation Strategies:\n  * Traffic scrubbing and filtering\n\
      \  * Distributed defense across POPs\n  * Capacity scaling during attacks\n\
      \  * Origin protection measures\n\n2. Web Application Firewall (WAF):\n- Rule-based\
      \ Filtering:\n  * OWASP Top 10 protection\n  * Custom security rules\n  * Geoblocking\
      \ and IP filtering\n  * Bot detection and mitigation\n- Advanced Security:\n\
      \  * SSL/TLS termination and optimization\n  * Certificate management\n  * HTTP\
      \ security headers\n  * Content Security Policy enforcement\n\nReal-world Implementation\
      \ Examples:\n\n1. Netflix CDN Strategy:\n- Open Connect Appliances:\n  * ISP-embedded\
      \ cache servers\n  * Content pre-positioning\n  * Adaptive bitrate streaming\n\
      \  * Global content distribution\n- Content Delivery Optimization:\n  * Predictive\
      \ caching algorithms\n  * Regional content libraries\n  * Peak-hour traffic\
      \ management\n  * Quality of experience optimization\n\n2. YouTube Global Distribution:\n\
      - Video Content Delivery:\n  * Multi-resolution video caching\n  * Adaptive\
      \ streaming protocols\n  * Geographic content preferences\n  * Live streaming\
      \ optimization\n- Edge Storage Strategy:\n  * Popular content pre-caching\n\
      \  * Long-tail content optimization\n  * Storage cost optimization\n  * Bandwidth\
      \ utilization patterns\n\n3. Shopify Commerce CDN:\n- E-commerce Optimization:\n\
      \  * Product image delivery\n  * Dynamic inventory updates\n  * Checkout process\
      \ acceleration\n  * Mobile-first optimization\n- Global Scaling:\n  * Multi-region\
      \ deployment\n  * Currency and language adaptation\n  * Compliance and data\
      \ residency\n  * Performance monitoring\n\n4. Cloudflare Network:\n- Global\
      \ Anycast Network:\n  * 200+ city presence\n  * Edge computing platform\n  *\
      \ Security service integration\n  * Developer-friendly APIs\n- Innovation Features:\n\
      \  * Workers distributed computing\n  * Stream video platform\n  * R2 object\
      \ storage\n  * Zero Trust security\n\nPerformance Monitoring and Analytics:\n\
      \n1. Core Metrics:\n- Cache Performance:\n  * Hit ratio by content type\n  *\
      \ Origin offload percentage\n  * Average response times\n  * Bandwidth savings\
      \ analysis\n- User Experience:\n  * Time to First Byte (TTFB)\n  * Page load\
      \ time distributions\n  * Geographic performance variations\n  * Mobile vs desktop\
      \ performance\n\n2. Advanced Analytics:\n- Real-time Monitoring:\n  * Traffic\
      \ pattern analysis\n  * Error rate tracking\n  * Security threat detection\n\
      \  * Capacity utilization monitoring\n- Business Intelligence:\n  * Cost optimization\
      \ insights\n  * User behavior analysis\n  * Content popularity trends\n  * Revenue\
      \ impact measurement\n\nCost Optimization and Business Considerations:\n\n1.\
      \ Pricing Models:\n- Bandwidth-based Pricing:\n  * Data transfer cost optimization\n\
      \  * Peak vs off-peak pricing\n  * Regional pricing variations\n  * Commitment-based\
      \ discounts\n- Request-based Pricing:\n  * Cost per request analysis\n  * Caching\
      \ impact on costs\n  * Edge function execution costs\n  * Security service pricing\n\
      \n2. ROI and Business Impact:\n- Performance Benefits:\n  * Conversion rate\
      \ improvements\n  * User engagement metrics\n  * SEO ranking benefits\n  * Customer\
      \ satisfaction scores\n- Operational Savings:\n  * Origin server load reduction\n\
      \  * Bandwidth cost savings\n  * Infrastructure scaling benefits\n  * Operational\
      \ complexity reduction\n\nPractice Questions:\n\nCapacity Estimation:\n1. \"\
      Design CDN for global video streaming service: 100M users, 2 hours average watch\
      \ time, 1080p average quality. Calculate edge storage, bandwidth requirements,\
      \ and optimal POP placement.\"\n2. \"Estimate cache hit ratio improvement for\
      \ e-commerce site: 1M products, 100K daily users, 20% cache hit ratio currently.\
      \ Design caching strategy to achieve 85% hit ratio.\"\n3. \"Calculate CDN cost\
      \ optimization for news website: 50TB monthly traffic, 80% cacheable content,\
      \ $0.05/GB bandwidth cost. Compare origin vs CDN delivery costs.\"\n\nConceptual\
      \ Understanding:\n1. \"Compare pull vs push CDN models for different content\
      \ types. Analyze latency, storage costs, and complexity trade-offs.\"\n2. \"\
      Explain edge computing vs traditional CDN caching. When would you use edge functions\
      \ vs simple content caching?\"\n3. \"Design cache invalidation strategy for\
      \ social media platform. Handle user posts, profile updates, and real-time content\
      \ feeds.\"\n\nTrade-off Analysis:\n1. \"Analyze geographic CDN placement strategy:\
      \ Many small POPs vs fewer large regional centers. Consider costs, performance,\
      \ and maintenance.\"\n2. \"Compare CDN providers: CloudFlare vs AWS CloudFront\
      \ vs Fastly. Evaluate features, pricing, performance, and integration capabilities.\"\
      \n3. \"Design caching strategy balancing freshness vs performance for stock\
      \ trading platform with real-time price updates.\"\n\nScenario-based Design:\n\
      1. \"Handle viral content surge: News article getting 100x normal traffic in\
      \ 1 hour. Design cache warming, capacity scaling, and origin protection.\"\n\
      2. \"Design CDN for live sports streaming: 10M concurrent viewers, multiple\
      \ camera angles, global audience. Handle peak traffic and low latency requirements.\"\
      \n3. \"Implement CDN for mobile app with offline capabilities: Image caching,\
      \ delta updates, and background synchronization across global user base.\"\n\
      4. \"Design secure CDN for banking application: Sensitive document delivery,\
      \ compliance requirements, and DDoS protection.\"\n"
    resources:
    - title: 'Cloudflare: How CDNs Work'
      url: https://www.cloudflare.com/learning/cdn/what-is-a-cdn/
      description: Comprehensive CDN concepts and implementation
    - title: AWS CloudFront Developer Guide
      url: https://docs.aws.amazon.com/cloudfront/
      description: Production CDN service implementation
    - title: Fastly Edge Computing Platform
      url: https://docs.fastly.com/
      description: Edge computing and real-time processing
    - title: Netflix Open Connect
      url: https://openconnect.netflix.com/en/
      description: Global content delivery infrastructure
    - title: Akamai Edge Computing
      url: https://www.akamai.com/resources
      description: Enterprise CDN and edge services
    - title: HTTP Caching Best Practices
      url: https://developers.google.com/web/fundamentals/performance/optimizing-content-efficiency/http-caching
      description: Web performance optimization techniques
    - title: Edge-side Includes (ESI) Specification
      url: https://www.w3.org/TR/esi-lang
      description: Dynamic content assembly at edge
    - title: WebAssembly at the Edge
      url: https://webassembly.org/
      description: High-performance edge computing platform
    time_estimate: 120
    video_resources:
    - title: 'ByteByteGo: Design Autocomplete'
      url: https://www.youtube.com/@ByteByteGo
      duration: 10 min
      description: Typeahead search suggestions
      priority: high
    - title: 'Hello Interview: Design Google Autocomplete'
      url: https://www.youtube.com/@HelloInterview
      duration: 20 min
      description: Trie-based autocomplete system
      priority: high
  - day: 17
    topic: Search Systems & Information Retrieval
    activity: Master search engine architecture, advanced indexing strategies, ranking
      algorithms, and relevance optimization for large-scale information retrieval
      systems.
    detailed_content: "Search System Architecture:\n\n1. High-level Search Pipeline:\n\
      - Document Ingestion:\n  * Web crawling and discovery\n  * Content extraction\
      \ and parsing\n  * Document classification and filtering\n  * Quality assessment\
      \ and spam detection\n- Index Construction:\n  * Text processing and normalization\n\
      \  * Tokenization and linguistic analysis\n  * Inverted index building\n  *\
      \ Distributed index sharding\n- Query Processing:\n  * Query parsing and analysis\n\
      \  * Intent classification and understanding\n  * Query rewriting and expansion\n\
      \  * Result ranking and presentation\n\n2. Crawling and Data Acquisition:\n\
      - Web Crawling Strategies:\n  * Breadth-first vs depth-first crawling\n  * Politeness\
      \ policies and rate limiting\n  * Duplicate detection and deduplication\n  *\
      \ Incremental crawling for fresh content\n- Content Discovery:\n  * Sitemap\
      \ and robots.txt processing\n  * Link graph analysis\n  * Social media and API\
      \ integration\n  * User-submitted content handling\n- Quality Control:\n  *\
      \ Content quality assessment\n  * Spam and malicious content filtering\n  *\
      \ Language detection and processing\n  * Copyright and legal compliance\n\n\
      Advanced Indexing Strategies:\n\n1. Inverted Index Design:\n- Index Structure:\n\
      \  * Term dictionary with posting lists\n  * Document frequency and term positions\n\
      \  * Index compression techniques\n  * Delta encoding and variable-byte encoding\n\
      - Advanced Features:\n  * Phrase indexing for exact match queries\n  * Proximity\
      \ indexing for near-match queries\n  * Field-specific indexing (title, body,\
      \ metadata)\n  * Multi-language indexing and analysis\n- Distribution Strategies:\n\
      \  * Document-based partitioning\n  * Term-based partitioning\n  * Hybrid partitioning\
      \ approaches\n  * Replication for fault tolerance\n\n2. Real-time Indexing:\n\
      - Incremental Updates:\n  * Log-structured merge trees\n  * Delta indexing strategies\n\
      \  * Memory-mapped index structures\n  * Background merging and compaction\n\
      - Near Real-time Search:\n  * Buffer-based indexing\n  * Commit and refresh\
      \ strategies\n  * Index warming techniques\n  * Consistency guarantees\n\n3.\
      \ Specialized Indexes:\n- Geospatial Indexing:\n  * R-trees and spatial partitioning\n\
      \  * Geohash and spatial grid systems\n  * Location-based search optimization\n\
      \  * Distance and proximity calculations\n- Temporal Indexing:\n  * Time-series\
      \ data indexing\n  * Time-range queries optimization\n  * Historical data archiving\n\
      \  * Temporal relevance scoring\n- Multimedia Indexing:\n  * Image feature extraction\
      \ and indexing\n  * Video content analysis\n  * Audio fingerprinting\n  * Cross-modal\
      \ search capabilities\n\nRanking and Relevance Algorithms:\n\n1. Classical Ranking\
      \ Models:\n- TF-IDF (Term Frequency-Inverse Document Frequency):\n  * Term frequency\
      \ normalization strategies\n  * Document frequency calculations\n  * Variants\
      \ and improvements\n  * Implementation optimizations\n- BM25 and Okapi BM25:\n\
      \  * Parameter tuning (k1, b, k3)\n  * Document length normalization\n  * Field-specific\
      \ BM25 variants\n  * Performance characteristics\n- Vector Space Models:\n \
      \ * Cosine similarity calculations\n  * Term weighting schemes\n  * Dimensionality\
      \ reduction techniques\n  * Latent semantic analysis\n\n2. Link-based Algorithms:\n\
      - PageRank:\n  * Random walk model\n  * Damping factor and convergence\n  *\
      \ Personalized PageRank\n  * Computation and storage optimizations\n- HITS (Hyperlink-Induced\
      \ Topic Search):\n  * Authority and hub scores\n  * Topic-specific HITS\n  *\
      \ Web graph analysis\n  * Link spam detection\n- Modern Link Analysis:\n  *\
      \ TrustRank for spam detection\n  * Social signals integration\n  * Citation\
      \ analysis for academic search\n  * Temporal link dynamics\n\n3. Machine Learning\
      \ for Ranking:\n- Learning to Rank (LTR):\n  * Pointwise approach (classification/regression)\n\
      \  * Pairwise approach (ranking SVM, RankNet)\n  * Listwise approach (ListNet,\
      \ AdaRank)\n  * Feature engineering for ranking\n- Deep Learning Models:\n \
      \ * Neural ranking models\n  * BERT for search relevance\n  * Transformer-based\
      \ architectures\n  * Embedding-based semantic matching\n- Personalization:\n\
      \  * User behavior modeling\n  * Collaborative filtering integration\n  * Context-aware\
      \ ranking\n  * Privacy-preserving personalization\n\nQuery Processing and Understanding:\n\
      \n1. Query Analysis:\n- Query Parsing:\n  * Tokenization and normalization\n\
      \  * Phrase detection and handling\n  * Boolean query processing\n  * Wildcard\
      \ and regex support\n- Intent Classification:\n  * Navigational vs informational\
      \ queries\n  * Commercial intent detection\n  * Entity recognition in queries\n\
      \  * Ambiguity resolution\n- Query Expansion:\n  * Synonym expansion techniques\n\
      \  * Stemming and lemmatization\n  * Thesaurus-based expansion\n  * Automatic\
      \ query suggestion\n\n2. Advanced Query Features:\n- Faceted Search:\n  * Dynamic\
      \ facet generation\n  * Hierarchical faceting\n  * Range and numerical facets\n\
      \  * Facet value ordering and counting\n- Auto-complete and Suggestions:\n \
      \ * Trie-based completion\n  * Popular query completion\n  * Personalized suggestions\n\
      \  * Typo-tolerant completion\n- Spell Correction:\n  * Edit distance algorithms\n\
      \  * Phonetic matching techniques\n  * Context-aware correction\n  * Machine\
      \ learning-based correction\n\nPerformance Optimization:\n\n1. Query Performance:\n\
      - Index Optimization:\n  * Index warming strategies\n  * Cache-friendly data\
      \ structures\n  * Parallel query processing\n  * Query result caching\n- Distributed\
      \ Query Processing:\n  * Scatter-gather query execution\n  * Query routing and\
      \ load balancing\n  * Result merging and ranking\n  * Fault tolerance and retries\n\
      \n2. Scalability Patterns:\n- Horizontal Scaling:\n  * Shard management and\
      \ routing\n  * Cross-shard result aggregation\n  * Load balancing strategies\n\
      \  * Auto-scaling based on query load\n- Caching Layers:\n  * Query result caching\n\
      \  * Index segment caching\n  * Distributed cache coordination\n  * Cache invalidation\
      \ strategies\n\nReal-world Implementation Examples:\n\n1. Google Search Architecture:\n\
      - Web Scale Indexing:\n  * MapReduce for distributed indexing\n  * Bigtable\
      \ for metadata storage\n  * Distributed file system integration\n  * Real-time\
      \ index updates\n- Ranking Innovation:\n  * PageRank and authority signals\n\
      \  * RankBrain machine learning\n  * BERT for query understanding\n  * Knowledge\
      \ graph integration\n\n2. Elasticsearch at Scale:\n- Netflix Search Infrastructure:\n\
      \  * Multi-cluster deployment\n  * Content recommendation search\n  * Real-time\
      \ analytics integration\n  * Performance monitoring\n- GitHub Code Search:\n\
      \  * Programming language analysis\n  * Repository indexing strategies\n  *\
      \ Code structure understanding\n  * Developer workflow integration\n\n3. Amazon\
      \ Product Search:\n- E-commerce Optimization:\n  * Product catalog indexing\n\
      \  * Inventory-aware search results\n  * Conversion-optimized ranking\n  * Personalized\
      \ product recommendations\n- Search Features:\n  * Visual search capabilities\n\
      \  * Voice search integration\n  * Multi-language support\n  * Mobile search\
      \ optimization\n\n4. Slack Search System:\n- Workplace Search:\n  * Message\
      \ history indexing\n  * Real-time message search\n  * Channel and user filtering\n\
      \  * File content search\n- Performance Requirements:\n  * Sub-second response\
      \ times\n  * High availability guarantees\n  * Privacy and security compliance\n\
      \  * Cross-workspace search\n\nSearch Quality and Evaluation:\n\n1. Relevance\
      \ Metrics:\n- Traditional Metrics:\n  * Precision and recall\n  * F-measure\
      \ and F1 score\n  * Mean Average Precision (MAP)\n  * Normalized Discounted\
      \ Cumulative Gain (NDCG)\n- User-centric Metrics:\n  * Click-through rates (CTR)\n\
      \  * Dwell time and engagement\n  * Session success rate\n  * User satisfaction\
      \ surveys\n\n2. A/B Testing and Experimentation:\n- Ranking Experiments:\n \
      \ * Statistical significance testing\n  * Interleaving experiments\n  * Online\
      \ evaluation frameworks\n  * Long-term impact analysis\n- Quality Assurance:\n\
      \  * Query-result pair evaluation\n  * Human relevance judgments\n  * Automated\
      \ quality checks\n  * Regression testing frameworks\n\nSpecialized Search Domains:\n\
      \n1. Enterprise Search:\n- Content Sources:\n  * Document repositories\n  *\
      \ Email and collaboration tools\n  * Database and CRM systems\n  * Intranet\
      \ and knowledge bases\n- Security and Access Control:\n  * User permission integration\n\
      \  * Content security classification\n  * Audit trail and compliance\n  * GDPR\
      \ and privacy requirements\n\n2. Academic and Scientific Search:\n- Citation\
      \ Analysis:\n  * Paper ranking by citations\n  * Author authority metrics\n\
      \  * Journal impact factors\n  * Cross-reference networks\n- Content Processing:\n\
      \  * PDF and paper parsing\n  * Formula and equation handling\n  * Scientific\
      \ notation processing\n  * Multi-language academic content\n\nPractice Questions:\n\
      \nCapacity Estimation:\n1. \"Design search system for 1B web pages, 1M queries/second\
      \ peak. Calculate index size, memory requirements, and server count for sub-100ms\
      \ response time.\"\n2. \"Estimate storage for e-commerce search: 100M products,\
      \ 50 attributes each, 10 languages. Include inverted indexes, forward indexes,\
      \ and caching layers.\"\n3. \"Calculate ranking computation cost: BM25 scoring\
      \ for 10K candidate documents, 50-term queries, considering CPU and memory constraints.\"\
      \n\nConceptual Understanding:\n1. \"Compare inverted index vs forward index\
      \ structures. Analyze storage efficiency, query performance, and update complexity\
      \ for different use cases.\"\n2. \"Explain trade-offs between real-time indexing\
      \ vs batch indexing. Consider consistency, performance, and resource utilization.\"\
      \n3. \"Design ranking algorithm combining TF-IDF, PageRank, and user behavior\
      \ signals. Handle signal normalization and weight optimization.\"\n\nTrade-off\
      \ Analysis:\n1. \"Analyze document-based vs term-based index partitioning for\
      \ distributed search. Consider query latency, load balancing, and fault tolerance.\"\
      \n2. \"Compare Elasticsearch vs custom search solution for startup. Evaluate\
      \ development time, operational complexity, and scalability limits.\"\n3. \"\
      Design personalization strategy balancing relevance improvement vs privacy concerns\
      \ and computational overhead.\"\n\nScenario-based Design:\n1. \"Design search\
      \ for code repository with 10M files: Handle syntax highlighting, code structure\
      \ understanding, and developer workflow integration.\"\n2. \"Implement real-time\
      \ search for social media platform: Index 1000 posts/second, handle viral content,\
      \ and ensure content freshness.\"\n3. \"Build enterprise search across 100+\
      \ data sources: Handle heterogeneous schemas, access control, and query federation.\"\
      \n4. \"Design multilingual search for global e-commerce: Handle 20 languages,\
      \ cultural preferences, and localized ranking factors.\"\n"
    resources:
    - title: 'Elasticsearch: The Definitive Guide'
      url: https://www.elastic.co/guide/en/elasticsearch/guide/current/
      description: Comprehensive guide to distributed search
    - title: Introduction to Information Retrieval
      url: https://nlp.stanford.edu/IR-book/
      description: Academic foundation of search systems
    - title: 'Modern Search Engines: Architecture and Engineering'
      url: https://www.manning.com/books/relevant-search
      description: Practical search system implementation
    - title: Apache Lucene Documentation
      url: https://lucene.apache.org/core/documentation.html
      description: Core search library implementation
    - title: Google's Original PageRank Paper
      url: http://ilpubs.stanford.edu:8090/422/1/1999-66.pdf
      description: Foundational link analysis algorithm
    - title: Learning to Rank for Information Retrieval
      url: https://www.microsoft.com/en-us/research/publication/learning-to-rank-for-information-retrieval/
      description: Machine learning approaches to ranking
    - title: 'BM25 and Beyond: Relevance Scoring'
      url: https://opensourceconnections.com/blog/2015/10/16/bm25-the-next-generation-of-lucene-relevation/
      description: Modern relevance scoring techniques
    - title: 'Designing Data-Intensive Applications: Search'
      url: https://dataintensive.net/
      description: Search systems in distributed architectures
    time_estimate: 120
    video_resources:
    - title: 'Hello Interview: Design Uber'
      url: https://www.youtube.com/@HelloInterview
      duration: 35 min
      description: Ride-sharing platform design
      priority: high
    - title: 'ByteByteGo: Geospatial Indexing'
      url: https://www.youtube.com/@ByteByteGo
      duration: 12 min
      description: Location-based services architecture
      priority: high
  - day: 18
    topic: Big Data Processing Systems
    activity: Master distributed data processing architectures, MapReduce paradigms,
      stream processing frameworks, and modern big data platforms for petabyte-scale
      analytics.
    detailed_content: "Big Data Processing Fundamentals:\n\n1. Processing Paradigms\
      \ and Architectures:\n- Batch Processing:\n  * High-throughput, high-latency\
      \ processing\n  * Complete dataset availability for analysis\n  * Complex transformations\
      \ and aggregations\n  * Cost-effective for large historical datasets\n  * Examples:\
      \ Daily ETL pipelines, monthly reports\n- Stream Processing:\n  * Low-latency,\
      \ continuous data processing\n  * Incremental processing of data streams\n \
      \ * Real-time alerts and monitoring\n  * Event-driven architectures\n  * Examples:\
      \ Fraud detection, real-time analytics\n- Micro-batch Processing:\n  * Hybrid\
      \ approach combining batch and stream\n  * Small batches processed frequently\n\
      \  * Balance between latency and throughput\n  * Fault tolerance through batch\
      \ boundaries\n  * Examples: Near real-time reporting\n\n2. Lambda Architecture:\n\
      - Architecture Components:\n  * Batch Layer: Comprehensive, accurate processing\n\
      \  * Speed Layer: Real-time approximate results\n  * Serving Layer: Merges results\
      \ from both layers\n  * Master Dataset: Immutable, append-only storage\n- Benefits\
      \ and Challenges:\n  * Comprehensive data processing coverage\n  * Fault tolerance\
      \ and data recovery\n  * Complex operational overhead\n  * Code duplication\
      \ between layers\n- Implementation Patterns:\n  * Data ingestion strategies\n\
      \  * Batch view materialization\n  * Stream processing integration\n  * Query\
      \ serving optimization\n\n3. Kappa Architecture:\n- Unified Stream Processing:\n\
      \  * Single processing engine for all data\n  * Stream-first approach to data\
      \ processing\n  * Reprocessing through stream replay\n  * Simplified operational\
      \ model\n- Advantages over Lambda:\n  * Reduced complexity and maintenance\n\
      \  * Single codebase for all processing\n  * Easier debugging and testing\n\
      \  * Real-time and historical unified view\n- Implementation Considerations:\n\
      \  * Stream storage and replay capabilities\n  * Exactly-once processing guarantees\n\
      \  * State management and checkpointing\n  * Backpressure and flow control\n\
      \nMapReduce and Distributed Computing:\n\n1. MapReduce Programming Model:\n\
      - Core Concepts:\n  * Map Function: Transform input key-value pairs\n  * Reduce\
      \ Function: Aggregate values by key\n  * Partitioning: Distribute data across\
      \ reducers\n  * Combiners: Local aggregation optimization\n- Execution Flow:\n\
      \  * Input splitting and distribution\n  * Map task execution and intermediate\
      \ output\n  * Shuffle and sort phases\n  * Reduce task execution and output\n\
      - Optimization Techniques:\n  * Input format optimization\n  * Combiner function\
      \ usage\n  * Custom partitioning strategies\n  * Output compression techniques\n\
      \n2. Hadoop Ecosystem:\n- HDFS (Hadoop Distributed File System):\n  * Block-based\
      \ distributed storage\n  * Replication for fault tolerance\n  * NameNode and\
      \ DataNode architecture\n  * Block placement and load balancing\n- YARN (Yet\
      \ Another Resource Negotiator):\n  * Resource management and job scheduling\n\
      \  * ApplicationMaster and NodeManager\n  * Multi-tenancy and resource isolation\n\
      \  * Dynamic resource allocation\n- MapReduce Engine:\n  * Job execution framework\n\
      \  * Task scheduling and monitoring\n  * Fault tolerance and recovery\n  * Performance\
      \ tuning and optimization\n\n3. Apache Spark Architecture:\n- Core Components:\n\
      \  * Driver Program: Application coordination\n  * Cluster Manager: Resource\
      \ allocation\n  * Executors: Task execution and data storage\n  * RDD (Resilient\
      \ Distributed Datasets)\n- Processing Models:\n  * Batch processing with Spark\
      \ Core\n  * Stream processing with Spark Streaming\n  * SQL processing with\
      \ Spark SQL\n  * Machine learning with MLlib\n  * Graph processing with GraphX\n\
      - Optimization Features:\n  * In-memory computing capabilities\n  * Lazy evaluation\
      \ and query optimization\n  * Dynamic resource allocation\n  * Adaptive query\
      \ execution\n\nStream Processing Systems:\n\n1. Apache Flink Architecture:\n\
      - Core Features:\n  * True streaming with low latency\n  * Event-time processing\
      \ semantics\n  * Exactly-once state consistency\n  * Advanced windowing capabilities\n\
      - State Management:\n  * Operator state and keyed state\n  * State backends\
      \ and checkpointing\n  * Savepoints for application evolution\n  * State schema\
      \ evolution\n- Time and Windowing:\n  * Event time vs processing time\n  * Watermarks\
      \ for out-of-order events\n  * Window types (tumbling, sliding, session)\n \
      \ * Custom window functions\n\n2. Apache Kafka Streams:\n- Stream Processing\
      \ Library:\n  * Client-side stream processing\n  * No separate cluster management\n\
      \  * Integration with Kafka ecosystem\n  * Microservices-friendly architecture\n\
      - Processing Topology:\n  * Source and sink processors\n  * Stream and table\
      \ abstractions\n  * Stateful and stateless operations\n  * Joins and aggregations\n\
      - Exactly-once Semantics:\n  * Transactional processing guarantees\n  * Idempotent\
      \ producers and consumers\n  * State store consistency\n  * Failure recovery\
      \ mechanisms\n\n3. Apache Storm (Real-time Computing):\n- Architecture Components:\n\
      \  * Nimbus: Master node coordination\n  * Supervisors: Worker node management\n\
      \  * Spouts: Data source components\n  * Bolts: Processing components\n- Topology\
      \ Design:\n  * Stream grouping strategies\n  * Parallelism and scaling\n  *\
      \ Fault tolerance guarantees\n  * At-least-once processing\n\nModern Big Data\
      \ Platforms:\n\n1. Cloud-Native Solutions:\n- Google Cloud Dataflow:\n  * Unified\
      \ batch and stream processing\n  * Auto-scaling and managed infrastructure\n\
      \  * Apache Beam programming model\n  * Integration with GCP services\n- Amazon\
      \ EMR and Kinesis:\n  * Managed Hadoop and Spark clusters\n  * Real-time stream\
      \ processing\n  * Integration with AWS ecosystem\n  * Cost optimization and\
      \ spot instances\n- Azure Data Factory and Stream Analytics:\n  * ETL orchestration\
      \ and data movement\n  * Real-time stream processing\n  * Integration with Azure\
      \ services\n  * Visual pipeline development\n\n2. Modern Unified Platforms:\n\
      - Databricks Unified Analytics:\n  * Collaborative data science platform\n \
      \ * Managed Spark and Delta Lake\n  * MLOps and AutoML capabilities\n  * Multi-cloud\
      \ deployment options\n- Snowflake Data Cloud:\n  * Cloud-native data warehouse\n\
      \  * Separation of compute and storage\n  * Auto-scaling and concurrency\n \
      \ * Data sharing and marketplace\n\nReal-world Implementation Examples:\n\n\
      1. Netflix Big Data Platform:\n- Data Processing Pipeline:\n  * Real-time event\
      \ collection and processing\n  * Batch processing for recommendation models\n\
      \  * A/B testing data analysis\n  * Content performance analytics\n- Technology\
      \ Stack:\n  * Kafka for data ingestion\n  * Spark for batch processing\n  *\
      \ Flink for real-time analytics\n  * Cassandra for data storage\n\n2. Uber Data\
      \ Platform:\n- Real-time Processing:\n  * Trip matching and pricing\n  * Fraud\
      \ detection systems\n  * Real-time city operations\n  * Dynamic supply-demand\
      \ balancing\n- Architecture:\n  * Kafka for event streaming\n  * Flink for stream\
      \ processing\n  * Hadoop for historical analysis\n  * Presto for interactive\
      \ queries\n\n3. LinkedIn Data Infrastructure:\n- Kafka Origin Story:\n  * Activity\
      \ stream processing\n  * Real-time feed generation\n  * Metrics and monitoring\n\
      \  * Change data capture\n- Processing Evolution:\n  * Voldemort for serving\n\
      \  * Hadoop for batch analytics\n  * Samza for stream processing\n  * Brooklin\
      \ for data replication\n\n4. Spotify Music Recommendations:\n- Data Processing\
      \ Pipeline:\n  * User behavior stream processing\n  * Music feature extraction\n\
      \  * Collaborative filtering algorithms\n  * Real-time playlist generation\n\
      - Technology Choices:\n  * Google Cloud Dataflow\n  * BigQuery for analytics\n\
      \  * Pub/Sub for messaging\n  * TensorFlow for ML models\n\nPerformance Optimization\
      \ and Scalability:\n\n1. Resource Management:\n- Cluster Sizing:\n  * CPU, memory,\
      \ and storage planning\n  * Network bandwidth considerations\n  * Cost optimization\
      \ strategies\n  * Auto-scaling policies\n- Task Scheduling:\n  * Fair vs FIFO\
      \ vs capacity schedulers\n  * Resource pools and queues\n  * Priority-based\
      \ scheduling\n  * Speculative execution\n\n2. Data Locality and Partitioning:\n\
      - Data Locality:\n  * Co-locating computation and data\n  * Rack-aware scheduling\n\
      \  * Network topology optimization\n  * Storage tier optimization\n- Partitioning\
      \ Strategies:\n  * Hash-based partitioning\n  * Range-based partitioning\n \
      \ * Custom partitioning functions\n  * Dynamic repartitioning\n\n3. Fault Tolerance\
      \ and Recovery:\n- Checkpointing Strategies:\n  * Periodic state snapshots\n\
      \  * Incremental checkpointing\n  * Distributed checkpointing protocols\n  *\
      \ Recovery time optimization\n- Failure Handling:\n  * Task failure recovery\n\
      \  * Node failure detection\n  * Data corruption handling\n  * Network partition\
      \ tolerance\n\nPractice Questions:\n\nCapacity Estimation:\n1. \"Design big\
      \ data processing system for 1PB daily data: Calculate cluster size, processing\
      \ time, and resource requirements for batch and stream processing.\"\n2. \"\
      Estimate costs for processing 100TB monthly data using cloud services: Compare\
      \ Spark on EMR vs Dataflow vs on-premises Hadoop.\"\n3. \"Calculate stream processing\
      \ latency: 1M events/second through 10-stage pipeline with 5ms per stage, including\
      \ network and serialization overhead.\"\n\nConceptual Understanding:\n1. \"\
      Compare Lambda vs Kappa architectures for e-commerce analytics. Analyze complexity,\
      \ consistency, and operational overhead.\"\n2. \"Explain exactly-once processing\
      \ in distributed systems. Compare implementations in Kafka Streams, Flink, and\
      \ Spark Streaming.\"\n3. \"Design data partitioning strategy for time-series\
      \ analytics. Balance query performance, parallelism, and storage efficiency.\"\
      \n\nTrade-off Analysis:\n1. \"Analyze MapReduce vs Spark for large-scale ETL:\
      \ Consider memory usage, fault tolerance, development complexity, and performance.\"\
      \n2. \"Compare batch vs stream processing for fraud detection: Evaluate accuracy,\
      \ latency, cost, and operational complexity.\"\n3. \"Design resource allocation\
      \ strategy: Dedicated clusters vs shared multi-tenant environment. Consider\
      \ isolation, utilization, and cost.\"\n\nScenario-based Design:\n1. \"Design\
      \ real-time recommendation system: Process 100K user events/second, update ML\
      \ models, and serve recommendations under 100ms.\"\n2. \"Build data pipeline\
      \ for IoT sensor network: Handle 1M sensors, 1 datapoint/second each, with real-time\
      \ alerting and historical analytics.\"\n3. \"Implement financial risk calculation\
      \ system: Process market data streams, calculate portfolio risk in real-time,\
      \ and generate regulatory reports.\"\n4. \"Design log analytics platform: Ingest\
      \ 10TB daily logs, provide real-time dashboards, and enable ad-hoc queries for\
      \ debugging.\"\n"
    resources:
    - title: Designing Data-Intensive Applications
      url: https://dataintensive.net/
      description: Comprehensive guide to big data systems design
    - title: Apache Spark Documentation
      url: https://spark.apache.org/docs/latest/
      description: Unified analytics engine for big data processing
    - title: Apache Flink Documentation
      url: https://flink.apache.org/documentation.html
      description: Stream processing framework with low latency
    - title: Kafka Streams Developer Guide
      url: https://kafka.apache.org/documentation/streams/
      description: Stream processing library for Apache Kafka
    - title: 'MapReduce: Simplified Data Processing'
      url: https://research.google/pubs/pub62/
      description: Original Google MapReduce paper
    - title: The Lambda Architecture
      url: http://lambda-architecture.net/
      description: Big data architecture pattern
    - title: 'Streaming Systems: O''Reilly Book'
      url: https://www.oreilly.com/library/view/streaming-systems/9781491983867/
      description: Comprehensive guide to stream processing
    - title: Google Cloud Dataflow Model
      url: https://cloud.google.com/dataflow/docs/concepts/beam-programming-model
      description: Unified batch and stream processing model
    time_estimate: 130
    video_resources:
    - title: 'ByteByteGo: Design Chat System'
      url: https://www.youtube.com/@ByteByteGo
      duration: 18 min
      description: Real-time messaging system
      priority: high
    - title: 'Hello Interview: Design WhatsApp'
      url: https://www.youtube.com/@HelloInterview
      duration: 30 min
      description: Messaging app architecture
      priority: high
  - day: 19
    topic: NoSQL Database Deep Dive
    activity: Master NoSQL database architectures, data models, consistency patterns,
      and selection criteria for document, key-value, column-family, graph, and time-series
      databases.
    detailed_content: "NoSQL Database Categories and Architecture:\n\n1. Document\
      \ Stores (MongoDB, CouchDB, Amazon DocumentDB):\n- Data Model and Structure:\n\
      \  * JSON/BSON document storage with nested structures\n  * Schema flexibility\
      \ and schema-on-read capabilities\n  * Collections and databases organization\n\
      \  * Document size limitations and optimization\n- Query Capabilities:\n  *\
      \ Rich query language (MongoDB Query Language)\n  * Index support (compound,\
      \ text, geospatial, partial)\n  * Aggregation framework for complex operations\n\
      \  * MapReduce for large-scale data processing\n- Scaling and Distribution:\n\
      \  * Horizontal sharding with shard keys\n  * Replica sets for high availability\n\
      \  * Automatic failover and read preference\n  * Zones and cross-datacenter\
      \ replication\n- Use Cases and Examples:\n  * Content Management Systems (CMS)\n\
      \  * Product catalogs and inventory management\n  * User profiles and personalization\n\
      \  * Real-time analytics and logging\n  * Mobile and web application backends\n\
      \n2. Key-Value Stores (Redis, DynamoDB, Riak, Amazon ElastiCache):\n- Architecture\
      \ Patterns:\n  * Simple key-value pair storage model\n  * In-memory vs persistent\
      \ storage options\n  * Distributed hash tables and consistent hashing\n  * Partitioning\
      \ and replication strategies\n- Advanced Features:\n  * Data structures (strings,\
      \ hashes, lists, sets, sorted sets)\n  * Expiration and TTL mechanisms\n  *\
      \ Atomic operations and transactions\n  * Pub/Sub messaging capabilities\n \
      \ * Lua scripting for complex operations\n- Performance Characteristics:\n \
      \ * Sub-millisecond latency for in-memory systems\n  * High throughput for read\
      \ and write operations\n  * Predictable performance scaling\n  * Memory vs disk\
      \ trade-offs\n- Use Cases and Patterns:\n  * Session storage and user state\
      \ management\n  * Real-time caching and application acceleration\n  * Gaming\
      \ leaderboards and counters\n  * Shopping carts and temporary data\n  * Rate\
      \ limiting and throttling systems\n\n3. Column-Family Databases (Cassandra,\
      \ HBase, Amazon Keyspaces):\n- Data Model Deep Dive:\n  * Wide column storage\
      \ with row keys and column families\n  * Dynamic column creation and sparse\
      \ data handling\n  * Time-stamped values and versioning\n  * Composite keys\
      \ and clustering columns\n- Architecture and Distribution:\n  * Ring-based architecture\
      \ with consistent hashing\n  * Tunable consistency levels (ONE, QUORUM, ALL)\n\
      \  * Multi-datacenter replication strategies\n  * Gossip protocol for cluster\
      \ membership\n- Performance Optimization:\n  * Write-optimized storage with\
      \ LSM trees\n  * Bloom filters for efficient reads\n  * Compaction strategies\
      \ and tombstone handling\n  * Compression algorithms for storage efficiency\n\
      - Use Cases and Scenarios:\n  * Time-series data and event logging\n  * IoT\
      \ sensor data collection\n  * Large-scale analytics and data warehousing\n \
      \ * Content delivery and media storage\n  * Financial trading and market data\n\
      \n4. Graph Databases (Neo4j, Amazon Neptune, ArangoDB, TigerGraph):\n- Graph\
      \ Data Models:\n  * Nodes, edges, and properties representation\n  * Labeled\
      \ property graph model\n  * RDF (Resource Description Framework) support\n \
      \ * Multi-model capabilities (document + graph)\n- Query Languages and Processing:\n\
      \  * Cypher query language for graph traversal\n  * SPARQL for RDF queries\n\
      \  * Gremlin for graph computing\n  * Native graph processing vs graph layers\n\
      - Graph Algorithms and Analytics:\n  * Shortest path and pathfinding algorithms\n\
      \  * Centrality measures (betweenness, closeness, PageRank)\n  * Community detection\
      \ and clustering\n  * Graph neural networks integration\n- Scaling and Performance:\n\
      \  * Graph partitioning challenges\n  * Horizontal vs vertical scaling approaches\n\
      \  * Caching strategies for graph traversals\n  * ACID transactions and consistency\n\
      - Use Cases and Applications:\n  * Social networks and relationship mapping\n\
      \  * Fraud detection and risk analysis\n  * Recommendation engines and collaborative\
      \ filtering\n  * Knowledge graphs and semantic search\n  * Supply chain and\
      \ network optimization\n\n5. Time-Series Databases (InfluxDB, TimescaleDB, Prometheus,\
      \ OpenTSDB):\n- Specialized Architecture:\n  * Time-based partitioning and data\
      \ organization\n  * Columnar storage for temporal data\n  * Retention policies\
      \ and data lifecycle management\n  * Continuous queries and real-time processing\n\
      - Query and Analytics Features:\n  * Time-range queries and windowing functions\n\
      \  * Downsampling and data aggregation\n  * Gap-filling and interpolation functions\n\
      \  * Statistical functions and trend analysis\n- Performance Optimizations:\n\
      \  * Compression algorithms for time-series data\n  * Index strategies for temporal\
      \ queries\n  * Batch ingestion and write optimization\n  * Memory management\
      \ for recent data\n- Use Cases and Domains:\n  * Application performance monitoring\
      \ (APM)\n  * Infrastructure and system metrics\n  * IoT sensor data and industrial\
      \ telemetry\n  * Financial market data and trading systems\n  * Scientific data\
      \ collection and analysis\n\nAdvanced NoSQL Concepts:\n\n1. Consistency Models\
      \ and CAP Theorem:\n- Consistency Levels:\n  * Strong consistency (immediate\
      \ consistency)\n  * Eventual consistency (BASE properties)\n  * Causal consistency\
      \ and session consistency\n  * Tunable consistency per operation\n- CAP Theorem\
      \ Trade-offs:\n  * Consistency vs Availability during partitions\n  * CP systems\
      \ (MongoDB, HBase) vs AP systems (Cassandra, DynamoDB)\n  * Practical implications\
      \ for system design\n  * PACELC theorem extensions\n\n2. Replication and Partitioning\
      \ Strategies:\n- Replication Patterns:\n  * Master-slave vs master-master replication\n\
      \  * Synchronous vs asynchronous replication\n  * Multi-region replication for\
      \ global systems\n  * Conflict resolution strategies\n- Partitioning Techniques:\n\
      \  * Hash-based partitioning with consistent hashing\n  * Range-based partitioning\
      \ for ordered data\n  * Directory-based partitioning for flexibility\n  * Hybrid\
      \ approaches and repartitioning\n\n3. Transaction Support and ACID Properties:\n\
      - NoSQL Transaction Models:\n  * Single-document vs multi-document transactions\n\
      \  * Cross-partition transaction complexity\n  * Distributed transaction protocols\
      \ (2PC, Saga)\n  * Eventual consistency vs immediate consistency\n- ACID Alternatives:\n\
      \  * BASE (Basically Available, Soft state, Eventual consistency)\n  * CALM\
      \ theorem and monotonic operations\n  * Idempotent operations and retry safety\n\
      \  * Compensation patterns for failure handling\n\nReal-world Implementation\
      \ Examples:\n\n1. Netflix NoSQL Strategy:\n- Cassandra for Viewing History:\n\
      \  * Storing billions of viewing events\n  * Time-series data modeling\n  *\
      \ Multi-region replication\n  * Tuned consistency for performance\n- MongoDB\
      \ for Content Metadata:\n  * Rich content descriptions and schemas\n  * Search\
      \ and recommendation integration\n  * Global content distribution\n  * Dynamic\
      \ schema evolution\n\n2. Uber's Database Architecture:\n- Redis for Real-time\
      \ Operations:\n  * Driver location caching\n  * Session management\n  * Rate\
      \ limiting and throttling\n  * Pub/Sub for real-time updates\n- Cassandra for\
      \ Trip Data:\n  * Historical trip storage\n  * Analytics and reporting\n  *\
      \ Geographic partitioning\n  * Time-based retention policies\n\n3. LinkedIn's\
      \ Data Platform:\n- Voldemort (Key-Value):\n  * User profile storage\n  * Social\
      \ graph data\n  * High-availability requirements\n  * Consistent hashing distribution\n\
      - Graph Database for Social Connections:\n  * Connection recommendations\n \
      \ * Network analysis and insights\n  * Real-time relationship queries\n  * Privacy\
      \ and access control\n\n4. Discord Message Storage:\n- Cassandra for Message\
      \ History:\n  * Billions of messages storage\n  * Channel-based partitioning\n\
      \  * Real-time message delivery\n  * Cross-datacenter replication\n- Redis for\
      \ Real-time Features:\n  * Online presence tracking\n  * Message caching\n \
      \ * Rate limiting and spam protection\n  * Real-time notifications\n\nDatabase\
      \ Selection Framework:\n\n1. Data Model Considerations:\n- Structured vs Semi-structured\
      \ Data:\n  * Relational data with fixed schema → RDBMS\n  * Flexible documents\
      \ with varying fields → Document stores\n  * Simple key-value pairs → Key-value\
      \ stores\n  * Time-stamped metrics → Time-series databases\n  * Relationship-heavy\
      \ data → Graph databases\n- Query Pattern Analysis:\n  * Simple lookups → Key-value\
      \ stores\n  * Complex queries and joins → Document stores or RDBMS\n  * Graph\
      \ traversals → Graph databases\n  * Time-range analytics → Time-series databases\n\
      \  * Write-heavy workloads → Column-family stores\n\n2. Scalability and Performance\
      \ Requirements:\n- Read vs Write Patterns:\n  * Read-heavy: Document stores\
      \ with read replicas\n  * Write-heavy: Column-family databases\n  * Balanced:\
      \ Key-value stores with caching\n  * Complex queries: Graph databases with indexing\n\
      - Consistency Requirements:\n  * Strong consistency: RDBMS or CP NoSQL systems\n\
      \  * Eventual consistency: AP NoSQL systems\n  * Tunable consistency: Cassandra\
      \ or DynamoDB\n  * Real-time consistency: In-memory databases\n\n3. Operational\
      \ Considerations:\n- Team Expertise and Learning Curve:\n  * SQL familiarity\
      \ → SQL-like NoSQL (DocumentDB)\n  * DevOps complexity → Managed services\n\
      \  * Operational overhead → Cloud-native solutions\n  * Monitoring and debugging\
      \ capabilities\n- Cost and Resource Planning:\n  * Storage costs for different\
      \ data models\n  * Compute requirements for query processing\n  * Network costs\
      \ for distributed systems\n  * Operational overhead and maintenance\n\nPractice\
      \ Questions:\n\nCapacity Estimation:\n1. \"Design NoSQL storage for social media\
      \ platform: 1B users, 100B posts, 1TB daily growth. Choose appropriate databases\
      \ for user profiles, posts, and analytics.\"\n2. \"Estimate DynamoDB costs for\
      \ gaming leaderboard: 10M players, 1000 updates/second, 100 reads/second. Calculate\
      \ RCU/WCU requirements and monthly costs.\"\n3. \"Calculate Cassandra cluster\
      \ size for IoT platform: 1M sensors, 1 datapoint/second each, 2-year retention.\
      \ Consider replication factor and growth.\"\n\nConceptual Understanding:\n1.\
      \ \"Compare document databases vs relational databases for e-commerce product\
      \ catalog. Analyze schema flexibility, query capabilities, and scaling patterns.\"\
      \n2. \"Explain eventual consistency in Cassandra. How would you handle read-after-write\
      \ consistency for critical operations?\"\n3. \"Design data model for graph database\
      \ storing social network. Handle mutual connections, privacy settings, and recommendation\
      \ queries.\"\n\nTrade-off Analysis:\n1. \"Compare Redis vs DynamoDB for session\
      \ storage: Analyze latency, durability, cost, and operational complexity for\
      \ global web application.\"\n2. \"Choose between MongoDB and Cassandra for logging\
      \ system: 1TB daily logs, real-time queries, 3-month retention. Consider write\
      \ performance and query needs.\"\n3. \"Analyze graph database vs document database\
      \ for recommendation engine: Product recommendations based on user behavior\
      \ and social connections.\"\n\nScenario-based Design:\n1. \"Design NoSQL architecture\
      \ for multiplayer game: Player profiles, match history, leaderboards, and real-time\
      \ game state. Handle 1M concurrent players.\"\n2. \"Build time-series database\
      \ solution for financial trading: Store tick data, calculate moving averages,\
      \ and detect anomalies. Handle 100K updates/second.\"\n3. \"Design multi-tenant\
      \ SaaS database architecture: Isolated customer data, flexible schemas, and\
      \ cross-tenant analytics. Choose appropriate NoSQL solutions.\"\n4. \"Implement\
      \ content delivery system: Global content distribution, user preferences, and\
      \ real-time recommendations. Handle diverse content types and access patterns.\"\
      \n"
    resources:
    - title: 'NoSQL Distilled: A Brief Guide'
      url: https://martinfowler.com/books/nosql.html
      description: Comprehensive guide to NoSQL database concepts
    - title: MongoDB Architecture Guide
      url: https://docs.mongodb.com/manual/
      description: Document database design patterns and best practices
    - title: 'Cassandra: The Complete Reference'
      url: https://cassandra.apache.org/doc/latest/
      description: Column-family database architecture and operations
    - title: Redis Documentation and Patterns
      url: https://redis.io/documentation
      description: Key-value store advanced features and use cases
    - title: Neo4j Graph Database Concepts
      url: https://neo4j.com/docs/
      description: Graph database modeling and query optimization
    - title: Time-Series Database Comparison
      url: https://blog.timescale.com/blog/what-the-heck-is-time-series-data-and-why-do-i-need-a-time-series-database-dcf3b6dfd73/
      description: Time-series database architecture and use cases
    - title: CAP Theorem and NoSQL
      url: https://en.wikipedia.org/wiki/CAP_theorem
      description: Consistency, availability, and partition tolerance trade-offs
    - title: Amazon DynamoDB Design Patterns
      url: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/best-practices.html
      description: NoSQL design patterns and optimization techniques
    time_estimate: 130
    video_resources:
    - title: 'ByteByteGo: Design Distributed Cache'
      url: https://www.youtube.com/@ByteByteGo
      duration: 15 min
      description: Consistent hashing for distributed systems
      priority: high
    - title: 'Hello Interview: Consistent Hashing'
      url: https://www.youtube.com/@HelloInterview
      duration: 12 min
      description: Virtual nodes and ring structure
      priority: medium
  - day: 20
    topic: Week 3 Integration - Social Media Feed Design
    activity: Design a comprehensive social media platform integrating advanced sharding,
      CDN, search systems, big data processing, and NoSQL databases for global scale.
    detailed_content: "Week 3 Integration Project: Global Social Media Platform\n\n\
      This capstone project integrates all Week 3 concepts:\n- Advanced Database Sharding\
      \ (Day 15)\n- Content Delivery Networks (Day 16)\n- Search Systems & Information\
      \ Retrieval (Day 17)\n- Big Data Processing Systems (Day 18)\n- NoSQL Database\
      \ Deep Dive (Day 19)\n\nSystem Requirements and Scale:\n\nFunctional Requirements:\n\
      - User registration and profile management\n- Post creation (text, images, videos,\
      \ stories)\n- Timeline/feed generation with personalization\n- Social interactions\
      \ (likes, comments, shares, reactions)\n- User relationships (follow, unfollow,\
      \ block)\n- Content search and discovery\n- Real-time notifications\n- Direct\
      \ messaging and chat\n- Trending topics and hashtag tracking\n- Content moderation\
      \ and safety\n\nNon-functional Requirements:\n- 1.5 billion daily active users\n\
      - 500 million posts created daily\n- 50 billion interactions daily (likes, comments,\
      \ shares)\n- 10TB of media uploaded daily\n- Timeline load time: <200ms globally\n\
      - Search response time: <100ms\n- 99.9% availability (8.76 hours downtime/year)\n\
      - Global deployment across 6 regions\n- Real-time feed updates within 5 seconds\n\
      \nComprehensive Architecture Design:\n\n1. User Service (Advanced Sharding Applied):\n\
      - Database Sharding Strategy:\n  * Shard by user_id using consistent hashing\n\
      \  * 1000 shards across 200 database servers\n  * Each shard handles ~1.5M users\n\
      \  * Cross-shard queries for social graph operations\n- NoSQL Implementation:\n\
      \  * MongoDB for user profiles (flexible schema)\n  * Redis for session management\
      \ and online status\n  * Cassandra for activity logs and audit trails\n- Data\
      \ Model:\n  * User profiles with rich metadata\n  * Social graph relationships\
      \ (bidirectional)\n  * Privacy settings and preferences\n  * Account verification\
      \ and security settings\n\n2. Post Service (Multi-Database Strategy):\n- Content\
      \ Storage Sharding:\n  * Time-based + user_id composite sharding\n  * Recent\
      \ posts (last 30 days) in hot shards\n  * Older posts archived to cold storage\n\
      \  * Media metadata stored separately from content\n- NoSQL Database Selection:\n\
      \  * Cassandra for post metadata (high write throughput)\n  * MongoDB for rich\
      \ content objects (flexible schema)\n  * Time-series database for engagement\
      \ metrics\n  * Graph database for content relationships\n- Data Architecture:\n\
      \  * Post content with versioning support\n  * Media attachments and processing\
      \ status\n  * Engagement metrics (likes, shares, views)\n  * Content classification\
      \ and moderation flags\n\n3. Timeline Service (Hybrid Feed Generation):\n- Feed\
      \ Generation Strategy:\n  * Push model for users with <10K followers\n  * Pull\
      \ model for celebrities with >1M followers\n  * Hybrid model for mid-tier influencers\n\
      \  * Pre-computed feeds for active users\n- Big Data Processing Integration:\n\
      \  * Apache Kafka for real-time post streaming\n  * Apache Flink for real-time\
      \ feed updates\n  * Apache Spark for batch personalization\n  * Lambda architecture\
      \ for comprehensive processing\n- Caching and Performance:\n  * Redis for timeline\
      \ caches (last 500 posts)\n  * CDN for timeline API responses\n  * Preload next\
      \ page content predictively\n  * Cache warming for returning users\n\n4. Search\
      \ Service (Comprehensive Search System):\n- Search Architecture (Day 17 Applied):\n\
      \  * Elasticsearch cluster with 100+ nodes\n  * Real-time indexing with <5 second\
      \ latency\n  * Multi-language support and analysis\n  * Autocomplete and spell\
      \ correction\n- Index Design:\n  * Post content with full-text search\n  * User\
      \ profiles with fuzzy matching\n  * Hashtag and mention extraction\n  * Media\
      \ content analysis and tagging\n- Advanced Features:\n  * Personalized search\
      \ ranking\n  * Trending topics detection\n  * Content similarity and recommendations\n\
      \  * Real-time search suggestions\n\n5. Recommendation Service (ML-Driven Personalization):\n\
      - Big Data Pipeline (Day 18 Applied):\n  * Kafka Streams for real-time feature\
      \ extraction\n  * Spark MLlib for model training and inference\n  * Feature\
      \ store for user and content features\n  * A/B testing framework for algorithm\
      \ optimization\n- Recommendation Algorithms:\n  * Collaborative filtering for\
      \ user similarity\n  * Content-based filtering for post similarity\n  * Deep\
      \ learning models for engagement prediction\n  * Graph neural networks for social\
      \ influence\n- Real-time Processing:\n  * Stream processing for immediate personalization\n\
      \  * Feature vector updates within 1 second\n  * Model serving with <10ms inference\
      \ time\n  * Continuous learning from user interactions\n\n6. Media Service (Global\
      \ Content Distribution):\n- CDN Architecture (Day 16 Applied):\n  * Multi-tier\
      \ CDN with edge locations globally\n  * Adaptive bitrate streaming for videos\n\
      \  * Image optimization and format selection\n  * Progressive loading and lazy\
      \ loading\n- Media Processing Pipeline:\n  * Async video transcoding with multiple\
      \ resolutions\n  * Image compression and thumbnail generation\n  * Content analysis\
      \ for auto-tagging\n  * NSFW detection and content moderation\n- Storage Strategy:\n\
      \  * Object storage for original media files\n  * CDN caching for frequently\
      \ accessed content\n  * Geographically distributed storage\n  * Intelligent\
      \ storage tiering based on access patterns\n\n7. Analytics and Insights Service:\n\
      - Time-Series Database Implementation:\n  * InfluxDB for user engagement metrics\n\
      \  * Prometheus for system performance monitoring\n  * Custom dashboards for\
      \ business intelligence\n  * Real-time alerting for anomaly detection\n- Big\
      \ Data Analytics:\n  * Data lake for historical analysis\n  * ETL pipelines\
      \ for data warehouse\n  * Machine learning for trend prediction\n  * Privacy-compliant\
      \ data processing\n\nAdvanced Implementation Details:\n\nDatabase Sharding Strategy\
      \ (Day 15 Integration):\n- User Sharding:\n  * Consistent hashing with virtual\
      \ nodes\n  * Cross-shard relationship queries optimization\n  * Hot celebrity\
      \ shard splitting mechanisms\n  * Global secondary indexes for search\n- Content\
      \ Sharding:\n  * Time + geography based partitioning\n  * Content locality for\
      \ regional preferences\n  * Automated resharding based on growth\n  * Cross-shard\
      \ analytics aggregation\n\nCDN and Global Distribution (Day 16 Integration):\n\
      - Edge Computing:\n  * Timeline generation at edge locations\n  * Image resizing\
      \ and optimization at edge\n  * Personalization logic deployment\n  * Real-time\
      \ content filtering\n- Performance Optimization:\n  * Prefetching based on user\
      \ behavior\n  * Intelligent caching with ML predictions\n  * Network-aware content\
      \ delivery\n  * Mobile-optimized content serving\n\nSearch System Architecture\
      \ (Day 17 Integration):\n- Distributed Indexing:\n  * Real-time document indexing\
      \ pipeline\n  * Incremental index updates\n  * Cross-datacenter index replication\n\
      \  * Search result personalization\n- Advanced Search Features:\n  * Multi-modal\
      \ search (text, image, video)\n  * Semantic search with embeddings\n  * Real-time\
      \ trending search suggestions\n  * Privacy-preserving search personalization\n\
      \nBig Data Processing (Day 18 Integration):\n- Stream Processing:\n  * Real-time\
      \ engagement scoring\n  * Trend detection and viral content identification\n\
      \  * Abuse detection and content filtering\n  * User behavior pattern analysis\n\
      - Batch Processing:\n  * Daily recommendation model retraining\n  * Historical\
      \ analytics and reporting\n  * Data quality checks and validation\n  * Long-term\
      \ trend analysis\n\nNoSQL Database Strategy (Day 19 Integration):\n- Multi-Database\
      \ Architecture:\n  * MongoDB: User profiles, post content, rich objects\n  *\
      \ Cassandra: Activity feeds, time-series data, high-write workloads\n  * Redis:\
      \ Caching, session management, real-time features\n  * Neo4j: Social graph,\
      \ recommendation graphs, influence networks\n  * InfluxDB: Metrics, monitoring,\
      \ time-series analytics\n- Consistency and Performance:\n  * Eventually consistent\
      \ user feeds\n  * Strong consistency for financial transactions\n  * Tunable\
      \ consistency based on use case\n  * Cross-database transaction coordination\n\
      \nScalability and Performance Optimizations:\n\n1. Hot User Problem Resolution:\n\
      - Celebrity Timeline Optimization:\n  * Dedicated infrastructure for high-follower\
      \ accounts\n  * Fan-out limitations and sampling strategies\n  * Push notification\
      \ rate limiting\n  * Separate processing queues for viral content\n- Load Distribution:\n\
      \  * Geographic load balancing\n  * Peak hour traffic prediction and scaling\n\
      \  * Elastic resource allocation\n  * Circuit breakers for cascade failure prevention\n\
      \n2. Real-time Processing Challenges:\n- Stream Processing Optimization:\n \
      \ * Exactly-once processing guarantees\n  * Late-arriving data handling\n  *\
      \ Backpressure management\n  * Stateful processing with checkpoints\n- Data\
      \ Consistency:\n  * Event sourcing for audit trails\n  * CQRS for read/write\
      \ optimization\n  * Saga pattern for distributed transactions\n  * Conflict\
      \ resolution for concurrent updates\n\n3. Global Distribution Strategy:\n- Multi-Region\
      \ Architecture:\n  * Active-active deployment across regions\n  * Data locality\
      \ optimization\n  * Cross-region replication strategies\n  * Disaster recovery\
      \ and failover procedures\n- Network Optimization:\n  * Anycast routing for\
      \ global load balancing\n  * TCP optimization for long-distance connections\n\
      \  * Compression and protocol optimization\n  * Mobile network adaptation\n\n\
      Security and Privacy Considerations:\n\n1. Data Protection:\n- Privacy by Design:\n\
      \  * GDPR and CCPA compliance\n  * User data anonymization\n  * Right to be\
      \ forgotten implementation\n  * Consent management systems\n- Content Security:\n\
      \  * End-to-end encryption for private messages\n  * Content moderation and\
      \ abuse detection\n  * NSFW content filtering\n  * Spam and bot detection systems\n\
      \n2. System Security:\n- Access Control:\n  * OAuth 2.0 for third-party integrations\n\
      \  * Multi-factor authentication\n  * API rate limiting and DDoS protection\n\
      \  * Zero-trust security architecture\n- Infrastructure Security:\n  * Network\
      \ segmentation and firewalls\n  * Infrastructure as code with security scanning\n\
      \  * Secrets management and rotation\n  * Security monitoring and incident response\n\
      \nMonitoring and Observability:\n\n1. System Monitoring:\n- Comprehensive Metrics:\n\
      \  * Application performance monitoring (APM)\n  * Infrastructure monitoring\
      \ and alerting\n  * Business metric tracking and dashboards\n  * User experience\
      \ monitoring\n- Distributed Tracing:\n  * Request tracing across microservices\n\
      \  * Performance bottleneck identification\n  * Error tracking and debugging\n\
      \  * Capacity planning and optimization\n\n2. Business Intelligence:\n- Analytics\
      \ Pipeline:\n  * Real-time business dashboards\n  * User engagement analysis\n\
      \  * Content performance tracking\n  * Revenue and growth metrics\n- Machine\
      \ Learning Operations:\n  * Model performance monitoring\n  * Feature drift\
      \ detection\n  * A/B testing statistical analysis\n  * Recommendation system\
      \ optimization\n\nCost Optimization Strategies:\n\n1. Infrastructure Efficiency:\n\
      - Resource Optimization:\n  * Auto-scaling based on traffic patterns\n  * Spot\
      \ instance utilization for batch jobs\n  * Storage tiering for cost optimization\n\
      \  * Network traffic optimization\n- Technology Choices:\n  * Open source vs\
      \ managed service trade-offs\n  * Multi-cloud strategy for cost arbitrage\n\
      \  * Reserved capacity planning\n  * Development vs production environment optimization\n\
      \n2. Operational Efficiency:\n- Automation and DevOps:\n  * Infrastructure as\
      \ code deployment\n  * Automated testing and validation\n  * Continuous integration\
      \ and deployment\n  * Chaos engineering for resilience testing\n- Team Productivity:\n\
      \  * Developer productivity tools\n  * Monitoring and alerting automation\n\
      \  * Self-service platforms for teams\n  * Documentation and knowledge sharing\n\
      \nPerformance Metrics and SLAs:\n\nService Level Objectives:\n- Timeline Loading:\
      \ 95th percentile < 200ms globally\n- Search Response: 99th percentile < 100ms\n\
      - Post Publishing: 95th percentile < 500ms end-to-end\n- Notification Delivery:\
      \ 90% within 5 seconds\n- System Availability: 99.9% uptime (8.76 hours/year\
      \ downtime)\n- Data Durability: 99.999999999% (11 9's)\n\nCapacity Planning:\n\
      - User Growth: 20% year-over-year growth planning\n- Content Growth: 25% year-over-year\
      \ growth planning\n- Peak Traffic Handling: 3x average traffic capacity\n- Storage\
      \ Scaling: Automatic expansion with 6-month forecasting\n- Network Capacity:\
      \ Global backbone with 2x overprovisioning\n\nFuture Enhancements and Innovation:\n\
      \n1. Emerging Technologies:\n- AI/ML Integration:\n  * GPT-powered content generation\
      \ assistance\n  * Computer vision for automated content tagging\n  * Natural\
      \ language processing for sentiment analysis\n  * Reinforcement learning for\
      \ optimization\n- Next-Generation Features:\n  * AR/VR content support and processing\n\
      \  * Blockchain integration for creator monetization\n  * Edge AI for real-time\
      \ content analysis\n  * Quantum-resistant cryptography preparation\n\n2. Platform\
      \ Evolution:\n- Technical Debt Management:\n  * Legacy system migration strategies\n\
      \  * API versioning and backward compatibility\n  * Database migration with\
      \ zero downtime\n  * Technology stack modernization roadmap\n- Innovation Pipeline:\n\
      \  * Experimental feature development framework\n  * Technology evaluation and\
      \ adoption process\n  * Open source contribution strategy\n  * Research and\
      \ development partnerships\n\nPractice Questions and Exercises:\n\nArchitecture\
      \ Design Questions:\n1. \"Design database sharding strategy for 2B users with\
      \ uneven geographic distribution. Handle hot celebrity accounts and cross-shard\
      \ social graph queries.\"\n2. \"Implement CDN strategy for global media delivery\
      \ with 10TB daily uploads. Optimize for mobile users in emerging markets with\
      \ limited bandwidth.\"\n3. \"Design search system handling 1M queries/second\
      \ with personalized results. Support real-time indexing of 500M daily posts\
      \ with sub-second latency.\"\n4. \"Build recommendation pipeline processing\
      \ 50B daily interactions. Integrate collaborative filtering, content analysis,\
      \ and social signals for personalization.\"\n\nScalability Challenges:\n1. \"\
      Handle viral content spreading to 100M users in 1 hour. Design systems to prevent\
      \ cascade failures while maintaining real-time updates.\"\n2. \"Implement timeline\
      \ generation for user with 100M followers. Balance fan-out costs, latency requirements,\
      \ and storage constraints.\"\n3. \"Design cross-region consistency strategy\
      \ for global social media platform. Handle network partitions while maintaining\
      \ user experience.\"\n4. \"Optimize search performance for trending queries\
      \ during major events. Handle 10x traffic spikes while maintaining sub-100ms\
      \ response times.\"\n\nTechnology Integration:\n1. \"Compare NoSQL database\
      \ choices for different components: user profiles, content storage, activity\
      \ feeds, and analytics. Justify selection criteria.\"\n2. \"Design big data\
      \ processing pipeline integrating Kafka, Flink, and Spark. Handle real-time\
      \ features, batch analytics, and ML model training.\"\n3. \"Implement multi-tier\
      \ caching strategy using Redis, CDN, and application caches. Optimize for different\
      \ content types and access patterns.\"\n4. \"Build monitoring and alerting system\
      \ for distributed social media platform. Include business metrics, technical\
      \ metrics, and user experience monitoring.\"\n"
    resources:
    - title: Instagram Engineering Blog
      url: https://instagram-engineering.com/
      description: Real-world social media scaling challenges and solutions
    - title: Facebook (Meta) Engineering Blog
      url: https://engineering.fb.com/
      description: Large-scale social platform architecture insights
    - title: Twitter Engineering Blog
      url: https://blog.twitter.com/engineering
      description: Real-time social media system design patterns
    - title: Pinterest Engineering Blog
      url: https://medium.com/@Pinterest_Engineering
      description: Visual content platform scaling strategies
    - title: LinkedIn Engineering Blog
      url: https://engineering.linkedin.com/
      description: Professional social network architecture
    - title: 'High Scalability: Social Media Cases'
      url: http://highscalability.com/
      description: Collection of social media scaling case studies
    - title: System Design Interview Book
      url: https://www.amazon.com/System-Design-Interview-insiders-Second/dp/B08CMF2CQF
      description: Social media system design interview preparation
    - title: Designing Data-Intensive Applications
      url: https://dataintensive.net/
      description: Foundational concepts for large-scale system design
    time_estimate: 180
    video_resources:
    - title: 'ByteByteGo: Design Stock Exchange'
      url: https://www.youtube.com/@ByteByteGo
      duration: 20 min
      description: High-throughput trading system
      priority: high
    - title: 'Hello Interview: Design Payment System'
      url: https://www.youtube.com/@HelloInterview
      duration: 25 min
      description: Financial transactions at scale
      priority: high
  week4:
  - day: 22
    topic: Security & Authentication
    activity: Master enterprise-grade security architecture, authentication systems,
      authorization frameworks, threat modeling, and defense strategies for large-scale
      distributed systems.
    detailed_content: "Enterprise Security Architecture:\n\n1. Security-First Design\
      \ Principles:\n- Zero Trust Architecture:\n  * Never trust, always verify principle\n\
      \  * Network segmentation and micro-perimeters\n  * Identity-based security\
      \ boundaries\n  * Continuous verification and monitoring\n- Defense in Depth:\n\
      \  * Multiple security layers and controls\n  * Fail-safe and fail-secure designs\n\
      \  * Redundant security mechanisms\n  * Compartmentalization and isolation\n\
      - Principle of Least Privilege:\n  * Minimum necessary access rights\n  * Time-bound\
      \ and context-aware permissions\n  * Regular access reviews and audits\n  *\
      \ Automatic privilege escalation controls\n\nAuthentication Systems and Mechanisms:\n\
      \n1. Multi-Factor Authentication (MFA):\n- Authentication Factors:\n  * Something\
      \ you know (passwords, PINs)\n  * Something you have (tokens, phones, cards)\n\
      \  * Something you are (biometrics, behavior)\n  * Somewhere you are (location,\
      \ device)\n- Implementation Strategies:\n  * TOTP (Time-based One-Time Passwords)\n\
      \  * Push notifications and mobile approval\n  * Hardware security keys (FIDO2/WebAuthn)\n\
      \  * Biometric authentication integration\n- Risk-based Authentication:\n  *\
      \ Adaptive authentication based on context\n  * Device fingerprinting and recognition\n\
      \  * Behavioral analytics and anomaly detection\n  * Geographic and temporal\
      \ risk assessment\n\n2. Enterprise Identity Management:\n- Single Sign-On (SSO):\n\
      \  * SAML 2.0 for enterprise federation\n  * OpenID Connect for modern applications\n\
      \  * Active Directory and LDAP integration\n  * Cross-domain authentication\
      \ and trust\n- Identity Providers (IdP):\n  * Centralized identity management\n\
      \  * User provisioning and deprovisioning\n  * Group membership and role assignment\n\
      \  * Identity lifecycle management\n- Federation and Trust:\n  * Cross-organizational\
      \ authentication\n  * Identity provider federation\n  * Trust relationships\
      \ and metadata exchange\n  * Certificate-based authentication\n\n3. Modern Authentication\
      \ Protocols:\n- OAuth 2.0 and Extensions:\n  * Authorization code flow with\
      \ PKCE\n  * Client credentials for service-to-service\n  * Device authorization\
      \ flow for IoT\n  * Token introspection and revocation\n- JWT (JSON Web Tokens):\n\
      \  * Stateless authentication and claims\n  * Token structure (header, payload,\
      \ signature)\n  * Signing algorithms (RS256, ES256, HS256)\n  * Token validation\
      \ and security considerations\n- OpenID Connect:\n  * Identity layer on top\
      \ of OAuth 2.0\n  * ID tokens and userinfo endpoint\n  * Discovery and dynamic\
      \ registration\n  * Session management and logout\n\nAuthorization and Access\
      \ Control:\n\n1. Access Control Models:\n- Role-Based Access Control (RBAC):\n\
      \  * Role hierarchy and inheritance\n  * Permission assignment and management\n\
      \  * Static vs dynamic role assignment\n  * Role explosion and management challenges\n\
      - Attribute-Based Access Control (ABAC):\n  * Policy-based access decisions\n\
      \  * Subject, resource, action, and environment attributes\n  * Fine-grained\
      \ and context-aware permissions\n  * XACML (eXtensible Access Control Markup\
      \ Language)\n- Relationship-Based Access Control (ReBAC):\n  * Graph-based access\
      \ control models\n  * Social relationships and ownership\n  * Dynamic permission\
      \ calculation\n  * Zanzibar-inspired authorization systems\n\n2. Authorization\
      \ Patterns and Implementation:\n- Policy Decision Points (PDP):\n  * Centralized\
      \ policy evaluation\n  * Policy engines and rule management\n  * Decision caching\
      \ and performance optimization\n  * Policy versioning and deployment\n- Policy\
      \ Enforcement Points (PEP):\n  * Distributed enforcement mechanisms\n  * API\
      \ gateways and middleware integration\n  * Fine-grained resource protection\n\
      \  * Audit trail and compliance logging\n- Resource-Level Authorization:\n \
      \ * Object-level permissions\n  * Field-level access controls\n  * Dynamic resource\
      \ ownership\n  * Hierarchical resource structures\n\nCryptography and Data Protection:\n\
      \n1. Encryption at Rest and in Transit:\n- Data-at-Rest Encryption:\n  * Database-level\
      \ encryption (TDE)\n  * File system and disk encryption\n  * Application-level\
      \ field encryption\n  * Key management and rotation strategies\n- Data-in-Transit\
      \ Protection:\n  * TLS 1.3 for secure communications\n  * Perfect Forward Secrecy\
      \ (PFS)\n  * Certificate management and validation\n  * End-to-end encryption\
      \ for sensitive data\n- Advanced Encryption Techniques:\n  * Homomorphic encryption\
      \ for computation on encrypted data\n  * Format-preserving encryption (FPE)\n\
      \  * Searchable encryption for databases\n  * Secure multi-party computation\n\
      \n2. Key Management and PKI:\n- Key Management Systems (KMS):\n  * Hardware\
      \ Security Modules (HSM)\n  * Key generation and distribution\n  * Key rotation\
      \ and lifecycle management\n  * Envelope encryption and key hierarchy\n- Public\
      \ Key Infrastructure (PKI):\n  * Certificate authorities and trust chains\n\
      \  * Certificate lifecycle management\n  * Certificate transparency and monitoring\n\
      \  * Code signing and document encryption\n- Secrets Management:\n  * Centralized\
      \ secrets storage (Vault, AWS Secrets Manager)\n  * Dynamic secrets and short-lived\
      \ credentials\n  * Secrets rotation and versioning\n  * Application integration\
      \ patterns\n\nSecurity Threats and Mitigation:\n\n1. Common Attack Vectors:\n\
      - OWASP Top 10 Mitigation:\n  * Injection attacks (SQL, NoSQL, LDAP, OS)\n \
      \ * Broken authentication and session management\n  * Sensitive data exposure\
      \ and inadequate encryption\n  * XML external entities (XXE) and deserialization\n\
      \  * Security misconfiguration and known vulnerabilities\n- Advanced Persistent\
      \ Threats (APT):\n  * Multi-stage attack detection\n  * Lateral movement prevention\n\
      \  * Command and control communication blocking\n  * Data exfiltration monitoring\n\
      - Application Security:\n  * Input validation and sanitization\n  * Output encoding\
      \ and CSRF protection\n  * Secure coding practices and code review\n  * Dynamic\
      \ and static application security testing\n\n2. DDoS Protection and Rate Limiting:\n\
      - DDoS Mitigation Strategies:\n  * Traffic analysis and pattern recognition\n\
      \  * Geo-blocking and IP reputation filtering\n  * Content delivery network\
      \ (CDN) protection\n  * Upstream provider and ISP coordination\n- Rate Limiting\
      \ Implementation:\n  * Token bucket and leaky bucket algorithms\n  * Distributed\
      \ rate limiting across services\n  * User-based and IP-based limiting\n  * Adaptive\
      \ rate limiting based on system load\n- API Security:\n  * API authentication\
      \ and authorization\n  * Request signing and verification\n  * API versioning\
      \ and backward compatibility\n  * API gateway security features\n\nReal-world\
      \ Security Implementation Examples:\n\n1. Google's Zero Trust Model (BeyondCorp):\n\
      - Architecture Components:\n  * Device inventory and trust assessment\n  * User\
      \ and device authentication\n  * Network access proxy and enforcement\n  * Risk-based\
      \ access decisions\n- Implementation Principles:\n  * Remove network location\
      \ trust\n  * Encrypt and authenticate all communications\n  * Grant access based\
      \ on device and user state\n  * Continuous monitoring and validation\n\n2. Netflix\
      \ Security Architecture:\n- Cloud Security Strategy:\n  * Immutable infrastructure\
      \ and deployment\n  * Security monitoring and incident response\n  * Data classification\
      \ and protection\n  * Third-party risk management\n- Application Security:\n\
      \  * Security-focused microservices design\n  * Automated security testing in\
      \ CI/CD\n  * Runtime application self-protection (RASP)\n  * Security chaos\
      \ engineering\n\n3. Facebook (Meta) Security Systems:\n- Scale Security Challenges:\n\
      \  * 3 billion user authentication\n  * Real-time threat detection and response\n\
      \  * Privacy-preserving data processing\n  * Global regulatory compliance\n\
      - Technical Solutions:\n  * Machine learning for fraud detection\n  * Differential\
      \ privacy for analytics\n  * Secure computation for cross-platform data\n  *\
      \ Automated security policy enforcement\n\n4. Amazon Web Services (AWS) Security:\n\
      - Shared Responsibility Model:\n  * AWS infrastructure security responsibilities\n\
      \  * Customer application and data security\n  * Service-specific security configurations\n\
      \  * Compliance framework alignment\n- Security Services Integration:\n  * Identity\
      \ and Access Management (IAM)\n  * Key Management Service (KMS)\n  * Web Application\
      \ Firewall (WAF)\n  * Security Hub and GuardDuty threat detection\n\nSecurity\
      \ Monitoring and Incident Response:\n\n1. Security Information and Event Management\
      \ (SIEM):\n- Log Collection and Analysis:\n  * Centralized log aggregation and\
      \ correlation\n  * Real-time security event detection\n  * Threat intelligence\
      \ integration\n  * Machine learning for anomaly detection\n- Incident Detection\
      \ and Response:\n  * Security orchestration and automated response (SOAR)\n\
      \  * Threat hunting and investigation workflows\n  * Incident classification\
      \ and prioritization\n  * Forensic data collection and preservation\n\n2. Compliance\
      \ and Governance:\n- Regulatory Compliance:\n  * GDPR, CCPA, and privacy regulations\n\
      \  * SOX, HIPAA, and industry-specific requirements\n  * PCI DSS for payment\
      \ card data\n  * ISO 27001 and security management systems\n- Security Governance:\n\
      \  * Security policy development and enforcement\n  * Risk assessment and management\
      \ processes\n  * Security awareness training and culture\n  * Third-party security\
      \ assessments\n\nPerformance and Scalability Considerations:\n\n1. Authentication\
      \ Performance Optimization:\n- Caching Strategies:\n  * Token validation caching\n\
      \  * User session state management\n  * Permission evaluation caching\n  * Distributed\
      \ cache consistency\n- Load Balancing:\n  * Authentication service scaling\n\
      \  * Session affinity and stateless design\n  * Geographic distribution of auth\
      \ services\n  * Failover and disaster recovery\n\n2. Security vs Performance\
      \ Trade-offs:\n- Optimization Techniques:\n  * Asymmetric vs symmetric encryption\
      \ choices\n  * Hardware acceleration for cryptographic operations\n  * Batching\
      \ and bulk operations for authorization\n  * Lazy loading and just-in-time permission\
      \ evaluation\n- Monitoring and Alerting:\n  * Security metric collection and\
      \ analysis\n  * Performance impact measurement\n  * Security debt tracking and\
      \ remediation\n  * Cost-benefit analysis for security investments\n\nDevSecOps\
      \ and Security Automation:\n\n1. Security in CI/CD Pipelines:\n- Shift-Left\
      \ Security:\n  * Static Application Security Testing (SAST)\n  * Dynamic Application\
      \ Security Testing (DAST)\n  * Software Composition Analysis (SCA)\n  * Infrastructure\
      \ as Code security scanning\n- Automated Security Testing:\n  * Unit tests for\
      \ security functions\n  * Integration tests for authentication flows\n  * End-to-end\
      \ security scenario testing\n  * Penetration testing automation\n\n2. Infrastructure\
      \ Security Automation:\n- Configuration Management:\n  * Security baseline enforcement\n\
      \  * Compliance scanning and remediation\n  * Vulnerability management automation\n\
      \  * Patch management and deployment\n- Runtime Security:\n  * Container security\
      \ and image scanning\n  * Runtime application protection\n  * Network security\
      \ monitoring\n  * Behavioral analysis and alerting\n\nPractice Questions:\n\n\
      Architecture Design:\n1. \"Design authentication system for global SaaS platform:\
      \ 100M users, 1000 API calls/second, multi-region deployment. Include SSO, MFA,\
      \ and fraud detection.\"\n2. \"Implement authorization framework for microservices\
      \ architecture: 50 services, fine-grained permissions, cross-service calls.\
      \ Design policy engine and enforcement.\"\n3. \"Build security architecture\
      \ for financial trading platform: Real-time transactions, regulatory compliance,\
      \ fraud prevention, and audit requirements.\"\n\nSecurity Implementation:\n\
      1. \"Design DDoS protection for e-commerce platform during Black Friday: Handle\
      \ 100x traffic increase while maintaining legitimate user access.\"\n2. \"Implement\
      \ zero-trust network architecture for remote workforce: Device trust, identity\
      \ verification, and secure access to internal resources.\"\n3. \"Build data\
      \ privacy system for social media platform: GDPR compliance, user consent management,\
      \ and data anonymization at scale.\"\n\nThreat Modeling:\n1. \"Analyze attack\
      \ vectors for mobile banking application: Identify threats, assess risks, and\
      \ design countermeasures for each attack path.\"\n2. \"Design security monitoring\
      \ for cloud infrastructure: Detect insider threats, advanced persistent threats,\
      \ and automated attacks.\"\n3. \"Implement secure communication protocol for\
      \ IoT devices: Handle millions of devices, over-the-air updates, and device\
      \ authentication.\"\n\nCompliance and Governance:\n1. \"Design audit system\
      \ for healthcare platform: HIPAA compliance, access logging, data lineage tracking,\
      \ and regulatory reporting.\"\n2. \"Implement security governance framework\
      \ for startup scaling from 100 to 10,000 employees: Policy development, training,\
      \ and enforcement.\"\n3. \"Build privacy-preserving analytics system: Collect\
      \ user behavior data while maintaining individual privacy and regulatory compliance.\"\
      \n"
    resources:
    - title: OWASP Top 10 Security Risks
      url: https://owasp.org/www-project-top-ten/
      description: Most critical web application security risks
    - title: NIST Cybersecurity Framework
      url: https://www.nist.gov/cyberframework
      description: Comprehensive cybersecurity guidance and standards
    - title: OAuth 2.0 Security Best Practices
      url: https://tools.ietf.org/html/draft-ietf-oauth-security-topics
      description: Security considerations for OAuth 2.0 implementations
    - title: Zero Trust Architecture - NIST SP 800-207
      url: https://csrc.nist.gov/publications/detail/sp/800-207/final
      description: Zero trust architecture principles and implementation
    - title: Google BeyondCorp Papers
      url: https://research.google/pubs/?area=security
      description: Zero trust network architecture research and implementation
    - title: SANS Security Architecture
      url: https://www.sans.org/white-papers/
      description: Enterprise security architecture best practices
    - title: Designing Secure Systems (Microsoft)
      url: https://docs.microsoft.com/en-us/azure/architecture/framework/security/
      description: Cloud security architecture patterns and practices
    - title: Application Security Verification Standard (ASVS)
      url: https://owasp.org/www-project-application-security-verification-standard/
      description: Comprehensive application security requirements framework
    time_estimate: 140
    video_resources:
    - title: 'ByteByteGo: Design Netflix'
      url: https://www.youtube.com/@ByteByteGo
      duration: 20 min
      description: Video streaming at massive scale
      priority: high
    - title: 'Hello Interview: Content Delivery at Scale'
      url: https://www.youtube.com/@HelloInterview
      duration: 18 min
      description: CDN and caching strategies
      priority: medium
  - day: 23
    topic: Monitoring & Observability
    activity: Master comprehensive observability architecture, monitoring strategies,
      SLI/SLO frameworks, alerting systems, and operational excellence for large-scale
      distributed systems.
    detailed_content: "Modern Observability Architecture:\n\n1. Three Pillars of Observability:\n\
      - Metrics (What is Happening):\n  * Quantitative measurements of system behavior\n\
      \  * Time-series data for trend analysis\n  * Aggregatable data for statistical\
      \ analysis\n  * Low storage overhead and fast queries\n  * Examples: CPU usage,\
      \ request rate, error count\n- Logs (What Happened):\n  * Discrete events with\
      \ timestamps and context\n  * Detailed information about specific occurrences\n\
      \  * Searchable and filterable event records\n  * High storage overhead but\
      \ rich information\n  * Examples: Error messages, audit trails, debug info\n\
      - Traces (How Things Happen):\n  * Request flow through distributed systems\n\
      \  * Causal relationships between service calls\n  * Performance bottleneck\
      \ identification\n  * End-to-end transaction visibility\n  * Examples: User\
      \ request journey, dependency mapping\n\n2. Modern Extensions to Observability:\n\
      - Profiles (Why Things Happen):\n  * Continuous profiling of application performance\n\
      \  * CPU, memory, and resource usage analysis\n  * Code-level performance optimization\
      \ insights\n  * Production performance debugging\n- Service Maps and Topology:\n\
      \  * Real-time service dependency visualization\n  * Service health and performance\
      \ mapping\n  * Critical path identification\n  * Impact analysis for incidents\n\
      - User Experience Monitoring:\n  * Real user monitoring (RUM) data\n  * Synthetic\
      \ transaction monitoring\n  * Core web vitals and performance metrics\n  * Business-impact\
      \ correlation\n\nMetrics and Time-Series Systems:\n\n1. Metrics Design and Collection:\n\
      - Metric Types and Patterns:\n  * Counter: Monotonically increasing values (requests,\
      \ errors)\n  * Gauge: Point-in-time values (memory usage, queue size)\n  * Histogram:\
      \ Distribution of values (latency, request size)\n  * Summary: Quantiles and\
      \ aggregations over time windows\n- High-Cardinality Challenges:\n  * Tag explosion\
      \ and storage costs\n  * Query performance degradation\n  * Cardinality limits\
      \ and management\n  * Sampling and aggregation strategies\n- Instrumentation\
      \ Best Practices:\n  * Consistent naming conventions\n  * Meaningful labels\
      \ and dimensions\n  * Appropriate metric granularity\n  * Performance impact\
      \ considerations\n\n2. Time-Series Database Architecture:\n- Prometheus Ecosystem:\n\
      \  * Pull-based metrics collection model\n  * Service discovery and target configuration\n\
      \  * PromQL for flexible querying and alerting\n  * Federation for multi-cluster\
      \ monitoring\n- Alternative Solutions:\n  * InfluxDB for high-write throughput\
      \ scenarios\n  * Datadog for unified observability platform\n  * New Relic for\
      \ application performance monitoring\n  * Grafana Cloud for managed time-series\
      \ storage\n- Scaling Strategies:\n  * Prometheus sharding and federation\n \
      \ * Long-term storage solutions (Cortex, Thanos)\n  * Downsampling and retention\
      \ policies\n  * Cross-datacenter replication\n\nLogging Architecture and Management:\n\
      \n1. Structured Logging Design:\n- Log Format Standards:\n  * JSON structured\
      \ logs for machine parsing\n  * Consistent field naming and data types\n  *\
      \ Hierarchical context and nested objects\n  * Timestamp standardization (ISO\
      \ 8601)\n- Contextual Information:\n  * Correlation IDs for request tracing\n\
      \  * User and session identifiers\n  * Service and version information\n  *\
      \ Environment and deployment context\n- Log Levels and Categorization:\n  *\
      \ Debug: Detailed diagnostic information\n  * Info: General operational information\n\
      \  * Warn: Potentially harmful situations\n  * Error: Error events with potential\
      \ impact\n  * Fatal: Severe errors causing termination\n\n2. Log Aggregation\
      \ and Processing:\n- ELK Stack (Elasticsearch, Logstash, Kibana):\n  * Logstash\
      \ for log processing and transformation\n  * Elasticsearch for distributed search\
      \ and storage\n  * Kibana for visualization and dashboards\n  * Beats for lightweight\
      \ data shippers\n- Modern Alternatives:\n  * Fluent Bit and Fluentd for log\
      \ forwarding\n  * Vector for high-performance log processing\n  * Loki for logs-as-metrics\
      \ approach\n  * Splunk for enterprise log management\n- Cloud-Native Solutions:\n\
      \  * AWS CloudWatch Logs and Insights\n  * Google Cloud Logging (formerly Stackdriver)\n\
      \  * Azure Monitor and Log Analytics\n  * Datadog Log Management\n\n3. Log Storage\
      \ and Retention:\n- Storage Optimization:\n  * Log compression and archival\
      \ strategies\n  * Hot, warm, and cold storage tiers\n  * Index lifecycle management\n\
      \  * Cost optimization techniques\n- Retention Policies:\n  * Compliance and\
      \ regulatory requirements\n  * Business value vs storage costs\n  * Automated\
      \ deletion and archival\n  * Data sovereignty considerations\n\nDistributed\
      \ Tracing Systems:\n\n1. OpenTelemetry Standards:\n- Instrumentation Framework:\n\
      \  * Auto-instrumentation for popular frameworks\n  * Manual instrumentation\
      \ APIs\n  * Semantic conventions for consistent data\n  * Multi-language SDK\
      \ support\n- Trace Data Model:\n  * Spans: Individual units of work\n  * Trace\
      \ context propagation\n  * Baggage for cross-cutting concerns\n  * Sampling\
      \ strategies and decisions\n- Backend Integration:\n  * Jaeger for trace collection\
      \ and analysis\n  * Zipkin for distributed tracing\n  * AWS X-Ray for cloud-native\
      \ tracing\n  * Datadog APM for application monitoring\n\n2. Tracing Implementation\
      \ Patterns:\n- Sampling Strategies:\n  * Head-based sampling at trace start\n\
      \  * Tail-based sampling after trace completion\n  * Adaptive sampling based\
      \ on system load\n  * Priority sampling for critical transactions\n- Context\
      \ Propagation:\n  * HTTP header propagation (W3C Trace Context)\n  * Message\
      \ queue context passing\n  * Database and cache correlation\n  * Cross-language\
      \ context sharing\n- Performance Considerations:\n  * Instrumentation overhead\
      \ minimization\n  * Asynchronous trace export\n  * Batching and compression\n\
      \  * Circuit breakers for trace collection\n\nService Level Management:\n\n\
      1. SLI/SLO/SLA Framework:\n- Service Level Indicators (SLIs):\n  * Request latency:\
      \ 95th percentile response time\n  * Availability: Successful requests ratio\n\
      \  * Throughput: Requests processed per second\n  * Quality: Accuracy and correctness\
      \ metrics\n- Service Level Objectives (SLOs):\n  * Target performance levels\
      \ for SLIs\n  * Error budgets and burn rate calculations\n  * Time window considerations\
      \ (rolling vs calendar)\n  * Multi-SLO objectives and dependencies\n- Service\
      \ Level Agreements (SLAs):\n  * Customer-facing commitments\n  * Financial penalties\
      \ and credits\n  * Legal and contractual implications\n  * SLA vs SLO alignment\
      \ strategies\n\n2. Error Budget Management:\n- Error Budget Calculation:\n \
      \ * Available vs consumed error budget\n  * Burn rate analysis and alerting\n\
      \  * Multi-window burn rate policies\n  * Budget recovery and reset strategies\n\
      - Policy Implementation:\n  * Development velocity vs reliability trade-offs\n\
      \  * Release freezes and rollback procedures\n  * Post-incident improvement\
      \ actions\n  * Reliability investment decisions\n\nAlerting and Incident Management:\n\
      \n1. Alerting Strategy and Design:\n- Alert Types and Patterns:\n  * Symptom-based\
      \ alerting (user impact)\n  * Cause-based alerting (system failures)\n  * Threshold-based\
      \ static alerts\n  * Anomaly detection dynamic alerts\n- Alert Quality Optimization:\n\
      \  * Signal vs noise ratio improvement\n  * Alert fatigue prevention\n  * Actionable\
      \ and specific alert content\n  * Context-rich alert information\n- Escalation\
      \ and Routing:\n  * Multi-tier escalation policies\n  * Team-based alert routing\n\
      \  * Time-based escalation rules\n  * Holiday and vacation coverage\n\n2. Advanced\
      \ Alerting Techniques:\n- Machine Learning for Anomaly Detection:\n  * Baseline\
      \ establishment and drift detection\n  * Seasonal pattern recognition\n  * Multi-dimensional\
      \ anomaly detection\n  * False positive reduction strategies\n- Correlation\
      \ and Suppression:\n  * Related alert grouping\n  * Root cause vs symptom identification\n\
      \  * Dependency-based alert suppression\n  * Maintenance window handling\n-\
      \ Predictive Alerting:\n  * Trend-based capacity alerts\n  * Resource exhaustion\
      \ prediction\n  * Seasonal demand forecasting\n  * Proactive scaling recommendations\n\
      \nReal-world Implementation Examples:\n\n1. Google SRE Monitoring Practices:\n\
      - Four Golden Signals:\n  * Latency: Time to process requests\n  * Traffic:\
      \ Demand on the system\n  * Errors: Rate of failed requests\n  * Saturation:\
      \ Resource utilization levels\n- Implementation Strategies:\n  * Borgmon monitoring\
      \ system (inspiration for Prometheus)\n  * Distributed tracing with Dapper\n\
      \  * SLO-based alerting and error budgets\n  * Postmortem culture and learning\n\
      \n2. Netflix Observability Platform:\n- Atlas Monitoring:\n  * Dimensional time-series\
      \ data platform\n  * High-cardinality metrics support\n  * Real-time streaming\
      \ analytics\n  * Automated anomaly detection\n- Distributed Tracing:\n  * Zipkin\
      \ for request flow analysis\n  * Chaos engineering observability\n  * Microservices\
      \ dependency mapping\n  * Performance optimization insights\n\n3. Uber's M3\
      \ Monitoring Stack:\n- M3 Time-Series Platform:\n  * Distributed time-series\
      \ database\n  * High availability and scalability\n  * Multi-region replication\n\
      \  * Cost-efficient storage aggregation\n- Observability Architecture:\n  *\
      \ Jaeger for distributed tracing\n  * ELK stack for log analysis\n  * Real-time\
      \ alerting and dashboards\n  * Business metric correlation\n\n4. Shopify Observability\
      \ Evolution:\n- Monitoring Modernization:\n  * Migration from legacy monitoring\n\
      \  * Kubernetes-native observability\n  * Multi-cloud monitoring strategy\n\
      \  * Developer self-service platforms\n- Business Metric Integration:\n  * Revenue\
      \ and conversion tracking\n  * Customer experience monitoring\n  * Merchant\
      \ success metrics\n  * Platform reliability correlation\n\nObservability Platform\
      \ Architecture:\n\n1. Unified Observability Platforms:\n- Single Pane of Glass:\n\
      \  * Correlated metrics, logs, and traces\n  * Unified query and visualization\n\
      \  * Cross-signal navigation and drilling\n  * Consistent user experience\n\
      - Data Correlation Strategies:\n  * Trace ID propagation across signals\n  *\
      \ Service and resource tagging\n  * Time-based correlation windows\n  * Machine\
      \ learning for relationship discovery\n- Multi-Tenancy and Access Control:\n\
      \  * Team-based data isolation\n  * Role-based access controls\n  * Data retention\
      \ policies per tenant\n  * Cost allocation and chargeback\n\n2. Observability\
      \ Data Pipeline:\n- Data Collection Architecture:\n  * Agent-based vs agentless\
      \ collection\n  * Push vs pull data models\n  * Edge processing and filtering\n\
      \  * Data format standardization\n- Stream Processing:\n  * Real-time data transformation\n\
      \  * Aggregation and enrichment\n  * Anomaly detection pipelines\n  * Alert\
      \ generation and routing\n- Storage and Indexing:\n  * Time-series database\
      \ optimization\n  * Log storage and search indexing\n  * Trace sampling and\
      \ storage\n  * Data lifecycle management\n\nPerformance and Cost Optimization:\n\
      \n1. Observability Data Management:\n- Sampling and Aggregation:\n  * Intelligent\
      \ sampling strategies\n  * Pre-aggregation for common queries\n  * Data downsampling\
      \ over time\n  * Cardinality reduction techniques\n- Storage Optimization:\n\
      \  * Compression algorithms and techniques\n  * Storage tiering and archival\n\
      \  * Query optimization and caching\n  * Infrastructure cost monitoring\n\n\
      2. Operational Efficiency:\n- Self-Service Capabilities:\n  * Developer-friendly\
      \ instrumentation\n  * Automated dashboard generation\n  * Template-based monitoring\
      \ setup\n  * Documentation and best practices\n- Automation and Integration:\n\
      \  * Infrastructure as code for monitoring\n  * CI/CD pipeline integration\n\
      \  * Automated testing for observability\n  * Continuous improvement processes\n\
      \nPractice Questions:\n\nArchitecture Design:\n1. \"Design observability platform\
      \ for microservices architecture: 100 services, 1M requests/second. Include\
      \ metrics, logs, traces, and alerting with <5% overhead.\"\n2. \"Build monitoring\
      \ system for global e-commerce platform: Multi-region deployment, business metrics\
      \ integration, and real-time anomaly detection.\"\n3. \"Implement SLO framework\
      \ for cloud platform: Define SLIs, error budgets, alerting policies, and incident\
      \ response for 99.9% availability target.\"\n\nScalability and Performance:\n\
      1. \"Design log aggregation system for 10TB daily logs: Handle peak ingestion,\
      \ enable real-time search, and optimize storage costs.\"\n2. \"Implement distributed\
      \ tracing for high-throughput system: 1M traces/second with 1% sampling rate,\
      \ minimize performance impact.\"\n3. \"Build metrics platform for IoT monitoring:\
      \ 1M devices, 100 metrics/device/minute, real-time dashboards and alerting.\"\
      \n\nOperational Excellence:\n1. \"Design alerting strategy for reducing alert\
      \ fatigue: Current 1000 alerts/day with 90% false positive rate, target actionable\
      \ alerts only.\"\n2. \"Implement observability for chaos engineering: Monitor\
      \ system behavior during failure injection, measure blast radius and recovery\
      \ time.\"\n3. \"Build correlation system linking business metrics to technical\
      \ metrics: Revenue impact analysis for system outages and performance degradation.\"\
      \n\nCost and Efficiency:\n1. \"Optimize observability costs for startup scaling\
      \ 10x: Balance monitoring coverage with budget constraints, prioritize critical\
      \ signals.\"\n2. \"Design multi-tenant observability platform: Isolated data\
      \ access, shared infrastructure, cost allocation per team/product.\"\n3. \"\
      Implement observability data lifecycle: Hot/warm/cold storage strategy, automated\
      \ archival, compliance requirements.\"\n"
    resources:
    - title: Site Reliability Engineering (Google)
      url: https://sre.google/sre-book/table-of-contents/
      description: Comprehensive SRE practices including monitoring and alerting
    - title: Observability Engineering (O'Reilly)
      url: https://www.oreilly.com/library/view/observability-engineering/9781492076438/
      description: Modern observability practices and tooling
    - title: Prometheus Monitoring Documentation
      url: https://prometheus.io/docs/
      description: Open-source monitoring system and time series database
    - title: OpenTelemetry Specification
      url: https://opentelemetry.io/docs/
      description: Vendor-neutral observability framework and standards
    - title: Distributed Systems Observability (O'Reilly)
      url: https://www.oreilly.com/library/view/distributed-systems-observability/9781492033431/
      description: Observability strategies for distributed architectures
    - title: The Art of Monitoring
      url: https://artofmonitoring.com/
      description: Practical monitoring implementation guide
    - title: Netflix Atlas Monitoring Platform
      url: https://netflix.github.io/atlas-docs/
      description: Dimensional time-series monitoring system
    - title: 'Google SRE Workbook: SLI/SLO Implementation'
      url: https://sre.google/workbook/implementing-slos/
      description: Practical SLI/SLO implementation guidance
    time_estimate: 130
    video_resources:
    - title: 'Hello Interview: Design Amazon'
      url: https://www.youtube.com/@HelloInterview
      duration: 40 min
      description: E-commerce platform architecture
      priority: high
    - title: 'ByteByteGo: Design Shopping Cart'
      url: https://www.youtube.com/@ByteByteGo
      duration: 12 min
      description: Cart management and checkout
      priority: medium
  - day: 24
    topic: Rate Limiting & Traffic Management
    activity: Master advanced rate limiting algorithms, traffic shaping, DDoS protection,
      and resilience patterns for high-scale distributed systems.
    detailed_content: "Rate Limiting Fundamentals and Algorithms:\n\n1. Core Rate\
      \ Limiting Algorithms:\n- Token Bucket Algorithm:\n  * Bucket capacity and token\
      \ refill rate\n  * Burst handling and sustained rate control\n  * Implementation\
      \ with timestamps and counters\n  * Memory efficiency and performance characteristics\n\
      \  * Use cases: API rate limiting, network traffic shaping\n- Leaky Bucket Algorithm:\n\
      \  * Constant output rate regardless of input bursts\n  * Queue-based implementation\
      \ with overflow handling\n  * Smooth traffic shaping and congestion control\n\
      \  * Comparison with token bucket trade-offs\n  * Use cases: Network QoS, traffic\
      \ smoothing\n- Fixed Window Counter:\n  * Simple time-window based counting\n\
      \  * Reset behavior and boundary spike issues\n  * Memory and computational\
      \ efficiency\n  * Implementation with expiring counters\n  * Use cases: Basic\
      \ API limiting, simple quotas\n- Sliding Window Log:\n  * Precise rate limiting\
      \ with event timestamps\n  * Memory overhead for storing request logs\n  * Accurate\
      \ burst detection and control\n  * Scalability challenges with high request\
      \ rates\n  * Use cases: Premium API tiers, strict compliance\n- Sliding Window\
      \ Counter:\n  * Hybrid approach combining fixed windows\n  * Weighted calculation\
      \ for smoother transitions\n  * Balance between accuracy and efficiency\n  *\
      \ Implementation with multiple time buckets\n  * Use cases: Production API gateways,\
      \ fair usage\n\n2. Advanced Rate Limiting Techniques:\n- Adaptive Rate Limiting:\n\
      \  * Dynamic rate adjustment based on system load\n  * Machine learning for\
      \ traffic pattern recognition\n  * Predictive scaling and proactive limiting\n\
      \  * Performance metrics integration\n- Hierarchical Rate Limiting:\n  * Multi-level\
      \ rate limiting (global, tenant, user)\n  * Priority-based rate allocation\n\
      \  * Quota distribution and inheritance\n  * Fair sharing algorithms\n- Contextual\
      \ Rate Limiting:\n  * Request type and endpoint-specific limits\n  * Geographic\
      \ and time-based variations\n  * User tier and subscription-based limits\n \
      \ * Business rule integration\n\nDistributed Rate Limiting Architecture:\n\n\
      1. Centralized vs Distributed Approaches:\n- Centralized Rate Limiting:\n  *\
      \ Single source of truth for rate decisions\n  * Consistent global rate enforcement\n\
      \  * Single point of failure concerns\n  * Network latency and bottleneck issues\n\
      - Distributed Rate Limiting:\n  * Local rate limiting with global coordination\n\
      \  * Eventually consistent rate enforcement\n  * Improved performance and availability\n\
      \  * Synchronization and drift challenges\n- Hybrid Approaches:\n  * Local rate\
      \ limiting with periodic synchronization\n  * Emergency local limits with global\
      \ overrides\n  * Multi-tier rate limiting strategies\n  * Graceful degradation\
      \ during network partitions\n\n2. Implementation Patterns:\n- Redis-Based Distributed\
      \ Limiting:\n  * Lua scripts for atomic rate operations\n  * Redis Cluster for\
      \ high availability\n  * Key design and expiration strategies\n  * Performance\
      \ optimization techniques\n- Database-Based Rate Limiting:\n  * Time-series\
      \ tables for rate tracking\n  * Optimistic locking for concurrent updates\n\
      \  * Batch processing for efficiency\n  * Cleanup and maintenance strategies\n\
      - In-Memory Grid Solutions:\n  * Hazelcast and Apache Ignite integration\n \
      \ * Distributed counters and maps\n  * Replication and consistency models\n\
      \  * Performance and scalability characteristics\n\nDDoS Protection and Mitigation:\n\
      \n1. DDoS Attack Types and Characteristics:\n- Volumetric Attacks:\n  * UDP\
      \ floods and amplification attacks\n  * ICMP floods and network saturation\n\
      \  * Bandwidth exhaustion strategies\n  * Network-level detection and mitigation\n\
      - Protocol Attacks:\n  * SYN floods and TCP state exhaustion\n  * Ping of death\
      \ and fragmentation attacks\n  * Connection table exhaustion\n  * Protocol anomaly\
      \ detection\n- Application Layer Attacks:\n  * HTTP floods and slowloris attacks\n\
      \  * SQL injection and XSS amplification\n  * Resource exhaustion attacks\n\
      \  * Application-specific vulnerabilities\n\n2. Multi-Layer DDoS Protection:\n\
      - Network Layer Protection:\n  * BGP blackholing and null routing\n  * Upstream\
      \ provider coordination\n  * Anycast distribution and load spreading\n  * Network\
      \ appliance and firewall rules\n- Transport Layer Protection:\n  * SYN cookies\
      \ and TCP optimization\n  * Connection rate limiting\n  * Proxy and load balancer\
      \ protection\n  * Geographic and IP reputation filtering\n- Application Layer\
      \ Protection:\n  * Request validation and filtering\n  * CAPTCHA and human verification\n\
      \  * Behavioral analysis and bot detection\n  * Rate limiting and traffic shaping\n\
      \n3. Advanced Protection Techniques:\n- Machine Learning for Attack Detection:\n\
      \  * Traffic pattern analysis and anomaly detection\n  * Behavioral fingerprinting\
      \ and classification\n  * Real-time threat intelligence integration\n  * Adaptive\
      \ threshold adjustment\n- Proof of Work Challenges:\n  * Client-side computational\
      \ challenges\n  * Dynamic difficulty adjustment\n  * Resource exhaustion protection\n\
      \  * User experience balance\n- Traffic Fingerprinting:\n  * Request signature\
      \ analysis\n  * Device and browser fingerprinting\n  * Geolocation and timing\
      \ analysis\n  * Reputation scoring systems\n\nTraffic Management and Resilience\
      \ Patterns:\n\n1. Circuit Breaker Pattern:\n- Circuit States and Transitions:\n\
      \  * Closed: Normal operation with monitoring\n  * Open: Fast-fail to prevent\
      \ cascade failures\n  * Half-open: Gradual recovery testing\n  * State transition\
      \ thresholds and timeouts\n- Implementation Strategies:\n  * Failure rate and\
      \ latency-based triggers\n  * Exponential backoff and jitter\n  * Graceful degradation\
      \ responses\n  * Service mesh integration\n- Advanced Circuit Breaker Features:\n\
      \  * Per-endpoint and per-user circuit breakers\n  * Bulkhead isolation for\
      \ different resources\n  * Priority-based circuit breaking\n  * Monitoring and\
      \ alerting integration\n\n2. Bulkhead Pattern:\n- Resource Isolation Strategies:\n\
      \  * Thread pool isolation for different services\n  * Connection pool separation\n\
      \  * Memory and CPU resource partitioning\n  * Network bandwidth allocation\n\
      - Implementation Approaches:\n  * Container-based isolation\n  * Process-level\
      \ separation\n  * Application-level resource pools\n  * Cloud resource grouping\n\
      - Capacity Planning:\n  * Resource allocation optimization\n  * Load testing\
      \ and capacity modeling\n  * Dynamic resource scaling\n  * Cost-performance\
      \ trade-offs\n\n3. Load Shedding and Graceful Degradation:\n- Load Shedding\
      \ Techniques:\n  * Request priority classification\n  * Queue management and\
      \ dropping policies\n  * Sampling-based load reduction\n  * Feature toggle-based\
      \ shedding\n- Graceful Degradation Strategies:\n  * Non-essential feature disabling\n\
      \  * Simplified response modes\n  * Cached response serving\n  * Fallback service\
      \ integration\n- Quality of Service (QoS):\n  * Service level prioritization\n\
      \  * Resource reservation systems\n  * Dynamic priority adjustment\n  * SLA\
      \ enforcement mechanisms\n\nReal-world Implementation Examples:\n\n1. Netflix\
      \ Traffic Management:\n- Hystrix Circuit Breaker:\n  * Service-to-service protection\n\
      \  * Real-time failure detection\n  * Fallback mechanisms\n  * Dashboard and\
      \ monitoring\n- Zuul API Gateway:\n  * Dynamic routing and filtering\n  * Rate\
      \ limiting and throttling\n  * Request transformation\n  * Security policy enforcement\n\
      \n2. Twitter Rate Limiting:\n- Multi-Tier Rate Limiting:\n  * User-based and\
      \ application-based limits\n  * Endpoint-specific rate limits\n  * Time window\
      \ variations\n  * Premium tier allowances\n- Real-time Rate Enforcement:\n \
      \ * Redis-based distributed limiting\n  * Sub-second rate calculations\n  *\
      \ Burst handling strategies\n  * Rate limit headers and communication\n\n3.\
      \ Cloudflare DDoS Protection:\n- Global Anycast Network:\n  * Traffic distribution\
      \ across edge locations\n  * Local DDoS mitigation\n  * Capacity aggregation\n\
      \  * Attack source isolation\n- Intelligent Attack Detection:\n  * Machine learning\
      \ classification\n  * Real-time threat intelligence\n  * Behavioral analysis\n\
      \  * Automated mitigation rules\n\n4. GitHub API Rate Limiting:\n- Sophisticated\
      \ Rate Limiting:\n  * Multiple rate limit categories\n  * Search API special\
      \ handling\n  * Authenticated vs anonymous limits\n  * Abuse detection and response\n\
      - User Experience Optimization:\n  * Rate limit status communication\n  * Predictive\
      \ rate limit warnings\n  * Best practice documentation\n  * Developer tooling\
      \ integration\n\nTraffic Shaping and Quality of Service:\n\n1. Traffic Shaping\
      \ Algorithms:\n- Weighted Fair Queuing (WFQ):\n  * Bandwidth allocation based\
      \ on weights\n  * Fair sharing among competing flows\n  * Latency optimization\
      \ for high-priority traffic\n  * Implementation complexity considerations\n\
      - Class-Based Queuing (CBQ):\n  * Hierarchical traffic classification\n  * Multi-level\
      \ priority handling\n  * Bandwidth guarantees and limits\n  * Policy-based traffic\
      \ management\n- Random Early Detection (RED):\n  * Proactive congestion avoidance\n\
      \  * Probabilistic packet dropping\n  * Queue length management\n  * TCP-friendly\
      \ behavior\n\n2. Service Mesh Traffic Management:\n- Envoy Proxy Configuration:\n\
      \  * Rate limiting filter configuration\n  * Circuit breaker implementation\n\
      \  * Load balancing algorithms\n  * Traffic routing policies\n- Istio Traffic\
      \ Policies:\n  * DestinationRule rate limiting\n  * VirtualService routing rules\n\
      \  * Fault injection for testing\n  * Security policy integration\n- Linkerd\
      \ Traffic Management:\n  * Service profiles and routes\n  * Request-level load\
      \ balancing\n  * Retry and timeout policies\n  * Traffic splitting and canary\
      \ deployments\n\nPerformance Optimization and Scalability:\n\n1. Rate Limiting\
      \ Performance:\n- Memory Optimization:\n  * Efficient data structures for counters\n\
      \  * Memory pooling and recycling\n  * Garbage collection optimization\n  *\
      \ Cache-friendly algorithms\n- CPU Optimization:\n  * Lock-free data structures\n\
      \  * Batching and amortization\n  * Algorithmic complexity reduction\n  * Hardware\
      \ acceleration opportunities\n- Network Optimization:\n  * Local rate limiting\
      \ to reduce latency\n  * Batched coordination messages\n  * Compression and\
      \ protocol optimization\n  * Edge-based rate limiting\n\n2. Scalability Patterns:\n\
      - Horizontal Scaling:\n  * Stateless rate limiter design\n  * Consistent hashing\
      \ for distribution\n  * Auto-scaling based on traffic patterns\n  * Cross-region\
      \ coordination\n- Vertical Scaling:\n  * Multi-core optimization\n  * Memory\
      \ and storage scaling\n  * Hardware acceleration\n  * Performance profiling\
      \ and tuning\n\nMonitoring and Operations:\n\n1. Rate Limiting Metrics:\n- Key\
      \ Performance Indicators:\n  * Rate limit hit rates and violations\n  * Request\
      \ latency distribution\n  * Throughput and capacity utilization\n  * Error rates\
      \ and false positives\n- Business Impact Metrics:\n  * User experience degradation\n\
      \  * Revenue impact analysis\n  * Customer satisfaction correlation\n  * Competitive\
      \ benchmark comparison\n\n2. Operational Excellence:\n- Configuration Management:\n\
      \  * Dynamic rate limit adjustment\n  * A/B testing for rate policies\n  * Emergency\
      \ rate limit overrides\n  * Policy versioning and rollback\n- Incident Response:\n\
      \  * Automated DDoS response procedures\n  * Escalation policies and runbooks\n\
      \  * Communication and coordination\n  * Post-incident analysis and improvement\n\
      \nPractice Questions:\n\nAlgorithm Design:\n1. \"Design rate limiting system\
      \ for API gateway: 10K requests/second, per-user and global limits, distributed\
      \ across 50 servers. Choose appropriate algorithm and justify.\"\n2. \"Implement\
      \ adaptive rate limiting for e-commerce platform: Handle traffic spikes during\
      \ sales events, maintain performance for legitimate users.\"\n3. \"Build multi-tier\
      \ rate limiting for SaaS platform: Free, premium, enterprise tiers with different\
      \ limits, graceful degradation strategies.\"\n\nDDoS Protection:\n1. \"Design\
      \ DDoS protection for online gaming platform: Handle 1Tbps attacks, maintain\
      \ <50ms latency for legitimate players during attacks.\"\n2. \"Implement intelligent\
      \ bot detection system: Distinguish between legitimate crawlers, malicious bots,\
      \ and human traffic at scale.\"\n3. \"Build traffic analysis system: Real-time\
      \ anomaly detection, automatic mitigation, and false positive minimization.\"\
      \n\nResilience Patterns:\n1. \"Design circuit breaker system for microservices:\
      \ 100 services, cascading failure prevention, service dependency management.\"\
      \n2. \"Implement bulkhead pattern for multi-tenant system: Resource isolation,\
      \ fair sharing, performance guarantees per tenant.\"\n3. \"Build load shedding\
      \ mechanism for video streaming: Priority-based request handling, quality degradation,\
      \ capacity management.\"\n\nPerformance and Scale:\n1. \"Optimize rate limiting\
      \ performance: Handle 1M requests/second with <1ms overhead, memory-efficient\
      \ distributed counters.\"\n2. \"Design global rate limiting system: Multi-region\
      \ deployment, eventual consistency, network partition handling.\"\n3. \"Implement\
      \ traffic shaping for CDN: Bandwidth allocation, QoS policies, customer SLA\
      \ enforcement across global edge locations.\"\n"
    resources:
    - title: Rate Limiting Strategies (Google Cloud)
      url: https://cloud.google.com/architecture/rate-limiting-strategies-techniques
      description: Comprehensive rate limiting implementation strategies
    - title: AWS Shield DDoS Protection
      url: https://aws.amazon.com/shield/
      description: DDoS protection strategies and implementation
    - title: Netflix Hystrix Circuit Breaker
      url: https://github.com/Netflix/Hystrix/wiki
      description: Circuit breaker pattern implementation and best practices
    - title: Cloudflare DDoS Protection System
      url: https://blog.cloudflare.com/cloudflare-ddos-protection-system/
      description: Large-scale DDoS protection architecture
    - title: Envoy Rate Limiting Configuration
      url: https://www.envoyproxy.io/docs/envoy/latest/configuration/http/http_filters/rate_limit_filter
      description: Service mesh rate limiting implementation
    - title: Redis Rate Limiting Patterns
      url: https://redis.com/redis-best-practices/basic-rate-limiting/
      description: Distributed rate limiting with Redis
    - title: Traffic Management in Istio
      url: https://istio.io/latest/docs/concepts/traffic-management/
      description: Service mesh traffic management and policies
    - title: Resilience Engineering Patterns
      url: https://www.oreilly.com/library/view/release-it-2nd/9781680504552/
      description: Comprehensive resilience patterns and practices
    time_estimate: 120
    video_resources:
    - title: 'ByteByteGo: Design Dropbox'
      url: https://www.youtube.com/@ByteByteGo
      duration: 18 min
      description: File storage and sync system
      priority: high
    - title: 'Hello Interview: Design Google Drive'
      url: https://www.youtube.com/@HelloInterview
      duration: 30 min
      description: Cloud storage architecture
      priority: high
  - day: 25
    topic: Distributed Systems Consensus
    activity: Master consensus algorithms, leader election, distributed coordination,
      and Byzantine fault tolerance for building reliable distributed systems.
    detailed_content: 'Consensus Fundamentals:

      - The Consensus Problem: Achieving agreement among distributed processes despite
      failures

      - Safety Property: Never returning incorrect results (consistency)

      - Liveness Property: Eventually returning correct results (availability)

      - FLP Impossibility Theorem: Consensus impossible in asynchronous systems with
      one faulty process

      - Partial Synchrony: Real-world assumption for practical consensus algorithms


      Raft Consensus Algorithm:

      - Leader Election: Randomized timeouts, term numbers, majority votes

      - Log Replication: Append-only logs, consistency checks, commit index

      - Safety Guarantees: Leader completeness, state machine safety

      - Configuration Changes: Joint consensus for membership updates

      - Log Compaction: Snapshots to prevent unbounded log growth

      - Implementation Considerations: Heartbeats, election timeouts, batching

      - Real-world Usage: etcd (Kubernetes), TiKV (TiDB), CockroachDB Raft groups


      Paxos Family of Algorithms:

      - Basic Paxos: Single-decree consensus with prepare/promise/accept/accepted
      phases

      - Multi-Paxos: Optimization for multiple decisions with stable leader

      - Fast Paxos: Reduced latency by skipping prepare phase when possible

      - Flexible Paxos: Separate quorums for phase 1 and phase 2

      - EPaxos (Egalitarian): Leaderless consensus for geographic distribution

      - Roles and Phases: Proposers (leaders), Acceptors (voters), Learners (followers)

      - Production Examples: Google Chubby, Apache Cassandra (lightweight Paxos)


      Byzantine Fault Tolerance:

      - Byzantine Failures: Arbitrary behavior including malicious attacks

      - PBFT (Practical Byzantine Fault Tolerance): 3f+1 nodes to tolerate f failures

      - Three-phase Protocol: Pre-prepare, prepare, commit phases

      - View Changes: Leader replacement when primary is suspected faulty

      - Tendermint: Modern BFT consensus for blockchain applications

      - HotStuff: Linear communication complexity BFT with pipelining

      - Applications: Blockchain systems, critical infrastructure, financial systems


      Leader Election Patterns:

      - Bully Algorithm: Highest ID node becomes leader

      - Ring Algorithm: Token passing in logical ring topology

      - Raft Leader Election: Randomized timeouts, term-based voting

      - Apache Zookeeper: Ephemeral sequential nodes for leader election

      - Consul: Raft-based leadership with service discovery integration

      - Kubernetes Leader Election: Lease-based coordination using etcd


      Distributed Coordination Services:

      - Apache Zookeeper: Configuration management, service discovery, leader election

      - etcd: Kubernetes backing store, distributed key-value store

      - Consul: Service mesh coordination, health checking, KV store

      - Amazon DynamoDB Global Tables: Multi-region consensus via vector clocks

      - Google Spanner: TrueTime API for globally consistent timestamps

      - Coordination Primitives: Locks, barriers, queues, configuration management


      Implementation Challenges:

      - Network Partitions: Split-brain prevention, quorum-based decisions

      - Clock Synchronization: Logical clocks (Lamport, vector), physical clock skew

      - Failure Detection: Heartbeats, timeouts, phi-accrual failure detectors

      - Recovery and Catchup: Log replay, snapshot transfer, incremental updates

      - Performance Optimization: Batching, pipelining, read leases

      - Testing: Jepsen-style partition testing, chaos engineering, linearizability


      Trade-offs and Design Decisions:

      - Throughput vs Latency: Batching and pipelining effects

      - Fault Tolerance vs Performance: Replication factor impact

      - Consistency Models: Strong vs eventual consistency requirements

      - Geographical Distribution: Wide-area network considerations

      - Read vs Write Performance: Read replicas, leader leases

      - Operational Complexity: Monitoring, debugging, capacity planning


      Production Architecture Patterns:

      - Multi-Raft: Separate Raft groups per partition (TiKV, CockroachDB)

      - Hierarchical Consensus: Region-level and global-level coordination

      - Cross-Region Replication: Consensus across data centers

      - State Machine Replication: Application-level consistency via consensus

      - Consensus as a Service: Managed coordination services (etcd, Consul)

      - Hybrid Approaches: Combining different algorithms for different layers

      '
    practice_questions:
      capacity_estimation:
      - question: Design consensus system for 1000-node cluster with 10K ops/sec
        answer: 'Use Multi-Raft with 200 Raft groups (each 5 nodes). Per group: 50
          ops/sec = 1 leader + 4 followers. Leaders: 200 nodes (handle writes). Followers:
          800 nodes (replicas). Write amplification: 5x (1 leader + 4 followers) =
          50K total writes/sec. Log size: 1KB/op × 10K × 86400 = 864GB/day. Replication
          bandwidth: 1KB × 50K = 50MB/sec. Quorum: 3/5 nodes (majority). Latency:
          2 RTT (propose + commit) = 10ms same-region. Cost: $50K/month (1000 nodes
          × $50).'
      - question: Calculate quorum sizes for 5-region deployment with 3 nodes per
          region
        answer: 'Total: 15 nodes (5 regions × 3). For majority: Need ⌈15/2⌉ + 1 =
          8 nodes. Problem: Network partition splits regions. Solution: Use 3 nodes
          per region + cross-region quorum. Write quorum: 2 nodes in leader region
          + 1 node in 1 other region = 3 nodes (same as single-region, but cross-region
          latency). Read quorum: 1 node (leader serves reads with lease). Alternative:
          2-1-1-1-1 topology (2 nodes in primary, 1 in each other) = faster writes
          (only 2 local nodes needed for quorum), but less fault tolerance.'
      - question: Estimate bandwidth for Raft log replication with 1MB/sec write load
        answer: 'Replication factor: 3 (1 leader + 2 followers). Bandwidth: 1MB/sec
          × 3 = 3MB/sec total. Per follower: 1MB/sec. Network: Leader → Follower1
          (1MB/sec) + Leader → Follower2 (1MB/sec) = 2MB/sec outbound from leader.
          Compression: Gzip 3x = 667KB/sec per follower = 1.3MB/sec total (2.3x savings).
          Batching: 100 ops per batch reduces overhead by 10x. Snapshot transfer:
          Daily snapshot (10GB) = 116KB/sec avg. Total: 2MB/sec + 116KB/sec = 2.1MB/sec.'
      - question: Size etcd cluster for Kubernetes control plane with 10K nodes
        answer: 'etcd usage: Store cluster state (nodes, pods, services). Data: 10K
          nodes × 10KB = 100MB. Pods: 50K pods × 5KB = 250MB. ConfigMaps/Secrets:
          50MB. Total: 400MB. With history (1000 revisions): 400GB. Cluster: 5 nodes
          (leader + 4 followers). Quorum: 3/5. Hardware: 8 vCPU, 16GB RAM, 100GB SSD
          per node. Write rate: 5K ops/sec (pod updates). Read rate: 50K ops/sec (watch
          streams). Latency: 10ms writes (2 RTT), 1ms reads (local). Cost: $2K/month
          (5 nodes × $400).'
      conceptual:
      - question: Explain why FLP theorem doesn't prevent practical consensus systems
        answer: 'FLP theorem: Impossible to achieve consensus in asynchronous system
          with even 1 faulty node (proven by Fischer, Lynch, Paterson). Why practical
          systems work: 1) Assumption relaxation: Use partially synchronous model
          (bounded delays, not infinite). 2) Timeouts: Detect failures via heartbeats
          (not perfect but works 99.9%). 3) Randomization: Break determinism (e.g.,
          random leader election timeouts). 4) Practical trade-off: Accept temporary
          unavailability during partitions (CP systems) or accept weaker consistency
          (AP systems). Raft/Paxos: Partially synchronous + leader election with timeouts.
          Result: Consensus works in practice despite FLP.'
      - question: Compare Raft vs Paxos trade-offs for different use cases
        answer: 'Raft: Pros: Simpler to understand (log-based, strong leader), easier
          to implement, better for single-datacenter. Cons: Leader bottleneck, slower
          leader election (timeout-based). Use for: etcd, Consul, distributed databases
          (TiKV). Latency: 10ms. Paxos (Multi-Paxos): Pros: Faster leader election
          (no timeout), leader can be any node. Cons: Complex, harder to implement
          correctly. Use for: Google Chubby, Spanner. Latency: 5ms. EPaxos (Egalitarian
          Paxos): No leader, lower latency for cross-region (1 RTT). Use for: WAN
          consensus. Choose: Raft for simplicity, Paxos for performance.'
      - question: How does Byzantine fault tolerance change consensus requirements?
        answer: 'Crash fault tolerance (CFT): Nodes fail by stopping (Raft, Paxos).
          Tolerates f failures with 2f+1 nodes (majority). Byzantine fault tolerance
          (BFT): Nodes fail by sending malicious/incorrect messages. Tolerates f failures
          with 3f+1 nodes (vs 2f+1). Example: 1 Byzantine node needs 4 total nodes
          (vs 3 for crash). Reason: Need majority of honest nodes to override Byzantine
          nodes. PBFT algorithm: 3-phase commit (pre-prepare, prepare, commit) vs
          2-phase (Raft). Latency: 3-5× slower. Cost: 1.5× more nodes. Use cases:
          Blockchain (untrusted nodes), financial systems. Most systems: Use CFT (cheaper,
          faster, trusted environment).'
      - question: Why do most consensus algorithms require majority quorums?
        answer: 'Majority quorum (⌈N/2⌉ + 1): Ensures any two quorums overlap in at
          least 1 node = guarantees consistency. Example: 5 nodes, quorum = 3. Any
          2 quorums of 3 must share ≥1 node. Why critical: Prevents split-brain (two
          leaders with different state). Alternative: Minority quorums allow split-brain.
          Example: 2/5 quorum → 2 nodes accept write A, different 2 nodes accept write
          B = inconsistency. Trade-off: Majority = higher availability cost (need
          >50% nodes alive). Quorum intersection theorem: R + W > N guarantees consistency
          (read quorum + write quorum > total nodes).'
      - question: Explain the role of logical vs physical time in consensus
        answer: 'Physical time (wall clock): Actual time (NTP). Problem: Clock skew
          (10-100ms), clock drift. Can''t trust for ordering. Use: Timeouts, leases
          (with skew bounds). Logical time: Lamport timestamps, vector clocks. Guarantees:
          Causal ordering (if event A causes B, timestamp(A) < timestamp(B)). Use:
          Conflict resolution, ordering events without synchronized clocks. Hybrid:
          TrueTime (Google Spanner) = physical time with uncertainty bounds (<7ms).
          Use: Global ordering with bounded wait. Raft: Uses logical log indexes for
          ordering (not time). Result: No clock synchronization needed for correctness
          (only for performance/timeouts).'
      trade_offs:
      - question: 'Multi-Paxos vs Raft: when to choose each for your system?'
        answer: 'Multi-Paxos: Pros: Flexible leader (any node), faster leader election
          (no timeout), optimized for low latency (2 RTT vs 3). Cons: Complex implementation,
          edge cases hard to handle. Use for: Google (Chubby, Spanner), high-performance
          systems, cross-region WAN. Latency: 5ms. Raft: Pros: Simple, well-documented,
          easier to implement correctly, strong leader model. Cons: Slower leader
          election (timeout-based: 150-300ms), leader bottleneck. Use for: Most systems
          (etcd, Consul, TiKV, CockroachDB). Latency: 10ms. Choose Raft unless: Need
          absolute lowest latency or have expert team to implement Paxos correctly.'
      - question: 'Strong consistency vs performance: where to make compromises?'
        answer: 'Strong consistency (linearizable): Every read sees latest write.
          Cost: 2 RTT latency (10-50ms), leader bottleneck (10K writes/sec max per
          leader). Use for: Financial transactions, inventory, leader election. Compromise
          1: Read from followers (stale reads, but faster: 1ms vs 10ms). Use for:
          Analytics, dashboards. Compromise 2: Eventual consistency (0 RTT coordination).
          Use for: Social media feeds, view counts. Compromise 3: Session consistency
          (user sees own writes). Use for: User profiles. Hybrid: 5% strong (critical
          paths) + 95% eventual (everything else) = 10× throughput. Choose: Strong
          only when data loss/inconsistency costs > latency costs.'
      - question: 'Cross-region consensus: trade-offs between latency and availability'
        answer: 'Single-region: Latency 10ms, Availability 99.9% (1 DC failure = downtime).
          Multi-region (3 regions): Latency 100-200ms (cross-region RTT), Availability
          99.99% (survives 1 region failure). Trade-off: 10-20× higher latency for
          10× better availability. Mitigation: 1) Use read-only replicas in each region
          (reads 10ms, writes 100ms). 2) Partition data by geography (US users → US
          DC). 3) Async replication (eventual consistency, 1s lag). 4) Raft groups
          per region + global coordinator. Cost: 3× infrastructure. Choose: Single-region
          (latency critical, can tolerate downtime), multi-region (availability critical,
          can tolerate latency).'
      - question: 'Leader-based vs leaderless: pros and cons for different workloads'
        answer: 'Leader-based (Raft, Paxos): Pros: Simpler (single writer), strong
          consistency, total ordering. Cons: Leader bottleneck (10K writes/sec), single
          point of failure (need election on failure: 150ms). Use for: Metadata stores
          (etcd, ZooKeeper), configuration, locks. Leaderless (Dynamo, Cassandra):
          Pros: Higher throughput (all nodes accept writes), no election delay, multi-datacenter
          writes. Cons: Eventual consistency, conflict resolution needed, more complex.
          Use for: High-write workloads (IoT, logs), multi-region active-active. Hybrid:
          Leader per partition (Multi-Raft) = 100× throughput with strong consistency.
          Choose: Leader for consistency, leaderless for throughput/availability.'
      - question: 'Byzantine vs crash fault tolerance: cost-benefit analysis'
        answer: 'Crash fault tolerance (CFT - Raft, Paxos): Nodes fail by stopping.
          Cost: 2f+1 nodes to tolerate f failures = 3 nodes for 1 failure. Latency:
          2-3 RTT (10ms). Use: Trusted environment (same org). Byzantine fault tolerance
          (BFT - PBFT): Nodes fail by lying/attacking. Cost: 3f+1 nodes to tolerate
          f failures = 4 nodes for 1 failure (33% more). Latency: 3-5 RTT (25ms, 2.5×
          slower). Use: Untrusted environment (blockchain, multi-org). When to use
          BFT: Blockchain (untrusted miners), financial systems (regulatory). When
          CFT enough: Internal systems (trusted infrastructure). Cost: BFT = 1.5×
          more nodes + 2.5× latency = 3.75× total cost. Choose BFT only if Byzantine
          failures are real threat.'
      scenario_based:
      - question: Design leader election for microservices in Kubernetes environment
        answer: 'Use etcd (built into K8s) + leader election library. Architecture:
          1) Each service instance tries to create lease in etcd with TTL=10s, 2)
          First instance to create = leader, 3) Leader renews lease every 5s (heartbeat),
          4) If leader dies: Lease expires in 10s, new election starts. Library: Use
          client-go LeaderElector (Go) or kubernetes-client (Python). Failover: 10s
          (TTL expiry). Load: 1 write every 5s per service = negligible. Cost: Free
          (uses existing etcd). Alternative: Use distributed lock (Redis, ZooKeeper),
          but etcd already in K8s. Edge case: Network partition splits cluster → majority
          side continues, minority side blocks (safe).'
      - question: Handle network partition in 5-node consensus cluster
        answer: 'Partition: 3 nodes (Group A) + 2 nodes (Group B). Raft behavior:
          Group A (majority) → Continues serving requests (can form quorum: 3 > 5/2).
          Group B (minority) → Blocks writes (cannot form quorum: 2 < 3). Reads: Group
          A serves reads, Group B rejects or serves stale. When partition heals: Group
          B syncs from Group A (log replay). Edge case: 2-2-1 split → If leader in
          2-node group, new election in other 2-node group (both fail quorum). Node
          1 joins either side → 3-node group wins. Mitigation: Odd number of nodes
          (5, 7) reduces tie probability. Monitoring: Detect partition via heartbeat
          failures, alert operators.'
      - question: Migrate from single-region to multi-region consensus safely
        answer: 'Steps: 1) Deploy new region nodes (3 regions × 3 nodes = 9 total).
          2) Add nodes as followers (replication only, no voting). 3) Wait for full
          sync (check lag < 1s). 4) Update quorum config to include new nodes (9 nodes,
          quorum = 5). 5) Verify cross-region writes work (test failover). 6) Gradual
          traffic shift (10% → 50% → 100% over 1 week). Rollback: Keep old region
          nodes for 1 month (can revert config). Zero downtime: Yes (followers don''t
          need quorum, just replication). Challenges: Cross-region latency (100ms
          writes), network partitions (test with chaos engineering). Cost: 3× infrastructure.
          Time: 2 months (testing, rollout).'
      - question: Implement configuration management service using consensus
        answer: 'Use etcd/Consul for consensus + versioned key-value store. Architecture:
          1) Store configs as versioned keys (namespace/service/config → version N),
          2) Watch API for real-time updates (long polling), 3) Clients cache locally
          + watch for changes. Write: Client → Leader → Raft log → Replicate → Commit
          → Watchers notified (50ms e2e). Read: Local cache (1ms) or read from follower
          (10ms). Features: Atomic updates (compare-and-swap), rollback (revert to
          version N-1), audit log (who changed what when). Scale: 10K services × 10
          configs = 100K keys = 100MB. Cost: $2K/month (5-node etcd cluster). Alternative:
          ZooKeeper (older, less user-friendly).'
      - question: Design consensus-based distributed lock service
        answer: 'Use Raft (etcd, Consul) for distributed locks. API: AcquireLock(key,
          ttl) → lock_id, ReleaseLock(lock_id), RenewLock(lock_id). Implementation:
          1) Client creates ephemeral key with TTL=10s + unique client_id, 2) If key
          doesn''t exist → Lock acquired, 3) Client renews every 5s (heartbeat), 4)
          If client dies: TTL expires → Lock auto-released. Correctness: Fencing tokens
          (monotonic lock_id) prevent stale lock holder from writing after timeout.
          Scale: 100K locks = 100K keys = 10MB. Lock contention: Use queue (first-come-first-served).
          Cost: $2K/month. Alternative: Redis (single-node, not highly available).
          Use cases: Job scheduling, leader election, resource coordination.'
      - question: Build strongly consistent cache using consensus protocols
        answer: 'Use Raft + in-memory cache. Architecture: 1) Cache nodes form Raft
          cluster (5 nodes), 2) Writes go through Raft log (strong consistency), 3)
          Reads from local state machine (fast). Write: Client → Leader → Raft commit
          → Apply to cache (50ms). Read: Any node serves from local cache (1ms, linearizable
          with read quorum or leader lease). Eviction: LRU on each node (eventually
          consistent). Invalidation: Broadcast via Raft log. Scale: 100GB per node
          × 5 nodes = 500GB cache. Throughput: 10K writes/sec (leader limit), 1M reads/sec
          (all nodes). Cost: $10K/month. Use cases: Session store, feature flags,
          rate limiting. Alternative: Redis + Sentinel (weaker consistency, simpler).'
    resources:
    - title: Raft Consensus Algorithm
      url: https://raft.github.io/
      description: Complete Raft specification with visualization and interactive
        demo
    - title: Lamport's Paxos Made Simple
      url: http://lamport.azurewebsites.net/pubs/paxos-simple.pdf
      description: Definitive paper on Paxos algorithm by its creator
    - title: 'PBFT: Practical Byzantine Fault Tolerance'
      url: http://pmg.csail.mit.edu/papers/osdi99.pdf
      description: Foundational paper on practical Byzantine consensus
    - title: etcd Documentation
      url: https://etcd.io/docs/
      description: Production-grade consensus system powering Kubernetes
    - title: 'Jepsen: Distributed Systems Safety Research'
      url: https://jepsen.io/
      description: Testing framework and analysis of consensus system failures
    - title: Designing Data-Intensive Applications - Chapter 9
      url: https://dataintensive.net/
      description: Consensus and atomic commit in distributed systems
    - title: MIT 6.824 Distributed Systems
      url: https://pdos.csail.mit.edu/6.824/
      description: Academic course with Raft lab implementation
    - title: Apache Zookeeper Internals
      url: https://zookeeper.apache.org/doc/current/zookeeperInternals.html
      description: ZAB consensus protocol and coordination service design
    time_estimate: 135
    video_resources:
    - title: 'ByteByteGo: Design Metrics System'
      url: https://www.youtube.com/@ByteByteGo
      duration: 15 min
      description: Time-series data and monitoring
      priority: high
    - title: 'Hello Interview: Design Analytics Platform'
      url: https://www.youtube.com/@HelloInterview
      duration: 25 min
      description: Real-time analytics at scale
      priority: medium
  - day: 26
    topic: System Integration & Architecture Patterns
    activity: Master comprehensive system architectures, integration patterns, and
      avoid common anti-patterns.
    detailed_content: 'Core Architecture Patterns:

      - Monolithic: Single deployable unit, simpler initially but harder to scale

      - Layered: Separation of concerns (presentation, business, data layers)

      - Microservices: Service-oriented decomposition with independent deployment

      - Event-driven: Loose coupling through asynchronous event communication

      - Serverless/FaaS: Function-based execution, auto-scaling, pay-per-use

      - CQRS: Command Query Responsibility Segregation for read/write optimization

      - Hexagonal (Ports & Adapters): Business logic isolation from external concerns

      - Space-based: In-memory data grids for high scalability

      - Pipeline: Sequential data processing stages (ETL, stream processing)


      Integration Patterns:

      - Request-Response: Synchronous communication for immediate results

      - Fire-and-Forget: Asynchronous messaging without waiting for response

      - API Composition: Aggregate data from multiple services

      - Database per Service: Data ownership and independence

      - Saga Pattern: Distributed transaction management with compensation

      - Backend for Frontend (BFF): Service layer tailored for specific clients

      - Strangler Fig: Gradual legacy system migration

      - Circuit Breaker: Prevent cascade failures in service calls

      - Message Routing: Content-based and topic-based message distribution

      - Scatter-Gather: Parallel data collection and aggregation

      - Event Sourcing: Store events instead of current state

      - Bulkhead: Resource isolation to prevent resource exhaustion


      Anti-Patterns to Avoid:

      - Distributed Monolith: Microservices with tight coupling and shared databases

      - Chatty Interfaces: Excessive fine-grained API calls causing network overhead

      - Shared Database: Multiple services accessing the same database

      - Synchronous Communication Overuse: Creating tight coupling and cascade failures

      - God Service: Single service handling too many responsibilities

      - Data Inconsistency: Lack of eventual consistency handling

      - Distributed Transactions: Using 2PC in microservices (prefer saga pattern)


      Modern Architecture Concepts:

      - API Gateway: Single entry point for client requests with cross-cutting concerns

      - Service Mesh: Infrastructure layer for service-to-service communication

      - Sidecar Pattern: Deploy auxiliary services alongside main application

      - Ambassador Pattern: Proxy for external service communication

      - Container Orchestration: Kubernetes patterns for deployment and scaling

      - GitOps: Infrastructure and application deployment through Git workflows


      System Boundaries:

      - Domain-driven Design: Bounded contexts and ubiquitous language

      - Conway''s Law: System structure mirrors organization communication patterns

      - Service Interfaces: Contract-first design with API versioning

      - Data Consistency Boundaries: Eventual consistency across service boundaries

      - Failure Domain Isolation: Contain failures within service boundaries


      Quality Attributes:

      - Scalability: Horizontal (scale out) vs vertical (scale up) approaches

      - Availability: System uptime and fault tolerance (different from reliability)

      - Reliability: System correctness and error handling

      - Elasticity: Dynamic scaling based on demand (auto-scaling)

      - Performance: Latency (response time) and throughput (requests per second)

      - Security: Authentication, authorization, encryption, defense in depth

      - Maintainability: Code organization, testing, documentation

      - Observability: Monitoring, logging, tracing for system understanding

      - Testability: Unit, integration, contract, chaos testing strategies

      - Deployability: CI/CD pipelines, blue-green, canary deployments


      Integration Trade-offs:

      - Synchronous vs Asynchronous: Latency vs complexity

      - Push vs Pull: Real-time updates vs resource usage

      - Choreography vs Orchestration: Decentralized vs centralized coordination

      - REST vs GraphQL vs gRPC: Different API paradigms and use cases

      - Event Streaming vs Message Queues: Kafka vs RabbitMQ/SQS patterns

      '
    resources:
    - title: Software Architecture Patterns
      url: https://www.oreilly.com/library/view/software-architecture-patterns/9781491971437/
      description: Common architecture patterns
    - title: Building Evolutionary Architectures
      url: https://www.thoughtworks.com/insights/books/building-evolutionary-architectures
      description: Adaptable system design
    - title: Microservices Anti-patterns
      url: https://microservices.io/patterns/microservices.html
      description: What to avoid in microservices design
    - title: Service Mesh Patterns
      url: https://www.oreilly.com/library/view/istio-up-and/9781492043775/
      description: Modern service communication patterns
    practice_questions:
      estimation:
      - question: A monolith serves 10K RPS. Split into 5 microservices with 2ms network
          overhead per call. If each request needs 3 service calls, what's the latency
          impact?
        answer: 'Monolith latency: Assume 50ms (in-memory calls, no network). Microservices:
          3 service calls × 2ms network overhead = 6ms added latency. Total: 50ms
          + 6ms = 56ms (12% increase). But: Services now run in parallel (if independent).
          Parallel: max(service times) + network = 20ms + 6ms = 26ms (48% faster).
          Sequential (dependent calls): 50ms + 6ms = 56ms (12% slower). Real cost:
          Error rate (network failures: 0.1% per call × 3 = 0.3% vs 0% monolith).
          Throughput: Each service can scale independently = 10x throughput possible.
          Choose: Microservices for scale, monolith for latency.'
      - question: API Gateway handles 50K RPS with 5ms overhead. Backend services
          can handle 100K RPS. What's the bottleneck and how to scale?
        answer: 'Bottleneck: API Gateway (50K RPS < 100K backend capacity). Gateway
          overhead: 5ms processing + SSL termination + auth + routing. Scaling options:
          1) Horizontal scale: Add 1 more gateway = 100K RPS (matches backend). Cost:
          $500/month per gateway × 2 = $1K. 2) Vertical scale: Bigger instance (4×
          CPU) = 80K RPS. Cost: $2K/month. 3) Optimize: Move auth to sidecar (removes
          2ms) = 70K RPS. Choose: Horizontal (cheaper, better availability). New bottleneck:
          Backend at 100K RPS. Solution: Scale backends (add replicas).'
      - question: Event-driven system processes 1M events/day. Each event triggers
          3 downstream services. How many service calls per second at peak (assuming
          10x daily average)?
        answer: 'Average: 1M events/day = 11.6 events/sec. Per event: 3 service calls
          = 11.6 × 3 = 34.8 calls/sec avg. Peak: 10× daily average = 116 events/sec
          × 3 = 348 calls/sec. With fan-out: If services process in parallel, 348
          concurrent calls/sec. Service capacity: Each service needs 116 events/sec
          capacity. Failure handling: Dead letter queue for failed events (retry with
          backoff). Kafka: 3 topics (one per downstream service) = 348 messages/sec
          total = negligible load. Cost: $200/month for 3-node Kafka cluster.'
      concepts:
      - question: When would you choose microservices over monolith? What are the
          trade-offs?
        answer: 'Choose microservices when: 1) Scale (different services need different
          scaling: 100× for API, 1× for admin), 2) Team size (>20 engineers, need
          autonomy), 3) Polyglot (different tech stacks: Python ML, Go API), 4) Deployment
          frequency (deploy multiple times/day without full system restart). Monolith
          better when: Small team (<10), simple app, low traffic (<10K RPS), tight
          coupling (shared state). Trade-offs: Microservices = 3× complexity (distributed
          debugging, network failures, eventual consistency), 2× cost (overhead, duplication),
          but 10× scaling flexibility. Start monolith, migrate to microservices at
          scale inflection point.'
      - question: How do you handle distributed transactions without 2PC? Explain
          saga pattern.
        answer: 'Saga pattern: Break transaction into local transactions + compensating
          actions. Example (hotel + flight booking): 1) Reserve hotel (local tx),
          2) Book flight (local tx), 3) If flight fails: Cancel hotel (compensating
          tx). Types: Choreography (event-driven, services listen for events) vs Orchestration
          (central coordinator). Choreography: Hotel emits ''reserved'' event → Flight
          service listens → Books flight. Pro: Decentralized. Con: Hard to trace.
          Orchestration: Saga coordinator calls Hotel → Flight → Payment. Pro: Easy
          to trace. Con: Single point of failure. Failure: Each step has compensating
          action (cancel, refund). Consistency: Eventual (not ACID). Use when: Distributed
          data, can''t use 2PC.'
      - question: What's the difference between API Gateway and Service Mesh?
        answer: 'API Gateway: North-south traffic (external → internal). Functions:
          Auth, rate limiting, SSL termination, routing, caching. Sits at edge. Example:
          Kong, AWS API Gateway. Latency: 5-10ms overhead. Service Mesh: East-west
          traffic (service → service). Functions: Load balancing, retry, circuit breaking,
          tracing, mTLS. Runs as sidecar proxy (Envoy). Example: Istio, Linkerd. Latency:
          1-2ms overhead per hop. Overlap: Both do routing, observability. Use both:
          Gateway for external API, mesh for internal services. Alternative: Gateway
          only (simpler, less overhead). Cost: Gateway $500/month, mesh $2K/month
          (more infrastructure).'
      - question: How do you prevent cascade failures in a microservices architecture?
        answer: 'Cascade failure: Service A fails → Service B (depends on A) times
          out → Service C (depends on B) overloaded. Prevention: 1) Circuit breaker:
          After N failures (5), open circuit = fast fail (no timeout wait). Close
          after cooldown (30s). 2) Timeouts: Aggressive timeouts (1s) prevent thread
          exhaustion. 3) Bulkheads: Separate thread pools per dependency (Service
          A gets 10 threads, Service B gets 10). 4) Rate limiting: Limit requests
          to downstream (100 RPS max). 5) Graceful degradation: Return cached/default
          data when service unavailable. 6) Async: Use message queue (decouples services).
          Tools: Hystrix, Resilience4j. Result: Isolated failures, no cascade.'
      - question: Explain the difference between availability and reliability with
          examples.
        answer: 'Availability: Uptime percentage (can system respond?). Example: 99.9%
          = 8.7h downtime/year. Measured: Total uptime / Total time. Reliability:
          Correctness over time (does system work correctly?). Example: Returns correct
          results 99.9% of time. Measured: MTBF (mean time between failures). Can
          have: High availability, low reliability (system responds but wrong data).
          Low availability, high reliability (system rarely available but correct
          when it is). Trade-off: Replication increases availability (more replicas
          = less downtime) but can decrease reliability (more replicas = more consistency
          issues). Goal: Both high (99.99% available + correct results).'
      - question: When would you use CQRS and what are the complexities it introduces?
        answer: 'CQRS (Command Query Responsibility Segregation): Separate write model
          (commands) from read model (queries). Use when: 1) Read/write patterns differ
          (95% reads, 5% writes → optimize differently), 2) Complex queries (joins
          across services), 3) Different scaling needs (100K reads/sec, 1K writes/sec),
          4) Event sourcing (replay events to build read models). Example: E-commerce
          (write to order DB, read from denormalized product catalog). Complexities:
          1) Eventual consistency (write delay to read model: 100ms-1s), 2) Data duplication
          (2× storage), 3) Sync complexity (keep models in sync via events), 4) More
          infrastructure (2 databases). Cost: 2× database cost. Use only when read/write
          patterns significantly different.'
      - question: How do you handle data consistency in an event-driven architecture?
        answer: 'Event-driven = eventual consistency (events propagate with delay:
          100ms-1s). Strategies: 1) Event ordering: Use partition key (user_id) →
          all user events in order. 2) Idempotency: Use event ID to deduplicate (handle
          duplicate deliveries). 3) Compensating events: If error, emit reverse event
          (OrderCancelled after OrderCreated). 4) Saga pattern: Coordinate multi-step
          workflows with events. 5) Event sourcing: Store events as source of truth,
          rebuild state by replay. 6) Versioning: Version events (OrderCreatedV1 →
          OrderCreatedV2) for schema evolution. Challenges: No ACID transactions across
          services. Debugging: Hard to trace (distributed events). Monitoring: Track
          event lag (<1s), replay capability (rebuild state from events).'
      tradeoffs:
      - question: 'Compare synchronous vs asynchronous communication: when to use
          each?'
        answer: 'Synchronous (REST/gRPC): Request → Wait → Response. Pros: Simple,
          immediate feedback (10ms). Cons: Tight coupling, caller blocks (thread held),
          cascade failures. Use for: User-facing APIs (need instant response), critical
          path (payment). Latency: 10-50ms. Asynchronous (Message Queue): Send message
          → Return immediately → Process later. Pros: Decoupled (sender continues),
          resilient (queue buffers), scalable (parallel processing). Cons: Complex,
          eventual consistency (1s delay), no immediate feedback. Use for: Background
          jobs (email, analytics), high volume (1M events/day), non-critical path.
          Latency: 100ms-1s. Hybrid: Sync for user requests (95%), async for background
          (5%). Result: Balance responsiveness + resilience.'
      - question: 'Choreography vs Orchestration for microservices coordination: pros
          and cons'
        answer: 'Choreography: Decentralized, event-driven. Service A emits event
          → Service B listens → Service C listens. Pros: No single point of failure,
          loose coupling, scalable. Cons: Hard to trace (no central view), implicit
          dependencies, debugging difficult. Use for: Simple workflows (2-3 steps),
          high autonomy teams. Example: Order created → Notification sent + Inventory
          updated. Orchestration: Central coordinator (Saga orchestrator). Coordinator
          calls Service A → Service B → Service C. Pros: Easy to trace (central log),
          explicit workflow, timeouts/retries centralized. Cons: Coordinator is bottleneck
          + single point of failure, tighter coupling. Use for: Complex workflows
          (>3 steps), compensation logic. Example: Book flight → hotel → car (rollback
          if any fails). Choose: Orchestration for complex, choreography for simple.'
      - question: 'API Gateway vs Backend for Frontend (BFF): when to use which pattern?'
        answer: 'API Gateway: Single entry point for all clients (web, mobile, IoT).
          Handles: Auth, rate limiting, routing, SSL. Generic aggregation (same for
          all clients). Pro: Centralized control, consistent security. Con: One-size-fits-all
          (mobile gets same data as web, over-fetching). Cost: $500/month. BFF: Separate
          backend per client type (web-bff, mobile-bff, iot-bff). Each BFF tailored
          to client needs (mobile-bff returns less data, web-bff aggregates more).
          Pro: Client-optimized APIs (mobile gets 1KB, web gets 10KB), independent
          deployment. Con: Code duplication, more services to manage. Cost: $1.5K/month
          (3 BFFs). Choose: Gateway for simple apps, BFF for multiple distinct clients
          (web + mobile with different needs).'
      - question: 'Event Sourcing vs traditional CRUD: benefits and drawbacks'
        answer: 'CRUD (traditional): Store current state only (UPDATE users SET balance=100).
          Pros: Simple, fast reads (10ms), small storage. Cons: Lose history (can''t
          answer ''what was balance yesterday?''), hard to debug, no audit trail.
          Storage: 1GB. Event Sourcing: Store all events (BalanceSet(100), BalanceAdded(50)).
          Rebuild state by replaying events. Pros: Full audit trail (regulatory compliance),
          time travel (query historical state), debugging (replay events). Cons: Complex,
          slower reads (replay 1000 events = 100ms), more storage (10× events vs state).
          Storage: 10GB. Requires: Snapshots (rebuild from snapshot + recent events,
          not all history). Use for: Financial systems (audit trail), analytics (need
          history), undo/replay. Cost: 10× storage + CQRS for fast reads.'
      - question: 'Service Mesh vs API Gateway: overlapping concerns and complementary
          use'
        answer: 'Overlap: Both do routing, load balancing, observability, retries,
          timeouts. Difference: Gateway (north-south, external → internal), Mesh (east-west,
          internal ↔ internal). Gateway features: Auth, rate limiting (external clients),
          SSL termination, API composition. Mesh features: mTLS (service-to-service
          encryption), fine-grained routing (per-service), distributed tracing (spans
          across services), zero-trust security. Use both: Gateway at edge (external
          traffic) + mesh for internal (service-to-service). Alternative: Gateway
          only (simpler, less overhead). Cost: Gateway $500/month, mesh $2K/month
          (sidecar per pod = 2× pods). Choose mesh when: Need service-to-service security
          (mTLS), advanced routing (canary per service), observability at scale. Otherwise:
          Gateway sufficient.'
      - question: 'Database per service vs shared database: data consistency challenges'
        answer: 'Database per service: Each microservice has own DB. Pros: Independent
          scaling (scale order DB separately), loose coupling (schema changes isolated),
          polyglot (MySQL for orders, MongoDB for products). Cons: No ACID transactions
          across services, data duplication, eventual consistency. Consistency: Use
          Saga pattern (compensating transactions) or event-driven sync. Query: Can''t
          JOIN across services → need API composition or CQRS. Cost: 5 services ×
          $200 = $1K/month. Shared database: All services access same DB. Pros: ACID
          transactions (strong consistency), easy queries (JOIN works). Cons: Tight
          coupling (schema change breaks all), scaling bottleneck (single DB), deployment
          risk (shared state). Cost: $500/month. Choose: Per-service for true microservices
          (loose coupling), shared for transitional architecture (monolith → microservices).'
      - 'REST vs GraphQL vs gRPC: when to choose each API style?'
      scenarios:
      - Design integration between a legacy monolith and new microservices. How would
        you use Strangler Fig pattern?
      - Your microservices architecture has tight coupling. Identify anti-patterns
        and propose solutions.
      - A service is overwhelmed with requests. Design a solution using Circuit Breaker
        and Bulkhead patterns.
      - You need to migrate from synchronous to event-driven architecture. What are
        the challenges and steps?
      - Design a system where user actions trigger multiple downstream processes.
        How do you ensure consistency?
    time_estimate: 60
    video_resources:
    - title: 'ByteByteGo: Design Ad Click Aggregator'
      url: https://www.youtube.com/@ByteByteGo
      duration: 18 min
      description: Real-time data processing
      priority: high
    - title: 'Hello Interview: Design Ad Serving System'
      url: https://www.youtube.com/@HelloInterview
      duration: 22 min
      description: Ad targeting and delivery
      priority: medium
  - day: 27
    topic: Mock Interview 1 - Ride Sharing System (Uber/Lyft)
    activity: Complete mock system design interview for ride-sharing platform with
      real-time matching, dynamic pricing, and global scale considerations.
    detailed_content: "Interview Structure & Timing:\n- Requirements Clarification\
      \ (8-10 minutes): Scope, users, scale, constraints\n- High-Level Architecture\
      \ (15-18 minutes): Core services, data flow, APIs\n- Deep Dive Components (15-18\
      \ minutes): Critical systems detailed design\n- Scale & Trade-offs (5-7 minutes):\
      \ Bottlenecks, optimization, monitoring\n- Q&A (3-5 minutes): Edge cases, failure\
      \ scenarios, next steps\n\nRequirements Gathering Framework:\n- Functional Requirements:\n\
      \  * User registration and profile management (drivers, riders)\n  * Real-time\
      \ location tracking and updates\n  * Driver-rider matching and assignment\n\
      \  * Trip lifecycle management (request, start, end, payment)\n  * Dynamic pricing\
      \ based on supply/demand\n  * ETA calculation and route optimization\n  * Payment\
      \ processing and billing\n  * Rating and feedback system\n  * Trip history and\
      \ analytics\n\n- Non-Functional Requirements:\n  * Scale: 10M active riders,\
      \ 1M active drivers daily\n  * Geographic: Global service across 100+ cities\n\
      \  * Availability: 99.9% uptime with graceful degradation\n  * Latency: <500ms\
      \ for matching, <100ms for location updates\n  * Consistency: Strong for payments,\
      \ eventual for location data\n  * Real-time: Location updates every 2-4 seconds\n\
      \nCore System Components:\n1. User Service: Authentication, profiles, preferences,\
      \ driver verification\n2. Location Service: Real-time GPS tracking, geospatial\
      \ indexing, location history\n3. Matching Service: Driver-rider pairing algorithms,\
      \ supply-demand optimization\n4. Trip Service: State management, lifecycle tracking,\
      \ route calculation\n5. Pricing Service: Dynamic fare calculation, surge pricing,\
      \ promotions\n6. Payment Service: Transaction processing, billing, refunds,\
      \ driver payouts\n7. Notification Service: Real-time updates, push notifications,\
      \ SMS/email\n8. Analytics Service: Business intelligence, operational metrics,\
      \ ML training data\n\nDetailed Design Deep Dives:\n\nLocation Service Architecture:\n\
      - Real-time Location Updates: WebSocket connections, Redis for caching\n- Geospatial\
      \ Indexing: QuadTree or Geohash for efficient proximity searches\n- Location\
      \ Storage: Time-series database for historical tracking\n- Data Pipeline: Kafka\
      \ for streaming location events to analytics\n- Optimization: Location prediction,\
      \ batch updates, data compression\n\nMatching Algorithm Design:\n- Proximity\
      \ Search: Find drivers within 5km radius using geospatial index\n- Scoring System:\
      \ Distance, driver rating, acceptance rate, trip direction\n- Assignment Strategy:\
      \ Greedy vs optimal assignment with timeout\n- Fairness: Prevent driver starvation,\
      \ rotation policies\n- Machine Learning: Demand prediction, driver positioning\
      \ optimization\n\nTrip State Management:\n- State Machine: REQUESTED → ACCEPTED\
      \ → STARTED → COMPLETED → PAID\n- Fault Tolerance: Distributed coordination,\
      \ timeout handling, retries\n- Consistency: Event sourcing for trip audit trail\n\
      - Real-time Updates: WebSocket for trip status to both parties\n- Compensation:\
      \ Handle payment failures, trip cancellations\n\nDynamic Pricing Engine:\n-\
      \ Surge Pricing: Supply/demand ratio, geographic heat maps\n- ML Models: Demand\
      \ forecasting, price elasticity, revenue optimization\n- Real-time Updates:\
      \ Price recalculation every 30 seconds\n- Transparency: Clear pricing communication\
      \ to users\n- Regulation Compliance: Price caps, fare regulations per city\n\
      \nScalability Considerations:\n- Geographic Partitioning: City-based sharding,\
      \ regional data centers\n- Database Scaling: Read replicas, write sharding,\
      \ caching strategies\n- Service Mesh: Circuit breakers, load balancing, service\
      \ discovery\n- Auto-scaling: Kubernetes HPA based on location update volume\n\
      - CDN: Static content, mobile app assets, configuration data\n\nData Storage\
      \ Strategy:\n- User Data: PostgreSQL with read replicas\n- Location Data: Redis\
      \ for real-time, InfluxDB for historical\n- Trip Data: PostgreSQL with partitioning\
      \ by time/geography\n- Analytics: Data lake (S3) + Spark for batch processing\n\
      - Caching: Redis for hot data, application-level caching\n\nMonitoring & Observability:\n\
      - Business Metrics: Successful matches/minute, average ETA accuracy\n- Technical\
      \ Metrics: API latency, error rates, database performance\n- Real-time Dashboards:\
      \ Geographic heat maps, system health\n- Alerting: SLA violations, spike in\
      \ failed matches, payment issues\n- Distributed Tracing: End-to-end request\
      \ flow across microservices\n\nSecurity & Compliance:\n- Authentication: OAuth\
      \ 2.0, JWT tokens, multi-factor for drivers\n- Data Privacy: PII encryption,\
      \ GDPR compliance, data retention policies\n- Payment Security: PCI DSS compliance,\
      \ tokenization, fraud detection\n- Location Privacy: Data anonymization, user\
      \ consent, location expiry\n- Driver Verification: Background checks, document\
      \ verification, real-time monitoring\n\nCommon Interview Questions & Answers:\n\
      - \"How do you handle driver going offline during trip?\"\n- \"What happens\
      \ when payment fails after trip completion?\"\n- \"How do you prevent fake GPS\
      \ locations from drivers?\"\n- \"How do you handle network partitions between\
      \ regions?\"\n- \"What's your strategy for expanding to a new city?\"\n\nFailure\
      \ Scenarios & Recovery:\n- Location Service Down: Use cached locations, degrade\
      \ to manual assignment\n- Matching Service Overload: Queue requests, increase\
      \ timeout, circuit breaker\n- Payment Service Failure: Async retry, manual reconciliation,\
      \ credit system\n- Database Partition: Regional failover, read-only mode, eventual\
      \ consistency\n- Network Issues: Offline capability, request queuing, graceful\
      \ degradation\n\nPerformance Optimization:\n- Location Updates: Batch processing,\
      \ compression, adaptive frequency\n- Matching Algorithm: Pre-computed driver\
      \ pools, parallel processing\n- Database: Connection pooling, query optimization,\
      \ proper indexing\n- Caching: Multi-level caching, cache warming, intelligent\
      \ eviction\n- API Design: Pagination, filtering, async processing for heavy\
      \ operations\n"
    practice_questions:
      capacity_estimation:
      - question: Calculate storage for 10M trips/day with location updates every
          3 seconds
        answer: 'Trip data: 10M trips/day × 2KB/trip = 20GB/day. Location updates:
          10M trips × 20 min avg duration × (60/3) updates/min × 100B/update = 10M
          × 20 × 20 × 100B = 400GB/day. Total: 20GB + 400GB = 420GB/day = 153TB/year.
          With compression (3×): 51TB/year. Hot storage (30 days): 12.6TB. Cold storage
          (11 months): 38.4TB. Database: Use time-series DB (InfluxDB) for locations
          = 10× compression. Cost: Hot $1.5K/month, Cold $500/month = $2K/month total.'
      - question: Estimate bandwidth for real-time location updates from 1M drivers
        answer: 'Active drivers: 1M concurrent. Update frequency: 3 seconds. Payload:
          100B (lat, lon, heading, speed, timestamp). Upload: 1M × 100B / 3s = 33MB/sec
          upload. Broadcast to riders: 10M riders watching drivers × 100B / 3s = 333MB/sec
          download. Total: 33MB + 333MB = 366MB/sec = 2.9Gbps. With WebSocket compression
          (2×): 1.5Gbps. Peak (5× avg): 7.3Gbps. CDN for rider updates: $5K/month.
          Direct upload from drivers: $2K/month. Total: $7K/month bandwidth.'
      - question: Size matching service to handle 100K concurrent ride requests
        answer: 'Requests: 100K concurrent = peak load. Processing: 10ms per match
          (QuadTree search + scoring). Throughput: 1 core handles 100 req/sec. Cores
          needed: 100K / 100 = 1000 cores. Servers: 1000 cores / 32 cores per server
          = 32 servers. With 2× headroom: 64 servers. Memory: 10GB per server (cache
          driver locations) = 640GB total. Database: 1K queries/sec (read-heavy) =
          5 read replicas. Cost: 64 servers × $500 = $32K/month. Alternative: Pre-compute
          driver pools per geohash = reduce to 5ms/match = 16 servers = $8K/month.'
      - question: Calculate database capacity for storing 1 year of trip history
        answer: 'Trips: 10M/day × 365 = 3.65B trips/year. Per trip: 2KB (user_id,
          driver_id, route, price, timestamps) = 7.3TB. Indexes (50%): 3.65TB. Total:
          10.95TB ≈ 11TB. Sharding: 100GB per shard = 110 shards. Replication (3×):
          33TB storage. Growth: 11TB/year, plan for 3 years = 33TB capacity. Cost:
          $3K/month for storage. Query pattern: 80% recent (30 days), 20% historical.
          Solution: Hot tier (1 month: 600GB SSD), Cold tier (rest: HDD). Cost: $1K/month.'
      architecture_decisions:
      - question: Why use QuadTree vs Geohash for location indexing?
        answer: 'QuadTree: Recursively divide space into 4 quadrants. Pros: Efficient
          radius search (O(log n) within cell), dynamic (adapts to density). Cons:
          In-memory only (hard to shard), complex updates. Use for: Real-time matching
          (drivers within 5km). Latency: 1-5ms. Geohash: Encode lat/lon as string
          (e.g., ''9q5''). Pros: Simple, DB-friendly (range query), easy sharding
          (by prefix). Cons: Edge problem (nearby points may have different prefixes),
          fixed precision. Use for: Persistent storage, coarse filtering. Latency:
          10ms. Hybrid: Geohash for initial filter (100 drivers) → QuadTree for final
          ranking. Result: Best of both (DB sharding + fast search).'
      - question: SQL vs NoSQL for different data types in the system?
        answer: 'Use SQL for: 1) Users (PostgreSQL): ACID transactions (payment, trips),
          complex queries (analytics), structured. 2) Trips (PostgreSQL): Strong consistency
          (no double-billing), foreign keys, reporting. Use NoSQL for: 1) Locations
          (Cassandra): Write-heavy (1M drivers × 0.33 writes/sec = 330K writes/sec),
          time-series, partition by driver_id. 2) Driver pool (Redis): In-memory,
          fast reads (<1ms), geospatial index (GEORADIUS). 3) Notifications (DynamoDB):
          Event-driven, high availability. Polyglot persistence: 5 databases = complexity,
          but optimized per use case. Cost: SQL $2K/month, NoSQL $3K/month = $5K total.'
      - question: Microservices vs monolith for a startup building ride-sharing?
        answer: 'Start with modular monolith: Single deployment, but organized into
          modules (matching, payment, trips). Pros: Simple (1 codebase), fast iteration
          (no network overhead), easier debugging. Cons: Shared database, single scaling
          unit. When to split: 1) Team size >20 (coordination overhead), 2) Independent
          scaling (matching 100×, admin 1×), 3) Polyglot needs (Python ML, Go API).
          Timeline: Monolith for MVP (6 months), microservices at scale (1M users).
          Migration: Strangler Fig pattern (gradually extract services). Cost: Monolith
          $5K/month, microservices $15K/month. Choose: Monolith until pain points
          justify 3× cost increase.'
      - question: Event-driven vs request-response for trip state updates?
        answer: 'Request-response (sync): Client → API → Update DB → Return status.
          Pros: Immediate feedback (10ms), simple debugging. Cons: Tight coupling,
          blocks client. Use for: Critical path (rider requests trip, driver accepts).
          Latency: 50ms. Event-driven (async): Client → Emit event → Queue → Subscribers
          update. Pros: Decoupled (add notification without changing trip service),
          resilient (queue buffers), scalable. Cons: Eventual consistency (100ms lag),
          complex debugging. Use for: Non-critical updates (notifications, analytics,
          billing). Hybrid: Sync for critical (trip creation), async for side effects
          (notify rider, update analytics). Result: 80% event-driven, 20% sync.'
      scalability_challenges:
      - question: How do you scale matching service for 10x traffic growth?
        answer: 'Current: 100K concurrent requests. Target: 1M concurrent. Strategy:
          1) Horizontal scale: 64 servers → 640 servers (10×). Cost: $320K/month.
          2) Optimization: Pre-compute driver pools per geohash = 5× faster. New:
          128 servers. Cost: $64K/month (5× cheaper). 3) Caching: Cache driver locations
          in Redis (1M drivers × 100B = 100MB, fits in memory). Update every 3s. Reduces
          DB load 10×. 4) Async processing: Queue ride requests, process in batches
          (100 req/batch) = 2× throughput. 5) Multi-region: 5 regions × 128 servers
          = 640 servers, but local processing (10ms vs 100ms cross-region). Final:
          128 servers + Redis cluster = $70K/month.'
      - question: Design disaster recovery for multi-region deployment
        answer: 'Architecture: Active-active across 3 regions (US-West, US-East, EU).
          Per region: Full stack (API, matching, DB). Data: Users/Trips (sync replicated:
          strong consistency, 100ms lag), Locations (local only: eventual consistency).
          Failure: Region goes down → DNS/load balancer redirects to nearest region
          (30s failover). User impact: In-progress trips reassigned to backup region
          (drivers/riders in same region mostly). Trips in failed region: Use location
          history (last known) + manual assignment. RTO: 30s, RPO: 0 (sync replication).
          Cost: 3× infrastructure = $150K/month (vs $50K single region). Alternative:
          Active-passive (cheaper: 1.5× cost, but 5min RTO).'
      - question: Handle hotspots during peak hours (concerts, airports)
        answer: 'Problem: 10K riders request at same location (concert) = 100× normal
          density. Solution: 1) Dynamic pricing (surge): Increase price 2-5× → attract
          more drivers, reduce demand. 2) Pre-positioning: ML predicts events → notify
          drivers in advance (incentives). 3) Geohash expansion: If no drivers in
          cell, search adjacent cells (expand radius 1km → 3km → 5km). 4) Request
          queuing: Queue requests, process FIFO (estimated wait time: 10 min). 5)
          Dedicated servers: Spin up extra matching servers for event (auto-scaling:
          10× capacity for 2 hours). Cost: Surge pricing revenue offsets capacity
          cost. User experience: Transparent wait times + alternatives (carpool, shuttle).'
      - question: Scale payment processing to handle Black Friday volumes
        answer: 'Normal: 10M trips/day = 116 payments/sec. Black Friday: 100M trips/day
          = 1160 payments/sec (10×). Strategy: 1) Async processing: Charge after trip
          (not during) = decouples user experience from payment latency. Queue in
          Kafka (1160 msg/sec = trivial). 2) Payment provider scaling: Use Stripe
          (handles millions TPS), pre-warm with test transactions. 3) Retry logic:
          Exponential backoff (1s, 2s, 4s) for failed payments. Dead letter queue
          for manual review. 4) Database: Shard by user_id (100 shards), each handles
          11.6 payments/sec. 5) Idempotency: Use payment_id to prevent double-charge.
          Cost: Stripe fees (2.9% + 30¢) = $2.9M on $100M revenue. Capacity: No extra
          cost (Stripe auto-scales).'
      real_world_scenarios:
      - Driver reports wrong trip completion - design resolution flow
      - Network partition isolates 20% of drivers - how to handle?
      - Competitor enters market with 50% lower prices - system adaptations?
      - Government requires real-time location sharing - compliance design
    resources:
    - title: Designing Uber - High Scalability
      url: http://highscalability.com/blog/2022/1/11/designing-uber.html
      description: Comprehensive system design breakdown with architecture patterns
    - title: Uber Engineering Blog - Real-time Architecture
      url: https://eng.uber.com/category/architecture/
      description: Production insights from Uber's engineering team
    - title: Geospatial Indexing at Scale
      url: https://medium.com/@buckhx/unwinding-uber-s-most-efficient-service-406413c5871d
      description: Deep dive into location services and geospatial algorithms
    - title: System Design Interview Guide
      url: https://github.com/donnemartin/system-design-primer
      description: Complete preparation framework for system design interviews
    - title: Grokking System Design - Uber Design
      url: https://www.educative.io/courses/grokking-system-design-fundamentals
      description: Step-by-step system design interview preparation
    - title: Distributed Systems Patterns
      url: https://martinfowler.com/articles/patterns-of-distributed-systems/
      description: Common patterns for building distributed systems like ride-sharing
    - title: Real-time Data Processing at Uber
      url: https://eng.uber.com/real-time-exactly-once-ad-event-processing/
      description: Stream processing architecture for real-time systems
    time_estimate: 120
    video_resources:
    - title: 'ByteByteGo: Design Hotel Reservation System'
      url: https://www.youtube.com/@ByteByteGo
      duration: 15 min
      description: Booking system with concurrency
      priority: high
    - title: 'Hello Interview: Design Ticketmaster'
      url: https://www.youtube.com/@HelloInterview
      duration: 28 min
      description: High-concurrency ticket booking
      priority: high
  - day: 28
    topic: Mock Interview 2 - Video Streaming Platform (Netflix/YouTube)
    activity: Complete comprehensive capstone mock interview for video streaming platform
      demonstrating mastery of all Week 4 concepts with advanced scalability challenges.
    detailed_content: "Final Assessment Structure & Timing:\n- Requirements Clarification\
      \ (8-10 minutes): Scope, scale, features, constraints\n- High-Level Architecture\
      \ (15-18 minutes): Core services, data flow, global distribution\n- Deep Dive\
      \ Components (20-22 minutes): Video pipeline, CDN, recommendations, storage\n\
      - Advanced Challenges (8-10 minutes): Live streaming, ML, content protection\n\
      - Scale & Optimization (5-7 minutes): Bottlenecks, cost optimization, future\
      \ growth\n\nRequirements Gathering Framework:\n- Functional Requirements:\n\
      \  * User registration, authentication, and profile management\n  * Video upload,\
      \ processing, and transcoding pipeline\n  * Video streaming with adaptive bitrate\
      \ (ABR)\n  * Content discovery through search and browse\n  * Personalized recommendation\
      \ system\n  * User interactions (like, comment, share, subscribe)\n  * Content\
      \ creator monetization and analytics\n  * Offline downloads for mobile devices\n\
      \  * Live streaming capabilities\n  * Content moderation and copyright protection\n\
      \n- Non-Functional Requirements:\n  * Scale: 2B registered users, 1B daily active\
      \ users\n  * Content: 500M hours watched daily, 1M hours uploaded daily\n  *\
      \ Geographic: Global service across 190+ countries\n  * Availability: 99.95%\
      \ uptime with regional redundancy\n  * Latency: <200ms for video start, <3 seconds\
      \ buffering\n  * Storage: Petabytes of video content with multiple formats\n\
      \  * Bandwidth: Multi-Tbps global distribution capacity\n\nCore System Architecture:\n\
      \nVideo Ingestion & Processing Pipeline:\n- Upload Service: Chunked upload,\
      \ resume capability, metadata extraction\n- Transcoding Service: Multiple resolutions\
      \ (4K, 1080p, 720p, 480p, 360p)\n- Format Conversion: H.264, H.265/HEVC, VP9,\
      \ AV1 for different devices\n- Thumbnail Generation: Multiple timestamps, A/B\
      \ testing thumbnails\n- Quality Control: Automated content analysis, duplicate\
      \ detection\n- Storage: Raw videos in blob storage, processed videos in distributed\
      \ storage\n\nContent Delivery Network (CDN):\n- Global PoPs: 200+ edge locations\
      \ worldwide\n- Cache Hierarchy: Origin → Regional → Edge caches\n- Intelligent\
      \ Routing: Geographic and network-aware content delivery\n- Adaptive Streaming:\
      \ DASH/HLS protocols for bitrate adaptation\n- Prefetching: ML-driven content\
      \ pre-positioning based on predictions\n- Multi-CDN Strategy: Primary/secondary\
      \ CDN providers for redundancy\n\nRecommendation Engine Architecture:\n- Data\
      \ Collection: View history, engagement metrics, social signals\n- Feature Engineering:\
      \ User profiles, content features, contextual data\n- ML Models: Collaborative\
      \ filtering, deep neural networks, reinforcement learning\n- Real-time Serving:\
      \ Sub-100ms recommendation generation\n- A/B Testing: Continuous experimentation\
      \ on recommendation algorithms\n- Cold Start: New user/content recommendations\
      \ without historical data\n\nUser & Content Management:\n- User Service: Authentication,\
      \ profiles, preferences, viewing history\n- Content Catalog: Metadata service\
      \ with search indexing and faceting\n- Creator Studio: Upload interface, analytics\
      \ dashboard, monetization tools\n- Comment System: Threaded discussions, moderation,\
      \ spam detection\n- Subscription Service: Follow relationships, notification\
      \ management\n\nSearch & Discovery Service:\n- Search Index: Elasticsearch with\
      \ video metadata, transcripts, tags\n- Query Processing: Auto-complete, spell\
      \ correction, semantic search\n- Ranking Algorithm: Relevance, popularity, personalization,\
      \ recency\n- Browse Categories: Trending, genre-based, personalized sections\n\
      - Content Tagging: Automated tagging using computer vision and NLP\n\nDetailed\
      \ Component Deep Dives:\n\nVideo Processing Pipeline:\n- Distributed Transcoding:\
      \ Kubernetes-based auto-scaling workers\n- Format Optimization: Device-specific\
      \ encoding (mobile, TV, web)\n- Quality Metrics: PSNR, SSIM automated quality\
      \ assessment\n- Watermarking: Content protection and tracking\n- Closed Captions:\
      \ Auto-generation using speech recognition\n- Content Analysis: Violence, copyright,\
      \ spam detection using ML\n\nGlobal Distribution Strategy:\n- Regional Data\
      \ Centers: Primary content storage in 10+ regions\n- Edge Computing: Real-time\
      \ processing at edge locations\n- Bandwidth Optimization: Compression, efficient\
      \ protocols (QUIC)\n- Network Peering: Direct connections with ISPs for better\
      \ performance\n- Cost Optimization: Intelligent cache eviction, traffic engineering\n\
      \nRecommendation System Implementation:\n- Candidate Generation: Retrieve top-k\
      \ videos from billion+ catalog\n- Ranking Model: Deep neural networks with embeddings\n\
      - Diversity & Freshness: Balance popular and new content\n- Contextual Features:\
      \ Time of day, device type, location\n- Real-time Updates: Stream processing\
      \ for immediate feedback integration\n- Explainability: Transparent recommendations\
      \ with reasoning\n\nData Storage Architecture:\n- Video Storage: Distributed\
      \ object storage (S3, GCS) with replication\n- Metadata Database: Sharded SQL\
      \ databases for structured data\n- User Data: NoSQL for profiles, preferences,\
      \ viewing history\n- Analytics: Data lake with time-series databases for metrics\n\
      - Caching: Multi-level Redis/Memcached for hot data\n- Archive Storage: Cold\
      \ storage for old/unpopular content\n\nAdvanced Features & Challenges:\n\nLive\
      \ Streaming Infrastructure:\n- RTMP Ingestion: Real-time video input from creators\n\
      - Low-latency Streaming: WebRTC for interactive experiences\n- Transcoding:\
      \ Real-time processing with minimal delay\n- Chat Integration: Real-time messaging\
      \ with moderation\n- DVR Capability: Record live streams for later viewing\n\
      - Scalability: Handle millions of concurrent viewers\n\nMachine Learning Integration:\n\
      - Content Understanding: Automatic tagging, scene detection\n- Recommendation\
      \ Models: Collaborative and content-based filtering\n- Abuse Detection: Spam,\
      \ harassment, fake account identification\n- Thumbnail Selection: A/B testing\
      \ optimal video thumbnails\n- Ad Targeting: Personalized advertisement placement\n\
      - Creator Analytics: Audience insights and growth recommendations\n\nContent\
      \ Protection & Moderation:\n- DRM: Digital rights management for premium content\n\
      - Geo-blocking: Regional content restrictions and compliance\n- Copyright Detection:\
      \ Content ID system for automated protection\n- Community Guidelines: ML-based\
      \ content policy enforcement\n- Human Review: Escalation workflow for complex\
      \ moderation cases\n- DMCA Compliance: Takedown request processing and appeals\n\
      \nMonetization Systems:\n- Ad Serving: Real-time bidding, personalized ad insertion\n\
      - Subscription Management: Premium tiers, billing, content access\n- Creator\
      \ Revenue: Ad revenue sharing, super chat, memberships\n- Analytics: Revenue\
      \ tracking, creator dashboards, audience insights\n- Payment Processing: Global\
      \ payment methods, fraud detection\n\nScalability & Performance:\n- Auto-scaling:\
      \ Dynamic resource allocation based on traffic\n- Load Balancing: Geographic\
      \ and algorithm-aware traffic distribution\n- Database Sharding: Horizontal\
      \ partitioning strategies\n- Caching Strategy: Content popularity prediction\
      \ and preloading\n- Monitoring: Real-time alerting on performance degradation\n\
      - Capacity Planning: Predictive scaling for traffic growth\n\nSecurity & Privacy:\n\
      - Authentication: OAuth 2.0, multi-factor authentication\n- Data Protection:\
      \ Encryption at rest and in transit\n- Privacy Controls: User data management,\
      \ GDPR compliance\n- Access Control: Role-based permissions for content and\
      \ features\n- Security Monitoring: Anomaly detection, intrusion prevention\n\
      - Child Safety: Age verification, restricted content filtering\n\nOperational\
      \ Excellence:\n- Monitoring: Business and technical metrics dashboards\n- Alerting:\
      \ SLA violations, system health, user experience\n- Incident Response: Runbooks,\
      \ escalation procedures, post-mortems\n- Deployment: Blue-green deployments,\
      \ feature flags, rollback procedures\n- Cost Management: Resource optimization,\
      \ usage analytics\n- Compliance: Regional regulations, content standards, data\
      \ governance\n\nInterview Deep Dive Questions:\n- \"How do you handle a viral\
      \ video that suddenly gets 100M views?\"\n- \"Design the recommendation system\
      \ to avoid filter bubbles\"\n- \"How do you detect and prevent copyright infringement\
      \ at scale?\"\n- \"What's your strategy for expanding to a country with poor\
      \ internet?\"\n- \"How do you handle live streaming for 1M concurrent viewers?\"\
      \n\nFailure Scenarios & Mitigation:\n- CDN Failure: Multi-CDN failover, degraded\
      \ quality serving\n- Recommendation System Down: Fallback to trending/popular\
      \ content\n- Upload Service Overload: Queue management, priority processing\n\
      - Database Partition: Read-only mode, eventual consistency handling\n- DDoS\
      \ Attack: Rate limiting, traffic filtering, CDN protection\n\nPerformance Optimization\
      \ Strategies:\n- Video Compression: Next-gen codecs (AV1), quality-bitrate optimization\n\
      - Predictive Caching: ML-driven content pre-positioning\n- Edge Computing: Process\
      \ transcoding closer to users\n- Network Optimization: QUIC protocol, HTTP/3\
      \ adoption\n- Mobile Optimization: Adaptive streaming, offline capability\n\n\
      Cost Optimization:\n- Storage Tiering: Hot, warm, cold storage based on access\
      \ patterns\n- CDN Optimization: Traffic engineering, peering agreements\n- Transcoding\
      \ Efficiency: GPU acceleration, smart encoding\n- Infrastructure: Spot instances,\
      \ reserved capacity, auto-scaling\n- Content Lifecycle: Automated archival,\
      \ duplicate detection\n"
    practice_questions:
      capacity_estimation:
      - question: Calculate storage needs for 1M hours uploaded daily with multiple
          formats
        answer: 'Uploads: 1M hours/day. Formats: 4K (25GB/h), 1080p (5GB/h), 720p
          (2GB/h), 480p (1GB/h), 360p (500MB/h). Per upload: Average 50 min × (25
          + 5 + 2 + 1 + 0.5) = 50/60 × 33.5GB = 27.9GB per upload. Daily: 1M hours
          × 27.9GB = 27.9PB/day. With 2× (raw + processed): 55.8PB/day. Monthly: 1.67EB.
          Yearly: 20EB. Compression (HEVC 30% savings): 14EB/year. Storage tiers:
          Hot (30 days): 420PB, Warm (1 year): 14EB, Cold (archive): rest. Cost: Hot
          $42K/month, Warm $140K/month, Cold $20K/month = $202K/month.'
      - question: Estimate bandwidth for 1B users watching 500M hours daily
        answer: 'Watch time: 500M hours/day = 20.8M hours/hour = 5780 hours/sec. Average
          bitrate: 3Mbps (mix of qualities). Bandwidth: 5780 × 3Mbps = 17.3Tbps =
          17,300Gbps. Peak (5× avg): 86.5Tbps. CDN cost: $0.02/GB. Daily transfer:
          500M hours × 3Mbps × 3600s / 8 = 675PB/day. Monthly: 20.25EB. Cost: 20.25EB
          × $0.02/GB = 20,250TB × $20 = $405K/month. Optimization: Peer-to-peer (WebRTC)
          reduces CDN by 30% = $283K/month. Multi-CDN (Akamai + Cloudflare + Fastly):
          3× redundancy, negotiate bulk discount (50% off) = $202K/month.'
      - question: Size recommendation system to serve 2B users with <100ms latency
        answer: 'Users: 2B. Requests: 10 req/day per user = 20B req/day = 231K req/sec.
          Peak: 5× = 1.15M req/sec. Latency budget: <100ms. Model serving: 10ms (TensorFlow
          Serving on GPU). Feature lookup: 20ms (Redis). Post-processing: 10ms (ranking).
          Total: 40ms (60ms headroom). Throughput: 1 GPU server handles 100 req/sec
          (10ms × 100 = 1s). Servers needed: 1.15M / 100 = 11,500 GPU servers. Cost:
          $2/hour × 11,500 = $23K/hour = $16.6M/month. Optimization: Pre-compute top
          1000 recommendations per user (batch job overnight) → reduce real-time to
          100 servers = $150K/month. Trade-off: Freshness (12h old) vs cost (100×
          cheaper).'
      - question: Calculate transcoding capacity for real-time processing of uploads
        answer: 'Uploads: 1M hours/day = 41.6K hours/hour = 694 min/min = 694 concurrent
          transcodes. Per video: Transcode to 5 formats (4K, 1080p, 720p, 480p, 360p).
          Time: 1 hour source → 30 min transcode time (2× realtime). Parallel: 5 formats
          × 694 videos = 3470 concurrent transcode jobs. Hardware: 1 server (32 cores,
          2 GPUs) handles 8 concurrent transcodes. Servers needed: 3470 / 8 = 434
          servers. Cost: $500/month × 434 = $217K/month. Peak handling (5× avg): 2170
          servers = $1.08M/month (use spot instances 70% off) = $324K/month. Alternative:
          AWS MediaConvert (pay-per-minute) = $250K/month.'
      architecture_decisions:
      - question: SQL vs NoSQL for different data types in video platform?
        answer: 'Use SQL for: 1) Users (PostgreSQL): ACID transactions (subscriptions,
          payments), complex queries (churn analysis). 2) Videos metadata (PostgreSQL):
          Structured (title, description, upload_date), foreign keys (user_id, channel_id),
          analytics. Use NoSQL for: 1) View events (Cassandra): Write-heavy (20B events/day),
          time-series, partition by user_id + day. 2) Recommendations (Redis): In-memory,
          fast reads (<1ms), precomputed lists. 3) Comments (MongoDB): Nested structure
          (replies), high write volume (100M/day). Polyglot persistence: 5 databases.
          Cost: SQL $10K/month, NoSQL $50K/month = $60K total. Alternative: All-SQL
          (simpler, but 10× cost for view events).'
      - question: 'CDN vs regional data centers: cost and performance trade-offs'
        answer: 'CDN (Cloudflare, Akamai): Pros: Global edge locations (200+), <50ms
          latency anywhere, DDoS protection, handles traffic spikes. Cons: Expensive
          ($0.02/GB), less control, vendor lock-in. Cost: 20EB/month × $0.02/GB =
          $400K/month. Regional data centers (self-managed): Pros: Cheaper (owned
          bandwidth $0.005/GB), full control, custom optimizations. Cons: Limited
          coverage (10 regions), higher latency (100-200ms), need to manage. Cost:
          $100K/month bandwidth + $200K infra = $300K/month. Hybrid: CDN for popular
          content (80% of views, 20% of catalog) = $80K, data centers for long-tail
          = $300K. Total: $380K (5% savings + better control).'
      - question: Microservices vs monolith for video processing pipeline?
        answer: 'Video processing: Upload → Transcode → Thumbnail → Metadata → Publish.
          Monolith: Single service handles all steps. Pros: Simple deployment, easy
          debugging, low latency (in-memory). Cons: Single point of failure, hard
          to scale (transcoding needs 100× more resources than thumbnail). Microservices:
          Separate services (Upload, Transcode, Thumbnail, Metadata, Publish). Pros:
          Independent scaling (transcode 100× thumbnail), fault isolation (thumbnail
          fails, video still published), polyglot (Python ML, Go API). Cons: Complex
          (5 services), network overhead (10ms per hop), distributed tracing needed.
          Choose: Microservices (pipeline stages have vastly different resource needs).
          Cost: Monolith $50K/month, microservices $150K/month (3×, but necessary
          for scale).'
      - question: Real-time vs batch processing for recommendation updates?
        answer: 'Real-time: Update recommendations after each view (within seconds).
          Pros: Personalized instantly (user watches cat video → immediately see more
          cats), higher engagement. Cons: Expensive (1.15M req/sec × 10ms = 11,500
          GPU servers = $16.6M/month), complex (streaming ML). Batch: Update once
          per day (overnight job). Pros: Cheap (100 servers = $150K/month), simple
          (batch Spark job). Cons: Stale (12h old recommendations), lower engagement.
          Hybrid: Batch for base recommendations (nightly) + real-time for signals
          (trending, new uploads). Result: 90% batch, 10% real-time = $2M/month (vs
          $16.6M pure real-time). Trade-off: 95% of accuracy at 8× lower cost.'
      scalability_challenges:
      - question: Design disaster recovery for multi-region video platform
        answer: 'Architecture: 5 regions (US-West, US-East, EU, Asia, South America),
          active-active. Data: Videos (replicated to all regions via CDN), Metadata
          (PostgreSQL with cross-region replication: async, <1s lag), View events
          (Cassandra multi-DC: async, eventual consistency). Failure: Region down
          → CDN redirects to nearest region (10s DNS update). Impact: Users in failed
          region see 50-100ms higher latency, all content available. User uploads:
          S3 cross-region replication (async, <15 min). RTO: 10s (CDN failover), RPO:
          15 min (upload replication lag). Cost: 5× infrastructure = $1M/month (vs
          $200K single region). Monitoring: Health checks every 10s, auto-failover.
          Alternative: Active-passive = 2× cost, 5 min RTO.'
      - question: Handle 10x traffic growth during major live event
        answer: 'Normal: 500M hours/day = 5780 hours/sec = 17.3Tbps. Live event: 10×
          = 57,800 hours/sec = 173Tbps. Preparation: 1) Pre-scale CDN (notify providers
          24h advance, pre-warm caches). 2) Spin up 10× transcoding servers (spot
          instances, 2h before event). 3) Add read replicas (5× DB replicas for metadata).
          4) Rate limiting (non-live content: degrade to 720p max). 5) Graceful degradation
          (turn off recommendations, use cached trending). 6) Multi-CDN (Akamai +
          Cloudflare + Fastly = 3× capacity). Cost: Normal $400K/month, event day
          $4M (10× for 24h). Revenue: Ad revenue 5× higher during event offsets cost.
          Post-event: Scale down within 1h.'
      - question: Scale recommendation system for emerging markets with poor connectivity
        answer: 'Problem: Poor bandwidth (1-5Mbps), high latency (200-500ms), expensive
          data. Solution: 1) Lightweight models (10MB model vs 1GB, deploy to client).
          2) Offline-first (preload 50 videos when on WiFi). 3) P2P sharing (WebRTC
          between nearby users, reduce CDN 50%). 4) Lower bitrate (480p max, 1Mbps).
          5) Simplified UI (reduce API calls by 5×). 6) Regional data centers (closer
          to users, <100ms latency). 7) Partnerships (ISP caching, zero-rating deals).
          Cost: Additional regional DCs $50K/month, P2P infrastructure $20K/month
          = $70K investment. User growth: 10× in emerging markets. Revenue: Lower
          ARPU ($2/month vs $10), but volume compensates.'
      - question: Optimize video delivery for mobile-first regions
        answer: 'Mobile constraints: Small screen (6 inch), limited data (2GB/month),
          variable network (3G/4G/WiFi). Optimizations: 1) Adaptive bitrate: Start
          360p (500KB/min), upgrade to 720p if bandwidth allows. 2) Data saver mode:
          480p max, skip video previews = 50% data savings. 3) Smart preloading: Next
          2 videos when on WiFi (ML predicts what user will watch). 4) Thumbnail efficiency:
          WebP format (30% smaller than JPEG). 5) API optimization: Reduce JSON payload
          (send only mobile-needed fields) = 10× smaller. 6) Edge computing: Process
          recommendations at edge (reduce latency 200ms → 50ms). 7) Progressive download:
          Watch while downloading. Result: 50% less data usage, 4× faster load, 2×
          engagement. Cost: Edge servers $100K/month.'
      advanced_scenarios:
      - question: Competitor launches with AI-generated personalized content - response
          strategy
        answer: 'Competitor: AI generates unique videos per user (infinite content,
          perfect personalization). Challenges: We have static content (1B videos,
          same for all users). Response: 1) AI content generation (OpenAI Sora, Runway):
          Generate summaries, clips, mashups from existing videos. 2) Hyper-personalization:
          AI-generated thumbnails/titles per user (A/B test 100 variants, pick best).
          3) Interactive videos: Let users choose storyline (branching videos). 4)
          Creator tools: Give creators AI tools (auto-edit, captions, effects) = 10×
          more content. 5) AI dubbing: Translate to 100 languages with voice cloning.
          Timeline: MVP in 6 months, full rollout in 18 months. Cost: $50M/year (GPUs,
          engineering). Revenue impact: 20% engagement increase = $10B/year. ROI:
          200×.'
      - question: Government requires local data storage - compliance architecture
        answer: 'Requirement: User data (profiles, view history) must stay in-country
          (GDPR, China data laws). Architecture: 1) Data residency: Store user data
          in regional PostgreSQL (EU users → EU DB, China users → China DB). 2) Video
          content: Global CDN (content not user data, allowed). 3) Cross-border queries:
          Not allowed (EU user can''t query US data). 4) Data export: Manual approval
          (user requests data, gov''t signs off). 5) Encryption: At-rest + in-transit
          (AES-256). Implementation: Shard by country (200 countries = 200 DB shards).
          Cost: 10× more infrastructure (vs single global DB) = $2M/month. Alternative:
          Exit market (China, Russia = too complex). Revenue: Keep 80% of global market.'
      - question: Major content creator threatens platform switch - retention features
        answer: 'Problem: Creator (100M subscribers) threatens to move to competitor
          (better revenue share 70/30 vs our 55/45). Cost of losing: $100M/year ad
          revenue. Retention strategy: 1) Custom revenue deal (negotiate 70/30 for
          top creators). 2) Exclusive features (early access to new tools, priority
          support). 3) Audience lock-in (subscribers can''t easily follow to new platform).
          4) Analytics (better creator analytics, A/B testing tools). 5) Monetization
          tools (memberships, super chat, merch integration). 6) Multi-platform (let
          them post elsewhere, but premiere on our platform). Cost: $10M/year (custom
          deal + features). Alternative: Let them go, focus on 1000 mid-tier creators
          (diversification). Risk: Domino effect (other creators follow).'
      - question: New codec offers 50% compression - migration strategy
        answer: 'New codec: AV2 (50% smaller than H.265/HEVC), but slower encode (5×
          CPU). Benefits: Halve storage (20EB → 10EB = $100K/month savings), halve
          bandwidth (20EB/month → 10EB = $200K/month savings). Total: $300K/month
          = $3.6M/year. Migration: 1) Pilot: Re-encode top 1% of videos (most-watched)
          in AV2 (10TB, 1 week). 2) Measure: Verify 50% savings, check playback compatibility
          (95% of devices support AV2). 3) Gradual rollout: Re-encode 1%/month (entire
          catalog in 8 years, high-value content in 6 months). 4) New uploads: Encode
          in AV2 immediately. Cost: Re-encoding $1M (5× slower = 5× more servers ×
          6 months). ROI: Payback in 4 months. Challenge: Old devices (5% users) can''t
          play AV2 → dual-encode (AV2 + H.265) for 2 years.'
    resources:
    - title: 'ByteByteGo: Design YouTube'
      url: https://www.youtube.com/@ByteByteGo
      description: Complete walkthrough of video streaming platform architecture with
        CDN and encoding
    - title: 'ByteByteGo: Design Netflix'
      url: https://www.youtube.com/@ByteByteGo
      description: Visual explanation of content recommendation and adaptive bitrate
        streaming
    - title: 'Hello Interview: Design YouTube'
      url: https://www.hellointerview.com/learn/system-design/problem-breakdowns/youtube
      description: Interactive practice with level-specific guidance for video platform
        design
    - title: Netflix Technology Blog
      url: https://netflixtechblog.com/
      description: Production insights from Netflix engineering on streaming architecture
    - title: YouTube Engineering Blog
      url: https://blog.youtube/inside-youtube/engineering/
      description: Technical deep dives from YouTube's engineering team
    - title: Video Streaming at Scale
      url: https://www.infoq.com/presentations/netflix-streaming-scale/
      description: Netflix's approach to global video distribution
    - title: Video Compression and Encoding
      url: https://netflixtechblog.com/high-quality-video-encoding-at-scale-d159db052746
      description: Advanced video processing and compression techniques
    - title: System Design Interview - Volume 2
      url: https://www.amazon.com/System-Design-Interview-Insiders-Guide/dp/1736049119
      description: Comprehensive guide including video streaming system design
    time_estimate: 135
    video_resources:
    - title: 'Hello Interview: Mock System Design Interview'
      url: https://www.youtube.com/@HelloInterview
      duration: 45 min
      description: Full mock interview with feedback
      priority: high
    - title: 'ByteByteGo: System Design Interview Framework'
      url: https://www.youtube.com/@ByteByteGo
      duration: 20 min
      description: How to approach any system design problem
      priority: high
