track: ml_algorithms
description: 14-day ML algorithms curriculum covering deep learning and classical
  ML for Staff/Principal interviews
weeks:
  week1:
  - day: 1
    topic: Neural Network Fundamentals
    activity: Master forward/backward propagation, activation functions, loss functions,
      and optimization basics
    detailed_content: "Neural Network Mechanics:\n- Forward propagation: Layer-by-layer\
      \ computation from input to output\n  * Input → Hidden layers → Output\n  *\
      \ Each layer: z = Wx + b, a = activation(z)\n- Backpropagation: Computing gradients\
      \ via chain rule\n  * ∂L/∂W = ∂L/∂a * ∂a/∂z * ∂z/∂W\n  * Efficient gradient\
      \ computation through computational graph\n\nActivation Functions:\n- ReLU:\
      \ f(x) = max(0, x)\n  * Pros: No vanishing gradient, computationally efficient\n\
      \  * Cons: Dead neurons (always outputs 0), not zero-centered\n- Sigmoid: f(x)\
      \ = 1/(1 + e^-x)\n  * Pros: Output in [0,1], smooth gradient\n  * Cons: Vanishing\
      \ gradient for extreme values, not zero-centered\n- Tanh: f(x) = (e^x - e^-x)/(e^x\
      \ + e^-x)\n  * Pros: Zero-centered, output in [-1,1]\n  * Cons: Still has vanishing\
      \ gradient problem\n- Softmax: f(x_i) = e^x_i / Σe^x_j\n  * Use: Multi-class\
      \ classification output layer\n  * Properties: Outputs sum to 1, differentiable\n\
      \nLoss Functions:\n- Mean Squared Error (MSE): L = 1/n * Σ(y - ŷ)²\n  * Use:\
      \ Regression problems\n  * Properties: Penalizes large errors heavily\n- Cross-Entropy:\
      \ L = -Σ y*log(ŷ)\n  * Binary: -[y*log(ŷ) + (1-y)*log(1-ŷ)]\n  * Categorical:\
      \ -Σ y_i * log(ŷ_i)\n  * Use: Classification problems\n  * Why better than MSE\
      \ for classification: Addresses vanishing gradient\n- Hinge Loss: L = max(0,\
      \ 1 - y*ŷ)\n  * Use: SVMs, margin-based classifiers\n\nWeight Initialization:\n\
      - Xavier/Glorot: Var(W) = 2/(n_in + n_out)\n  * Good for sigmoid, tanh activations\n\
      - He Initialization: Var(W) = 2/n_in\n  * Good for ReLU activations\n  * Prevents\
      \ vanishing/exploding gradients\n\nBasic Optimizers:\n- SGD (Stochastic Gradient\
      \ Descent): w = w - η*∇L\n  * Simple but slow convergence\n- SGD with Momentum:\
      \ v = βv + ∇L; w = w - ηv\n  * Accelerates in relevant directions\n  * β typically\
      \ 0.9\n- Adam: Combines momentum + adaptive learning rates\n  * m = β₁m + (1-β₁)∇L\
      \ (momentum)\n  * v = β₂v + (1-β₂)(∇L)² (adaptive)\n  * Most popular optimizer\
      \ in practice\n"
    practice_questions:
      concepts:
      - question: Explain backpropagation and the chain rule. Why is it computationally
          efficient?
        answer: 'Backpropagation computes gradients by applying chain rule backward
          through the network. For loss L and weights W in layer l: ∂L/∂W_l = ∂L/∂a_l
          * ∂a_l/∂z_l * ∂z_l/∂W_l. Efficient because we reuse intermediate gradients
          (∂L/∂a_l) computed in later layers, avoiding redundant calculations. Time
          complexity: O(edges) in computational graph, same as forward pass.'
      - question: Why does ReLU help with vanishing gradient problem compared to sigmoid?
        answer: 'Sigmoid gradient: σ''(x) = σ(x)(1-σ(x)), max value 0.25. For deep
          networks, gradients multiply: ∂L/∂w₁ = ∂L/∂a_n * σ''(z_n) * ... * σ''(z₁)
          * x. With many layers, this product → 0 (vanishing). ReLU gradient: 1 if
          x>0, 0 otherwise. No saturation for positive values, so gradients don''t
          vanish through deep networks.'
      - question: Derive the gradient of cross-entropy loss with softmax output layer.
        answer: 'Let ŷ_i = e^z_i/Σe^z_j (softmax), L = -Σy_i*log(ŷ_i). Through chain
          rule: ∂L/∂z_i = ŷ_i - y_i. Remarkably simple! This is why cross-entropy
          + softmax is standard for classification. Gradient is just (prediction -
          label), which is intuitive and numerically stable.'
      tradeoffs:
      - question: When would you use sigmoid vs tanh vs ReLU activations?
        answer: 'ReLU: Default choice for hidden layers (fast, no vanishing gradient).
          Use in CNNs, deep networks. Sigmoid: Output layer for binary classification
          (outputs probability 0-1). Tanh: Hidden layers when zero-centered outputs
          beneficial (RNNs sometimes). Avoid sigmoid/tanh in deep networks due to
          vanishing gradients. Special cases: LeakyReLU for dead neuron problem, GELU
          for transformers.'
      - question: Compare SGD, SGD+Momentum, and Adam. When to use each?
        answer: 'SGD: Simple, works for convex problems but slow. SGD+Momentum: Better
          convergence, good for non-convex. Need to tune learning rate carefully.
          Adam: Adaptive learning rates, works well out-of-box, most popular for deep
          learning. Trade-offs: SGD can generalize better (reach flatter minima),
          Adam faster convergence but may overfit. Use Adam for quick prototyping,
          SGD+Momentum for final tuning in research.'
      estimation:
      - question: 'Calculate memory for 3-layer MLP: 784→256→128→10. Assume float32
          weights.'
        answer: 'Layer 1: 784*256 + 256 (bias) = 200,960 params. Layer 2: 256*128
          + 128 = 32,896 params. Layer 3: 128*10 + 10 = 1,290 params. Total: 235,146
          params. Memory: 235,146 * 4 bytes = 940KB for weights. Also need gradients
          (same size) + activations. Total: ~3-5MB including activations for batch
          size 32.'
    time_estimate: 90
    video_resources:
    - title: '3Blue1Brown: But what is a neural network?'
      url: https://www.youtube.com/@3blue1brown
      duration: 19 min
      description: Visual explanation of neural networks and backpropagation
      priority: high
    - title: 'StatQuest: Neural Networks Clearly Explained'
      url: https://www.youtube.com/@statquest
      duration: 15 min
      description: Intuitive explanation of forward and backward propagation
      priority: high
  - day: 2
    topic: Convolutional Neural Networks (CNNs)
    activity: Understand convolution operations, pooling, receptive fields, and classic
      architectures (VGG, ResNet)
    detailed_content: "Convolution Operation:\n- 2D Convolution: Output(i,j) = Σ Σ\
      \ Input(i+m, j+n) * Kernel(m,n)\n  * Kernel/Filter: Small matrix (3x3, 5x5)\
      \ sliding over input\n  * Stride: Step size for sliding (stride=1 means move\
      \ 1 pixel)\n  * Padding: Add zeros around border (same padding keeps spatial\
      \ dims)\n- Parameters in Conv layer: K filters of size (C_in × F × F) + K biases\n\
      \  * Example: 64 filters, 3x3, input 128 channels → 64*(128*3*3) + 64 = 73,792\
      \ params\n\nPooling Layers:\n- Max Pooling: Take maximum value in each region\
      \ (2x2)\n  * Downsamples spatial dimensions (H/2, W/2)\n  * Provides translation\
      \ invariance\n  * No learnable parameters\n- Average Pooling: Take average value\n\
      \  * Used in final layers (global average pooling)\n- Why pooling: Reduce computation,\
      \ increase receptive field, invariance\n\nReceptive Field:\n- Receptive field:\
      \ Region of input that affects one output neuron\n- Calculation: For L layers\
      \ with kernel k and stride s:\n  * RF = 1 + Σ(k_i - 1) * Π(s_j) for j<i\n- Example:\
      \ 2 conv layers (3x3, stride 1) → RF = 5x5\n- Deep networks have larger receptive\
      \ fields (see more context)\n\nClassic Architectures:\n- LeNet-5 (1998): Conv→Pool→Conv→Pool→FC→FC\n\
      \  * 60K parameters, designed for MNIST\n- VGG-16 (2014): Stack of 3x3 convs,\
      \ max pools\n  * Key insight: Multiple 3x3 conv = larger kernel but fewer params\n\
      \  * 2 layers of 3x3 = 5x5 receptive field\n  * 138M parameters (very large)\n\
      - ResNet (2015): Residual connections F(x) + x\n  * Solves degradation problem\
      \ (deeper networks worse)\n  * Skip connections allow gradients to flow directly\n\
      \  * ResNet-50: 25M parameters, 152 layers possible\n\nBatch Normalization:\n\
      - Normalize: z_norm = (z - μ_batch) / √(σ²_batch + ε)\n- Scale and shift: y\
      \ = γ*z_norm + β (learnable γ, β)\n- Benefits: Faster training, higher learning\
      \ rates, regularization\n- Where to use: After conv/linear, before activation\n\
      \nKey Design Patterns:\n- Bottleneck blocks: 1x1 conv to reduce channels, 3x3\
      \ conv, 1x1 to expand\n  * Reduces computation: (64→16→16→64) < (64→64)\n- Depthwise\
      \ separable conv: Separate spatial and channel convolutions\n  * Used in MobileNet\
      \ for efficiency\n"
    practice_questions:
      concepts:
      - question: Why are multiple 3x3 conv layers better than one 5x5 layer?
        answer: '1. Parameters: Two 3x3 layers have 2*(C²*9) = 18C² params vs one
          5x5 with 25C² params (28% fewer). 2. Receptive field: Same 5x5 RF. 3. Non-linearity:
          Two ReLU activations vs one, more expressive. 4. VGG popularized this: deeper
          networks with fewer parameters.'
      - question: Explain how residual connections in ResNet solve the degradation
          problem.
        answer: 'Without skip connections: deeper networks have worse training accuracy
          (not just overfitting). With F(x) + x: if identity mapping is optimal, network
          can learn F(x)=0. Gradients flow through skip connections directly: ∂L/∂x
          = ∂L/∂(F+x) * (∂F/∂x + 1). The ''+1'' ensures gradient doesn''t vanish.
          Enables training 100+ layer networks.'
      - question: How does batch normalization help training? What are the downsides?
        answer: 'Benefits: 1) Reduces internal covariate shift (layer inputs stay
          normalized), 2) Allows higher learning rates, 3) Acts as regularization
          (batch statistics add noise). Downsides: 1) Batch size dependency (small
          batches have noisy statistics), 2) Different behavior train vs test, 3)
          Doesn''t work well with batch size 1. Alternatives: Layer Norm (for transformers,
          RNNs), Group Norm (for small batches).'
      tradeoffs:
      - question: Compare VGG, ResNet, and EfficientNet. When to use each?
        answer: 'VGG: Simple architecture (easy to understand), 138M params, slow.
          Use for: teaching, small datasets with transfer learning. ResNet: 25M params
          (ResNet-50), skip connections, good accuracy. Use for: general CV, ImageNet
          baseline. EfficientNet: Best accuracy/params ratio via neural architecture
          search, 5M params. Use for: production (mobile, edge devices), when efficiency
          matters. Trade-off: VGG easiest to implement, ResNet best researched, EfficientNet
          most efficient.'
      estimation:
      - question: 'Calculate output dimensions and parameters for: Input 224x224x3,
          Conv(64 filters, 3x3, stride=2, padding=1), MaxPool(2x2, stride=2).'
        answer: 'Conv output: ((224+2*1-3)/2 + 1) = 112x112x64. Parameters: 64*(3*3*3)
          + 64 = 1,792. MaxPool output: 112/2 = 56x56x64. MaxPool parameters: 0 (no
          learnable params). Total output: 56x56x64 = 200,704 activations. Memory
          for batch=32: 32*200,704*4bytes = 25MB just for this layer''s activations.'
    time_estimate: 90
    video_resources:
    - title: '3Blue1Brown: Convolution'
      url: https://www.youtube.com/@3blue1brown
      duration: 21 min
      description: Beautiful visual explanation of convolutions
      priority: high
    - title: 'Andrej Karpathy: CNNs from Scratch'
      url: https://www.youtube.com/@AndrejKarpathy
      duration: 25 min
      description: Building CNNs step by step
      priority: high
  - day: 3
    topic: Recurrent Neural Networks (RNNs) and LSTMs
    activity: Master sequence modeling, vanishing gradients, LSTM/GRU architecture,
      and bidirectional RNNs
    detailed_content: "RNN Basics:\n- Recurrent computation: h_t = tanh(W_hh * h_{t-1}\
      \ + W_xh * x_t + b_h)\n- Output: y_t = W_hy * h_t + b_y\n- Hidden state h_t\
      \ acts as memory of past sequence\n- Shared weights across time steps\n\nVanishing/Exploding\
      \ Gradient Problem:\n- Gradient through time: ∂L/∂h_0 = ∂L/∂h_T * Π ∂h_t/∂h_{t-1}\n\
      - ∂h_t/∂h_{t-1} = diag(tanh'(·)) * W_hh\n- If largest eigenvalue of W < 1: gradients\
      \ vanish (→0)\n- If largest eigenvalue of W > 1: gradients explode (→∞)\n- Result:\
      \ RNNs can't learn long-term dependencies (T > 10-20 steps)\n\nLSTM (Long Short-Term\
      \ Memory):\n- Cell state c_t: runs through time with minimal modification\n\
      - Gates (all sigmoid σ, output 0-1):\n  * Forget gate: f_t = σ(W_f * [h_{t-1},\
      \ x_t] + b_f)\n  * Input gate: i_t = σ(W_i * [h_{t-1}, x_t] + b_i)\n  * Output\
      \ gate: o_t = σ(W_o * [h_{t-1}, x_t] + b_o)\n- Cell update:\n  * Candidate:\
      \ c̃_t = tanh(W_c * [h_{t-1}, x_t] + b_c)\n  * New cell: c_t = f_t ⊙ c_{t-1}\
      \ + i_t ⊙ c̃_t\n  * Hidden: h_t = o_t ⊙ tanh(c_t)\n- Key insight: Cell state\
      \ allows unobstructed gradient flow\n\nGRU (Gated Recurrent Unit):\n- Simplified\
      \ LSTM: 2 gates instead of 3\n- Update gate: z_t = σ(W_z * [h_{t-1}, x_t])\n\
      - Reset gate: r_t = σ(W_r * [h_{t-1}, x_t])\n- Hidden: h_t = (1-z_t) ⊙ h_{t-1}\
      \ + z_t ⊙ h̃_t\n- Fewer parameters than LSTM, often similar performance\n\n\
      Bidirectional RNN:\n- Forward pass: h→_t processes sequence left-to-right\n\
      - Backward pass: h←_t processes sequence right-to-left\n- Concatenate: h_t =\
      \ [h→_t; h←_t]\n- Use: When future context available (not for generation)\n\
      - Examples: BERT, sentence classification\n\nSequence-to-Sequence:\n- Encoder:\
      \ RNN that processes input sequence → context vector\n- Decoder: RNN that generates\
      \ output sequence from context\n- Bottleneck: Single context vector for entire\
      \ input\n- Solution: Attention mechanism (see Day 4)\n"
    practice_questions:
      concepts:
      - question: Explain why vanilla RNNs suffer from vanishing gradients. Derive
          the gradient flow.
        answer: 'Gradient at time 0: ∂L/∂h_0 = ∂L/∂h_T * (∂h_T/∂h_{T-1}) * ... * (∂h_1/∂h_0).
          Each ∂h_t/∂h_{t-1} = diag(tanh''(·)) * W_hh. For T steps, gradient scales
          as (W_hh)^T. If largest eigenvalue λ < 1, then λ^T → 0 exponentially. For
          T=20 and λ=0.9: gradient is 0.9^20 = 0.12, very small. Can''t learn dependencies
          beyond 10-20 steps.'
      - question: How do LSTM gates solve the vanishing gradient problem? Explain
          the cell state gradient flow.
        answer: 'Cell state gradient: ∂c_t/∂c_{t-1} = f_t (forget gate). Unlike RNN
          where ∂h_t/∂h_{t-1} involves W matrix multiplication, LSTM uses element-wise
          multiplication by forget gate. If f_t ≈ 1, gradient flows unchanged. Cell
          state acts as ''highway'' for gradients. Key insight: Additive update (c_t
          = f_t⊙c_{t-1} + i_t⊙c̃_t) vs multiplicative in RNN. Allows learning dependencies
          100+ steps.'
      - question: Compare LSTM and GRU. When would you choose one over the other?
        answer: 'LSTM: 3 gates (forget, input, output), more parameters, more expressive.
          GRU: 2 gates (update, reset), fewer parameters, faster training. Performance:
          Similar on most tasks, GRU slightly faster. Choose LSTM: When you have lots
          of data and computational resources, need maximum expressiveness. Choose
          GRU: Limited data/compute, faster prototyping, mobile deployment. In practice:
          Try both, GRU often wins in speed vs accuracy trade-off.'
      tradeoffs:
      - question: When would you use an RNN/LSTM vs a Transformer for sequence modeling?
        answer: 'RNN/LSTM advantages: 1) Constant memory O(1) for sequence length,
          2) Good for streaming/online processing, 3) Natural for variable-length
          sequences. Transformer advantages: 1) Parallelizable training (RNN is sequential),
          2) Better at long-range dependencies, 3) State-of-the-art results. Use RNN:
          Edge devices, streaming data, very long sequences (>1000 tokens), limited
          compute. Use Transformer: When accuracy is critical, have GPUs for training,
          sequences <512 tokens. In 2024: Transformers dominate most NLP tasks.'
      estimation:
      - question: Calculate parameters for LSTM with input size 300, hidden size 512,
          and explain memory for sequence length 100.
        answer: 'LSTM has 4 weight matrices (forget, input, output, candidate), each
          size (hidden+input) × hidden. Params: 4 * (512+300) * 512 = 1,662,976 ≈
          1.7M params. For sequence length 100: need to store hidden states h_t and
          cell states c_t for all timesteps. Memory: 100 * 512 * 2 * 4 bytes * batch_size.
          For batch=32: 13MB just for hidden states. Add gradients (same size) for
          backprop through time: total ~40MB per batch.'
    time_estimate: 75
    video_resources:
    - title: 'StatQuest: RNNs and LSTMs Clearly Explained'
      url: https://www.youtube.com/@statquest
      duration: 18 min
      description: How RNNs and LSTMs handle sequences
      priority: high
    - title: 'Andrej Karpathy: The Unreasonable Effectiveness of RNNs'
      url: https://www.youtube.com/@AndrejKarpathy
      duration: 20 min
      description: Deep dive into RNN applications
      priority: medium
  - day: 4
    topic: Transformers and Attention Mechanisms
    activity: Master self-attention, multi-head attention, positional encoding, and
      understand BERT vs GPT architectures
    detailed_content: "Self-Attention Mechanism:\n- Query, Key, Value: Q = X*W_Q,\
      \ K = X*W_K, V = X*W_V\n- Attention scores: A = softmax(Q*K^T / √d_k)\n- Output:\
      \ Attention(Q,K,V) = A*V\n- Intuition: Each token attends to all other tokens\n\
      - √d_k scaling: Prevents softmax saturation for large d_k\n\nScaled Dot-Product\
      \ Attention:\n- Score(q, k) = q·k / √d_k\n- Why scaling: Without it, dot products\
      \ grow large for high dimensions\n  * Softmax(large values) → near one-hot,\
      \ vanishing gradients\n- Time complexity: O(n² d) where n is sequence length\n\
      \nMulti-Head Attention:\n- h parallel attention heads, each with d_k = d_model/h\n\
      - Head_i = Attention(QW^Q_i, KW^K_i, VW^V_i)\n- Concat all heads, project: MultiHead\
      \ = Concat(heads)*W_O\n- Why multiple heads: Attend to different representation\
      \ subspaces\n  * Example: One head for syntax, one for semantics\n\nPositional\
      \ Encoding:\n- Transformers have no recurrence → no positional information\n\
      - Add position embeddings to input embeddings\n- Sinusoidal encoding: PE(pos,\
      \ 2i) = sin(pos/10000^(2i/d))\n                       PE(pos, 2i+1) = cos(pos/10000^(2i/d))\n\
      - Why sinusoidal: Can extrapolate to longer sequences than training\n- Alternative:\
      \ Learned positional embeddings (BERT uses this)\n\nTransformer Encoder:\n-\
      \ Multi-head attention → Add & Norm → FFN → Add & Norm\n- Feed-forward: FFN(x)\
      \ = max(0, xW_1 + b_1)W_2 + b_2\n  * Position-wise (same FFN for all positions)\n\
      - Residual connections: Prevent gradient vanishing\n- Layer normalization: Normalize\
      \ across features (not batch)\n\nBERT (Bidirectional Encoder Representations):\n\
      - Architecture: Stack of transformer encoders (12 or 24 layers)\n- Pretraining\
      \ tasks:\n  * Masked Language Modeling: Predict masked tokens [MASK]\n  * Next\
      \ Sentence Prediction: Classify if sentence B follows A\n- Fine-tuning: Add\
      \ task-specific head, train on downstream task\n- Use cases: Classification,\
      \ NER, question answering\n\nGPT (Generative Pre-trained Transformer):\n- Architecture:\
      \ Stack of transformer decoders (autoregressive)\n- Causal masking: Can only\
      \ attend to previous tokens\n- Pretraining: Next token prediction (language\
      \ modeling)\n- Use cases: Text generation, few-shot learning\n\nVision Transformer\
      \ (ViT):\n- Split image into patches (16x16 pixels)\n- Linear embed each patch\
      \ → add positional embeddings\n- Pass through transformer encoder\n- Use [CLS]\
      \ token for image classification\n- Key insight: Transformers can work on any\
      \ sequence (not just text)\n"
    practice_questions:
      concepts:
      - question: Derive the scaled dot-product attention formula and explain why
          scaling by √d_k is necessary.
        answer: 'Attention(Q,K,V) = softmax(QK^T/√d_k)V. Derivation: For query q and
          key k, score is dot product q·k = Σq_i*k_i. If q,k are random with variance
          1, then q·k has variance d_k. For large d_k, q·k can be very large → softmax([10,
          1, 0.1]) ≈ [1, 0, 0] (near one-hot) → gradient ≈ 0. Dividing by √d_k normalizes
          variance to 1, keeping softmax gradients healthy.'
      - question: How does multi-head attention differ from single-head? What are
          the benefits?
        answer: 'Single-head: One set of Q,K,V projections, attends to one representation.
          Multi-head (h heads): Split d_model into h heads, each with d_k=d_model/h
          dimensions. Each head learns different attention patterns (syntax, semantics,
          long-range, local). Concatenate outputs: O(n²d) complexity same as single-head
          but more expressive. Analogy: Ensemble of attention mechanisms. BERT uses
          12 heads, GPT-3 uses 96 heads.'
      - question: Explain the difference between BERT and GPT architectures. When
          to use each?
        answer: 'BERT: Encoder-only, bidirectional attention (sees full context).
          Trained with MLM + NSP. Best for: Understanding tasks (classification, NER,
          QA). GPT: Decoder-only, causal masking (only sees past). Trained with next-token
          prediction. Best for: Generation tasks (text completion, dialogue). Key
          difference: BERT can''t generate text (no autoregressive structure), GPT
          less effective at understanding (can''t see future context). For fine-tuning:
          BERT for discriminative, GPT for generative.'
      tradeoffs:
      - question: Compare Transformers vs RNNs for sequence modeling. What are the
          trade-offs?
        answer: 'Transformers: 1) Parallelizable (train fast on GPUs), 2) Better long-range
          dependencies (direct attention), 3) O(n²) memory/computation. RNNs: 1) Sequential
          (slow training), 2) O(1) memory for sequence length, 3) Struggle with long
          dependencies. Use Transformers: Most NLP (translation, QA, generation),
          have GPU resources, sequences <2000 tokens. Use RNNs: Edge devices, streaming
          data, very long sequences, limited memory. In practice: Transformers won
          in NLP, but RNNs still used for time series with constraints.'
      estimation:
      - question: 'Calculate parameters for BERT-base: 12 layers, hidden=768, heads=12,
          vocab=30K. Estimate memory for sequence length 512.'
        answer: 'Per layer: Q,K,V projections 3*(768*768) + FFN 768*3072 + 3072*768
          = 7.1M params. 12 layers: 85M. Embeddings: 30K*768 = 23M. Position: 512*768
          = 0.4M. Total: ~110M params (matches BERT-base). Memory for batch=8, seq=512:
          Activations 8*512*768*12 layers = 38M floats = 150MB. Attention matrices
          8*12*512*512 = 25M = 100MB. Total: ~350MB for forward pass, double for backward.'
    time_estimate: 90
    video_resources:
    - title: '3Blue1Brown: Attention in Transformers'
      url: https://www.youtube.com/@3blue1brown
      duration: 25 min
      description: Visual explanation of attention mechanisms
      priority: high
    - title: 'Andrej Karpathy: Let''s build GPT from scratch'
      url: https://www.youtube.com/@AndrejKarpathy
      duration: 120 min
      description: Complete transformer implementation
      priority: high
  - day: 5
    topic: Generative Models - Autoencoders, VAEs, and GANs
    activity: Understand autoencoder architectures, variational inference, and GAN
      training dynamics
    detailed_content: "Autoencoder (AE):\n- Architecture: Encoder → Latent code →\
      \ Decoder\n- Encoder: Maps input x to latent z (dimensionality reduction)\n\
      - Decoder: Reconstructs x̂ from z\n- Loss: Reconstruction loss ||x - x̂||²\n\
      - Uses: Dimensionality reduction, denoising, feature learning\n\nVariational\
      \ Autoencoder (VAE):\n- Probabilistic autoencoder: Learn distribution p(z|x)\n\
      - Encoder outputs: μ(x) and σ(x) (mean and std of z)\n- Reparameterization trick:\
      \ z = μ + σ⊙ε where ε ~ N(0,1)\n  * Allows backpropagation through sampling\n\
      - Loss: ELBO = E[log p(x|z)] - KL(q(z|x)||p(z))\n  * Reconstruction term + regularization\n\
      \  * KL divergence: Forces q(z|x) to match prior N(0,1)\n- Sampling: Generate\
      \ new x by sampling z ~ N(0,1), decode\n\nDenoising Autoencoder (DAE):\n- Add\
      \ noise to input x̃ = x + noise\n- Train to reconstruct clean x from x̃\n- Forces\
      \ robust feature learning\n- Use: Pretraining, data augmentation\n\nGenerative\
      \ Adversarial Network (GAN):\n- Two networks playing minimax game:\n  * Generator\
      \ G: Maps noise z → fake image x̂\n  * Discriminator D: Classifies real vs fake\n\
      - Adversarial loss:\n  * D wants to maximize: E[log D(x)] + E[log(1-D(G(z)))]\n\
      \  * G wants to minimize: E[log(1-D(G(z)))]\n- Training: Alternating optimization\n\
      \  1. Train D to distinguish real from G(z)\n  2. Train G to fool D\n\nGAN Training\
      \ Challenges:\n- Mode collapse: G produces limited variety of outputs\n  * Symptom:\
      \ All generated images look similar\n  * Solutions: Minibatch discrimination,\
      \ Wasserstein GAN\n- Non-convergence: Losses oscillate, no stable equilibrium\n\
      \  * Nash equilibrium is hard to find\n  * Solutions: Spectral normalization,\
      \ progressive growing\n- Vanishing gradients: If D is too good, G gets no signal\n\
      \  * Solution: Non-saturating loss, Wasserstein distance\n\nGAN Variants:\n\
      - DCGAN (Deep Convolutional GAN): Use conv layers, batch norm\n- Conditional\
      \ GAN: Condition on label y: G(z, y) and D(x, y)\n- CycleGAN: Unpaired image-to-image\
      \ translation\n- StyleGAN: Control different levels of detail\n\nDiffusion Models\
      \ (Conceptual):\n- Forward process: Gradually add noise to data (T steps)\n\
      - Reverse process: Learn to denoise (neural network)\n- Sampling: Start from\
      \ noise, iteratively denoise\n- Advantages over GANs: More stable training,\
      \ better mode coverage\n- Current state-of-art for image generation\n"
    practice_questions:
      concepts:
      - question: Explain the reparameterization trick in VAEs. Why is it necessary
          for backpropagation?
        answer: 'Problem: Can''t backprop through sampling z ~ N(μ,σ²). Gradients
          don''t flow through random sampling. Solution: z = μ + σ⊙ε where ε ~ N(0,1).
          Now randomness is in ε (doesn''t depend on μ,σ), and transformation is deterministic.
          ∂z/∂μ = 1, ∂z/∂σ = ε. Gradients can flow through μ and σ. This allows end-to-end
          training of encoder and decoder.'
      - question: Why do GANs suffer from mode collapse? What are potential solutions?
        answer: 'Mode collapse: G finds a few samples that fool D, ignores rest of
          data distribution. Why: G optimizes to maximize D error, not to cover all
          modes. If one mode is easy to fool D, G focuses there. Solutions: 1) Minibatch
          discrimination (penalize similar outputs in batch), 2) Unrolled GAN (G looks
          ahead at D''s response), 3) Wasserstein GAN (better gradient signal), 4)
          Mode regularization (encourage diversity).'
      - question: Compare VAE vs GAN for image generation. What are the trade-offs?
        answer: 'VAE: Pros: Stable training, can encode/decode, latent space interpolation.
          Cons: Blurry images (MSE loss), lower sample quality. GAN: Pros: Sharp,
          realistic images, state-of-art quality. Cons: Training instability, mode
          collapse, can''t encode images. Use VAE: Need stable training, want latent
          space manipulation, understand data distribution. Use GAN: Need highest
          quality samples, don''t need encoding. Modern: Diffusion models surpassing
          both.'
      tradeoffs:
      - question: When would you use an autoencoder vs PCA for dimensionality reduction?
        answer: 'PCA: Linear projection, fast, interpretable, closed-form solution.
          Good for: Linear structure, small data, need interpretability. Autoencoder:
          Non-linear, can learn complex manifolds, needs training. Good for: High-dimensional
          data (images), non-linear structure, large datasets. Trade-offs: PCA is
          faster and guaranteed to find optimal linear subspace. Autoencoders are
          more powerful but can overfit and require tuning. For images: autoencoders
          much better. For tabular: try PCA first.'
      estimation:
      - question: Design a VAE for MNIST (28x28 grayscale). Choose architecture and
          calculate parameters.
        answer: 'Encoder: 784→400→200→20 (latent). Output: μ (20-dim) and log_σ² (20-dim).
          Decoder: 20→200→400→784. Parameters: Encoder: 784*400 + 400*200 + 200*40
          = 401,600. Decoder: 20*200 + 200*400 + 400*784 = 397,600. Total: ~800K params.
          Training: Batch size 128, Adam optimizer, β=0.001. Loss weights: Recon=1.0,
          KL=0.01 (anneal from 0). Generate: Sample z~N(0,1), pass through decoder.'
    time_estimate: 75
    video_resources:
    - title: 'StatQuest: VAEs Clearly Explained'
      url: https://www.youtube.com/@statquest
      duration: 22 min
      description: Variational autoencoders intuition
      priority: high
    - title: 'StatQuest: GANs Clearly Explained'
      url: https://www.youtube.com/@statquest
      duration: 18 min
      description: Generative adversarial networks fundamentals
      priority: high
  week2:
  - day: 6
    topic: Optimization and Regularization Techniques
    activity: Master advanced optimizers (Adam, AdamW), learning rate schedules, and
      regularization methods
    detailed_content: "Advanced Optimizers:\n- Adam (Adaptive Moment Estimation):\n\
      \  * First moment: m_t = β₁*m_{t-1} + (1-β₁)*∇L (momentum)\n  * Second moment:\
      \ v_t = β₂*v_{t-1} + (1-β₂)*(∇L)² (adaptive LR)\n  * Bias correction: m̂ = m_t/(1-β₁^t),\
      \ v̂ = v_t/(1-β₂^t)\n  * Update: θ = θ - α*m̂/√(v̂ + ε)\n  * Default: β₁=0.9,\
      \ β₂=0.999, α=0.001\n- AdamW (Adam with Weight Decay):\n  * Decouples weight\
      \ decay from gradient update\n  * Better generalization than Adam with L2\n\
      - RMSprop: Second moment only (no momentum)\n  * Good for RNNs with non-stationary\
      \ objectives\n\nLearning Rate Schedules:\n- Step Decay: Reduce LR by factor\
      \ every N epochs\n  * Example: α = α₀ * 0.1^(epoch/30)\n- Cosine Annealing:\
      \ α_t = α_min + 0.5*(α_max - α_min)*(1 + cos(πt/T))\n  * Smooth decrease, avoids\
      \ sudden drops\n- Warm Restarts: Periodically reset to high LR\n  * Helps escape\
      \ local minima\n- Linear Warmup: Start with small LR, gradually increase\n \
      \ * Prevents instability early in training\n\nRegularization Techniques:\n-\
      \ L1 Regularization (Lasso): Loss + λ*Σ|w_i|\n  * Induces sparsity (many weights\
      \ → 0)\n  * Feature selection effect\n- L2 Regularization (Ridge): Loss + λ*Σw_i²\n\
      \  * Shrinks weights uniformly\n  * Preferred for neural networks\n- Elastic\
      \ Net: L1 + L2 combined\n\nDropout:\n- Training: Randomly set neurons to 0 with\
      \ probability p\n- Test: Use all neurons, scale by (1-p)\n- Effect: Prevents\
      \ co-adaptation of features\n- Where to apply: After dense layers, before final\
      \ layer\n- Typical p: 0.3-0.5 for hidden layers\n\nBatch/Layer/Instance Normalization:\n\
      - Batch Norm: Normalize over batch dimension\n  * Good for CNNs, large batches\n\
      \  * Problem: Small batch size → noisy statistics\n- Layer Norm: Normalize over\
      \ feature dimension\n  * Good for RNNs, Transformers, batch size 1\n  * Independent\
      \ of batch size\n- Instance Norm: Normalize per sample, per channel\n  * Good\
      \ for style transfer\n- Group Norm: Compromise between batch and layer\n\nEarly\
      \ Stopping:\n- Monitor validation loss during training\n- Stop when val loss\
      \ stops improving (patience=5-10 epochs)\n- Restore best model weights\n- Prevents\
      \ overfitting without sacrificing capacity\n\nData Augmentation:\n- Images:\
      \ Random crop, flip, rotation, color jitter\n- Text: Synonym replacement, back-translation\n\
      - Why it works: Expands effective training set, regularization\n\nGradient Clipping:\n\
      - Clip gradients to max norm: if ||g|| > θ, g = θ*g/||g||\n- Prevents exploding\
      \ gradients in RNNs\n- Typical threshold: θ = 1.0 or 5.0\n"
    practice_questions:
      concepts:
      - question: Explain how Adam optimizer combines momentum and adaptive learning
          rates. Derive the update rule.
        answer: 'Adam maintains two moving averages: m_t (first moment, like momentum)
          and v_t (second moment, adaptive LR). m_t = β₁m_{t-1} + (1-β₁)∇L tracks
          gradient direction with momentum. v_t = β₂v_{t-1} + (1-β₂)(∇L)² tracks gradient
          magnitude for adaptive scaling. Bias correction: m̂=m_t/(1-β₁^t) compensates
          for initialization at 0. Update: θ = θ - α*m̂/√(v̂+ε). Benefits: Fast convergence
          (momentum) + adaptive per-parameter LR (handles sparse gradients).'
      - question: Why does L1 regularization induce sparsity but L2 doesn't? Provide
          geometric and analytical explanations.
        answer: 'Geometric: L1 constraint is diamond-shaped (|w₁|+|w₂|≤C), L2 is circular
          (w₁²+w₂²≤C). Optimization moves along contours of loss until hitting constraint.
          Diamond has corners on axes → sparse solutions. Analytical: L1 gradient
          is constant (±λ), pushes weights to exactly 0. L2 gradient is proportional
          to weight (2λw), shrinks but never reaches 0. For weight w: L1 pushes by
          λ regardless of size. L2 pushes by 2λw, slows as w→0.'
      - question: Compare Batch Normalization vs Layer Normalization. When to use
          each?
        answer: 'Batch Norm: Normalizes over batch dimension (N, H, W, C) → compute
          μ,σ over N. Good for CNNs, large batches. Problem: Batch-dependent, breaks
          with small batches or batch size 1. Layer Norm: Normalizes over feature
          dimension (N, H, W, C) → compute μ,σ over (H,W,C) per sample. Good for RNNs,
          Transformers, any batch size. Use BN: CNNs with batch size ≥32. Use LN:
          Transformers, RNNs, or when batch size varies/small. BERT/GPT use Layer
          Norm.'
      tradeoffs:
      - question: Compare SGD+Momentum vs Adam for training deep networks. When would
          you choose each?
        answer: 'SGD+Momentum: Pros: Better generalization (flatter minima), fewer
          hyperparams. Cons: Sensitive to LR, slower convergence. Adam: Pros: Fast
          convergence, works out-of-box, handles sparse gradients. Cons: May overfit,
          more hyperparams (β₁, β₂, ε). Use SGD: Final tuning for best accuracy, research
          papers. Use Adam: Quick prototyping, unknown problem, limited tuning time.
          Best practice: Start with Adam for quick baseline, switch to SGD+Momentum
          if overfitting.'
      estimation:
      - question: Calculate memory overhead for Adam optimizer training a model with
          100M parameters.
        answer: 'Adam requires: 1) Model weights: 100M params * 4 bytes = 400MB. 2)
          Gradients: 100M * 4 bytes = 400MB. 3) First moment m_t: 100M * 4 bytes =
          400MB. 4) Second moment v_t: 100M * 4 bytes = 400MB. Total: 1.6GB just for
          optimizer state. Add activations (depends on batch size). For batch=32 and
          deep network: ~2-4GB activations. Total training memory: ~4-6GB. This is
          why large models need multiple GPUs.'
    time_estimate: 75
    video_resources:
    - title: 'StatQuest: Regularization (L1, L2, Dropout)'
      url: https://www.youtube.com/@statquest
      duration: 16 min
      description: Preventing overfitting techniques
      priority: high
    - title: 'Andrej Karpathy: Training Neural Nets'
      url: https://www.youtube.com/@AndrejKarpathy
      duration: 30 min
      description: Practical optimization tips
      priority: high
  - day: 7
    topic: Transfer Learning and Fine-tuning
    activity: Master pretrained models, feature extraction, fine-tuning strategies,
      and domain adaptation
    detailed_content: "Transfer Learning Fundamentals:\n- Motivation: Training from\
      \ scratch needs massive data and compute\n- Key insight: Low-level features\
      \ transfer across domains\n  * CNNs: Edges, textures, patterns (ImageNet → medical\
      \ images)\n  * Transformers: Language understanding (BERT → domain-specific\
      \ text)\n- When to use: Limited labeled data (< 10K samples), related domains\n\
      \nPretrained Models:\n- Vision:\n  * ResNet-50/101: ImageNet (1M images, 1000\
      \ classes)\n  * EfficientNet: State-of-art accuracy/efficiency\n  * Vision Transformer\
      \ (ViT): Transformer for images\n- NLP:\n  * BERT: Bidirectional understanding,\
      \ 110M-340M params\n  * GPT-2/3: Text generation, 1.5B-175B params\n  * RoBERTa:\
      \ Improved BERT training\n- Multimodal:\n  * CLIP: Vision + language alignment\n\
      \  * DALL-E: Text → image generation\n\nFeature Extraction vs Fine-tuning:\n\
      - Feature Extraction:\n  * Freeze all pretrained layers\n  * Only train new\
      \ classification head\n  * Fast, needs less data\n  * Use when: Very different\
      \ domain or very small data\n- Fine-tuning:\n  * Unfreeze some/all layers, train\
      \ with small LR\n  * Adapts features to new domain\n  * Better accuracy but\
      \ needs more data\n  * Use when: >1K labeled samples, related domain\n\nFine-tuning\
      \ Strategies:\n- Gradual unfreezing: Start with head, progressively unfreeze\
      \ layers\n- Discriminative learning rates: Lower LR for earlier layers\n  *\
      \ Head: 1e-3, Middle: 1e-4, Base: 1e-5\n- Layer-wise learning rate decay: LR\
      \ decreases deeper in network\n- Warm-up: Train head first (freeze base), then\
      \ unfreeze\n\nDomain Adaptation:\n- Problem: Training data distribution ≠ test\
      \ data distribution\n- Techniques:\n  * Domain adversarial training: Force domain-invariant\
      \ features\n  * Self-training: Use model predictions as pseudo-labels\n  * Data\
      \ augmentation: Match target domain characteristics\n- Example: ImageNet → medical\
      \ images (different color distribution)\n\nFew-Shot Learning:\n- Goal: Learn\
      \ from few examples (1-10 per class)\n- Meta-learning: Learn to learn (MAML,\
      \ Prototypical Networks)\n- Prompting (GPT-3): Provide examples in prompt\n\
      - Use case: Rare diseases, new product categories\n\nPractical Considerations:\n\
      - Data size vs strategy:\n  * < 100 samples: Feature extraction only\n  * 100-1K:\
      \ Fine-tune last few layers\n  * 1K-10K: Fine-tune most layers with careful\
      \ LR\n  * > 10K: Can train from scratch or full fine-tune\n- Compute budget:\n\
      \  * Feature extraction: 10× faster than fine-tuning\n  * Full fine-tuning:\
      \ Can take days on single GPU\n- Evaluation: Compare against training from scratch\n"
    practice_questions:
      concepts:
      - question: Explain the difference between feature extraction and fine-tuning.
          When would you use each approach?
        answer: 'Feature extraction: Freeze pretrained weights, train only new head.
          Fast (10× faster), needs less data, features might not be optimal for task.
          Fine-tuning: Unfreeze layers, train with low LR. Slower, needs more data,
          adapts features to task. Use feature extraction: <1K samples, very different
          domain (ImageNet → X-rays), limited compute. Use fine-tuning: >1K samples,
          related domain, have GPU time. Best practice: Start with feature extraction,
          then fine-tune if results unsatisfactory.'
      - question: Why use discriminative learning rates when fine-tuning? What's the
          intuition?
        answer: 'Early layers learn general features (edges, textures), later layers
          learn task-specific features. When fine-tuning: early layers already good,
          later layers need more adaptation. Use lower LR for early layers (small
          changes), higher LR for later layers (larger changes). Typical: Layer 1:
          1e-5, Layer 2: 1e-4, Head: 1e-3. Prevents catastrophic forgetting of useful
          pretrained features while allowing task adaptation. Alternative: Progressive
          unfreezing.'
      - question: How does domain adaptation address distribution shift? Explain domain
          adversarial training.
        answer: 'Problem: Source domain (ImageNet: natural images) ≠ Target domain
          (medical images: X-rays). Standard fine-tuning may not bridge gap. Domain
          adversarial: 1) Feature extractor learns features. 2) Task classifier predicts
          labels. 3) Domain classifier predicts source vs target. 4) Train feature
          extractor to fool domain classifier (make features domain-invariant). Loss:
          L_task - λ*L_domain. Features that work well for task but can''t distinguish
          domain → transferable features.'
      tradeoffs:
      - question: Compare using BERT vs training a custom LSTM for text classification.
          What factors determine the choice?
        answer: 'BERT: Pros: Pretrained on massive corpus, state-of-art accuracy,
          transfer learning. Cons: Large (110M params), slower inference (100ms),
          needs GPU. LSTM: Pros: Smaller (1M params), faster inference (10ms), can
          train on CPU. Cons: Needs more labeled data, lower accuracy. Choose BERT:
          Have >1K labeled samples, accuracy critical, have GPU. Choose LSTM: Limited
          data, low latency required (<50ms), edge deployment. Data: BERT wins with
          1K+ samples, LSTM competitive with <500 samples if well-tuned.'
      estimation:
      - question: Estimate compute and time to fine-tune ResNet-50 on 5K medical images
          (224×224) for 10 epochs.
        answer: 'ResNet-50: 4B FLOPs per forward pass. Backward: 2× forward = 8B FLOPs
          total. Training: 5K images * 10 epochs = 50K iterations. Total: 50K * 8B
          = 400T FLOPs. On V100 (125 TFLOPS): 400T / 125T = 3,200 seconds = 53 minutes.
          Add data loading, batch size effects: ~2 hours total. Cost: V100 on AWS
          = $3/hour → $6 for fine-tuning. Memory: Batch size 32, activations ~500MB,
          total ~4GB. Conclusion: Feasible on single GPU in few hours.'
    time_estimate: 75
    video_resources:
    - title: 'StatQuest: Transfer Learning Explained'
      url: https://www.youtube.com/@statquest
      duration: 12 min
      description: Using pre-trained models effectively
      priority: high
    - title: 'Two Minute Papers: Transfer Learning Applications'
      url: https://www.youtube.com/@TwoMinutePapers
      duration: 5 min
      description: Real-world transfer learning examples
      priority: medium
  - day: 8
    topic: Tree-Based Methods - Decision Trees, Random Forests, Gradient Boosting
    activity: Understand tree splitting, ensemble methods, XGBoost, LightGBM, and
      when trees beat neural networks
    detailed_content: "Decision Tree Fundamentals:\n- Recursive binary splitting:\
      \ Choose best split at each node\n- Split criteria:\n  * Classification (Gini):\
      \ Gini = 1 - Σ p_i²\n  * Classification (Entropy): H = -Σ p_i log(p_i)\n  *\
      \ Regression (MSE): Σ(y_i - ȳ)²\n- Split selection: Try all features and thresholds,\
      \ pick best\n- Stopping criteria: Max depth, min samples per leaf, min impurity\
      \ decrease\n\nTree Pruning:\n- Problem: Trees overfit easily\n- Pre-pruning:\
      \ Stop early (max_depth, min_samples_split)\n- Post-pruning: Grow full tree,\
      \ then prune based on validation\n- Cost-complexity pruning: Trade-off between\
      \ accuracy and tree size\n\nRandom Forest:\n- Ensemble of decision trees with\
      \ randomness:\n  * Bootstrap sampling: Each tree trained on random subset (with\
      \ replacement)\n  * Feature sampling: Each split considers random subset of\
      \ features\n- Prediction: Majority vote (classification) or average (regression)\n\
      - Why it works: Decorrelates trees, reduces variance\n- Hyperparameters: n_estimators\
      \ (100-1000), max_features (√p or log₂p)\n\nGradient Boosting:\n- Key insight:\
      \ Sequentially fit trees to residuals\n- Algorithm:\n  1. Initialize: F₀(x)\
      \ = mean(y)\n  2. For m = 1 to M:\n     - Compute residuals: r_i = y_i - F_{m-1}(x_i)\n\
      \     - Fit tree h_m to residuals r\n     - Update: F_m(x) = F_{m-1}(x) + η*h_m(x)\n\
      - η is learning rate (0.01-0.3)\n- Equivalence to gradient descent in function\
      \ space\n\nXGBoost (Extreme Gradient Boosting):\n- Improvements over standard\
      \ gradient boosting:\n  * Regularization: L1/L2 on leaf weights\n  * Second-order\
      \ approximation (Newton's method)\n  * Sparsity-aware splits (handles missing\
      \ values)\n  * Parallel tree construction\n- Loss function: L = Σ l(y_i, ŷ_i)\
      \ + Ω(tree)\n  * Ω(tree) = γT + ½λΣw_j² (T=num leaves, w=leaf weights)\n- Excellent\
      \ for tabular data, wins Kaggle competitions\n\nLightGBM:\n- Gradient-based\
      \ One-Side Sampling (GOSS):\n  * Keep samples with large gradients, random sample\
      \ small gradients\n  * Speeds up training without losing accuracy\n- Exclusive\
      \ Feature Bundling (EFB):\n  * Bundle mutually exclusive features\n  * Reduces\
      \ dimensionality\n- Leaf-wise growth (vs level-wise in XGBoost)\n  * Faster\
      \ but can overfit (needs max_depth limit)\n\nCatBoost:\n- Handles categorical\
      \ features natively\n- Ordered boosting: Prevents target leakage\n- Symmetric\
      \ trees: Faster inference\n- Good default parameters, less tuning needed\n\n\
      When Trees Beat Neural Networks:\n- Tabular data with:\n  * Mixed feature types\
      \ (categorical + numerical)\n  * < 100K samples\n  * Feature interactions important\n\
      \  * Interpretability needed\n- Examples: Fraud detection, credit scoring, sales\
      \ forecasting\n- Why: Trees handle non-monotonic relationships, missing values\
      \ naturally\n"
    practice_questions:
      concepts:
      - question: Explain gradient boosting as gradient descent in function space.
          Derive the residual fitting step.
        answer: 'Gradient boosting minimizes loss L(y, F(x)) where F is a function
          (tree ensemble). Gradient descent: F_{m+1} = F_m - η*∇L. In function space,
          gradient is: ∇L = ∂L/∂F = ∂L/∂ŷ. For MSE loss: ∂L/∂ŷ = -(y - ŷ) = -residual.
          So fitting tree to residuals = following negative gradient. Update: F_{m+1}(x)
          = F_m(x) + η*h_m(x) where h_m fits residuals. This is gradient descent with
          learning rate η and direction h_m.'
      - question: How does Random Forest reduce variance compared to a single decision
          tree? Use bias-variance decomposition.
        answer: 'Single tree: High variance (sensitive to data), low bias (can fit
          any function). Random Forest: Average of B trees: F_RF = (1/B)Σf_i. Variance:
          Var(F_RF) = (ρσ²/B) + ((1-ρ)/B)σ² where ρ=correlation, σ²=tree variance.
          As B→∞: Var→ρσ² if trees perfectly correlated, Var→0 if uncorrelated. Bootstrap
          + feature sampling decorrelates trees (reduces ρ), averaging reduces variance.
          Bias stays same (average of unbiased estimators is unbiased). Result: Lower
          variance, same bias.'
      - question: Why does XGBoost use second-order approximation (Newton's method)
          instead of first-order (gradient)?
        answer: 'Gradient descent (first-order): Uses only ∂L/∂F. Newton (second-order):
          Uses ∂L/∂F and ∂²L/∂F². Advantage: Better curvature information, faster
          convergence. For loss L(y, F): Taylor expansion L ≈ L₀ + g*ΔF + ½h*(ΔF)²
          where g=∂L/∂F (gradient), h=∂²L/∂F² (Hessian). Optimal step: ΔF = -g/h (Newton
          step). XGBoost uses this for leaf weight calculation. Converges in fewer
          iterations, but requires computing Hessian.'
      tradeoffs:
      - question: When would you use XGBoost vs a neural network for tabular data?
          Consider sample size, feature types, and interpretability.
        answer: 'XGBoost: Pros: 1) Works well with <100K samples, 2) Handles categorical
          features natively, 3) Missing values handled automatically, 4) More interpretable
          (feature importance), 5) Faster training. Neural Network: Pros: 1) Scales
          better to >1M samples, 2) Can learn complex interactions automatically,
          3) Better for high-cardinality categoricals, 4) Transfer learning possible.
          Use XGBoost: <100K samples, mixed features, need interpretability, limited
          compute. Use NN: >100K samples, complex patterns, have GPUs. In practice:
          Try XGBoost first for tabular (faster experimentation), then NN if needed.'
      estimation:
      - question: Estimate training time for XGBoost on 100K samples, 100 features,
          1000 trees, max_depth=6.
        answer: 'Per tree: Try each feature (100) at each node. Tree has 2^6 - 1 =
          63 nodes max. Per node: Scan all samples for best split. Complexity: O(samples
          * features * depth) = O(100K * 100 * 6) = 60M operations per tree. 1000
          trees: 60B operations. On modern CPU (10 GFLOPS): 60B / 10G = 6 seconds
          per tree × 1000 = 6000 seconds = 1.7 hours. With XGBoost optimizations (column
          block, cache-aware, parallel): ~30-60 minutes. Memory: 100K * 100 * 4 bytes
          = 40MB for data, minimal overhead.'
    time_estimate: 90
    video_resources:
    - title: 'StatQuest: Decision Trees Clearly Explained'
      url: https://www.youtube.com/@statquest
      duration: 17 min
      description: How decision trees work
      priority: high
    - title: 'StatQuest: Random Forests and Gradient Boosting'
      url: https://www.youtube.com/@statquest
      duration: 20 min
      description: Ensemble methods explained
      priority: high
  - day: 9
    topic: Classical ML - Linear Models, SVMs, and Dimensionality Reduction
    activity: Master logistic regression, support vector machines, PCA, and understand
      when simple models work best
    detailed_content: "Linear Regression:\n- Model: y = w^T x + b\n- Loss: MSE = (1/n)\
      \ Σ(y_i - ŷ_i)²\n- Closed-form solution: w = (X^T X)^{-1} X^T y\n  * Works when\
      \ X^T X is invertible (features < samples)\n- Gradient descent: w = w - η *\
      \ (1/n) X^T (Xw - y)\n- Regularization:\n  * Ridge (L2): Add λΣw_i² → w = (X^T\
      \ X + λI)^{-1} X^T y\n  * Lasso (L1): Add λΣ|w_i| → no closed form, use coordinate\
      \ descent\n\nLogistic Regression:\n- Model: p(y=1|x) = σ(w^T x + b) where σ(z)\
      \ = 1/(1+e^{-z})\n- Loss: Binary cross-entropy = -[y log(ŷ) + (1-y)log(1-ŷ)]\n\
      - No closed form, use gradient descent or Newton's method\n- Gradient: ∇L =\
      \ (1/n) X^T (ŷ - y)\n- Interpretation: w_i is log-odds ratio for feature i\n\
      \nSupport Vector Machines (SVM):\n- Goal: Find maximum margin hyperplane\n-\
      \ Hard margin: min ||w||² subject to y_i(w^T x_i + b) ≥ 1\n- Soft margin: Add\
      \ slack ξ_i for misclassified points\n  * min (½||w||² + C Σξ_i) subject to\
      \ y_i(w^T x_i + b) ≥ 1 - ξ_i\n- Dual problem: Involves only dot products x_i^T\
      \ x_j\n  * Enables kernel trick\n\nKernel Trick:\n- Problem: Data not linearly\
      \ separable\n- Solution: Map to higher dimension φ(x), find linear separator\
      \ there\n- Key insight: Don't need explicit φ(x), only K(x_i, x_j) = φ(x_i)^T\
      \ φ(x_j)\n- Common kernels:\n  * Linear: K(x,y) = x^T y\n  * Polynomial: K(x,y)\
      \ = (x^T y + c)^d\n  * RBF (Gaussian): K(x,y) = exp(-γ||x-y||²)\n- RBF most\
      \ popular: Infinite dimensional space, single hyperparameter γ\n\nPrincipal\
      \ Component Analysis (PCA):\n- Goal: Find directions of maximum variance\n-\
      \ Method: Eigendecomposition of covariance matrix Σ = X^T X\n- Principal components:\
      \ Eigenvectors with largest eigenvalues\n- Dimensionality reduction: Keep top\
      \ k components\n- Variance explained: λ_k / Σλ_i (choose k to explain 95% variance)\n\
      \nNaive Bayes:\n- Bayes rule: P(y|x) ∝ P(x|y) P(y)\n- Naive assumption: Features\
      \ independent given class\n  * P(x|y) = P(x_1|y) * P(x_2|y) * ... * P(x_n|y)\n\
      - Training: Estimate P(x_i|y) and P(y) from data\n- Types:\n  * Gaussian NB:\
      \ Features are continuous, normally distributed\n  * Multinomial NB: Features\
      \ are counts (text classification)\n- Fast training and inference, works well\
      \ for text with small data\n\nK-Means Clustering:\n- Algorithm:\n  1. Initialize\
      \ k centroids randomly\n  2. Assign each point to nearest centroid\n  3. Update\
      \ centroids as mean of assigned points\n  4. Repeat until convergence\n- Loss:\
      \ Σ min_c ||x_i - μ_c||² (within-cluster sum of squares)\n- Choosing k: Elbow\
      \ method (plot loss vs k)\n- Limitations: Assumes spherical clusters, sensitive\
      \ to initialization\n"
    practice_questions:
      concepts:
      - question: Derive the closed-form solution for linear regression with L2 regularization
          (Ridge regression).
        answer: 'Loss: L = ||Xw - y||² + λ||w||². Take gradient: ∂L/∂w = 2X^T(Xw -
          y) + 2λw = 0. Rearrange: X^T Xw + λw = X^T y. Factor: (X^T X + λI)w = X^T
          y. Solve: w = (X^T X + λI)^{-1} X^T y. Key insight: λI makes X^T X + λI
          always invertible (even if X^T X is singular). This is why Ridge works with
          more features than samples, unlike standard linear regression.'
      - question: Explain the kernel trick in SVMs. Why is it computationally efficient
          for high-dimensional spaces?
        answer: 'SVM dual formulation: α* = argmax Σα_i - ½ΣΣ α_i α_j y_i y_j (x_i^T
          x_j). Notice only dot products x_i^T x_j appear. Kernel trick: Replace x_i^T
          x_j with K(x_i, x_j) = φ(x_i)^T φ(x_j). Example: RBF kernel maps to infinite
          dimensions, but K(x,y) = exp(-γ||x-y||²) is O(d) to compute. Never compute
          φ(x) explicitly! Enables learning in high/infinite-dim space with O(d) cost
          per kernel evaluation. Dual has O(n²) kernel evaluations.'
      - question: How does PCA find directions of maximum variance? Relate to eigenvalues
          and eigenvectors.
        answer: 'Goal: Find direction w (||w||=1) that maximizes variance Var(X w).
          Variance: w^T Σ w where Σ = X^T X / n (covariance). Constrained optimization:
          max w^T Σ w subject to ||w||²=1. Lagrangian: L = w^T Σ w - λ(w^T w - 1).
          Take derivative: ∂L/∂w = 2Σw - 2λw = 0. This gives: Σw = λw (eigenvalue
          equation). w is eigenvector, λ is eigenvalue = variance along w. First PC:
          eigenvector with largest eigenvalue.'
      tradeoffs:
      - question: Compare Logistic Regression vs Neural Network for binary classification.
          When to use each?
        answer: 'Logistic Regression: Pros: 1) Interpretable coefficients, 2) Fast
          training and inference, 3) Works with small data (<1K samples), 4) No hyperparameter
          tuning. Cons: 1) Assumes linear decision boundary, 2) Can''t learn interactions
          without manual feature engineering. Neural Network: Pros: 1) Learns non-linear
          boundaries automatically, 2) Can learn feature interactions, 3) Scales to
          large data. Cons: 1) Black box, 2) Needs more data (>10K), 3) Slower, needs
          tuning. Use Logistic Regression: <10K samples, need interpretability, linear/simple
          problem. Use NN: >10K samples, complex patterns, have compute.'
      estimation:
      - question: Calculate complexity of training SVM with RBF kernel on n=10K samples,
          d=100 features.
        answer: 'SVM dual problem: Solve QP with O(n²) kernel evaluations. Kernel
          matrix: 10K × 10K = 100M entries. Each RBF kernel: K(x,y) = exp(-γ||x-y||²)
          costs O(d)=O(100) ops. Total: 100M * 100 = 10B operations to build kernel
          matrix. QP solve: O(n³) in worst case = (10K)³ = 10^12 ops. With SMO algorithm
          (used in practice): O(n² to n³) depending on data. Memory: Store kernel
          matrix = 100M * 4 bytes = 400MB. Training time: Minutes to hours depending
          on C, γ. Prediction for new point: O(n*d) = 1M ops.'
    time_estimate: 75
    video_resources:
    - title: 'StatQuest: Linear Regression and Logistic Regression'
      url: https://www.youtube.com/@statquest
      duration: 22 min
      description: Classical ML foundations
      priority: high
    - title: 'StatQuest: Support Vector Machines (SVMs)'
      url: https://www.youtube.com/@statquest
      duration: 20 min
      description: SVM intuition and kernels
      priority: high
  - day: 10
    topic: Model Evaluation and Metrics
    activity: Master classification metrics (precision, recall, F1, ROC-AUC), regression
      metrics, and cross-validation strategies
    detailed_content: "Classification Metrics:\n- Confusion Matrix:\n  * True Positive\
      \ (TP), False Positive (FP), True Negative (TN), False Negative (FN)\n- Accuracy:\
      \ (TP + TN) / Total\n  * Misleading for imbalanced classes (99% class 0 → accuracy=99%\
      \ by predicting all 0)\n- Precision: TP / (TP + FP)\n  * \"Of predicted positives,\
      \ how many are correct?\"\n  * High precision: Few false alarms\n- Recall (Sensitivity,\
      \ TPR): TP / (TP + FN)\n  * \"Of actual positives, how many did we find?\"\n\
      \  * High recall: Few missed positives\n- F1 Score: 2 * (Precision * Recall)\
      \ / (Precision + Recall)\n  * Harmonic mean of precision and recall\n  * Use\
      \ when you need balance between both\n\nPrecision-Recall Trade-off:\n- Increase\
      \ threshold → higher precision, lower recall\n- Decrease threshold → higher\
      \ recall, lower precision\n- F_β score: Weighted harmonic mean\n  * β > 1: Favor\
      \ recall (e.g., cancer screening)\n  * β < 1: Favor precision (e.g., spam filtering)\n\
      \nROC and AUC:\n- ROC curve: TPR vs FPR at different thresholds\n  * FPR = FP\
      \ / (FP + TN)\n- AUC (Area Under Curve): Single number summarizing ROC\n  *\
      \ AUC = 1: Perfect classifier\n  * AUC = 0.5: Random classifier\n  * Interpretation:\
      \ Probability that model ranks random positive higher than random negative\n\
      - When to use: Binary classification, balanced classes\n\nMulti-class Metrics:\n\
      - Macro-average: Compute metric per class, average\n  * Treats all classes equally\
      \ (good for imbalanced)\n- Micro-average: Aggregate TP, FP, FN across classes,\
      \ then compute\n  * Weighted by class frequency\n- Weighted average: Average\
      \ weighted by class support\n\nRegression Metrics:\n- Mean Squared Error (MSE):\
      \ (1/n) Σ(y_i - ŷ_i)²\n  * Penalizes large errors heavily, sensitive to outliers\n\
      - Root MSE (RMSE): √MSE\n  * Same units as target variable\n- Mean Absolute\
      \ Error (MAE): (1/n) Σ|y_i - ŷ_i|\n  * More robust to outliers than MSE\n- R²\
      \ (Coefficient of Determination): 1 - (SS_res / SS_tot)\n  * Proportion of variance\
      \ explained\n  * R² = 1: Perfect fit, R² = 0: Predicts mean\n\nCross-Validation:\n\
      - K-Fold CV: Split data into K folds, train on K-1, test on 1\n  * Repeat K\
      \ times, average performance\n  * Typical: K=5 or K=10\n- Stratified K-Fold:\
      \ Preserves class distribution in each fold\n  * Important for imbalanced data\n\
      - Leave-One-Out CV (LOOCV): K=n (expensive but low bias)\n- Time-series CV:\
      \ Respect temporal order\n  * Train on [1:t], test on [t+1:t+h]\n\nBias-Variance\
      \ Trade-off:\n- Total Error = Bias² + Variance + Irreducible Error\n- Bias:\
      \ Error from wrong assumptions (underfitting)\n  * High bias: Model too simple\
      \ (linear for non-linear data)\n  * Fix: More complex model, more features,\
      \ less regularization\n- Variance: Error from sensitivity to training data (overfitting)\n\
      \  * High variance: Model too complex, memorizes noise\n  * Fix: More data,\
      \ regularization, simpler model\n\nLearning Curves:\n- Plot train/val error\
      \ vs training set size\n- High bias: Train and val error both high, converge\
      \ together\n  * Adding more data doesn't help\n- High variance: Large gap between\
      \ train and val error\n  * Adding more data helps\n\nOverfitting Diagnosis:\n\
      - Symptoms: Train accuracy >> val accuracy\n- Solutions: More data, regularization,\
      \ simpler model, early stopping, data augmentation\n\nUnderfitting Diagnosis:\n\
      - Symptoms: Train and val accuracy both low\n- Solutions: More complex model,\
      \ more features, less regularization, train longer\n"
    practice_questions:
      concepts:
      - question: Explain the precision-recall trade-off. In what scenarios would
          you optimize for high precision vs high recall?
        answer: 'Precision-recall trade-off: Increasing threshold → fewer positive
          predictions → higher precision (fewer FP) but lower recall (more FN). Decreasing
          threshold → more positive predictions → higher recall but lower precision.
          High precision scenarios: Spam detection (don''t want legitimate emails
          marked spam), ad click prediction (don''t waste money on unlikely clicks).
          High recall scenarios: Cancer screening (can''t miss cases), fraud detection
          (catch all fraud, even with false alarms). Use F_β score: β=2 for recall
          priority, β=0.5 for precision priority.'
      - question: What is AUC-ROC measuring? Provide an intuitive interpretation and
          explain when it's appropriate vs misleading.
        answer: 'AUC = P(model ranks random positive higher than random negative).
          Intuitive: If you pick one positive and one negative sample, what''s probability
          model scores positive > negative? ROC plots TPR vs FPR across all thresholds.
          Appropriate: Binary classification, care about ranking, balanced classes.
          Misleading: Imbalanced data (99% negative) - high AUC doesn''t mean good
          precision at operating point. Alternative for imbalanced: Precision-Recall
          curve. Example: 1% fraud - AUC=0.95 looks good but precision at 1% threshold
          might be only 10%.'
      - question: Explain bias-variance trade-off using a mathematical decomposition.
          How do you diagnose high bias vs high variance?
        answer: 'Expected error: E[(y - ŷ)²] = Bias[ŷ]² + Var[ŷ] + σ² where Bias[ŷ]
          = E[ŷ] - y, Var[ŷ] = E[(ŷ - E[ŷ])²]. Bias: Error from model assumptions
          (linear model for quadratic data). Variance: Error from sensitivity to training
          set. Diagnosis: 1) High bias: Train error high, gap small → model too simple.
          2) High variance: Train error low, val error high, large gap → overfitting.
          Learning curves: Bias → errors converge high. Variance → large gap persists
          with more data.'
      tradeoffs:
      - question: For an imbalanced classification problem (1% positive class), compare
          accuracy, F1, and AUC. Which metric is most appropriate?
        answer: 'Accuracy: Misleading. Predict all negative → 99% accuracy but useless.
          F1: Better, balances precision and recall. Penalizes missing positives and
          false alarms. AUC: Good for ranking, but doesn''t reflect performance at
          operating threshold. Best: Precision-Recall curve or F1 at desired operating
          point. For imbalanced: Report precision, recall, F1 at relevant threshold.
          Example: If business tolerates 10% FPR, report precision/recall at that
          point. AUC can be high while precision at operating point is low.'
      scenarios:
      - question: You have a binary classifier with 95% train accuracy and 70% validation
          accuracy. Diagnose the problem and suggest solutions.
        answer: 'Diagnosis: High variance (overfitting). Large gap (25%) between train
          and val indicates memorization. Solutions: 1) More training data (most effective
          if possible), 2) Regularization (L1/L2, dropout), 3) Simpler model (reduce
          capacity), 4) Early stopping, 5) Data augmentation (effectively increases
          data), 6) Cross-validation to select hyperparameters. Not solutions: More
          training epochs (makes worse), more complex model (makes worse). Verify:
          Check learning curves - gap should decrease with more data.'
    time_estimate: 75
    video_resources:
    - title: 'StatQuest: ROC and AUC Clearly Explained'
      url: https://www.youtube.com/@statquest
      duration: 16 min
      description: Evaluation metrics for classification
      priority: high
    - title: 'StatQuest: Precision, Recall, and F1 Score'
      url: https://www.youtube.com/@statquest
      duration: 12 min
      description: Understanding classification metrics
      priority: high
  week3:
  - day: 11
    topic: Embedding Techniques and Representation Learning
    activity: Master word embeddings (Word2Vec, GloVe), entity embeddings, and modern
      contextualized representations
    detailed_content: "Word Embeddings Motivation:\n- One-hot encoding: High dimensional,\
      \ no semantic relationships\n- Embedding: Dense vector in R^d (d=100-300) captures\
      \ semantics\n- Distributional hypothesis: Words in similar contexts have similar\
      \ meanings\n\nWord2Vec:\n- Skip-gram: Predict context words given center word\n\
      \  * Maximize: log P(w_{t-k},...,w_{t+k} | w_t)\n  * Model: P(w_o|w_i) = exp(u_o^T\
      \ v_i) / Σ exp(u_j^T v_i)\n- CBOW (Continuous Bag of Words): Predict center\
      \ word from context\n  * Faster than skip-gram\n- Negative sampling: Avoid expensive\
      \ softmax\n  * Sample k negative words, maximize log σ(u_o^T v_i) + Σ log σ(-u_j^T\
      \ v_i)\n- Hierarchical softmax: Use binary tree instead of flat softmax\n\n\
      GloVe (Global Vectors):\n- Idea: Combine global matrix factorization with local\
      \ context\n- Co-occurrence matrix X: X_ij = # times word j appears in context\
      \ of word i\n- Objective: Minimize Σ f(X_ij) (w_i^T w̃_j + b_i + b̃_j - log\
      \ X_ij)²\n  * f(x): Weighting function (less weight to rare co-occurrences)\n\
      - Captures linear relationships: king - man + woman ≈ queen\n\nEntity Embeddings:\n\
      - Categorical features in tabular data\n- Learn embedding for each category\
      \ during training\n- Example: Country → 50-dim vector (captures geographical/economic\
      \ similarity)\n- Benefits: Dimensionality reduction, semantic relationships,\
      \ better generalization\n\nContextualized Embeddings:\n- Problem with Word2Vec:\
      \ \"bank\" has same embedding in \"river bank\" and \"savings bank\"\n- ELMo\
      \ (Embeddings from Language Models):\n  * Bidirectional LSTM trained on language\
      \ modeling\n  * Embedding depends on entire sentence context\n- BERT: Transformer-based,\
      \ better than ELMo\n  * Each token's embedding depends on entire sequence via\
      \ attention\n\nSentence Embeddings:\n- Averaging word embeddings: Simple but\
      \ loses word order\n- Doc2Vec: Extension of Word2Vec to documents\n- Sentence-BERT:\
      \ Fine-tuned BERT for sentence similarity\n  * Siamese network architecture\n\
      \  * Cosine similarity in embedding space\n\nMetric Learning:\n- Goal: Learn\
      \ embedding space where similar items are close\n- Contrastive loss: Minimize\
      \ distance for similar pairs, maximize for dissimilar\n- Triplet loss: (anchor,\
      \ positive, negative)\n  * Minimize: max(0, ||a-p||² - ||a-n||² + margin)\n\
      - Applications: Face recognition, image retrieval, recommendation\n\nt-SNE (t-distributed\
      \ Stochastic Neighbor Embedding):\n- Non-linear dimensionality reduction for\
      \ visualization\n- Preserves local structure (similar points stay close)\n-\
      \ Algorithm: Model probability distributions in high-dim and 2D, minimize KL\
      \ divergence\n- Use: Visualize embeddings (Word2Vec, BERT), cluster analysis\n\
      - Caveat: Non-deterministic, hyperparameter-sensitive\n"
    practice_questions:
      concepts:
      - question: Explain negative sampling in Word2Vec. Why is it more efficient
          than full softmax?
        answer: 'Full softmax: P(w_o|w_i) = exp(u_o^T v_i) / Σ_j exp(u_j^T v_i). Denominator
          sums over entire vocab (10K-100K words), very expensive. Negative sampling:
          For context pair (w_i, w_o), sample k negative words (k=5-20). Maximize
          log σ(u_o^T v_i) + Σ_{j∈NEG} log σ(-u_j^T v_i). Only k+1 words per update
          instead of vocab size. Complexity: O(k) vs O(|V|). Still approximates softmax
          well in practice. Sampling distribution: P(w) ∝ freq(w)^{3/4} (balance frequency).'
      - question: How do contextualized embeddings (BERT) differ from static embeddings
          (Word2Vec)? Explain with an example.
        answer: 'Word2Vec: Each word has fixed embedding regardless of context. ''bank''
          has same vector in ''river bank'' and ''savings bank''. BERT: Embedding
          depends on entire sentence via transformer attention. ''bank'' in ''river
          bank'' attends to ''river'', ''bank'' in ''savings bank'' attends to ''savings''
          → different embeddings. Mechanism: Self-attention computes weighted average
          of all tokens. Each word''s representation is context-dependent. Use Word2Vec:
          Fast lookup, limited memory. Use BERT: Need context-awareness, have compute
          for transformer.'
      - question: Derive the triplet loss for metric learning. Why use margin?
        answer: 'Goal: Anchor a should be closer to positive p than to negative n.
          Distance constraint: ||a-p||² + margin < ||a-n||². Loss: L = max(0, ||a-p||²
          - ||a-n||² + margin). Why margin: Ensures separation, not just ordering.
          Without margin: Could have ||a-p||²=5.0, ||a-n||²=5.1 (barely separated).
          With margin=1.0: Forces ||a-n||² ≥ ||a-p||² + 1.0, creating buffer. Typical
          margin: 0.2-1.0. Used in FaceNet: Learn face embeddings where same person
          is close.'
      tradeoffs:
      - question: Compare Word2Vec, GloVe, and BERT embeddings for a text classification
          task with 10K training samples.
        answer: 'Word2Vec: Pros: Fast, pretrained on large corpus, works with small
          data. Cons: No context (polysemy problem), average pooling loses order.
          GloVe: Similar to Word2Vec but incorporates global statistics. BERT: Pros:
          Context-aware, state-of-art accuracy. Cons: Slow (transformer forward pass),
          needs GPU, large memory. For 10K samples: Word2Vec/GloVe sufficient (99%
          accuracy achievable), BERT gains 1-2% but 10× slower. Use BERT: If accuracy
          critical and have compute. Use Word2Vec: Fast baseline, limited resources,
          interpretability.'
      estimation:
      - question: 'Calculate memory for storing Word2Vec embeddings: vocab=100K words,
          embedding dim=300.'
        answer: 'Embeddings: 100K words × 300 dimensions × 4 bytes/float = 120MB.
          If also store context vectors (skip-gram has input and output embeddings):
          2 × 120MB = 240MB. In practice: Only keep input embeddings after training
          → 120MB. Compared to one-hot: 100K × 100K sparse matrix (impractical). Lookup
          time: O(1) hash table. For sentence: Average 20 words → 20 × 300 = 6K floats
          = 24KB per sentence. Batch of 32 sentences: 768KB (fits in CPU cache).'
    time_estimate: 60
    video_resources:
    - title: 'StatQuest: Word2Vec and Embeddings'
      url: https://www.youtube.com/@statquest
      duration: 18 min
      description: How embeddings capture meaning
      priority: high
    - title: '3Blue1Brown: Visualizing High-Dimensional Data'
      url: https://www.youtube.com/@3blue1brown
      duration: 15 min
      description: t-SNE and embedding visualization
      priority: medium
  - day: 12
    topic: Advanced Deep Learning Topics - Normalization, Initialization, Residual
      Learning
    activity: Deep dive into batch normalization variants, weight initialization strategies,
      and architectural patterns
    detailed_content: "Normalization Techniques:\n- Batch Normalization:\n  * Normalize:\
      \ μ_B = (1/m)Σx_i, σ²_B = (1/m)Σ(x_i-μ_B)²\n  * Transform: x̂ = (x-μ_B)/√(σ²_B+ε),\
      \ y = γx̂ + β\n  * Benefits: Faster training, higher LR, regularization\n  *\
      \ Drawbacks: Batch-size dependent, train/test discrepancy\n- Layer Normalization:\n\
      \  * Normalize across features per sample (not across batch)\n  * Independence\
      \ from batch size\n  * Used in: Transformers (BERT, GPT), RNNs\n- Instance Normalization:\n\
      \  * Normalize per sample, per channel\n  * Used in: Style transfer, GANs\n\
      - Group Normalization:\n  * Divide channels into groups, normalize within groups\n\
      \  * Compromise between batch and layer norm\n  * Good for small batches (<8)\n\
      \nWhen to use each:\n- CNNs, large batch (≥32): Batch Norm\n- Transformers,\
      \ RNNs: Layer Norm\n- Style transfer: Instance Norm\n- Small batch (<8), object\
      \ detection: Group Norm\n\nWeight Initialization:\n- Zero initialization: All\
      \ neurons learn same features (symmetry problem)\n- Random small values: Vanishing\
      \ gradients in deep networks\n- Xavier/Glorot: Var(W) = 2/(n_in + n_out)\n \
      \ * Derivation: Keep variance of activations constant across layers\n  * Good\
      \ for: sigmoid, tanh activations\n- He initialization: Var(W) = 2/n_in\n  *\
      \ Designed for ReLU (accounts for half neurons being zero)\n  * Modern default\
      \ for deep networks\n\nResidual Learning:\n- Skip connections: y = F(x) + x\n\
      - Why it works:\n  * Gradients flow directly through skip connection\n  * ∂L/∂x\
      \ = ∂L/∂y * (∂F/∂x + 1) - the \"+1\" prevents vanishing\n- ResNet variants:\n\
      \  * Pre-activation: BN → ReLU → Conv (better than Conv → BN → ReLU)\n  * Wide\
      \ ResNet: Wider layers (more channels) instead of deeper\n  * ResNeXt: Multiple\
      \ parallel paths (group convolutions)\n\nAttention Mechanisms Beyond Transformers:\n\
      - Squeeze-and-Excitation (SE) Networks:\n  * Channel attention: Which channels\
      \ are important?\n  * Global pool → FC → Sigmoid → Scale channels\n- CBAM (Convolutional\
      \ Block Attention Module):\n  * Channel attention + Spatial attention\n- Non-local\
      \ Neural Networks:\n  * Self-attention for CNNs (captures long-range dependencies)\n\
      \nNeural Architecture Search (NAS):\n- Automated design of architectures\n-\
      \ Search space: Operations, connections, hyperparameters\n- Search strategy:\
      \ Reinforcement learning, evolutionary, gradient-based\n- Results: EfficientNet,\
      \ AmoebaNet\n- Drawback: Expensive (thousands of GPU-hours)\n\nKnowledge Distillation:\n\
      - Train small \"student\" model to mimic large \"teacher\" model\n- Loss: L\
      \ = αL_CE(y, student) + (1-α)L_KL(teacher, student)\n- Soft targets from teacher\
      \ contain more information than hard labels\n- Application: Deploy large BERT\
      \ → distill to DistilBERT (40% smaller, 97% accuracy)\n"
    practice_questions:
      concepts:
      - question: Why does Layer Normalization work better than Batch Normalization
          for Transformers?
        answer: 'Transformers process variable-length sequences. Batch Norm normalizes
          across batch dimension - different sequence lengths lead to different statistics
          per position. Layer Norm normalizes across feature dimension per sample
          - independent of batch or sequence length. For transformer with hidden size
          768: LN computes μ,σ over 768 dimensions per token. Consistent statistics
          regardless of batch size or sequence length. Also: Transformers often use
          batch size 1 during inference (generation), where BN statistics would be
          noisy.'
      - question: Derive He initialization variance for ReLU activations. Why is it
          different from Xavier?
        answer: 'Goal: Preserve variance through layers. For layer l: y = W·x where
          x has variance σ²_x. Var(y) = E[W²]·Var(x) = n_in·Var(W)·σ²_x. To keep Var(y)=σ²_x,
          need Var(W) = 1/n_in. But ReLU zeros half the neurons: E[ReLU(x)] = E[max(0,x)]
          ≈ σ_x/2 for zero-mean x. Variance is also halved. To compensate: Var(W)
          = 2/n_in (He init). Xavier: Var(W) = 2/(n_in+n_out) assumes linear activation.
          He accounts for ReLU''s nonlinearity.'
      - question: How does knowledge distillation work? Why do soft targets from teacher
          model help student learning?
        answer: 'Student learns from: 1) Hard labels (one-hot), 2) Soft targets from
          teacher (probability distribution). Teacher output at temperature T: p_i
          = exp(z_i/T)/Σexp(z_j/T). Higher T → softer distribution. Why soft targets
          help: Encode relative similarities between classes. Example: For image of
          dog, teacher outputs [0.7 dog, 0.2 wolf, 0.1 cat] (soft) vs [1 dog, 0 wolf,
          0 cat] (hard). Soft targets contain information about similarity (wolf closer
          to dog than cat). Student learns these relationships.'
      tradeoffs:
      - question: Compare using a 50-layer ResNet vs a 20-layer plain CNN. What are
          the trade-offs in accuracy, training time, and complexity?
        answer: '50-layer ResNet: Pros: Higher accuracy (skip connections enable training),
          can learn more complex features. Cons: 2.5× more layers → 2× slower training/inference,
          2× memory. 20-layer plain CNN: Pros: Faster, smaller. Cons: Plain networks
          degrade with depth (degradation problem), lower accuracy. Empirical: ResNet-50
          > Plain-20 accuracy by ~3-5% on ImageNet. Training: ResNet-50 takes 2× time
          of Plain-20 but achieves better accuracy. Use ResNet: When accuracy is critical.
          Use plain: Limited compute, shallow enough to train (< 20 layers).'
      scenarios:
      - question: You're training a 100-layer network and observe training accuracy
          stuck at 60% (random is 50%). Diagnose and fix.
        answer: 'Diagnosis: Likely vanishing gradients. Deep network without skip
          connections can''t backprop gradients effectively. Other possibilities:
          Bad initialization, too high LR. Fixes: 1) Add residual connections (ResNet-style)
          - most effective. 2) Check initialization (use He init for ReLU). 3) Reduce
          learning rate (try 10× smaller). 4) Use Batch Normalization after each conv
          layer. 5) Gradient clipping to prevent explosion. Verification: Monitor
          gradient norms - if < 1e-5 in early layers, it''s vanishing. After fixes:
          Should see training accuracy increase to 80-90%.'
    time_estimate: 60
    video_resources:
    - title: 'Andrej Karpathy: Training Deep Networks'
      url: https://www.youtube.com/@AndrejKarpathy
      duration: 35 min
      description: Batch norm, layer norm, weight init
      priority: high
    - title: 'Two Minute Papers: ResNet and Skip Connections'
      url: https://www.youtube.com/@TwoMinutePapers
      duration: 6 min
      description: Why residual connections work
      priority: high
  - day: 13
    topic: Unsupervised and Self-Supervised Learning
    activity: Understand clustering, autoencoders, contrastive learning, and modern
      self-supervised methods like SimCLR and MAE
    detailed_content: "Clustering Algorithms:\n- K-Means: Already covered (Day 9)\n\
      - Hierarchical Clustering:\n  * Agglomerative (bottom-up): Start with each point\
      \ as cluster, merge\n  * Linkage: Single (min distance), complete (max distance),\
      \ average\n  * Output: Dendrogram (tree of merges)\n- DBSCAN (Density-Based):\n\
      \  * Core points: ≥minPts neighbors within radius ε\n  * Border points: Within\
      \ ε of core point\n  * Noise points: Neither core nor border\n  * Advantage:\
      \ Finds arbitrary-shaped clusters, handles noise\n\nSelf-Supervised Learning:\n\
      - Motivation: Leverage unlabeled data (abundant) before fine-tuning on labeled\
      \ (scarce)\n- Pretext tasks: Create supervised task from unlabeled data\n\n\
      Contrastive Learning:\n- Idea: Learn representations where similar samples are\
      \ close, dissimilar are far\n- SimCLR (Simple Framework for Contrastive Learning):\n\
      \  1. Augment image twice (crop, color jitter) → x_i, x_j\n  2. Encode through\
      \ CNN: h_i = f(x_i), h_j = f(x_j)\n  3. Project: z_i = g(h_i) (small MLP)\n\
      \  4. Contrastive loss: Maximize agreement between z_i, z_j, minimize with other\
      \ samples\n  * Loss: -log[exp(sim(z_i,z_j)/τ) / Σ_k exp(sim(z_i,z_k)/τ)]\n-\
      \ MoCo (Momentum Contrast):\n  * Maintains queue of negative samples\n  * Momentum\
      \ encoder for consistency\n\nMasked Language Modeling:\n- BERT pretraining:\
      \ Mask 15% of tokens, predict them\n- Why it works: Forces model to understand\
      \ context\n- T5, RoBERTa: Improved masking strategies\n\nMasked Image Modeling:\n\
      - MAE (Masked Autoencoders):\n  * Mask 75% of image patches (16×16 pixels)\n\
      \  * Encode visible patches with transformer\n  * Decode to reconstruct masked\
      \ patches\n  * Forces learning of spatial relationships\n- BEiT: Similar but\
      \ uses discrete tokens (VQ-VAE)\n\nRotation Prediction:\n- Rotate image by 0°,\
      \ 90°, 180°, 270°\n- Train network to predict rotation angle\n- Forces learning\
      \ of object structure\n\nJigsaw Puzzle:\n- Split image into 3×3 grid, shuffle\
      \ patches\n- Train network to predict correct permutation\n- Learns spatial\
      \ relationships\n\nColorization:\n- Input: Grayscale image\n- Output: Predicted\
      \ color channels\n- Forces semantic understanding (sky is blue, grass is green)\n\
      \nRelative Positioning:\n- Predict relative position of two image patches\n\
      - Learns spatial context\n\nTransfer Learning Pipeline:\n1. Pretrain on large\
      \ unlabeled data (self-supervised)\n2. Fine-tune on small labeled data (supervised)\n\
      3. Achieves competitive accuracy with less labeled data\n\nApplications:\n-\
      \ Medical imaging: Limited labeled data (expert annotations expensive)\n- Satellite\
      \ imagery: Lots of unlabeled data\n- Video understanding: Temporal consistency\
      \ as supervision\n"
    practice_questions:
      concepts:
      - question: Explain the intuition behind contrastive learning (SimCLR). How
          does it learn useful representations without labels?
        answer: 'Intuition: Different augmentations of same image should have similar
          embeddings (positive pairs), different images should have dissimilar embeddings
          (negative pairs). SimCLR: For image x, create two augmented views x_i, x_j.
          Loss maximizes similarity between (x_i, x_j) and minimizes with other images
          in batch. Why it works: Forces encoder to learn features invariant to augmentations
          but discriminative across images. Must capture semantic content (object
          identity) while ignoring superficial changes (color, crop). No labels needed
          - supervision comes from data augmentation.'
      - question: How does Masked Autoencoding (MAE) work for images? Why mask such
          a high percentage (75%)?
        answer: 'MAE: Randomly mask 75% of image patches, encode visible patches with
          transformer, decode to reconstruct masked patches. High masking (75%): Makes
          task challenging, forces learning global structure. With low masking (15%
          like BERT), could reconstruct by interpolating neighboring pixels (no semantic
          understanding). 75% masking: Can''t just interpolate, must understand what
          object is to fill in. Example: Mask most of cat → must infer ''this is a
          cat'' to reconstruct whiskers, ears. Also: Computational benefit - only
          encode 25% of patches.'
      - question: Compare self-supervised learning (SimCLR, MAE) vs supervised pretraining
          (ImageNet classification). Pros and cons?
        answer: 'Self-supervised: Pros: No labels needed (leverage billions of unlabeled
          images), learns general representations, recent results match/exceed supervised.
          Cons: Needs large batch sizes (SimCLR uses 4096), more compute, less mature.
          Supervised (ImageNet): Pros: Well-established, works with smaller batches,
          pretrained models widely available. Cons: Needs labeled data (ImageNet =
          1.2M labels), may learn label biases. Trend: Self-supervised catching up.
          Use self-supervised: Have lots of unlabeled domain data. Use supervised:
          Need stable baseline, limited compute.'
      tradeoffs:
      - question: For a medical imaging task with 1K labeled X-rays and 100K unlabeled,
          design a learning strategy. Compare options.
        answer: 'Options: 1) Supervised only (1K): Simple but likely underfits, accuracy
          ~70%. 2) ImageNet pretrain + fine-tune: Good baseline, accuracy ~85%. But
          ImageNet photos ≠ X-rays. 3) Self-supervised on 100K unlabeled, then fine-tune
          on 1K: Best accuracy ~90-92%. Recommendation: Option 3 (MAE or SimCLR).
          Steps: 1) Pretrain MAE on 100K unlabeled X-rays (learn X-ray-specific features).
          2) Fine-tune on 1K labeled. Why better: Domain-specific pretraining > generic
          ImageNet. Trade-off: More engineering complexity but worth for 5-7% accuracy
          gain in medical domain.'
      estimation:
      - question: 'Estimate compute for SimCLR pretraining: 1M unlabeled images, ResNet-50
          encoder, batch size 4096, 800 epochs.'
        answer: 'Per image: ResNet-50 forward pass = 4B FLOPs. Batch: 4096 * 4B =
          16T FLOPs. Contrastive loss: 4096 × 4096 similarity matrix = 16M similarities,
          each O(dim) = O(2048). Total: ~32B FLOPs for loss. Per epoch: 1M / 4096
          ≈ 244 batches. Epoch compute: 244 * 16T = 4P FLOPs. 800 epochs: 3200P FLOPs
          = 3.2 exaFLOPs. On 8× V100 (1 PFLOPS total): 3.2P / 1P = 3200 seconds =
          53 minutes per epoch. Total: 53 × 800 = 42K minutes = 700 GPU-hours. Cost
          on AWS: 8 V100s = $25/hour → $17,500.'
    time_estimate: 60
    video_resources:
    - title: 'StatQuest: Self-Supervised Learning Explained'
      url: https://www.youtube.com/@statquest
      duration: 15 min
      description: Learning without labels
      priority: high
    - title: 'Two Minute Papers: Contrastive Learning'
      url: https://www.youtube.com/@TwoMinutePapers
      duration: 7 min
      description: SimCLR and other approaches
      priority: medium
  - day: 14
    topic: Production ML Concerns - Model Compression, Quantization, Distillation
    activity: 'Master techniques for deploying models efficiently: pruning, quantization,
      knowledge distillation, and serving optimization'
    detailed_content: "Model Compression Motivation:\n- Production constraints: Latency\
      \ < 100ms, memory < 100MB, mobile/edge devices\n- Large models: BERT (110M params,\
      \ 440MB), ResNet-50 (25M params, 100MB)\n- Need: 10× smaller models with <5%\
      \ accuracy drop\n\nKnowledge Distillation:\n- Train small student to mimic large\
      \ teacher\n- Loss: L = α*L_hard(y, student) + (1-α)*L_soft(teacher, student)\n\
      - Temperature scaling: Softmax with T>1 makes distribution softer\n  * p_i =\
      \ exp(z_i/T) / Σexp(z_j/T)\n- Dark knowledge: Soft targets encode similarity\
      \ between classes\n- Example: DistilBERT (66M params) = 97% of BERT (110M) accuracy\n\
      \nQuantization:\n- Reduce precision: FP32 → INT8 (4× smaller, 4× faster)\n-\
      \ Post-training quantization (PTQ):\n  * Calibrate on small dataset (100-1000\
      \ samples)\n  * Map FP32 weights to INT8: w_int8 = round(w_fp32 / scale)\n \
      \ * Fast, no retraining, 1-2% accuracy drop\n- Quantization-aware training (QAT):\n\
      \  * Simulate quantization during training\n  * Better accuracy (< 0.5% drop)\
      \ but slower\n- Per-channel vs per-tensor quantization\n- Dynamic quantization:\
      \ Activations quantized at runtime\n\nPruning:\n- Remove unimportant weights\n\
      - Magnitude pruning: Remove weights with |w| < threshold\n- Structured pruning:\
      \ Remove entire neurons/channels\n- Iterative pruning:\n  1. Train full model\n\
      \  2. Prune small weights (remove 20%)\n  3. Fine-tune remaining weights\n \
      \ 4. Repeat\n- Lottery Ticket Hypothesis: Dense networks contain sparse subnetworks\n\
      - Typical: 50-90% weights can be pruned with <1% accuracy drop\n\nArchitecture-Specific\
      \ Optimization:\n- MobileNet: Depthwise separable convolutions\n  * Factorize\
      \ 3×3 conv → 3×3 depthwise + 1×1 pointwise\n  * 8-9× fewer parameters\n- SqueezeNet:\
      \ Fire modules (squeeze + expand)\n  * 50× smaller than AlexNet with similar\
      \ accuracy\n- EfficientNet: Neural architecture search for efficiency\n  * Compound\
      \ scaling (depth, width, resolution)\n\nServing Optimizations:\n- Batching:\
      \ Combine multiple requests, amortize overhead\n  * Trade-off: Latency (wait\
      \ for batch) vs throughput\n- Model caching: Cache embeddings for popular items\n\
      - Approximate nearest neighbors (ANN): FAISS, ScaNN\n  * 100× faster than exact\
      \ search with 99% recall\n- GPU optimizations: TensorRT, ONNX Runtime\n- ONNX\
      \ (Open Neural Network Exchange):\n  * Framework-agnostic model format\n  *\
      \ Deploy PyTorch model in TensorFlow Serving\n\nInference Frameworks:\n- TensorFlow\
      \ Serving: Production-ready, supports batching, model versioning\n- TorchServe:\
      \ PyTorch native serving\n- Triton Inference Server: NVIDIA, multi-framework\n\
      - Cloud options: AWS SageMaker, GCP AI Platform, Azure ML\n\nEdge Deployment:\n\
      - TensorFlow Lite: Mobile and edge devices\n- Core ML: iOS devices\n- Edge TPU:\
      \ Google's edge accelerator\n- Challenges: Limited memory (<100MB), CPU-only,\
      \ battery constraints\n- Solution: Distillation + quantization + pruning (combo\
      \ can achieve 100× compression)\n"
    practice_questions:
      concepts:
      - question: Explain how INT8 quantization achieves 4× speedup. What is the accuracy
          trade-off?
        answer: 'INT8 vs FP32: 1) Memory: 1 byte vs 4 bytes = 4× reduction. 2) Speed:
          INT8 ops are 4× faster on modern CPUs/GPUs (SIMD instructions). 3) Bandwidth:
          4× less data transfer (major bottleneck). Quantization: w_int8 = round(w_fp32
          / scale) where scale = max(|w_fp32|) / 127. Dequantize: w_fp32 ≈ w_int8
          * scale. Accuracy: Post-training quantization (PTQ): 1-2% drop. Quantization-aware
          training (QAT): <0.5% drop. Works because: Neural networks are overparameterized,
          robust to small perturbations.'
      - question: Compare pruning vs quantization vs distillation for model compression.
          When to use each?
        answer: 'Pruning: Removes weights (sparsity). Pros: Can remove 50-90% weights.
          Cons: Needs sparse matrix libraries to see speedup (not all hardware supports).
          Quantization: Reduces precision (FP32→INT8). Pros: 4× smaller, 4× faster
          on all hardware, minimal accuracy loss. Cons: Limited to 4× compression.
          Distillation: Train small model. Pros: Can design arbitrary small architecture,
          maintains accuracy. Cons: Requires retraining, may need unlabeled data.
          Best practice: Combine all three - distill to small architecture, quantize
          to INT8, prune if needed. Example: BERT→DistilBERT (distill) →INT8 (quantize)
          = 16× smaller.'
      - question: How does knowledge distillation transfer 'dark knowledge' from teacher
          to student? Why use temperature scaling?
        answer: 'Dark knowledge: Soft targets from teacher contain relative probabilities.
          Example: Teacher outputs [0.7 dog, 0.2 wolf, 0.1 cat] vs hard label [1 dog,
          0 others]. Soft targets encode: wolf is more similar to dog than cat. Temperature
          scaling: p_i = exp(z_i/T)/Σexp(z_j/T). T=1: Normal softmax (sharp). T>1:
          Softer distribution (spreads probability). Why: At T=1, teacher might output
          [0.99, 0.005, 0.005] (near one-hot, little info). At T=10, outputs [0.7,
          0.2, 0.1] (reveals similarity structure). Student learns these relationships,
          generalizes better.'
      tradeoffs:
      - question: 'For deploying BERT on mobile (latency <100ms, model <50MB), compare
          options: DistilBERT, quantization, pruning, or combination.'
        answer: 'BERT: 110M params = 440MB FP32, latency ~500ms on mobile CPU. DistilBERT:
          66M params = 264MB FP32, latency ~300ms (40% smaller, 40% faster). INT8
          quantization: 110M params = 110MB, latency ~125ms (4× smaller, 4× faster).
          Pruning 50%: 55M params = 220MB FP32, latency ~250ms (needs sparse lib).
          Best: DistilBERT + INT8: 66M params = 66MB INT8, latency ~75ms. Still too
          large. Final: DistilBERT + INT8 + 12→6 layers: 33M params = 33MB, latency
          ~40ms. Meets requirements (< 50MB, <100ms).'
      estimation:
      - question: Calculate memory and latency savings from quantizing ResNet-50 (25M
          params, 4B FLOPs) from FP32 to INT8.
        answer: 'Memory: FP32: 25M * 4 bytes = 100MB. INT8: 25M * 1 byte = 25MB. Savings:
          75MB (4× reduction). Latency: FP32 on CPU (2 GFLOPS): 4B FLOPs / 2G = 2
          seconds. INT8 on CPU (8 GOPS INT8): 4B ops / 8G = 0.5 seconds. Savings:
          1.5 seconds (4× faster). Accuracy: Post-training quantization typically
          <1% drop. On GPU: FP32 (125 TFLOPS): 32ms. INT8 (250 TOPS): 16ms. Conclusion:
          Quantization essential for deployment - 4× smaller and faster with minimal
          accuracy loss.'
    time_estimate: 75
    video_resources:
    - title: 'Two Minute Papers: Model Compression Techniques'
      url: https://www.youtube.com/@TwoMinutePapers
      duration: 8 min
      description: Quantization and pruning overview
      priority: high
    - title: 'Andrej Karpathy: Making Neural Networks Efficient'
      url: https://www.youtube.com/@AndrejKarpathy
      duration: 25 min
      description: Knowledge distillation and production optimization
      priority: high
