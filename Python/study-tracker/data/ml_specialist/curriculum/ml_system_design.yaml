track: "ml_system_design"
description: "14-day ML system design curriculum covering production ML infrastructure for Staff/Principal interviews"
weeks:
  week1:
    - day: 1
      topic: "Recommendation Systems - Architecture and Candidate Generation"
      activity: "Master the three-stage funnel (retrieval → ranking → re-ranking) and candidate generation methods"
      detailed_content: |
        Industry Standard: Three-Stage Funnel
        - Stage 1 - Candidate Generation (Retrieval):
          * Goal: 100M items → 10K candidates (99.99% reduction)
          * Latency budget: 10-50ms
          * Methods: Multiple retrieval sources in parallel

        - Stage 2 - Ranking:
          * Goal: Score 10K candidates → Top 500 items
          * Latency budget: 50-100ms
          * Model: Complex ML (neural network, gradient boosting)

        - Stage 3 - Re-ranking:
          * Goal: Final ordering with diversity, business rules
          * Latency budget: 10-20ms
          * Methods: Rule-based, lightweight model, MMR

        Candidate Generation Methods:
        1. Collaborative Filtering (CF):
           - User-Based CF: Find similar users, recommend their items
           - Item-Based CF: Find similar items to user's history (more stable)
           - Why Item-Based better: Fewer items than users, precompute daily, better cold start

        2. Matrix Factorization (ALS):
           - Decompose R ≈ U × V^T where U=user embeddings, V=item embeddings
           - Training: Alternating Least Squares (fix U optimize V, fix V optimize U)
           - Serving: ANN search on item embeddings

        3. Two-Tower Neural Network:
           - User Tower: user features → user embedding (128-dim)
           - Item Tower: item features → item embedding (128-dim)
           - Score: dot product of user and item embeddings
           - Training: Sampled softmax with negative sampling

        4. Sequential Models (GRU4Rec, SASRec):
           - Capture temporal patterns in user behavior
           - GRU4Rec: RNN for session-based recommendations
           - SASRec: Self-attention for sequences

        5. Graph-Based (Node2Vec, Item2Vec):
           - Build co-occurrence graph
           - Random walks to learn embeddings
           - Find similar items via embedding similarity

        Blending Multiple Sources:
        - Combine: CF + Two-tower + Sequential + Trending + Content-based
        - Each source contributes 100-300 candidates
        - Total: ~1000 candidates for ranking stage
      practice_questions:
        concepts:
          - question: "Why is the recommendation funnel split into three stages (retrieval, ranking, re-ranking) instead of one end-to-end model?"
            answer: "Performance and scale. Retrieval: Must process 100M items in <50ms, needs simple/fast methods (dot product, ANN search). Can't afford complex model. Ranking: Only 10K candidates, can afford 50-100ms for complex model (deep neural network). Re-ranking: Final 500 items, can apply slow business rules and diversity constraints. Trade-off: End-to-end would be too slow (complex model on 100M items = minutes). Staged approach: Simple fast model → complex accurate model → business logic. Total latency: 150ms."
          - question: "Explain negative sampling in two-tower models. Why is it necessary and how do you choose negatives?"
            answer: "Problem: Softmax over 100M items is infeasible. Negative sampling: For positive pair (user, item), sample K negative items. Loss: maximize log σ(u·v_pos) + Σ log σ(-u·v_neg). Effectively approximates full softmax. Strategies: 1) Random: uniform from catalog, 2) Popularity-based: P(item)^0.75 (word2vec trick), 3) Hard negatives: items shown but not clicked, 4) Batch negatives: use other positives in batch. K typically 100-1000. Hard negatives most effective but more complex."
          - question: "Compare Item-Based CF vs Matrix Factorization for candidate generation. When to use each?"
            answer: "Item-Based CF: Pros: Interpretable (users who liked X also liked Y), explainable, no training needed. Cons: Doesn't capture latent factors, sparse data issues. Matrix Factorization: Pros: Learns latent factors (genres, themes), dense embeddings work with ANN, better accuracy. Cons: Black box, needs retraining. Use Item-CF: Need explainability, small catalog (<10K items), sparse interactions. Use MF: Large catalog (>100K), have compute for training/ANN, accuracy critical. Modern systems use both: Item-CF for explainability + MF for accuracy."
        tradeoffs:
          - question: "For YouTube-scale recommendations (2B users, 500M videos), how would you design the candidate generation stage? What trade-offs would you make?"
            answer: "Scale challenges: Can't store all user-video pairs, can't compute scores for 500M videos in real-time. Design: 1) Multiple retrieval sources (5-7 methods), 2) User embeddings: 128-dim, 2B users = 1TB (shard by user_id), 3) Item embeddings: 128-dim, 500M videos = 256GB (fit in memory on single machine), 4) ANN index: FAISS with IVF+PQ, query time <10ms for top-1K, 5) Blend sources: 300 candidates each, total 1500 for ranking. Trade-offs: Embedding dim (128 vs 256): smaller=faster but less expressive. Num sources (3 vs 7): more=better coverage but higher latency. ANN recall (95% vs 99%): higher=better but slower. Choose: 128-dim (50ms ANN query), 5 sources (total 50ms parallel), 95% recall (acceptable for retrieval stage)."
        estimation:
          - question: "Calculate memory for storing 1B user embeddings (128-dim) and serving QPS if ANN search takes 10ms per query."
            answer: "Memory: 1B users × 128 dimensions × 4 bytes/float = 512GB. Too large for single machine. Sharding: Shard by user_id mod N. For N=16 shards: 32GB per machine (fits in memory). Item embeddings: 100M items × 128-dim = 51GB (fits on single machine for ANN index). QPS per machine: 1 query = 10ms → 100 QPS per machine. For 100K total QPS: need 1000 machines. Cost: c5.4xlarge (30GB RAM, $0.70/hour) × 1000 = $700/hour = $500K/month. Optimization: Use PQ compression (32 bytes → 8 bytes), 4× memory savings → 250 machines → $125K/month."
      time_estimate: 90

    - day: 2
      topic: "Recommendation Systems - Ranking and Re-ranking"
      activity: "Master ranking architectures (DeepFM, Wide&Deep), multi-objective optimization, and diversity techniques"
      detailed_content: |
        Ranking Stage:
        Goal: Precisely score 10K candidates using rich features and complex models

        Modern Ranking Architectures:
        1. Wide & Deep (Google Play):
           - Wide: Linear model on cross features (manual engineering)
           - Deep: DNN on embeddings (learned features)
           - Combine: Concatenate wide and deep outputs → final score
           - Benefits: Memorization (wide) + generalization (deep)

        2. DeepFM (Huawei):
           - FM component: Second-order feature interactions
             * y_FM = w_0 + Σw_i*x_i + ΣΣ<v_i,v_j>*x_i*x_j
           - DNN component: Higher-order interactions
           - Shared embeddings between FM and DNN
           - No manual feature engineering needed

        3. Multi-Task Learning (YouTube, Facebook):
           - Shared bottom layers
           - Task-specific towers for each objective:
             * Click probability (engagement)
             * Watch time (quality)
             * Like/share probability
             * Conversion (subscribe, purchase)
           - Loss: Weighted sum of task losses
           - Serving: Combine predictions into final score

        Feature Engineering:
        - User features: Demographics, history, session context
        - Item features: Content, popularity, freshness
        - Cross features: User-item interactions, category affinity
        - Context features: Time, device, location

        Multi-Objective Optimization:
        Problem: Conflicting objectives (engagement vs revenue vs diversity)

        Approaches:
        1. Weighted Sum: L = w1*L_ctr + w2*L_revenue + w3*L_diversity
           - Simple but hard to tune weights

        2. Pareto Frontier:
           - Explore trade-offs by varying weights
           - Visualize CTR vs revenue curve
           - Choose operating point based on business priorities

        3. Constraint-Based:
           - Maximize engagement subject to revenue ≥ threshold
           - Ensures minimum business metrics met

        4. Multi-Task Learning:
           - Train single model to predict all objectives
           - Adjust weights at serving time without retraining

        Re-ranking Stage:
        Goals: Diversity, freshness, business rules

        Techniques:
        1. Maximal Marginal Relevance (MMR):
           - Select items that are relevant AND diverse
           - MMR = λ*Relevance(item) - (1-λ)*MaxSimilarity(item, selected)
           - Greedy algorithm: iteratively select best MMR item

        2. Determinantal Point Process (DPP):
           - Probabilistic model for diverse subset selection
           - Kernel: L = Quality × Diversity
           - Sample diverse subset via DPP

        3. Exploration (Bandits):
           - Epsilon-greedy: Random items with probability ε
           - Thompson Sampling: Sample from Beta distribution
           - Balance exploration vs exploitation
      practice_questions:
        concepts:
          - question: "Explain how DeepFM combines FM and DNN. Why is this better than just using a DNN?"
            answer: "FM component: Learns pairwise feature interactions explicitly via v_i·v_j. Captures second-order relationships efficiently (O(k*n) where k=embedding dim). DNN component: Learns complex high-order interactions implicitly via layers. Both share same embeddings. Why better than DNN alone: FM provides explicit second-order interactions (strong inductive bias), DNN captures high-order. Result: Better accuracy with less data. Empirically: DeepFM outperforms DNN by 1-2% on CTR prediction tasks. Use case: CTR prediction where feature interactions critical (ad targeting, recommendation)."
          - question: "How does multi-task learning help recommendation systems? Explain with YouTube's ranking model."
            answer: "YouTube optimizes: 1) Click probability (engagement), 2) Watch time (quality), 3) Like/share (satisfaction). Single-task: Train 3 separate models, combine at serving. Multi-task: Shared bottom layers learn general representations, task-specific towers for each objective. Benefits: 1) Shared representations improve all tasks (transfer learning), 2) Single model deployment (simpler), 3) Can adjust weights at serving time. Architecture: User/item features → shared layers (256→128 dims) → 3 towers (64→1 each). Final score: 0.4*p_click + 0.4*watch_time/300 + 0.2*p_like. Adjust weights for business priorities."
          - question: "Explain MMR for diversity-aware re-ranking. When would you prefer MMR over just selecting top-K by score?"
            answer: "Top-K by score: May return very similar items (all sci-fi movies). Poor user experience, filter bubble. MMR: Balances relevance and diversity. Formula: MMR = λ*Relevance(item) - (1-λ)*MaxSim(item, selected). Greedy: Select item with max MMR, repeat. λ=1: Pure relevance (top-K). λ=0.5: Balance. λ=0: Pure diversity. Example: Netflix homepage - want mix of genres, not 10 similar movies. When to use: User-facing lists (homepage, search), need variety. When not: Precision-critical (search query 'iPhone 13') where user wants specific thing."
        tradeoffs:
          - question: "For ad CTR prediction, compare DeepFM vs simple logistic regression. Consider data size, feature engineering, and interpretability."
            answer: "Logistic Regression: Pros: 1) Interpretable coefficients, 2) Fast training/inference, 3) Works with small data (<100K). Cons: 1) Needs manual feature engineering (cross features), 2) Linear, can't learn interactions. DeepFM: Pros: 1) Learns interactions automatically (FM + DNN), 2) Better accuracy (+2-5% CTR), 3) No manual feature engineering. Cons: 1) Black box, 2) Needs more data (>1M), 3) Slower (10× inference time). Decision: <100K samples: Logistic Regression. >1M samples + accuracy critical: DeepFM. Startup: LR (quick iteration). FAANG: DeepFM (have data/compute, accuracy worth it)."
        scenarios:
          - question: "You're building YouTube homepage recommendations. How would you balance watch time (user satisfaction) vs ad revenue (business objective)?"
            answer: "Conflict: High watch time (long videos, engaging content) vs high ad revenue (more ads, shorter videos with ads). Multi-objective approach: 1) Train multi-task model predicting: a) Watch time (regression), b) Ad impressions (count), c) Revenue (regression). 2) Compute Pareto frontier: Test weights (watch time, revenue) = [(1.0, 0.0), (0.7, 0.3), (0.5, 0.5), (0.3, 0.7), (0.0, 1.0)]. 3) A/B test: Run 5 variants for 2 weeks, measure: watch time, revenue, retention (guardrail). 4) Results: (0.7, 0.3) gives 90% of max watch time, 70% of max revenue, best retention. 5) Business decision: Choose (0.7, 0.3) - prioritize user satisfaction while maintaining revenue. 6) Monitor: Track metrics over time, adjust if needed."
      time_estimate: 90

    - day: 3
      topic: "Search and Ranking Systems"
      activity: "Understand query understanding, candidate generation (inverted index), ranking (learning to rank), and evaluation (NDCG)"
      detailed_content: |
        Search vs Recommendations:
        - Search: User expresses intent via query → match documents
        - Recommendations: Predict preferences without query → discovery
        - Search: Query-dependent features dominate
        - Recommendations: User history & collaborative signals dominate

        Query Understanding:
        - Query rewriting: Spelling correction, expansion
        - Intent classification: Navigational, informational, transactional
        - Entity extraction: Recognize products, brands, locations
        - Query embeddings: BERT for semantic similarity

        Candidate Generation (Retrieval):
        1. Inverted Index:
           - Data structure: term → list of documents containing term
           - Boolean retrieval: Fast, scalable
           - Example: Query "machine learning" → docs with both terms

        2. BM25 Ranking:
           - Probabilistic ranking function
           - Score(D,Q) = Σ IDF(q_i) * (f(q_i,D) * (k+1)) / (f(q_i,D) + k*(1-b+b*|D|/avgdl))
           - Accounts for term frequency, document length, inverse document frequency

        3. Dense Retrieval (Neural):
           - Encode query and documents to embeddings
           - BERT-based: [CLS] token as embedding
           - Similarity: Dot product or cosine
           - ANN search: FAISS, ScaNN for fast retrieval

        4. Hybrid: Combine BM25 + Neural
           - BM25: Good for exact keyword match
           - Neural: Good for semantic similarity
           - Fusion: Weighted combination or reranking

        Ranking (Learning to Rank):
        1. Pointwise: Predict relevance score for each doc
           - Loss: Regression (MSE) or classification (cross-entropy)
           - Example: Predict ratings 1-5

        2. Pairwise: Learn to rank pairs
           - Given query, is doc A > doc B?
           - Loss: Hinge loss, logistic loss
           - Example: RankNet, LambdaRank

        3. Listwise: Directly optimize ranking metric
           - Loss: Approximation of NDCG or MAP
           - Example: LambdaMART (gradient boosting for ranking)
           - Most effective but complex

        Features for Ranking:
        - Query-document: BM25 score, BERT similarity
        - Document quality: PageRank, click-through rate
        - User context: Location, device, history
        - Cross features: Query-document interactions

        Evaluation Metrics:
        - NDCG@K (Normalized Discounted Cumulative Gain):
          * DCG@K = Σ (2^rel_i - 1) / log2(i+1)
          * NDCG = DCG / IDCG (ideal DCG)
          * Accounts for position and relevance grades
        - MAP (Mean Average Precision):
          * Precision at each relevant position, averaged
        - MRR (Mean Reciprocal Rank):
          * 1 / rank of first relevant document

        Production Considerations:
        - Caching: Cache popular queries
        - Personalization: User history, location
        - Freshness: Boost recent documents
        - Diversity: Avoid redundant results
      practice_questions:
        concepts:
          - question: "Explain BM25 ranking function. Why does it normalize by document length?"
            answer: "BM25: Score = Σ IDF(term) * TF_normalized. TF normalization: TF / (TF + k*(1-b+b*|D|/avgdl)) where |D|=doc length, avgdl=average doc length, b=0.75 (length normalization strength). Why normalize: Long documents naturally contain terms more times, would dominate short docs. Example: 'machine' appears 10 times in 1000-word doc vs 2 times in 100-word doc. Without normalization: long doc scores higher. With normalization: Penalize long docs, treat proportionally. b=0: No normalization (term frequency only). b=1: Full normalization. Typical b=0.75."
          - question: "Compare sparse retrieval (BM25) vs dense retrieval (BERT embeddings). When to use each?"
            answer: "BM25 (Sparse): Pros: 1) Exact keyword match (good for names, IDs), 2) Fast (inverted index), 3) Interpretable (term weights). Cons: 1) No semantic understanding ('auto' ≠ 'car'), 2) Vocabulary mismatch problem. BERT (Dense): Pros: 1) Semantic similarity ('auto' ≈ 'car'), 2) Handles synonyms, paraphrasing. Cons: 1) Slower (ANN search), 2) May miss exact matches, 3) Requires GPU for encoding. Best practice: Hybrid. Use BM25 for first-stage retrieval (fast, covers lexical), then BERT for re-ranking (semantic). Example: Google Search uses both - inverted index + neural ranking."
          - question: "Explain NDCG@K. Why is it better than accuracy for ranking evaluation?"
            answer: "NDCG@K: Accounts for 1) Relevance grades (not just binary), 2) Position (top results matter more). Formula: DCG = Σ (2^rel_i - 1)/log2(i+1). Denominator: Discounts lower positions. Numerator: Exponential gain for relevance. Normalize by ideal DCG (perfect ranking). Why better than accuracy: 1) Position matters: [rel, nonrel, nonrel] better than [nonrel, rel, nonrel] even though both have 1 relevant. 2) Graded relevance: Document with rel=3 better than rel=1. Accuracy treats equal. 3) Top-K focus: Only cares about top results, appropriate for search. Example: NDCG@10=0.8 means 80% as good as perfect ranking."
        tradeoffs:
          - question: "For e-commerce product search, design a retrieval+ranking pipeline. Balance latency, accuracy, and personalization."
            answer: "Constraints: <200ms latency, high relevance, personalized. Pipeline: 1) Candidate Generation (50ms): a) Query understanding: BERT for intent (buy vs browse), b) BM25 retrieval: 10K products from 10M catalog, c) User history: Past clicks/purchases → 1K products, d) Total: 11K candidates. 2) Ranking (100ms): a) Features: BM25, BERT similarity, price, user affinity, CTR, b) Model: LightGBM (fast), trained on click/purchase labels, c) Score all 11K, select top 500. 3) Re-ranking (50ms): a) Diversity: MMR to avoid duplicate brands, b) Business rules: Boost in-stock, margin, c) Final: Top 100 products. Total: 200ms. Latency breakdown: Retrieval 50ms (parallel BM25 + user), Ranking 100ms (batch inference), Re-rank 50ms."
        estimation:
          - question: "Estimate compute and memory for BERT-based dense retrieval on 1M product catalog. Query latency target <100ms."
            answer: "Offline (build index): Encode 1M products with BERT (768-dim). BERT forward: 10ms per product on GPU. Total: 1M * 10ms = 10M ms = 2.8 GPU-hours. Daily refresh: <3 hours on single GPU (acceptable). Memory: 1M × 768 × 4 bytes = 3GB for embeddings. ANN index (FAISS): ~5GB with IVF+PQ. Total: 8GB (fits in memory). Online (query): Encode query with BERT: 10ms. ANN search (top-1000): 20ms on CPU. Total: 30ms (meets <100ms target). Serving: c5.4xlarge ($0.70/hour) = $500/month. Conclusion: Feasible for 1M catalog, scales to 10M with more memory and index sharding."
      time_estimate: 90

    - day: 4
      topic: "Ad Click Prediction (CTR) - Features, Models, and Auction"
      activity: "Master CTR prediction with factorization machines, feature engineering, calibration, and ad auction mechanics"
      detailed_content: |
        Problem Definition:
        - Goal: Predict P(click | user, ad, context)
        - Data: Billions of impressions, click rate ~0.1-1% (highly imbalanced)
        - Latency: <10ms (user is waiting for page load)

        Feature Engineering:
        - User features: Demographics, click history, interests
        - Ad features: Creative, landing page, advertiser
        - Context features: Time, device, page content
        - Cross features: User-ad interactions
          * User clicked similar ads? User interest × ad category?
          * Manual engineering expensive → learn interactions

        Factorization Machines (FM):
        - Linear: y = w_0 + Σw_i*x_i (no interactions)
        - FM: y = w_0 + Σw_i*x_i + ΣΣ<v_i,v_j>*x_i*x_j
          * <v_i,v_j> = v_i · v_j (dot product of embeddings)
          * Learns feature interactions with O(kn) parameters (k=embedding dim)
        - Why FM: Captures interactions without manually crossing features
        - Efficient: Reformulate to avoid O(n²) computation

        DeepFM for CTR:
        - Combines FM (low-order interactions) + DNN (high-order)
        - Shared embeddings between FM and DNN
        - State-of-art for CTR prediction
        - Used by: Huawei, Criteo

        Training Challenges:
        - Class imbalance: 99% no-click, 1% click
          * Solution: Downsample negatives, adjust weights
        - Negative sampling: Can't use all negatives
          * Sample negatives proportional to frequency
        - Streaming data: Online learning, incremental updates
          * FTRL (Follow-The-Regularized-Leader) optimizer

        Calibration:
        - Problem: Model outputs not well-calibrated probabilities
        - Test: Plot predicted vs actual click rate in bins
        - Calibration methods:
          * Platt scaling: Train logistic regression on outputs
          * Isotonic regression: Learn monotonic mapping
          * Temperature scaling: Divide logits by T
        - Important for auction (expected value = bid × p_click)

        Ad Auction (Second-Price):
        - Bids: Advertiser bids $b per click
        - CTR prediction: Model predicts p(click)
        - Expected value: EV = b × p(click)
        - Rank by EV (not just bid)
        - Charge: Second-highest EV / winner's p(click)
        - Why: Incentivizes truthful bidding (game theory)

        Evaluation:
        - Offline: AUC, log loss, calibration curve
        - Online: CTR, revenue per impression, advertiser ROI
        - A/B testing: Randomly assign users to control/treatment
      practice_questions:
        concepts:
          - question: "Derive the efficient computation for Factorization Machines. Show how to avoid O(n²) complexity."
            answer: "Naive: ΣΣ<v_i,v_j>x_i*x_j requires O(n²) pairwise interactions. Efficient reformulation: ΣΣ<v_i,v_j>x_i*x_j = ΣΣ(Σv_i,f*v_j,f)x_i*x_j = Σ_f[(Σv_i,f*x_i)² - Σ(v_i,f*x_i)²] / 2. Key insight: (Σa_i)² - Σa_i² = 2*Σ_i<j a_i*a_j. Computation: For each factor f, compute sum = Σv_i,f*x_i (O(n)), then square (O(1)). Total: O(kn) where k=embedding dim. Typically k=10-100, n=millions → huge speedup. This makes FM practical for real-time serving."
          - question: "Why is calibration critical for ad CTR prediction? How does it affect auction outcomes?"
            answer: "Auction uses expected value: EV = bid × p(click). If model outputs are not calibrated (e.g., predicts 0.5 when true is 0.1), EV is wrong → incorrect ranking → revenue loss. Example: Ad A (bid=$1, true CTR=0.1, predicted=0.5) vs Ad B (bid=$0.8, true CTR=0.12, predicted=0.12). Uncalibrated: EV_A=0.5, EV_B=0.096 → show A (wrong, actual value=0.1). Calibrated: EV_A=0.1, EV_B=0.096 → show A (correct). Calibration ensures predicted probabilities match actual frequencies → correct ranking → maximize revenue."
          - question: "Explain how negative sampling addresses class imbalance in CTR prediction. What are the trade-offs?"
            answer: "Problem: 1B impressions, 10M clicks (1% CTR). Training on all negatives: 1) Computationally expensive, 2) Dominates gradients (99% weight on negatives). Negative sampling: Keep all positives (10M), sample negatives (e.g., 10% = 100M). Now 1:10 ratio instead of 1:99. During training: Adjust sample weights (negatives weighted 10×). Benefits: 10× faster training, balanced gradients. Drawbacks: Loses information from rare negatives, need to adjust for sampling in calibration. Trade-off: Sample more negatives (better accuracy, slower) vs fewer (faster, less accurate). Typical: 1:5 to 1:20 ratio."
        tradeoffs:
          - question: "Compare using logistic regression vs DeepFM for ad CTR prediction. Consider training time, interpretability, and accuracy."
            answer: "Logistic Regression: Pros: 1) Fast training (1 hour on 1B samples), 2) Fast serving (<1ms), 3) Interpretable weights. Cons: 1) Need manual feature crosses, 2) Linear (limited). DeepFM: Pros: 1) Learns interactions automatically (FM + DNN), 2) +5-10% AUC improvement, 3) State-of-art. Cons: 1) Slower training (10× longer), 2) Slower serving (5-10ms), 3) Black box. Decision: Startup/small scale: Logistic regression (simpler, faster iteration). Large scale (Google/Facebook): DeepFM (accuracy worth complexity, have infrastructure). Hybrid: LR for baseline, DeepFM for accuracy-critical campaigns."
        scenarios:
          - question: "Design a real-time ad CTR prediction system for Google Ads scale (1M QPS). Discuss architecture, features, and latency."
            answer: "Scale: 1M QPS, <10ms latency, 1B impressions/day. Architecture: 1) Feature extraction (2ms): a) User features: Lookup from Redis (demographics, history), b) Ad features: Lookup from cache (creative, landing page), c) Context: Parse from request (time, device, page). 2) Model serving (5ms): a) Model: DeepFM (optimized with TensorRT), b) Deployment: 1000× model servers (c5.2xlarge), c) Load balancing: Route by user_id, d) Batching: Micro-batches of 32 (5ms latency limit). 3) Calibration (1ms): Platt scaling on model output. 4) Total: 8ms. Throughput: 1000 QPS per server → need 1000 servers. Cost: $0.35/hour × 1000 = $350/hour = $250K/month. Optimizations: Model quantization (INT8), feature caching (Redis), ANN for similar ads."
      time_estimate: 75

    - day: 5
      topic: "Computer Vision Systems - Image Classification and Object Detection"
      activity: "Design production CV systems including transfer learning, model optimization, and edge deployment"
      detailed_content: |
        Image Classification System:
        Example: Content moderation (detect inappropriate images)

        Model Selection:
        - Transfer learning from ImageNet:
          * ResNet-50: 25M params, 4B FLOPs, good accuracy
          * EfficientNet-B0: 5M params, 0.4B FLOPs, best efficiency
          * Vision Transformer: State-of-art but slower
        - Fine-tuning strategy:
          * <1K labels: Feature extraction only (freeze backbone)
          * 1K-10K: Fine-tune last few layers
          * >10K: Full fine-tuning with low LR

        Data Pipeline:
        - Data augmentation: Random crop, flip, color jitter
        - Class imbalance: Oversample minority, weighted loss
        - Active learning: Label hard examples (low confidence)

        Training Infrastructure:
        - Distributed training: Data parallelism across GPUs
        - Mixed precision (FP16): 2× faster, same accuracy
        - Gradient accumulation: Simulate large batch size

        Model Optimization for Production:
        - Quantization: FP32 → INT8 (4× smaller, 4× faster)
        - Pruning: Remove 50-90% of weights
        - Distillation: Train small student from large teacher
        - ONNX: Framework-agnostic deployment

        Serving:
        - Batch inference: Group requests for efficiency
        - GPU serving: TensorRT, TensorFlow Serving
        - Caching: Cache predictions for popular images
        - Latency: <100ms for API, <50ms for real-time

        Object Detection:
        Example: Self-driving cars, surveillance

        Two-Stage Detectors:
        - Faster R-CNN: Region proposals → classify + localize
        - High accuracy but slow (100-500ms per image)

        One-Stage Detectors:
        - YOLO, SSD: Single pass, predict class + bbox directly
        - Faster (10-50ms) but slightly lower accuracy

        Trade-offs:
        - Accuracy vs Speed: Faster R-CNN vs YOLO
        - Resolution: Higher = better accuracy but slower
        - Backbone: ResNet vs EfficientNet vs MobileNet

        Edge Deployment:
        - Model compression: Quantization + pruning + distillation
        - Target: <50MB model, <100ms inference on mobile CPU
        - Frameworks: TensorFlow Lite, Core ML, ONNX Runtime
        - Hardware acceleration: Neural Engine (iPhone), EdgeTPU
      practice_questions:
        concepts:
          - question: "Explain transfer learning for image classification. Why does it work and when would you use it?"
            answer: "Transfer learning: Pretrain on large dataset (ImageNet: 1M images, 1000 classes), fine-tune on target task. Why it works: Low-level features (edges, textures) are general, transfer across domains. High-level features (object parts) partially transfer. Fine-tuning adapts to new domain. When to use: 1) Limited labeled data (<10K samples) - training from scratch would overfit. 2) Related domain - ImageNet photos → medical images (some transfer). 3) Need quick baseline - pretrained models widely available. Not when: 1) Very different domain (photos → X-rays: limited transfer), 2) Massive data (>1M samples: can train from scratch)."
          - question: "How does model quantization (FP32 → INT8) work? What's the accuracy vs speed trade-off?"
            answer: "Quantization: Map FP32 weights/activations to INT8 (256 discrete values). Process: 1) Determine range: max_val, min_val from calibration data. 2) Scale: scale = (max_val - min_val) / 255. 3) Quantize: w_int8 = round((w_fp32 - min_val) / scale). 4) Dequantize: w_fp32 ≈ w_int8 * scale + min_val. Benefits: 4× memory reduction (4 bytes → 1 byte), 4× faster inference (INT8 ops faster). Accuracy trade-off: Post-training quantization: 1-2% accuracy drop. Quantization-aware training: <0.5% drop. Why acceptable: Neural networks overparameterized, robust to small perturbations."
          - question: "Compare Faster R-CNN vs YOLO for object detection. When would you choose each?"
            answer: "Faster R-CNN (Two-stage): 1) Region Proposal Network generates ~2K proposals, 2) Classify + refine each. Accuracy: Higher (mAP ~40-45% on COCO). Speed: Slow (100-500ms per image). YOLO (One-stage): Single CNN predicts class + bbox for all grid cells. Accuracy: Slightly lower (mAP ~35-40%). Speed: Fast (10-50ms). Use Faster R-CNN: Accuracy critical, offline processing (e.g., medical imaging). Use YOLO: Real-time required (self-driving, video surveillance), mobile/edge deployment. Trend: One-stage detectors closing accuracy gap (YOLOv8, EfficientDet)."
        tradeoffs:
          - question: "For Instagram content moderation (detect inappropriate images), design a CV system. Balance accuracy, latency, and scale (1M images/day)."
            answer: "Requirements: High recall (catch all violations), <500ms latency (not user-blocking), 1M images/day. Design: 1) Model: EfficientNet-B3 (fine-tuned on NSFW dataset). Accuracy: 95% recall, 90% precision. Latency: 50ms on GPU. 2) Serving: a) 10 GPU servers (P3.2xlarge, $3/hour), b) Batch size 32 (optimize throughput), c) Load balancer distributes requests. 3) Throughput: 1M images/day = 12 images/sec. 1 GPU handles 640 images/sec (batch 32, 50ms). Need < 1 GPU, but use 10 for redundancy. 4) Post-processing: Low-confidence images (0.4-0.6) sent to human review queue. 5) Cost: 10 × $3/hour × 24 hours = $720/day. Optimizations: Model quantization (INT8) → 25ms latency, 5 GPUs → $360/day."
        estimation:
          - question: "Estimate training time and cost for fine-tuning ResNet-50 on 100K images for 10 epochs on AWS."
            answer: "ResNet-50: 25M params, 4B FLOPs per forward pass. Training: Forward (4B) + Backward (8B) = 12B FLOPs per image. Total: 100K images × 10 epochs × 12B FLOPs = 12P FLOPs. Hardware: P3.2xlarge (V100, 125 TFLOPS, $3/hour). Compute time: 12P / 125T = 96K seconds = 27 hours. But: Data loading, batch processing, overhead → 2× = 54 hours. Cost: 54 hours × $3 = $162. Memory: Batch size 32, activations ~2GB, model+optimizer ~400MB → need 4GB. V100 has 16GB → sufficient. Reality check: Practitioners report 1-2 days for this scale, matches estimate."
      time_estimate: 75

  week2:
    - day: 6
      topic: "NLP Systems - Text Classification and Question Answering"
      activity: "Design chatbots, spam detection, and QA systems using BERT, fine-tuning, and RAG"
      detailed_content: |
        Text Classification:
        Examples: Spam detection, sentiment analysis, topic classification

        Classical Approach:
        - TF-IDF + Logistic Regression:
          * Fast, interpretable, works with <10K samples
          * Baseline: 80-85% accuracy on sentiment

        Deep Learning Approach:
        - BERT fine-tuning:
          * Add classification head on [CLS] token
          * Fine-tune last few layers (or all)
          * Accuracy: 90-95% on sentiment
          * Latency: 50-100ms on CPU

        Model Selection:
        - Small data (<1K): TF-IDF + LR or pretrained embeddings + LSTM
        - Medium (1K-10K): DistilBERT (faster, 97% of BERT accuracy)
        - Large (>10K): BERT-base or RoBERTa
        - Latency critical: TinyBERT (4 layers, 20ms latency)

        Question Answering Systems:
        Two types:
        1. Extractive QA: Extract answer span from passage
           - Example: SQuAD dataset
           - Model: BERT with span prediction heads
           - Output: Start and end positions in text

        2. Generative QA: Generate answer from knowledge
           - Example: Open-domain QA
           - Model: T5, GPT
           - Output: Free-form text

        Retrieval-Augmented Generation (RAG):
        - Combine retrieval + generation
        - Architecture:
          1. Query → retrieve relevant documents (BM25 or dense retrieval)
          2. Concatenate query + documents
          3. Generate answer with T5 or GPT
        - Benefits: Grounded in documents, up-to-date information
        - Use case: Customer support chatbot with knowledge base

        Chatbot Design:
        Example: Customer support chatbot

        Components:
        1. Intent Classification: Identify user intent
           - Training data: <intent, example utterances>
           - Model: BERT fine-tuned for classification

        2. Entity Extraction: Extract key information
           - NER: Named Entity Recognition
           - Model: BERT + CRF layer

        3. Dialogue Management: Track conversation state
           - Rule-based or RL-based policy

        4. Response Generation:
           - Retrieval-based: Select from templates
           - Generative: GPT-based generation

        5. Knowledge Base: FAQ, product docs
           - Vector database for semantic search

        Production Considerations:
        - Latency: <500ms for chatbot response
        - Fallback: Human handoff for low confidence
        - Monitoring: Track intent accuracy, resolution rate
        - Continuous learning: Retrain on new conversations
      practice_questions:
        concepts:
          - question: "Explain how BERT is fine-tuned for text classification. What happens to the [CLS] token?"
            answer: "BERT pretraining: Learns contextualized embeddings via MLM + NSP. Fine-tuning for classification: 1) Add linear layer on [CLS] token output, 2) [CLS] embedding (768-dim) → FC layer → num_classes logits, 3) Loss: Cross-entropy, 4) Backprop through entire BERT (or freeze early layers). [CLS] token: Special token at start of sequence, its embedding aggregates entire sequence information via self-attention. Why it works: During pretraining, [CLS] learns to represent sentence-level semantics (NSP task). Fine-tuning adapts this representation to classification task. Alternatives: Average all token embeddings, max pooling (but [CLS] works best empirically)."
          - question: "How does Retrieval-Augmented Generation (RAG) work? Why is it better than pure generation for QA?"
            answer: "RAG: 1) Retrieve: Query → dense retrieval (BERT embeddings) → top-K relevant documents. 2) Augment: Concatenate query + retrieved docs as context. 3) Generate: Pass to T5/GPT → generate answer. Why better: 1) Grounded: Answers are based on retrieved docs (not hallucinated). 2) Up-to-date: Update document index without retraining model. 3) Interpretable: Can show source documents. 4) Efficient: Don't need to store all knowledge in model parameters. Pure generation (GPT): Stores knowledge in parameters, can hallucinate, hard to update. Use RAG: Customer support (knowledge base), medical QA (research papers). Use pure gen: Creative tasks, general conversation."
          - question: "For a spam detection system, compare TF-IDF+LR vs BERT. Consider data size, latency, and interpretability."
            answer: "TF-IDF+LR: Pros: 1) Fast training (minutes on 100K emails), 2) Fast inference (1ms), 3) Interpretable (can see which words indicate spam). Cons: 1) Bag-of-words (ignores order), 2) Lower accuracy (~85%). BERT: Pros: 1) Understands context ('free money' vs 'money-free vacation'), 2) Higher accuracy (~92%). Cons: 1) Slow training (hours), 2) Slow inference (50ms), 3) Black box. Decision: <10K samples + need interpretability: TF-IDF+LR. >10K samples + accuracy critical: DistilBERT (40ms). Latency critical (<10ms): TF-IDF+LR. Real systems: Use TF-IDF+LR for first-stage (fast, filters 90%), BERT for second-stage (accurate, filters remaining 10%)."
        tradeoffs:
          - question: "Design a customer support chatbot with 1000 FAQs. Compare retrieval-based vs generative approaches."
            answer: "Retrieval-based: 1) Encode 1000 FAQs with BERT → embeddings, 2) User query → encode → ANN search → top-3 FAQs, 3) Return FAQ answers. Pros: Controlled responses (no hallucination), fast (<100ms), interpretable. Cons: Limited to FAQs, can't handle variations. Generative (GPT): 1) Encode FAQs as context, 2) User query + context → GPT, 3) Generate custom answer. Pros: Flexible, natural responses, handles follow-ups. Cons: May hallucinate, slow (500ms), expensive ($0.01 per query). Hybrid (best): 1) Retrieval: Find top-3 FAQs, 2) Rerank: BERT cross-encoder for better ranking, 3) Generate: GPT to rephrase answer naturally. Pros: Grounded + flexible. Cons: More complex. For 1000 FAQs: Start with retrieval (simple, fast), add generative if user feedback requests more flexibility."
        scenarios:
          - question: "You're building a chatbot for a bank (10K daily users). Estimate infrastructure costs for BERT-based intent classification and response generation."
            answer: "Scale: 10K users/day, avg 5 messages/user = 50K messages/day = 0.6 QPS average, 10 QPS peak. Intent classification: DistilBERT (40ms on CPU). Peak: 10 QPS × 40ms = 400ms total → need 1 CPU core. Response: Retrieval (FAQ search, 10ms) + GPT for rephrasing (200ms). Total latency: 250ms (acceptable). Infrastructure: 1) API servers: 2× c5.xlarge (4 vCPUs, $0.17/hour) = $8/day. 2) GPT API: 50K messages × $0.001 = $50/day. 3) Vector DB (FAISS): 1× r5.large (16GB RAM, $0.13/hour) = $3/day. Total: $61/day = $1,830/month. Optimizations: Cache popular intents (reduce GPT calls 50%) → $900/month. Scale to 100K users: 10× → ~$9K/month."
      time_estimate: 75

    - day: 7
      topic: "Fraud Detection and Anomaly Detection Systems"
      activity: "Design real-time fraud detection for payments, fake accounts, and bot detection with graph features"
      detailed_content: |
        Fraud Detection Overview:
        - Types: Credit card fraud, payment fraud, fake accounts, bot traffic
        - Characteristics: Rare events (<0.1%), adversarial (fraudsters adapt)
        - Requirements: Real-time scoring (<100ms), high precision (minimize false positives)

        Feature Engineering:
        1. Transaction Features:
           - Amount, merchant, location, time
           - Velocity: # transactions in last 1h, 24h, 7d
           - Deviation: Compare to user's historical patterns

        2. User Features:
           - Account age, verification status
           - Historical fraud rate
           - Device fingerprint

        3. Graph Features:
           - User-merchant bipartite graph
           - User-device graph
           - PageRank, clustering coefficient
           - Fraud rings: Connected components

        4. Behavioral Features:
           - Typing speed, mouse movements (bot detection)
           - Session duration, click patterns

        Model Selection:
        - XGBoost / LightGBM:
          * Pros: Great for tabular data, handles categorical features
          * Fast inference (<10ms)
          * Feature importance for interpretability
        - Graph Neural Networks:
          * For fraud rings (connected fraudulent accounts)
          * Learn from graph structure
          * Slower but more powerful

        Class Imbalance:
        - Fraud rate: 0.01-0.1% (1 in 1000-10000)
        - Techniques:
          * Downsampling: Keep all positives, sample negatives
          * Class weights: Weight positives 100-1000×
          * Focal loss: Focus on hard examples
          * Precision@K: Optimize for top K predictions

        Real-Time Scoring:
        - Latency budget: <100ms (user waiting for transaction approval)
        - Architecture:
          1. Feature extraction (30ms): Lookup from Redis, compute velocity
          2. Model inference (20ms): XGBoost on CPU
          3. Rule engine (10ms): Hard rules (e.g., amount > $10K → flag)
          4. Total: 60ms
        - Serving: 1000 QPS → need 10 servers (100 QPS each)

        Precision-Recall Trade-off:
        - High precision (minimize false positives):
          * Legitimate transactions flagged → customer frustration
          * Set high threshold (predict fraud if score > 0.9)
        - High recall (catch all fraud):
          * Minimize losses to fraud
          * Set low threshold (score > 0.1)
        - Typical: Optimize for precision (few false alarms), accept some fraud

        Evaluation:
        - Offline: Precision@1%, Precision@0.1%, AUC-PR (not AUC-ROC for imbalanced)
        - Online: Dollar loss prevented, false positive rate, customer complaints
        - A/B testing: Random buckets, measure fraud loss

        Continuous Learning:
        - Fraudsters adapt → model degrades
        - Retrain daily/weekly on recent data
        - Monitor: Drift detection (feature distributions, model performance)
        - Feedback loop: Confirmed fraud → add to training

        Explainability:
        - Regulators require explanations for declined transactions
        - SHAP values: Top features contributing to fraud score
        - Rules: Hard rules (e.g., "Location mismatch with billing address")
      practice_questions:
        concepts:
          - question: "Why are graph features important for fraud detection? Give an example of a fraud ring."
            answer: "Fraud rings: Multiple fake accounts controlled by same fraudster. Individual accounts look normal, but graph reveals connections. Example: 10 accounts share same device ID, IP address, or phone number. Graph features: 1) Degree: # connections to other accounts (fraud ring has high degree). 2) Clustering coefficient: Fraud ring is densely connected. 3) PageRank: Fraudulent accounts link to each other (low PageRank). Detection: Build user-device-IP graph, run community detection, flag dense subgraphs. GNNs: Learn embeddings that capture graph structure, predict fraud on nodes. Why effective: Fraudsters can fake user features but harder to fake graph structure."
          - question: "Explain the precision-recall trade-off in fraud detection. How do you choose the operating point?"
            answer: "Precision: Of flagged transactions, % that are actually fraud. Recall: Of all fraud, % that we caught. Trade-off: Increasing threshold → higher precision, lower recall. Decreasing → higher recall, lower precision. Fraud detection: Prioritize precision (minimize false positives). Why: False positive = legitimate transaction declined → customer frustration, lost sale. Missing fraud = financial loss but less immediate impact. Choosing operating point: 1) Business decision: How much fraud loss is acceptable vs customer complaints? 2) Cost: False positive costs $X (lost sale), false negative costs $Y (fraud loss). 3) Optimize: Precision subject to recall ≥ threshold (e.g., catch 80% of fraud). Typical: Set threshold at 95% precision, accept 50% recall."
          - question: "How do you handle model drift in fraud detection? Why is continuous retraining important?"
            answer: "Drift types: 1) Data drift: Feature distributions change (new fraud tactics). 2) Concept drift: Relationship between features and fraud changes. Why critical: Fraudsters adapt (if model blocks tactic A, they switch to B). Model trained on old tactics becomes ineffective. Detection: 1) Monitor feature distributions (KL divergence from training), 2) Monitor model performance (precision drops), 3) A/B test new model vs old. Continuous retraining: 1) Retrain daily/weekly on recent data (30-day window), 2) Use online learning (FTRL) for real-time updates, 3) Feedback loop: Confirmed fraud → add to training immediately. Guardrails: Shadow mode new model (don't affect production), gradual rollout, rollback if metrics drop."
        tradeoffs:
          - question: "For payment fraud detection, compare XGBoost vs Graph Neural Networks. When to use each?"
            answer: "XGBoost: Pros: 1) Fast inference (<10ms), 2) Works well on tabular features (amount, merchant, velocity), 3) Interpretable (SHAP values), 4) Mature libraries. Cons: 1) Doesn't capture graph structure (fraud rings), 2) Treats transactions independently. GNN: Pros: 1) Captures graph structure (user-merchant, user-device), 2) Detects fraud rings (connected accounts), 3) Better recall for sophisticated fraud. Cons: 1) Slower inference (50-100ms), 2) Complex, harder to debug, 3) Needs graph data (edges). Decision: Solo fraudsters (single transactions): XGBoost. Fraud rings (coordinated attacks): GNN. Hybrid: XGBoost for real-time scoring (fast), GNN for batch analysis (find rings), combine scores."
        scenarios:
          - question: "Design a real-time fraud detection system for Stripe (100K transactions/second). Estimate infrastructure and latency."
            answer: "Scale: 100K TPS, <100ms latency, 0.1% fraud rate. Architecture: 1) Feature Store (Redis): a) User features: Account age, fraud history (10ms lookup), b) Velocity: # transactions in 1h (real-time aggregation), c) Graph features: Precomputed PageRank (10ms lookup). 2) Model Serving (XGBoost): a) Input: 100 features, b) XGBoost inference: 10ms on CPU, c) Deployment: 1000 servers (c5.2xlarge, 8 vCPUs), d) Each server: 100 QPS (10ms per inference). 3) Rule Engine: Hard rules (amount > $50K, location mismatch) - 5ms. Total latency: 10ms (feature) + 10ms (model) + 5ms (rules) = 25ms. Infrastructure: 1000 servers × $0.40/hour = $400/hour = $290K/month. Optimizations: Model caching (cache scores for repeat patterns), feature caching (popular merchants), reduce to 500 servers → $145K/month."
      time_estimate: 75

  week3:
    - day: 8
      topic: "Feed Ranking Systems - Multi-Objective Learning and Diversity"
      activity: "Design social media feed ranking with engagement prediction, multi-task learning, and diversity constraints"
      detailed_content: |
        Feed Ranking Overview:
        - Examples: Facebook News Feed, Twitter Timeline, Instagram Feed
        - Goal: Show most engaging content to each user
        - Challenges: Multiple objectives, diversity, freshness

        Multi-Objective Optimization:
        Conflicting objectives:
        - User engagement: Clicks, likes, shares, time spent
        - Business revenue: Ad clicks, conversions
        - Platform health: Reduce misinformation, toxic content
        - Creator satisfaction: Fair distribution of impressions

        Approaches:
        1. Weighted Sum: Score = w1*p(click) + w2*p(like) + w3*p(share)
           - Simple but hard to tune weights

        2. Multi-Task Learning:
           - Shared bottom layers
           - Task-specific towers for each objective
           - Benefits: Transfer learning, single model deployment

        3. Pareto Frontier:
           - Explore trade-offs by varying weights
           - Business chooses operating point

        4. Constraint-Based:
           - Maximize engagement subject to revenue ≥ threshold

        Features:
        - User features: Demographics, interests, activity level
        - Content features: Type (photo/video/link), creator, topic
        - User-content: Has user engaged with creator before?
        - Context: Time of day, device, location
        - Social: Friends' interactions (likes, shares)

        Model Architecture:
        - Two-tower: User tower + content tower → dot product
        - Deep & Wide: Wide (cross features) + Deep (embeddings)
        - Multi-task: Shared layers → towers for click, like, share, time spent

        Diversity Constraints:
        - Problem: Top-K by score may be all similar content (filter bubble)
        - MMR (Maximal Marginal Relevance): Balance relevance and diversity
        - DPP (Determinantal Point Process): Probabilistic diverse subset
        - Position-based: Force different types in top-10 (1 video, 2 photos, etc.)

        Freshness:
        - Problem: Popular old posts dominate new posts
        - Time decay: Score = base_score × e^(-λ × age)
        - Recency boost: Add bonus for posts < 1 hour old
        - Balance: Ensure new creators get visibility

        Exploration:
        - Bandits: Epsilon-greedy, Thompson sampling
        - Why: Discover new content users might like
        - Trade-off: Exploitation (show known good content) vs exploration (try new)

        Evaluation:
        - Offline: AUC for engagement prediction, NDCG for ranking
        - Online: Time spent, engagement rate, retention
        - Guardrail: Ensure no harm to platform health metrics
      practice_questions:
        concepts:
          - question: "How does multi-task learning help feed ranking? Explain the architecture and benefits."
            answer: "Feed ranking optimizes multiple objectives: click, like, share, time spent. Multi-task: Shared bottom layers learn general user/content representations, task-specific towers predict each objective. Architecture: User/content features → shared layers (512→256 dims) → 4 towers (128→1 each). Benefits: 1) Transfer learning: Shared layers learn features useful for all tasks (e.g., user interests), 2) Efficiency: Single model deployment, 3) Flexibility: Adjust weights at serving time (score = 0.3*click + 0.2*like + 0.3*share + 0.2*time). Training: Joint loss = w1*L_click + w2*L_like + w3*L_share + w4*L_time. Empirically: Multi-task outperforms separate models by 2-5% on each metric."
          - question: "Explain the filter bubble problem in feed ranking. How do diversity techniques address it?"
            answer: "Filter bubble: If rank purely by predicted engagement, user sees only similar content (e.g., all political posts). Causes echo chamber, poor user experience. Why happens: Top-K by score selects most similar to user's past behavior. Diversity techniques: 1) MMR: Select items that are relevant AND dissimilar to already-selected. Formula: MMR = λ*relevance - (1-λ)*max_similarity. 2) DPP: Sample diverse subset, kernel = quality × diversity. 3) Position-based: Force mix (1 news, 1 photo, 1 video in top-3). Trade-off: Diversity vs engagement (showing diverse content may reduce short-term engagement but improve long-term retention)."
          - question: "How do you balance freshness vs relevance in feed ranking? Explain time decay."
            answer: "Problem: Popular old posts have high engagement score (many likes), dominate new posts. But users want fresh content. Time decay: Reduce score of old posts. Formula: Score_final = Score_base × e^(-λ×age) where age in hours, λ controls decay rate. Example: Post with score=10 at 1 hour old: 10×e^(-0.1×1) ≈ 9. Same post at 24 hours: 10×e^(-0.1×24) ≈ 0.9. Choosing λ: Small λ (0.01): Slow decay, old posts remain. Large λ (0.5): Fast decay, prioritize fresh. Typical: λ=0.05-0.1 (half-life ~7-14 hours). Balance: Ensure viral content can rise while promoting fresh posts."
        tradeoffs:
          - question: "For Facebook News Feed, how would you balance engagement (time spent) vs platform health (reduce misinformation)? Discuss multi-objective optimization."
            answer: "Conflict: High engagement content (clickbait, sensational) may be misinformation. Business must balance. Approaches: 1) Weighted sum: Score = 0.7×engagement - 0.3×misinformation_score. Cons: Hard to tune weights. 2) Constraint-based: Maximize engagement subject to misinformation_rate < 1%. Ensures minimum health. 3) Multi-task learning: Predict both engagement and misinformation, combine at serving. 4) Pareto frontier: Test multiple weight combinations, A/B test, show trade-off curve to leadership. Recommendation: Start with constraint (maximize engagement, cap misinformation at acceptable level). A/B test: Vary constraint threshold (0.5%, 1%, 2%), measure: time spent, retention (long-term health), user complaints. Choose: Threshold that maximizes engagement while maintaining retention."
        scenarios:
          - question: "Design Instagram feed ranking (500M DAU, 100M posts/day). Estimate compute for ranking model serving."
            answer: "Scale: 500M users, each opens app 5× per day = 2.5B sessions/day = 29K QPS average, 100K QPS peak. Per session: Show 50 posts (rank top-50 from 1000 candidates). Features: 200 features (user, content, cross). Model: Multi-task neural network, 2M params. Inference: 200 features × 1000 candidates = 200K features to process. Batch inference (1000 candidates): 10ms on GPU. Peak: 100K QPS × 10ms = 1000 GPU-seconds per second → need 1000 GPUs. Infrastructure: 1000× P3.2xlarge (V100, $3/hour) = $3K/hour = $2.2M/month. Optimizations: 1) Two-stage: Retrieve 1000 candidates (fast), rank 50 with complex model (slow). 2) Caching: Cache scores for popular posts (50% reduction). 3) Model quantization (INT8): 4× faster → 250 GPUs → $550K/month. Final: ~$500K/month for feed ranking."
      time_estimate: 75

    - day: 9
      topic: "Training Infrastructure - Distributed Training and Experiment Tracking"
      activity: "Design scalable training pipelines with data parallelism, hyperparameter tuning, and experiment management"
      detailed_content: |
        Distributed Training:
        Why: Single GPU training too slow for large datasets/models

        Data Parallelism:
        - Replicate model on N GPUs
        - Split batch across GPUs (each processes batch_size/N)
        - Each GPU computes gradients on its data
        - Synchronize: Average gradients across GPUs
        - Update model weights
        - Linear speedup (ideally): 8 GPUs → 8× faster

        Synchronization:
        - Synchronous: Wait for all GPUs before updating (slow)
        - Asynchronous: GPUs update independently (faster but less stable)
        - Gradient accumulation: Simulate large batch with small batches

        Model Parallelism:
        - Split model across GPUs (for very large models)
        - Each GPU holds part of model
        - Example: GPT-3 (175B params) doesn't fit on one GPU
        - Pipeline parallelism: Split into stages

        Mixed Precision Training:
        - Use FP16 for forward/backward, FP32 for weights
        - Benefits: 2× speedup, 2× memory reduction
        - Loss scaling: Prevent underflow in FP16
        - Automatic Mixed Precision (AMP): PyTorch/TensorFlow

        Hyperparameter Tuning:
        - Grid search: Try all combinations (expensive)
        - Random search: Sample combinations (better than grid)
        - Bayesian optimization: Model performance, suggest next trial
          * Tools: Optuna, Hyperopt
        - Early stopping: Stop bad trials early

        Experiment Tracking:
        - Problem: 100s of experiments, lose track of hyperparameters, results
        - Tools: MLflow, Weights & Biases, TensorBoard
        - Log: Hyperparameters, metrics, model checkpoints, training curves
        - Compare: Visualize experiments side-by-side

        Training Pipeline:
        1. Data loading: tf.data, PyTorch DataLoader
           - Prefetching: Load next batch while training
           - Parallelism: Multi-threaded data loading
        2. Training loop: Forward, loss, backward, update
        3. Checkpointing: Save model every N steps
        4. Evaluation: Validate every epoch
        5. Monitoring: Log metrics to experiment tracker

        Resource Estimation:
        - Training time: (dataset_size × epochs × FLOPs_per_sample) / (GPU_FLOPS × num_GPUs)
        - Memory: Model params + gradients + activations + optimizer state
          * BERT-base: 110M params × 4 bytes × 4 (params + grads + optimizer) = 1.7GB
          * Add activations: batch_size × seq_len × hidden_dim × num_layers
        - Cost: GPU_hours × GPU_price

        Best Practices:
        - Start small: Overfit on tiny dataset to debug
        - Baseline: Train simple model first
        - Monitor: Track training curves (loss, validation accuracy)
        - Checkpoint: Save frequently in case of crashes
        - Reproducibility: Set random seeds, log hyperparameters
      practice_questions:
        concepts:
          - question: "Explain data parallelism for distributed training. How does gradient synchronization work?"
            answer: "Data parallelism: Replicate model on N GPUs. Split batch (size B) into N parts (B/N each). Each GPU: 1) Forward pass on its B/N samples, 2) Compute loss, 3) Backward pass → gradients. Synchronization: All-reduce operation averages gradients across GPUs. GPU1 has ∇L1, GPU2 has ∇L2, ... → Compute ∇L_avg = (∇L1 + ∇L2 + ... + ∇LN) / N. Update: All GPUs update with ∇L_avg (stay synchronized). Speedup: Ideal N× (N GPUs). Reality: 0.8-0.9× N due to communication overhead. Tools: PyTorch DistributedDataParallel (DDP), Horovod."
          - question: "Why use mixed precision training (FP16)? What problems does it solve and what new challenges does it introduce?"
            answer: "Benefits: 1) Speed: FP16 ops 2× faster on Tensor Cores (V100, A100), 2) Memory: 2× less memory (2 bytes vs 4 bytes), fits larger batches. Challenges: 1) Underflow: FP16 range is smaller (6e-8 to 65K) vs FP32 (1e-38 to 3e38). Small gradients → 0 in FP16. Solution: Loss scaling: Multiply loss by scale factor (e.g., 1024) before backward → gradients larger → no underflow. Then divide gradients by scale before update. 2) Precision: FP32 weights accumulator (update in FP32, store in FP16). Tools: Automatic Mixed Precision (AMP) handles this automatically."
          - question: "Explain Bayesian optimization for hyperparameter tuning. Why is it better than random search?"
            answer: "Random search: Sample hyperparameters randomly, train, evaluate. Inefficient: Treats trials independently. Bayesian optimization: 1) Build surrogate model (Gaussian Process) of performance f(hyperparams), 2) Acquisition function chooses next hyperparams (balance exploration vs exploitation), 3) Train with chosen hyperparams, observe performance, 4) Update surrogate model, repeat. Why better: Uses information from past trials to suggest promising hyperparams. Fewer trials needed (10-20 vs 100+ for random). Example: After trying learning_rate=0.1 (bad) and 0.001 (good), BO suggests 0.005 (interpolate), not 0.5 (random might try). Tools: Optuna, Hyperopt."
        tradeoffs:
          - question: "For training a BERT model on 1M samples, compare single GPU vs 8-GPU data parallelism. Consider time, cost, and batch size trade-offs."
            answer: "Single GPU (V100): Batch size 32 (memory limit), 1M samples / 32 = 31K batches, 100ms per batch → 3100 seconds/epoch = 52 minutes. 10 epochs: 520 minutes = 8.7 hours. Cost: $3/hour × 8.7 = $26. 8 GPUs (data parallel): Batch size 256 (32 × 8), 1M / 256 = 3.9K batches, 100ms per batch (same, but parallelized) → 390 seconds/epoch = 6.5 minutes. 10 epochs: 65 minutes = 1.1 hours. Cost: $24/hour × 1.1 = $26. Same cost but 8× faster! Caveat: Large batch (256) may require tuning (LR warmup, higher LR). Trade-off: 8 GPUs faster training (better for iteration speed), same cost."
        estimation:
          - question: "Estimate memory requirements for training ResNet-50 with batch size 128 on a single GPU."
            answer: "ResNet-50: 25M parameters. Memory breakdown: 1) Model weights: 25M × 4 bytes = 100MB. 2) Gradients: 25M × 4 bytes = 100MB. 3) Optimizer state (Adam): 2 × 25M × 4 bytes = 200MB (first + second moment). 4) Activations: Batch size × feature maps. Estimate: 128 × 2048 (avg feature map size) × 50 layers × 4 bytes = 50MB (rough, varies by layer). Total: 100 + 100 + 200 + 50 = 450MB. Add buffer (intermediate tensors): 2× = 900MB. Reality check: Practitioners report ~1-2GB for batch size 128 on ResNet-50 (matches estimate). V100 has 16GB → comfortable fit."
      time_estimate: 60

    - day: 10
      topic: "Model Serving Infrastructure - Batching, Caching, and Deployment Strategies"
      activity: "Design production serving systems with TensorFlow Serving, optimization, and deployment strategies"
      detailed_content: |
        Model Serving Overview:
        - Goal: Serve predictions at low latency and high throughput
        - Challenges: Latency (<100ms), scale (1000s QPS), cost

        Serving Frameworks:
        - TensorFlow Serving: Production-ready, model versioning, batching
        - TorchServe: PyTorch native
        - Triton Inference Server: Multi-framework (TF, PyTorch, ONNX)
        - Cloud: SageMaker, AI Platform, Azure ML

        Optimization Techniques:
        1. Batching:
           - Combine multiple requests into batch
           - Amortize GPU overhead
           - Trade-off: Latency (wait for batch) vs throughput
           - Dynamic batching: Wait up to T ms or B requests

        2. Model Optimization:
           - Quantization: FP32 → INT8 (4× faster, 4× smaller)
           - Pruning: Remove unimportant weights
           - Distillation: Small model mimics large model
           - ONNX: Framework-agnostic, optimized runtime

        3. Caching:
           - Cache predictions for popular inputs
           - Example: Recommendation cache for popular items
           - TTL: Expire after T minutes

        4. GPU vs CPU:
           - GPU: High throughput (batch), expensive
           - CPU: Low latency (single), cheap
           - Decision: Latency critical → CPU, throughput → GPU

        Deployment Strategies:
        1. Blue-Green Deployment:
           - Run old (blue) and new (green) versions
           - Switch traffic to green
           - Rollback to blue if issues

        2. Canary Deployment:
           - Route small % (5%) to new version
           - Monitor metrics
           - Gradually increase % if successful

        3. Shadow Deployment:
           - New model receives traffic but doesn't affect users
           - Compare predictions with old model
           - Validate before switching

        Model Versioning:
        - Store multiple model versions
        - Gradual rollout: Route % of traffic to each version
        - A/B testing: Random assignment to versions
        - Rollback: Quick switch to previous version

        Monitoring:
        - Latency: p50, p95, p99 percentiles
        - Throughput: QPS
        - Error rate: Failed predictions
        - Resource: CPU, GPU, memory usage
        - Model metrics: Prediction distribution, drift

        Autoscaling:
        - Monitor QPS, latency
        - Scale up: Add instances if latency > threshold
        - Scale down: Remove instances if underutilized
        - Kubernetes Horizontal Pod Autoscaler

        Load Balancing:
        - Distribute requests across instances
        - Round-robin, least connections, or consistent hashing
        - Health checks: Remove unhealthy instances

        Cold Start:
        - Problem: New instance takes time to load model
        - Solution: Pre-warm instances, model caching
      practice_questions:
        concepts:
          - question: "Explain dynamic batching in model serving. What's the latency-throughput trade-off?"
            answer: "Dynamic batching: Collect requests over time window T (e.g., 50ms) or until batch size B (e.g., 32), whichever comes first. Send batch to model for inference. Benefits: Batching amortizes GPU overhead → higher throughput. Trade-offs: Latency = wait time + inference time. Wait: 0-50ms (avg 25ms). Inference: 50ms for batch. Total: 75ms. Without batching: 0ms wait + 50ms inference = 50ms. So: Batching adds 25ms latency but increases throughput 10× (batch 32 vs 1). Decision: Latency critical (<50ms): No batching or small T (10ms). Throughput critical: Large T (100ms), large B (128)."
          - question: "Compare canary deployment vs blue-green deployment for ML models. When to use each?"
            answer: "Blue-Green: Run two full environments (old=blue, new=green). Switch 100% traffic blue→green instantly. Rollback: Switch back to blue. Pros: Fast rollback, simple. Cons: Need 2× capacity (expensive), all-or-nothing risk. Canary: Route small % (5%) to new model, rest to old. Monitor: Compare metrics (latency, accuracy). Gradually increase % if good. Pros: Risk mitigation (only 5% affected), cheaper (no 2× capacity). Cons: Slower rollout, complex traffic routing. Use Blue-Green: Small-scale, confident in new model, need fast rollback. Use Canary: Large-scale (millions of users), uncertain about new model, want gradual validation."
          - question: "How does model caching improve serving performance? What are the trade-offs?"
            answer: "Caching: Store prediction for input. On repeat request, return cached prediction (1ms) instead of model inference (50ms). Example: Recommendation for popular item requested 1000× per second. Benefits: 50× speedup, reduce compute cost. Trade-offs: 1) Memory: Store input→output map. If 1M popular items × 1KB = 1GB. 2) Staleness: Cached prediction may be outdated (user's interests changed). 3) Cache miss: Unpopular items still need inference. TTL: Set expiration (e.g., 1 hour) to balance freshness vs hit rate. Decision: Cache when: High QPS, inputs repeat often (search queries, popular items), predictions don't change frequently. Don't cache: Personalized predictions change rapidly."
        tradeoffs:
          - question: "For a recommendation API (1000 QPS, <100ms latency), compare CPU vs GPU serving. Consider cost and performance."
            answer: "CPU serving: Instance: c5.2xlarge (8 vCPUs, $0.34/hour). Inference: 50ms per request (no batching, sequential). Throughput: 20 QPS per instance. 1000 QPS needs: 50 instances. Cost: 50 × $0.34 = $17/hour = $12K/month. Latency: 50ms (meets requirement). GPU serving: Instance: P3.2xlarge (V100, $3/hour). Batching: Batch size 32, inference 20ms per batch. Throughput: 1600 QPS (32/20ms). 1000 QPS needs: 1 GPU. Cost: $3/hour = $2.2K/month. Latency: 25ms wait + 20ms inference = 45ms (meets requirement). Decision: GPU is 5× cheaper and faster! Use GPU unless: Very low QPS (<100), no batching possible, want simple deployment."
        scenarios:
          - question: "Design model serving infrastructure for Google Ads CTR prediction (1M QPS, <10ms latency). Estimate cost and architecture."
            answer: "Requirements: 1M QPS, <10ms latency, high availability. Architecture: 1) Model: DeepFM optimized with TensorRT (INT8 quantization). Inference: 5ms on CPU (batch 32). 2) Batching: Dynamic batching with 5ms timeout, batch size 32. 3) Throughput: 32 requests / 5ms = 6400 QPS per instance. 4) Instances: 1M QPS / 6400 = 157 instances (round to 200 for redundancy). 5) Hardware: c5.4xlarge (16 vCPUs, $0.68/hour). 6) Load balancer: Route by user_id (sticky routing). Cost: 200 × $0.68 = $136/hour = $100K/month. Optimizations: 1) Model caching (popular ads): 50% hit rate → 100 instances → $50K/month. 2) Regional deployment (lower latency): 5 regions × 40 instances. Monitoring: Latency, QPS, error rate per region. Autoscaling: Scale 150-250 instances based on traffic."
      time_estimate: 60

    - day: 11
      topic: "Feature Stores and Data Pipelines"
      activity: "Design feature engineering pipelines with Spark, feature stores for training-serving consistency, and data quality"
      detailed_content: |
        Feature Store Overview:
        - Problem: Feature engineering duplicated for training and serving
        - Training: Spark batch processing
        - Serving: Real-time API lookup
        - Mismatch: Training-serving skew (features computed differently)

        Solution: Feature Store
        - Centralized repository for features
        - Offline store: Historical features for training (Parquet, Delta Lake)
        - Online store: Real-time features for serving (Redis, Cassandra)
        - Consistency: Same feature logic for training and serving

        Feature Store Tools:
        - Feast: Open source, supports offline+online stores
        - Tecton: Managed, supports streaming features
        - SageMaker Feature Store: AWS managed
        - Databricks Feature Store: Integrated with Delta Lake

        Feature Engineering Pipeline:
        1. Raw data ingestion: Logs, events, database
        2. Feature computation: Spark, SQL, Python
        3. Feature validation: Check schema, distributions
        4. Feature storage: Write to offline+online stores
        5. Feature serving: Lookup at serving time

        Batch Features (Offline):
        - Computed daily/hourly on historical data
        - Example: User's avg purchase amount (last 30 days)
        - Storage: Parquet files in S3
        - Workflow: Airflow, Spark

        Real-Time Features (Online):
        - Computed on-the-fly for serving
        - Example: User's last 5 clicked items
        - Storage: Redis (key-value, <1ms lookup)
        - Update: Streaming (Kafka, Spark Streaming)

        Streaming Features:
        - Computed from event streams
        - Example: # clicks in last 1 hour (tumbling window)
        - Tools: Spark Streaming, Flink, Kafka Streams
        - Challenge: Late arrivals, out-of-order events

        Feature Monitoring:
        - Data drift: Distribution changes (e.g., avg age shifts)
        - Schema drift: Columns added/removed
        - Data quality: Null rates, outliers
        - Alerts: Trigger retraining if drift detected

        Training-Serving Consistency:
        - Problem: Training uses Spark (batch), serving uses Python (real-time)
          * Different implementations → different results
        - Solution: Feature store ensures same logic
          * Define feature once, materialize for both offline+online

        Point-in-Time Correctness:
        - Problem: Training must use features available at prediction time
          * Can't use future information (data leakage)
        - Solution: Feature store tracks timestamps
          * Retrieve features as of time T (historical)

        Data Labeling:
        - Active learning: Label data where model is uncertain
        - Weak supervision: Programmatic labeling (Snorkel)
        - Human labeling: Mechanical Turk, internal team
        - Quality control: Multiple labelers, consensus
      practice_questions:
        concepts:
          - question: "What is training-serving skew and how does a feature store prevent it?"
            answer: "Training-serving skew: Features computed differently for training vs serving → model sees different data distributions → poor performance. Example: Training: user_avg_purchase = Spark SQL (last 30 days). Serving: Python function (buggy, uses 7 days). Model trained on 30-day avg but serves with 7-day avg → skew. Feature store solution: Define feature logic once (SQL, Python), materialize to offline store (training) and online store (serving). Same logic guaranteed. Feast example: @feature_view(user_avg_purchase, aggregation='avg', window='30d'). Generates Spark code for training, Redis lookup for serving."
          - question: "Explain point-in-time correctness in feature stores. Why is it critical for model training?"
            answer: "Point-in-time correctness: Features must reflect what was known at prediction time, not future info. Example: Predict fraud on Jan 1. Can use user's purchases up to Jan 1, NOT Jan 2 (data leakage). Without: Join features as of 'now' → accidentally use future data → model overfits → fails in production. With: Feature store tracks timestamps. Request: get_features(user_id, timestamp=Jan 1) → returns features as of Jan 1. Implementation: Each feature has (entity_id, timestamp, value). At serving: Use current time. At training: Use label timestamp."
          - question: "Compare batch features vs streaming features. When to use each?"
            answer: "Batch features: Computed periodically (daily, hourly) on historical data. Example: User's avg purchase amount (last 30 days). Compute: Spark job runs nightly. Storage: Parquet in S3 (offline), Redis (online). Use when: Features change slowly, can tolerate staleness. Streaming features: Computed in real-time from event streams. Example: # clicks in last 1 hour. Compute: Kafka Streams / Spark Streaming. Storage: Redis. Use when: Features change rapidly, need fresh data (fraud detection). Trade-offs: Streaming is complex, expensive (always running). Batch is simple, cheap (periodic). Most systems use both: Batch for slow-changing (demographics), streaming for fast-changing (recent clicks)."
        tradeoffs:
          - question: "For a fraud detection system, design a feature store with both batch and streaming features. What features go where?"
            answer: "Batch features (daily Spark job): 1) User account age, 2) # transactions last 30 days, 3) Avg transaction amount, 4) Historical fraud rate. Storage: Parquet (offline), Redis (online, keyed by user_id). Streaming features (Kafka + Spark Streaming): 1) # transactions in last 1 hour, 2) # unique merchants last 1 hour, 3) Max transaction amount last 1 hour. Storage: Redis (sliding window). Feature serving: At transaction time, lookup both batch (from Redis) and streaming (from Redis) → combine → model. Trade-offs: Streaming adds latency (5-10ms Redis lookup) but critical for fraud (velocity features). Batch features cheaper, cover long-term patterns."
        scenarios:
          - question: "You have 1B historical transactions for training and 10K transactions/second for serving. Design the feature pipeline."
            answer: "Offline (Training): 1) Data: 1B transactions in Parquet (S3). 2) Features: Spark job computes user/merchant features (avg amount, frequency). 3) Output: Feature table (user_id, feature1, feature2, ..., timestamp). 4) Storage: Delta Lake (S3). 5) Training: Join features with labels (fraud/not), train XGBoost. Online (Serving): 1) Batch features: Daily Spark job writes to Redis (user_id → features). 2) Streaming features: Kafka Streams aggregates 1-hour windows → Redis. 3) Serving: a) Transaction arrives, b) Lookup user_id in Redis (batch features, 1ms), c) Lookup user_id in Redis (streaming features, 1ms), d) Model inference (10ms), e) Total: 12ms. Throughput: 10K TPS × 2ms Redis = 20K Redis QPS. Redis cluster: 10 nodes (2K QPS each)."
      time_estimate: 60

    - day: 12
      topic: "Monitoring, Drift Detection, and Model Retraining"
      activity: "Design monitoring systems for data drift, concept drift, and automated retraining pipelines"
      detailed_content: |
        ML System Monitoring:
        Unlike traditional software, ML systems degrade over time (data drift)

        Metrics to Monitor:
        1. Model performance:
           - Offline: AUC, precision, recall (on validation set)
           - Online: CTR, conversion rate, revenue
        2. Data quality:
           - Feature distributions (mean, std, quantiles)
           - Null rates, missing values
           - Schema changes
        3. System health:
           - Latency (p50, p95, p99)
           - Throughput (QPS)
           - Error rate
        4. Business metrics:
           - Revenue, user engagement, retention

        Data Drift Detection:
        - Data drift: Input distribution changes
        - Example: User demographics shift (more young users)
        - Detection:
          * Compare training vs production feature distributions
          * KL divergence, PSI (Population Stability Index)
          * Two-sample tests (Kolmogorov-Smirnov)
        - Action: Retrain on recent data

        Concept Drift:
        - Concept drift: Relationship between features and target changes
        - Example: User behavior changes (clicks different content)
        - Detection:
          * Monitor online metrics (AUC, CTR)
          * Performance drop indicates drift
        - Action: Retrain with new data

        Prediction Drift:
        - Prediction distribution changes
        - Example: Fraud model predicts fraud rate 5% → 20%
        - Detection: Compare prediction distribution over time
        - Action: Investigate cause (data drift? model issue?)

        Monitoring Tools:
        - Prometheus: Metrics collection
        - Grafana: Visualization, dashboards
        - CloudWatch, Datadog: Cloud monitoring
        - WhyLabs, Arize: ML-specific monitoring

        Alerting:
        - Define thresholds: Latency > 200ms, error rate > 1%
        - Alert channels: Email, Slack, PagerDuty
        - Severity: P0 (urgent, page on-call), P1 (investigate within hour)

        Model Retraining:
        Triggers:
        1. Scheduled: Retrain weekly/monthly
        2. Performance: Online metrics drop below threshold
        3. Data drift: KL divergence exceeds threshold
        4. Manual: New features, architecture change

        Retraining Pipeline:
        1. Data collection: Gather recent labeled data
        2. Feature engineering: Recompute features
        3. Training: Train new model
        4. Evaluation: Compare new vs old on validation set
        5. Deployment: Canary → full rollout if better
        6. Monitoring: Track new model performance

        Continuous Training:
        - Automated retraining pipeline
        - Triggered daily/weekly
        - Example: Fraud detection retrains daily on last 30 days
        - Tools: Airflow, Kubeflow Pipelines

        A/B Testing:
        - Old model (control) vs new model (treatment)
        - Random assignment (50/50 or 95/5 canary)
        - Monitor: Online metrics (CTR, revenue, latency)
        - Statistical significance: Two-proportion z-test
        - Duration: Run until significance or 2 weeks
      practice_questions:
        concepts:
          - question: "Explain data drift vs concept drift. Provide examples and detection methods for each."
            answer: "Data drift: Input distribution P(X) changes. Features shift but relationship to target stays same. Example: E-commerce user base shifts younger (age distribution changes), but young users' purchase behavior same as historical. Detection: Compare feature distributions (training vs production). KL divergence: KL(P_prod || P_train). If KL > threshold, drift detected. Concept drift: Relationship P(Y|X) changes. Features same but meaning changes. Example: User clicks change (e.g., pandemic changes content preferences). Detection: Monitor online performance. If CTR drops from 2% to 1%, concept drift. Can't detect offline (distribution of X unchanged). Action: Data drift → Retrain on recent data (new distribution). Concept drift → Retrain with new X-Y relationships."
          - question: "How do you design an alerting system for ML model degradation? What metrics and thresholds?"
            answer: "Metrics: 1) Online performance: CTR, conversion rate, revenue. 2) Prediction distribution: Avg fraud score, quantiles. 3) Data quality: Feature null rates, outlier rates. 4) System: Latency, error rate. Thresholds: 1) Performance: Alert if CTR drops 5% (relative) from baseline. 2) Prediction: Alert if fraud rate prediction > 10% (historical 1%). 3) Data: Alert if feature null rate > 5% (historical 1%). 4) System: Latency p99 > 200ms, error rate > 1%. Severity: P0 (immediate): Error rate > 10%, system down. P1 (1 hour): Performance drop 5-10%. P2 (1 day): Data quality issues. Actions: P0 → Rollback to old model. P1 → Investigate, prepare new model. P2 → Fix data pipeline."
          - question: "Explain continuous training for ML models. When is it necessary and what are the trade-offs?"
            answer: "Continuous training: Automated pipeline retrains model regularly (daily, weekly) on fresh data. When necessary: 1) Concept drift: User behavior changes rapidly (fraud, recommendations), 2) Seasonality: Patterns change (holidays, trends), 3) Fresh data: Recent data more predictive (news, finance). Pipeline: 1) Daily: Collect last 30 days labeled data, 2) Retrain: Spark job, 3) Evaluate: New model vs old on holdout, 4) Deploy: If new > old by 1%, deploy via canary. Trade-offs: Pros: Stay current, adapt to drift. Cons: 1) Cost: Daily retraining expensive (GPU hours), 2) Complexity: Automated pipeline needs monitoring, 3) Risk: New model may be worse (need safeguards). Recommendation: Start with weekly, move to daily only if drift severe."
        tradeoffs:
          - question: "For a recommendation system, how would you decide between retraining weekly vs daily? Consider drift, cost, and complexity."
            answer: "Weekly retraining: Pros: 1) Cheaper (7× less compute), 2) Simpler (fewer deployments), 3) More stable (less noise). Cons: 1) Slower adaptation to trends, 2) May miss fast-changing preferences (viral content). Daily retraining: Pros: 1) Adapts quickly to trends, 2) Better for fast-changing platforms (TikTok). Cons: 1) 7× more expensive, 2) Complex (daily deployments), 3) Risk of instability (noise in 1-day data). Decision factors: 1) Drift rate: A/B test weekly vs daily, measure CTR improvement. If daily +0.5%, worth it. 2) Platform: Slow-changing (Netflix movies): weekly. Fast-changing (TikTok): daily. 3) Cost: Startup with limited budget: weekly. FAANG scale: daily. Recommendation: Start weekly, monitor drift. If performance degrades mid-week, switch to daily."
        scenarios:
          - question: "You notice your fraud detection model's precision dropped from 90% to 70% over 2 weeks. Diagnose and fix."
            answer: "Diagnosis: 1) Check data drift: Compare feature distributions (training vs last 2 weeks). KL divergence, histogram comparison. Likely: Fraudster tactics changed (new features). 2) Check prediction drift: Fraud rate jumped 1% → 5% (model predicting more fraud). 3) Check concept drift: Precision drops → more false positives. Model not adapting. Root cause: Fraudsters using new tactics (e.g., stolen cards from new bank). Model trained on old patterns. Fix: 1) Immediate: Raise threshold (predict fraud if score > 0.9 instead of 0.5) → fewer false positives, precision recovers to 85%. Accept lower recall. 2) Short-term (1 week): Retrain on last 30 days data (includes new fraud patterns). 3) Long-term: Continuous training (retrain weekly), add features (detect new patterns). Validation: A/B test new model vs old, measure precision, recall, fraud loss."
      time_estimate: 60

  week4:
    - day: 13
      topic: "Responsible AI - Fairness, Bias, and Explainability"
      activity: "Address fairness constraints, bias detection, and interpretability for ML systems in production"
      detailed_content: |
        Fairness in ML:
        Problem: Models can discriminate against protected groups (gender, race)

        Fairness Metrics:
        1. Demographic Parity: P(ŷ=1|A=a) = P(ŷ=1|A=b)
           - Equal positive prediction rate across groups
        2. Equal Opportunity: P(ŷ=1|Y=1,A=a) = P(ŷ=1|Y=1,A=b)
           - Equal true positive rate (recall) across groups
        3. Equalized Odds: Equal TPR and FPR across groups
        4. Calibration: P(Y=1|ŷ=p,A=a) = p for all groups
           - Predicted probabilities are accurate for all groups

        Bias Sources:
        - Historical bias: Training data reflects past discrimination
        - Measurement bias: Features measured differently for groups
        - Aggregation bias: Model assumes all groups are same
        - Evaluation bias: Metrics don't capture fairness

        Bias Mitigation:
        1. Pre-processing:
           - Reweigh: Weight training samples to balance groups
           - Sampling: Oversample minority group
           - Remove biased features: Drop gender, race (but proxies remain)
        2. In-processing:
           - Fairness constraints: Add penalty for disparate impact
           - Adversarial debiasing: Train to be fair and accurate
        3. Post-processing:
           - Threshold optimization: Different thresholds per group
           - Equalized odds post-processing

        Explainability:
        Why: Regulators, users want to understand model decisions

        Model-Agnostic Methods:
        1. SHAP (Shapley Additive Explanations):
           - Game-theoretic feature attribution
           - Global: Which features most important overall?
           - Local: Why this prediction for this user?
        2. LIME (Local Interpretable Model-agnostic Explanations):
           - Approximate model locally with linear model
           - Explain individual predictions
        3. Permutation Importance:
           - Shuffle feature, measure performance drop
           - Important features cause large drop

        Model-Specific Methods:
        - Linear models: Coefficient = feature importance
        - Tree models: Gini importance, split frequency
        - Neural networks: Attention weights, saliency maps

        Interpretable Models:
        - Linear regression, logistic regression: Coefficients
        - Decision trees: Rules (max depth 5-7)
        - Rule-based models: If-then rules
        - Generalized Additive Models (GAM): Sum of univariate functions

        Trade-off: Accuracy vs Interpretability
        - Interpretable: Linear, shallow trees (80-85% accuracy)
        - Black box: Neural networks, deep trees (90-95% accuracy)
        - Decision: Regulated domain → interpretable

        Privacy-Preserving ML:
        - Differential Privacy: Add noise to gradients
          * Guarantee: No single user affects model by more than ε
        - Federated Learning: Train on devices, aggregate gradients
          * Data never leaves devices
        - Secure Multi-Party Computation: Encrypted training

        Responsible AI Checklist:
        1. Audit training data for bias
        2. Measure fairness metrics (demographic parity, equal opportunity)
        3. Mitigate bias (reweighing, constraints, thresholds)
        4. Provide explanations (SHAP, LIME)
        5. Monitor fairness in production
        6. Human review for high-stakes decisions
      practice_questions:
        concepts:
          - question: "Explain demographic parity vs equalized odds. Which is more appropriate for hiring?"
            answer: "Demographic parity: P(ŷ=1|A=a) = P(ŷ=1|A=b). Equal acceptance rate regardless of group (gender, race). Example: 10% of women hired, 10% of men hired. Equalized odds: P(ŷ=1|Y=1,A=a) = P(ŷ=1|Y=1,A=b). Equal TPR and FPR across groups. If qualified, equal chance of being hired. For hiring: Equalized odds better. Demographic parity may hire unqualified candidates to meet quota. Equalized odds: Qualified candidates treated equally, fair. But: Hard to define 'qualified' (Y=1). Labels may be biased. Best: Combine with human review."
          - question: "How does SHAP provide local explanations for model predictions? Compare to LIME."
            answer: "SHAP: Game-theoretic approach. For prediction f(x), compute contribution of each feature x_i. Shapley value: φ_i = Avg over all subsets S of features: [f(S ∪ {i}) - f(S)]. Measures marginal contribution. Properties: Additive (Σφ_i = f(x) - f(∅)), consistent. LIME: Approximate f locally with linear model g. Sample points near x, weight by proximity, fit g. Coefficients = feature importance. Compare: SHAP: Theoretically grounded, slow (exponential subsets), global + local. LIME: Fast, heuristic, local only. Use SHAP: Need rigorous explanations (healthcare, finance). Use LIME: Need fast approximations. Both: Model-agnostic, work with any black-box model."
          - question: "What is differential privacy and why is it important for ML? Explain the ε parameter."
            answer: "Differential privacy: Guarantee that model trained on dataset D vs D' (differ by one user) produces similar outputs. Formally: P(M(D) ∈ S) ≤ e^ε * P(M(D') ∈ S) for all outputs S. Meaning: Observing model output reveals little about individual user. ε controls privacy: Small ε (0.1): Strong privacy, large noise. Large ε (10): Weak privacy, small noise. Implementation: Add Gaussian noise to gradients during training. Noise ~ N(0, σ²) where σ depends on ε. Why important: Protects user data (GDPR, CCPA). Model can't memorize specific users (prevent inference attacks). Trade-off: Privacy (low ε) vs accuracy (high ε). Google uses ε=8 for federated learning (moderate privacy)."
        tradeoffs:
          - question: "For a credit scoring model, compare using logistic regression (interpretable) vs XGBoost (accurate). Consider fairness, explainability, and regulations."
            answer: "Logistic Regression: Accuracy: 75%. Interpretability: Coefficients directly show feature importance (income → +0.5, debt → -0.3). Fairness: Easy to audit (remove biased features, check coefficients). Regulations: FCRA requires explanations for rejections → LR provides clear reasons. XGBoost: Accuracy: 82% (+7%). Interpretability: Black box, need SHAP for explanations (slower, less clear). Fairness: Harder to audit, may capture bias in complex interactions. Regulations: Can provide SHAP values but less transparent. Decision: Start with LR (regulatory requirement, interpretable). If 7% accuracy is critical (reduces default losses significantly), use XGBoost + SHAP. Prepare explanations for regulators. Hybrid: Use LR for most applications, XGBoost for high-risk loans (with human review)."
        scenarios:
          - question: "You're building a hiring screening model and find it has 20% lower acceptance rate for women than men (demographic parity violated). Diagnose and fix."
            answer: "Diagnosis: 1) Check historical data: Was past hiring biased (fewer women hired)? If yes, model learned bias. 2) Check features: Do features correlate with gender (e.g., 'years experience' if women take maternity leave)? 3) Check labels: Are 'qualified' labels biased (male interviewers rated women lower)? Root cause: Training data reflects historical discrimination. Fix: 1) Pre-processing: Reweigh training data (women's positive samples weighted 2×). 2) In-processing: Add fairness constraint to loss: L = L_accuracy + λ*L_fairness where L_fairness penalizes disparate impact. 3) Post-processing: Set different thresholds (men: 0.5, women: 0.4) to equalize acceptance rates. 4) Long-term: Relabel data (unbiased review), collect more representative data. Validation: Measure demographic parity, equal opportunity. A/B test fairness-aware model. Ensure accuracy drop <5%. Human review for borderline cases."
      time_estimate: 60

    - day: 14
      topic: "ML System Design Practice - End-to-End Case Studies"
      activity: "Practice complete ML system designs: YouTube recommendations, Uber ETA prediction, Airbnb search ranking"
      detailed_content: |
        System Design Framework (MADE):
        M - Model: What ML model to use?
        A - API: How users interact with system?
        D - Data: What data to collect, features, labels?
        E - Evaluation: Offline and online metrics?

        Case Study 1: YouTube Video Recommendations

        Requirements:
        - 2B users, 500M videos
        - Personalized homepage recommendations
        - Latency < 200ms

        Design:
        1. Candidate Generation (50ms):
           - User embedding (128-dim) from watch history
           - ANN search on video embeddings → 1000 candidates
           - Sources: CF, two-tower, trending

        2. Ranking (100ms):
           - Features: User, video, context, cross
           - Model: Multi-task (predict watch time, CTR, like)
           - XGBoost or DNN, batch inference

        3. Re-ranking (50ms):
           - Diversity (MMR): Avoid similar videos
           - Freshness: Boost recent uploads
           - Business rules: Ad frequency

        Data:
        - Logs: Watch events (video, duration, like/dislike)
        - Features: User demographics, video metadata
        - Labels: Watch time, engagement

        Evaluation:
        - Offline: Precision@K, NDCG, AUC
        - Online: Watch time per session, retention

        Case Study 2: Uber ETA Prediction

        Requirements:
        - Predict arrival time for ride
        - Latency < 1s, high accuracy (within 2 min)

        Design:
        1. Model: Gradient boosting (XGBoost)
           - Why: Tabular data, need interpretability

        2. Features:
           - Route: Distance, # turns, highway %
           - Traffic: Real-time traffic speed
           - Historical: Avg time for route at time-of-day
           - Driver: Recent speeds
           - Weather: Rain, snow

        3. Training:
           - Labels: Actual arrival time
           - Data: Historical rides (1B trips)
           - Retrain: Daily on last 30 days

        4. Serving:
           - Real-time: Lookup features, model inference
           - Latency: 50ms

        Evaluation:
        - Offline: MAE, RMSE, % within 2 min
        - Online: Actual error, user complaints

        Case Study 3: Airbnb Search Ranking

        Requirements:
        - Search query → rank listings
        - Personalized, diverse results

        Design:
        1. Candidate Generation:
           - Geo filtering: Listings in location
           - Filters: Price, beds, amenities
           - ~1000 candidates

        2. Ranking:
           - Features: User-listing affinity, price, reviews
           - Model: LambdaMART (learning to rank)
           - Objective: Bookings

        3. Re-ranking:
           - Diversity: Mix price ranges, neighborhoods
           - Personalization: Boost user preferences

        Evaluation:
        - Offline: NDCG
        - Online: Booking rate, revenue

        Interview Tips:
        1. Clarify requirements: Scale, latency, metrics
        2. Start high-level: Candidate → rank → re-rank
        3. Dive deep on one area (interviewer's interest)
        4. Discuss trade-offs: Accuracy vs latency
        5. Estimate: Compute, storage, cost
        6. Address failure modes: Cold start, drift
      practice_questions:
        concepts:
          - question: "Walk through the three-stage funnel for YouTube recommendations. Why not use a single end-to-end model?"
            answer: "Three stages: 1) Candidate Generation: 500M videos → 1K candidates (ANN search, 50ms). 2) Ranking: 1K → 500 (complex model, 100ms). 3) Re-ranking: 500 → 50 (diversity, 50ms). Why not end-to-end: Latency. Complex model on 500M videos = hours. Even simple model (dot product) on 500M = seconds. Solution: Stage 1 uses fast methods (ANN), Stage 2 uses accurate methods (DNN) on small set. Trade-off: Stage 1 may miss some relevant videos (99.9% reduction), but Stage 2 re-ranks well. Empirically: Three stages achieves 90% of end-to-end quality at 200ms vs 10s latency."
          - question: "For Uber ETA prediction, why use XGBoost instead of a neural network? What are the advantages?"
            answer: "XGBoost advantages: 1) Tabular data: Features are structured (distance, traffic, weather), not images/text. XGBoost excels on tabular. 2) Interpretability: Feature importance (traffic=40%, distance=30%) helps debugging, stakeholder trust. 3) Small data (<1B samples): NNs need more data to outperform. 4) Fast training: Hours vs days for NN. 5) Fast inference: <10ms on CPU. NN disadvantages: Need GPU, hyperparameter tuning complex, black box. When to use NN: If >10M samples and accuracy critical, can try. But XGBoost baseline is strong. Empirical: XGBoost achieves 90% accuracy, NN 91% (+1%) at 10× training cost. Not worth it."
          - question: "How would you handle cold start for new listings in Airbnb search ranking?"
            answer: "Cold start: New listing has no bookings → no collaborative signal. Solutions: 1) Content-based: Use listing features (price, location, photos, description). Similar to popular listings in area. 2) Host history: If host has other listings, use their booking patterns. 3) Geographic: Use avg performance of listings in same neighborhood. 4) Exploration: Show new listings to small % of users (epsilon-greedy), collect data. 5) Hybrid: Combine content + CF. Start with content-based (first week), gradually incorporate CF as bookings accumulate. Implementation: For listings with <10 bookings, use content-based score. For >10 bookings, blend: 0.7*CF + 0.3*content. After 100 bookings, use pure CF."
        scenarios:
          - question: "Design a spam detection system for Gmail (1B users, 100M emails/day). Discuss model, features, latency, and accuracy trade-offs."
            answer: "Requirements: 1B users, 100M emails/day = 1.2K emails/sec, <100ms latency, 99% precision (minimize false positives). Design: 1) Model: Two-stage. Stage 1: TF-IDF + Logistic Regression (10ms, 95% precision). Stage 2: BERT fine-tuned (50ms, 99% precision). 2) Features: Email content, sender reputation, headers, attachments. 3) Serving: Stage 1 filters 90% (obvious spam). Stage 2 on remaining 10% (borderline cases). Total latency: 10ms + 5ms (10% × 50ms) = 15ms. 4) Scale: 1.2K emails/sec. Stage 1: CPU (c5.2xlarge, 100 QPS) → 12 instances. Stage 2: GPU (P3.2xlarge, 200 QPS on 10%) → 1 instance. Cost: $8/hour. 5) Data: Labels from user feedback (mark spam). Continuous training: Retrain weekly. 6) Evaluation: Offline: Precision, recall, F1. Online: User reports, false positive rate."
          - question: "You're building a real-time fraud detection system for Stripe (100K TPS). Walk through your design from data collection to serving."
            answer: "Requirements: 100K transactions/sec, <100ms latency, 0.1% fraud rate. Design: 1) Data Collection: Transaction events (amount, merchant, user, device) → Kafka. 2) Feature Engineering: a) Real-time: # transactions in last 1h (Kafka Streams), b) Batch: User history, merchant risk (daily Spark job). 3) Feature Store: Redis for fast lookup (<10ms). 4) Model: XGBoost (10ms inference on CPU). Features: 100 (transaction, user, velocity, graph). 5) Serving: a) Lookup features from Redis (10ms), b) XGBoost inference (10ms), c) Rules engine (5ms, flag high-risk). Total: 25ms. 6) Scale: 100K TPS → need 2500 QPS per instance → 40 instances (c5.2xlarge). Cost: $16/hour = $11K/month. 7) Labels: Feedback from investigations, chargebacks (delayed). 8) Retraining: Daily on last 30 days. 9) Monitoring: Fraud loss, false positive rate, latency."
      time_estimate: 90
