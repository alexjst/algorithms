track: ml_system_design
description: 14-day ML system design curriculum covering production ML infrastructure
  for Staff/Principal interviews
weeks:
  week1:
  - day: 1
    topic: Recommendation Systems - Architecture and Candidate Generation
    activity: Master the three-stage funnel (retrieval → ranking → re-ranking) and
      candidate generation methods
    detailed_content: "Industry Standard: Three-Stage Funnel\n- Stage 1 - Candidate\
      \ Generation (Retrieval):\n  * Goal: 100M items → 10K candidates (99.99% reduction)\n\
      \  * Latency budget: 10-50ms\n  * Methods: Multiple retrieval sources in parallel\n\
      \n- Stage 2 - Ranking:\n  * Goal: Score 10K candidates → Top 500 items\n  *\
      \ Latency budget: 50-100ms\n  * Model: Complex ML (neural network, gradient\
      \ boosting)\n\n- Stage 3 - Re-ranking:\n  * Goal: Final ordering with diversity,\
      \ business rules\n  * Latency budget: 10-20ms\n  * Methods: Rule-based, lightweight\
      \ model, MMR\n\nCandidate Generation Methods:\n1. Collaborative Filtering (CF):\n\
      \   - User-Based CF: Find similar users, recommend their items\n   - Item-Based\
      \ CF: Find similar items to user's history (more stable)\n   - Why Item-Based\
      \ better: Fewer items than users, precompute daily, better cold start\n\n2.\
      \ Matrix Factorization (ALS):\n   - Decompose R ≈ U × V^T where U=user embeddings,\
      \ V=item embeddings\n   - Training: Alternating Least Squares (fix U optimize\
      \ V, fix V optimize U)\n   - Serving: ANN search on item embeddings\n\n3. Two-Tower\
      \ Neural Network:\n   - User Tower: user features → user embedding (128-dim)\n\
      \   - Item Tower: item features → item embedding (128-dim)\n   - Score: dot\
      \ product of user and item embeddings\n   - Training: Sampled softmax with negative\
      \ sampling\n\n4. Sequential Models (GRU4Rec, SASRec):\n   - Capture temporal\
      \ patterns in user behavior\n   - GRU4Rec: RNN for session-based recommendations\n\
      \   - SASRec: Self-attention for sequences\n\n5. Graph-Based (Node2Vec, Item2Vec):\n\
      \   - Build co-occurrence graph\n   - Random walks to learn embeddings\n   -\
      \ Find similar items via embedding similarity\n\nBlending Multiple Sources:\n\
      - Combine: CF + Two-tower + Sequential + Trending + Content-based\n- Each source\
      \ contributes 100-300 candidates\n- Total: ~1000 candidates for ranking stage\n"
    practice_questions:
      concepts:
      - question: Why is the recommendation funnel split into three stages (retrieval,
          ranking, re-ranking) instead of one end-to-end model?
        answer: 'Performance and scale. Retrieval: Must process 100M items in <50ms,
          needs simple/fast methods (dot product, ANN search). Can''t afford complex
          model. Ranking: Only 10K candidates, can afford 50-100ms for complex model
          (deep neural network). Re-ranking: Final 500 items, can apply slow business
          rules and diversity constraints. Trade-off: End-to-end would be too slow
          (complex model on 100M items = minutes). Staged approach: Simple fast model
          → complex accurate model → business logic. Total latency: 150ms.'
      - question: Explain negative sampling in two-tower models. Why is it necessary
          and how do you choose negatives?
        answer: 'Problem: Softmax over 100M items is infeasible. Negative sampling:
          For positive pair (user, item), sample K negative items. Loss: maximize
          log σ(u·v_pos) + Σ log σ(-u·v_neg). Effectively approximates full softmax.
          Strategies: 1) Random: uniform from catalog, 2) Popularity-based: P(item)^0.75
          (word2vec trick), 3) Hard negatives: items shown but not clicked, 4) Batch
          negatives: use other positives in batch. K typically 100-1000. Hard negatives
          most effective but more complex.'
      - question: Compare Item-Based CF vs Matrix Factorization for candidate generation.
          When to use each?
        answer: 'Item-Based CF: Pros: Interpretable (users who liked X also liked
          Y), explainable, no training needed. Cons: Doesn''t capture latent factors,
          sparse data issues. Matrix Factorization: Pros: Learns latent factors (genres,
          themes), dense embeddings work with ANN, better accuracy. Cons: Black box,
          needs retraining. Use Item-CF: Need explainability, small catalog (<10K
          items), sparse interactions. Use MF: Large catalog (>100K), have compute
          for training/ANN, accuracy critical. Modern systems use both: Item-CF for
          explainability + MF for accuracy.'
      tradeoffs:
      - question: For YouTube-scale recommendations (2B users, 500M videos), how would
          you design the candidate generation stage? What trade-offs would you make?
        answer: 'Scale challenges: Can''t store all user-video pairs, can''t compute
          scores for 500M videos in real-time. Design: 1) Multiple retrieval sources
          (5-7 methods), 2) User embeddings: 128-dim, 2B users = 1TB (shard by user_id),
          3) Item embeddings: 128-dim, 500M videos = 256GB (fit in memory on single
          machine), 4) ANN index: FAISS with IVF+PQ, query time <10ms for top-1K,
          5) Blend sources: 300 candidates each, total 1500 for ranking. Trade-offs:
          Embedding dim (128 vs 256): smaller=faster but less expressive. Num sources
          (3 vs 7): more=better coverage but higher latency. ANN recall (95% vs 99%):
          higher=better but slower. Choose: 128-dim (50ms ANN query), 5 sources (total
          50ms parallel), 95% recall (acceptable for retrieval stage).'
      estimation:
      - question: Calculate memory for storing 1B user embeddings (128-dim) and serving
          QPS if ANN search takes 10ms per query.
        answer: 'Memory: 1B users × 128 dimensions × 4 bytes/float = 512GB. Too large
          for single machine. Sharding: Shard by user_id mod N. For N=16 shards: 32GB
          per machine (fits in memory). Item embeddings: 100M items × 128-dim = 51GB
          (fits on single machine for ANN index). QPS per machine: 1 query = 10ms
          → 100 QPS per machine. For 100K total QPS: need 1000 machines. Cost: c5.4xlarge
          (30GB RAM, $0.70/hour) × 1000 = $700/hour = $500K/month. Optimization: Use
          PQ compression (32 bytes → 8 bytes), 4× memory savings → 250 machines →
          $125K/month.'
    time_estimate: 90
    video_resources:
    - title: 'ByteByteGo: How Netflix Recommends'
      url: https://www.youtube.com/@ByteByteGo
      duration: 12 min
      description: Recommendation system architecture overview
      priority: high
    - title: 'Chip Huyen: RecSys at Scale'
      url: https://www.youtube.com/@chiphuyen
      duration: 20 min
      description: Candidate generation strategies
      priority: high
  - day: 2
    topic: Recommendation Systems - Ranking and Re-ranking
    activity: Master ranking architectures (DeepFM, Wide&Deep), multi-objective optimization,
      and diversity techniques
    detailed_content: "Ranking Stage:\nGoal: Precisely score 10K candidates using\
      \ rich features and complex models\n\nModern Ranking Architectures:\n1. Wide\
      \ & Deep (Google Play):\n   - Wide: Linear model on cross features (manual engineering)\n\
      \   - Deep: DNN on embeddings (learned features)\n   - Combine: Concatenate\
      \ wide and deep outputs → final score\n   - Benefits: Memorization (wide) +\
      \ generalization (deep)\n\n2. DeepFM (Huawei):\n   - FM component: Second-order\
      \ feature interactions\n     * y_FM = w_0 + Σw_i*x_i + ΣΣ<v_i,v_j>*x_i*x_j\n\
      \   - DNN component: Higher-order interactions\n   - Shared embeddings between\
      \ FM and DNN\n   - No manual feature engineering needed\n\n3. Multi-Task Learning\
      \ (YouTube, Facebook):\n   - Shared bottom layers\n   - Task-specific towers\
      \ for each objective:\n     * Click probability (engagement)\n     * Watch time\
      \ (quality)\n     * Like/share probability\n     * Conversion (subscribe, purchase)\n\
      \   - Loss: Weighted sum of task losses\n   - Serving: Combine predictions into\
      \ final score\n\nFeature Engineering:\n- User features: Demographics, history,\
      \ session context\n- Item features: Content, popularity, freshness\n- Cross\
      \ features: User-item interactions, category affinity\n- Context features: Time,\
      \ device, location\n\nMulti-Objective Optimization:\nProblem: Conflicting objectives\
      \ (engagement vs revenue vs diversity)\n\nApproaches:\n1. Weighted Sum: L =\
      \ w1*L_ctr + w2*L_revenue + w3*L_diversity\n   - Simple but hard to tune weights\n\
      \n2. Pareto Frontier:\n   - Explore trade-offs by varying weights\n   - Visualize\
      \ CTR vs revenue curve\n   - Choose operating point based on business priorities\n\
      \n3. Constraint-Based:\n   - Maximize engagement subject to revenue ≥ threshold\n\
      \   - Ensures minimum business metrics met\n\n4. Multi-Task Learning:\n   -\
      \ Train single model to predict all objectives\n   - Adjust weights at serving\
      \ time without retraining\n\nRe-ranking Stage:\nGoals: Diversity, freshness,\
      \ business rules\n\nTechniques:\n1. Maximal Marginal Relevance (MMR):\n   -\
      \ Select items that are relevant AND diverse\n   - MMR = λ*Relevance(item) -\
      \ (1-λ)*MaxSimilarity(item, selected)\n   - Greedy algorithm: iteratively select\
      \ best MMR item\n\n2. Determinantal Point Process (DPP):\n   - Probabilistic\
      \ model for diverse subset selection\n   - Kernel: L = Quality × Diversity\n\
      \   - Sample diverse subset via DPP\n\n3. Exploration (Bandits):\n   - Epsilon-greedy:\
      \ Random items with probability ε\n   - Thompson Sampling: Sample from Beta\
      \ distribution\n   - Balance exploration vs exploitation\n"
    practice_questions:
      concepts:
      - question: Explain how DeepFM combines FM and DNN. Why is this better than
          just using a DNN?
        answer: 'FM component: Learns pairwise feature interactions explicitly via
          v_i·v_j. Captures second-order relationships efficiently (O(k*n) where k=embedding
          dim). DNN component: Learns complex high-order interactions implicitly via
          layers. Both share same embeddings. Why better than DNN alone: FM provides
          explicit second-order interactions (strong inductive bias), DNN captures
          high-order. Result: Better accuracy with less data. Empirically: DeepFM
          outperforms DNN by 1-2% on CTR prediction tasks. Use case: CTR prediction
          where feature interactions critical (ad targeting, recommendation).'
      - question: How does multi-task learning help recommendation systems? Explain
          with YouTube's ranking model.
        answer: 'YouTube optimizes: 1) Click probability (engagement), 2) Watch time
          (quality), 3) Like/share (satisfaction). Single-task: Train 3 separate models,
          combine at serving. Multi-task: Shared bottom layers learn general representations,
          task-specific towers for each objective. Benefits: 1) Shared representations
          improve all tasks (transfer learning), 2) Single model deployment (simpler),
          3) Can adjust weights at serving time. Architecture: User/item features
          → shared layers (256→128 dims) → 3 towers (64→1 each). Final score: 0.4*p_click
          + 0.4*watch_time/300 + 0.2*p_like. Adjust weights for business priorities.'
      - question: Explain MMR for diversity-aware re-ranking. When would you prefer
          MMR over just selecting top-K by score?
        answer: 'Top-K by score: May return very similar items (all sci-fi movies).
          Poor user experience, filter bubble. MMR: Balances relevance and diversity.
          Formula: MMR = λ*Relevance(item) - (1-λ)*MaxSim(item, selected). Greedy:
          Select item with max MMR, repeat. λ=1: Pure relevance (top-K). λ=0.5: Balance.
          λ=0: Pure diversity. Example: Netflix homepage - want mix of genres, not
          10 similar movies. When to use: User-facing lists (homepage, search), need
          variety. When not: Precision-critical (search query ''iPhone 13'') where
          user wants specific thing.'
      tradeoffs:
      - question: For ad CTR prediction, compare DeepFM vs simple logistic regression.
          Consider data size, feature engineering, and interpretability.
        answer: 'Logistic Regression: Pros: 1) Interpretable coefficients, 2) Fast
          training/inference, 3) Works with small data (<100K). Cons: 1) Needs manual
          feature engineering (cross features), 2) Linear, can''t learn interactions.
          DeepFM: Pros: 1) Learns interactions automatically (FM + DNN), 2) Better
          accuracy (+2-5% CTR), 3) No manual feature engineering. Cons: 1) Black box,
          2) Needs more data (>1M), 3) Slower (10× inference time). Decision: <100K
          samples: Logistic Regression. >1M samples + accuracy critical: DeepFM. Startup:
          LR (quick iteration). FAANG: DeepFM (have data/compute, accuracy worth it).'
      scenarios:
      - question: You're building YouTube homepage recommendations. How would you
          balance watch time (user satisfaction) vs ad revenue (business objective)?
        answer: 'Conflict: High watch time (long videos, engaging content) vs high
          ad revenue (more ads, shorter videos with ads). Multi-objective approach:
          1) Train multi-task model predicting: a) Watch time (regression), b) Ad
          impressions (count), c) Revenue (regression). 2) Compute Pareto frontier:
          Test weights (watch time, revenue) = [(1.0, 0.0), (0.7, 0.3), (0.5, 0.5),
          (0.3, 0.7), (0.0, 1.0)]. 3) A/B test: Run 5 variants for 2 weeks, measure:
          watch time, revenue, retention (guardrail). 4) Results: (0.7, 0.3) gives
          90% of max watch time, 70% of max revenue, best retention. 5) Business decision:
          Choose (0.7, 0.3) - prioritize user satisfaction while maintaining revenue.
          6) Monitor: Track metrics over time, adjust if needed.'
    time_estimate: 90
    video_resources:
    - title: 'ByteByteGo: Design YouTube Recommendations'
      url: https://www.youtube.com/@ByteByteGo
      duration: 15 min
      description: Ranking and re-ranking pipelines
      priority: high
    - title: 'Chip Huyen: Multi-Stage Recommendation'
      url: https://www.youtube.com/@chiphuyen
      duration: 18 min
      description: From millions to dozens of recommendations
      priority: medium
  - day: 3
    topic: Search and Ranking Systems
    activity: Understand query understanding, candidate generation (inverted index),
      ranking (learning to rank), and evaluation (NDCG)
    detailed_content: "Search vs Recommendations:\n- Search: User expresses intent\
      \ via query → match documents\n- Recommendations: Predict preferences without\
      \ query → discovery\n- Search: Query-dependent features dominate\n- Recommendations:\
      \ User history & collaborative signals dominate\n\nQuery Understanding:\n- Query\
      \ rewriting: Spelling correction, expansion\n- Intent classification: Navigational,\
      \ informational, transactional\n- Entity extraction: Recognize products, brands,\
      \ locations\n- Query embeddings: BERT for semantic similarity\n\nCandidate Generation\
      \ (Retrieval):\n1. Inverted Index:\n   - Data structure: term → list of documents\
      \ containing term\n   - Boolean retrieval: Fast, scalable\n   - Example: Query\
      \ \"machine learning\" → docs with both terms\n\n2. BM25 Ranking:\n   - Probabilistic\
      \ ranking function\n   - Score(D,Q) = Σ IDF(q_i) * (f(q_i,D) * (k+1)) / (f(q_i,D)\
      \ + k*(1-b+b*|D|/avgdl))\n   - Accounts for term frequency, document length,\
      \ inverse document frequency\n\n3. Dense Retrieval (Neural):\n   - Encode query\
      \ and documents to embeddings\n   - BERT-based: [CLS] token as embedding\n \
      \  - Similarity: Dot product or cosine\n   - ANN search: FAISS, ScaNN for fast\
      \ retrieval\n\n4. Hybrid: Combine BM25 + Neural\n   - BM25: Good for exact keyword\
      \ match\n   - Neural: Good for semantic similarity\n   - Fusion: Weighted combination\
      \ or reranking\n\nRanking (Learning to Rank):\n1. Pointwise: Predict relevance\
      \ score for each doc\n   - Loss: Regression (MSE) or classification (cross-entropy)\n\
      \   - Example: Predict ratings 1-5\n\n2. Pairwise: Learn to rank pairs\n   -\
      \ Given query, is doc A > doc B?\n   - Loss: Hinge loss, logistic loss\n   -\
      \ Example: RankNet, LambdaRank\n\n3. Listwise: Directly optimize ranking metric\n\
      \   - Loss: Approximation of NDCG or MAP\n   - Example: LambdaMART (gradient\
      \ boosting for ranking)\n   - Most effective but complex\n\nFeatures for Ranking:\n\
      - Query-document: BM25 score, BERT similarity\n- Document quality: PageRank,\
      \ click-through rate\n- User context: Location, device, history\n- Cross features:\
      \ Query-document interactions\n\nEvaluation Metrics:\n- NDCG@K (Normalized Discounted\
      \ Cumulative Gain):\n  * DCG@K = Σ (2^rel_i - 1) / log2(i+1)\n  * NDCG = DCG\
      \ / IDCG (ideal DCG)\n  * Accounts for position and relevance grades\n- MAP\
      \ (Mean Average Precision):\n  * Precision at each relevant position, averaged\n\
      - MRR (Mean Reciprocal Rank):\n  * 1 / rank of first relevant document\n\nProduction\
      \ Considerations:\n- Caching: Cache popular queries\n- Personalization: User\
      \ history, location\n- Freshness: Boost recent documents\n- Diversity: Avoid\
      \ redundant results\n"
    practice_questions:
      concepts:
      - question: Explain BM25 ranking function. Why does it normalize by document
          length?
        answer: 'BM25: Score = Σ IDF(term) * TF_normalized. TF normalization: TF /
          (TF + k*(1-b+b*|D|/avgdl)) where |D|=doc length, avgdl=average doc length,
          b=0.75 (length normalization strength). Why normalize: Long documents naturally
          contain terms more times, would dominate short docs. Example: ''machine''
          appears 10 times in 1000-word doc vs 2 times in 100-word doc. Without normalization:
          long doc scores higher. With normalization: Penalize long docs, treat proportionally.
          b=0: No normalization (term frequency only). b=1: Full normalization. Typical
          b=0.75.'
      - question: Compare sparse retrieval (BM25) vs dense retrieval (BERT embeddings).
          When to use each?
        answer: 'BM25 (Sparse): Pros: 1) Exact keyword match (good for names, IDs),
          2) Fast (inverted index), 3) Interpretable (term weights). Cons: 1) No semantic
          understanding (''auto'' ≠ ''car''), 2) Vocabulary mismatch problem. BERT
          (Dense): Pros: 1) Semantic similarity (''auto'' ≈ ''car''), 2) Handles synonyms,
          paraphrasing. Cons: 1) Slower (ANN search), 2) May miss exact matches, 3)
          Requires GPU for encoding. Best practice: Hybrid. Use BM25 for first-stage
          retrieval (fast, covers lexical), then BERT for re-ranking (semantic). Example:
          Google Search uses both - inverted index + neural ranking.'
      - question: Explain NDCG@K. Why is it better than accuracy for ranking evaluation?
        answer: 'NDCG@K: Accounts for 1) Relevance grades (not just binary), 2) Position
          (top results matter more). Formula: DCG = Σ (2^rel_i - 1)/log2(i+1). Denominator:
          Discounts lower positions. Numerator: Exponential gain for relevance. Normalize
          by ideal DCG (perfect ranking). Why better than accuracy: 1) Position matters:
          [rel, nonrel, nonrel] better than [nonrel, rel, nonrel] even though both
          have 1 relevant. 2) Graded relevance: Document with rel=3 better than rel=1.
          Accuracy treats equal. 3) Top-K focus: Only cares about top results, appropriate
          for search. Example: NDCG@10=0.8 means 80% as good as perfect ranking.'
      tradeoffs:
      - question: For e-commerce product search, design a retrieval+ranking pipeline.
          Balance latency, accuracy, and personalization.
        answer: 'Constraints: <200ms latency, high relevance, personalized. Pipeline:
          1) Candidate Generation (50ms): a) Query understanding: BERT for intent
          (buy vs browse), b) BM25 retrieval: 10K products from 10M catalog, c) User
          history: Past clicks/purchases → 1K products, d) Total: 11K candidates.
          2) Ranking (100ms): a) Features: BM25, BERT similarity, price, user affinity,
          CTR, b) Model: LightGBM (fast), trained on click/purchase labels, c) Score
          all 11K, select top 500. 3) Re-ranking (50ms): a) Diversity: MMR to avoid
          duplicate brands, b) Business rules: Boost in-stock, margin, c) Final: Top
          100 products. Total: 200ms. Latency breakdown: Retrieval 50ms (parallel
          BM25 + user), Ranking 100ms (batch inference), Re-rank 50ms.'
      estimation:
      - question: Estimate compute and memory for BERT-based dense retrieval on 1M
          product catalog. Query latency target <100ms.
        answer: 'Offline (build index): Encode 1M products with BERT (768-dim). BERT
          forward: 10ms per product on GPU. Total: 1M * 10ms = 10M ms = 2.8 GPU-hours.
          Daily refresh: <3 hours on single GPU (acceptable). Memory: 1M × 768 × 4
          bytes = 3GB for embeddings. ANN index (FAISS): ~5GB with IVF+PQ. Total:
          8GB (fits in memory). Online (query): Encode query with BERT: 10ms. ANN
          search (top-1000): 20ms on CPU. Total: 30ms (meets <100ms target). Serving:
          c5.4xlarge ($0.70/hour) = $500/month. Conclusion: Feasible for 1M catalog,
          scales to 10M with more memory and index sharding.'
    time_estimate: 90
    video_resources:
    - title: 'ByteByteGo: Design Google Search'
      url: https://www.youtube.com/@ByteByteGo
      duration: 18 min
      description: Search and ranking system architecture
      priority: high
    - title: 'Chip Huyen: Learning to Rank'
      url: https://www.youtube.com/@chiphuyen
      duration: 15 min
      description: Ranking algorithms for search
      priority: medium
  - day: 4
    topic: Ad Click Prediction (CTR) - Features, Models, and Auction
    activity: Master CTR prediction with factorization machines, feature engineering,
      calibration, and ad auction mechanics
    detailed_content: "Problem Definition:\n- Goal: Predict P(click | user, ad, context)\n\
      - Data: Billions of impressions, click rate ~0.1-1% (highly imbalanced)\n- Latency:\
      \ <10ms (user is waiting for page load)\n\nFeature Engineering:\n- User features:\
      \ Demographics, click history, interests\n- Ad features: Creative, landing page,\
      \ advertiser\n- Context features: Time, device, page content\n- Cross features:\
      \ User-ad interactions\n  * User clicked similar ads? User interest × ad category?\n\
      \  * Manual engineering expensive → learn interactions\n\nFactorization Machines\
      \ (FM):\n- Linear: y = w_0 + Σw_i*x_i (no interactions)\n- FM: y = w_0 + Σw_i*x_i\
      \ + ΣΣ<v_i,v_j>*x_i*x_j\n  * <v_i,v_j> = v_i · v_j (dot product of embeddings)\n\
      \  * Learns feature interactions with O(kn) parameters (k=embedding dim)\n-\
      \ Why FM: Captures interactions without manually crossing features\n- Efficient:\
      \ Reformulate to avoid O(n²) computation\n\nDeepFM for CTR:\n- Combines FM (low-order\
      \ interactions) + DNN (high-order)\n- Shared embeddings between FM and DNN\n\
      - State-of-art for CTR prediction\n- Used by: Huawei, Criteo\n\nTraining Challenges:\n\
      - Class imbalance: 99% no-click, 1% click\n  * Solution: Downsample negatives,\
      \ adjust weights\n- Negative sampling: Can't use all negatives\n  * Sample negatives\
      \ proportional to frequency\n- Streaming data: Online learning, incremental\
      \ updates\n  * FTRL (Follow-The-Regularized-Leader) optimizer\n\nCalibration:\n\
      - Problem: Model outputs not well-calibrated probabilities\n- Test: Plot predicted\
      \ vs actual click rate in bins\n- Calibration methods:\n  * Platt scaling: Train\
      \ logistic regression on outputs\n  * Isotonic regression: Learn monotonic mapping\n\
      \  * Temperature scaling: Divide logits by T\n- Important for auction (expected\
      \ value = bid × p_click)\n\nAd Auction (Second-Price):\n- Bids: Advertiser bids\
      \ $b per click\n- CTR prediction: Model predicts p(click)\n- Expected value:\
      \ EV = b × p(click)\n- Rank by EV (not just bid)\n- Charge: Second-highest EV\
      \ / winner's p(click)\n- Why: Incentivizes truthful bidding (game theory)\n\n\
      Evaluation:\n- Offline: AUC, log loss, calibration curve\n- Online: CTR, revenue\
      \ per impression, advertiser ROI\n- A/B testing: Randomly assign users to control/treatment\n"
    practice_questions:
      concepts:
      - question: Derive the efficient computation for Factorization Machines. Show
          how to avoid O(n²) complexity.
        answer: 'Naive: ΣΣ<v_i,v_j>x_i*x_j requires O(n²) pairwise interactions. Efficient
          reformulation: ΣΣ<v_i,v_j>x_i*x_j = ΣΣ(Σv_i,f*v_j,f)x_i*x_j = Σ_f[(Σv_i,f*x_i)²
          - Σ(v_i,f*x_i)²] / 2. Key insight: (Σa_i)² - Σa_i² = 2*Σ_i<j a_i*a_j. Computation:
          For each factor f, compute sum = Σv_i,f*x_i (O(n)), then square (O(1)).
          Total: O(kn) where k=embedding dim. Typically k=10-100, n=millions → huge
          speedup. This makes FM practical for real-time serving.'
      - question: Why is calibration critical for ad CTR prediction? How does it affect
          auction outcomes?
        answer: 'Auction uses expected value: EV = bid × p(click). If model outputs
          are not calibrated (e.g., predicts 0.5 when true is 0.1), EV is wrong →
          incorrect ranking → revenue loss. Example: Ad A (bid=$1, true CTR=0.1, predicted=0.5)
          vs Ad B (bid=$0.8, true CTR=0.12, predicted=0.12). Uncalibrated: EV_A=0.5,
          EV_B=0.096 → show A (wrong, actual value=0.1). Calibrated: EV_A=0.1, EV_B=0.096
          → show A (correct). Calibration ensures predicted probabilities match actual
          frequencies → correct ranking → maximize revenue.'
      - question: Explain how negative sampling addresses class imbalance in CTR prediction.
          What are the trade-offs?
        answer: 'Problem: 1B impressions, 10M clicks (1% CTR). Training on all negatives:
          1) Computationally expensive, 2) Dominates gradients (99% weight on negatives).
          Negative sampling: Keep all positives (10M), sample negatives (e.g., 10%
          = 100M). Now 1:10 ratio instead of 1:99. During training: Adjust sample
          weights (negatives weighted 10×). Benefits: 10× faster training, balanced
          gradients. Drawbacks: Loses information from rare negatives, need to adjust
          for sampling in calibration. Trade-off: Sample more negatives (better accuracy,
          slower) vs fewer (faster, less accurate). Typical: 1:5 to 1:20 ratio.'
      tradeoffs:
      - question: Compare using logistic regression vs DeepFM for ad CTR prediction.
          Consider training time, interpretability, and accuracy.
        answer: 'Logistic Regression: Pros: 1) Fast training (1 hour on 1B samples),
          2) Fast serving (<1ms), 3) Interpretable weights. Cons: 1) Need manual feature
          crosses, 2) Linear (limited). DeepFM: Pros: 1) Learns interactions automatically
          (FM + DNN), 2) +5-10% AUC improvement, 3) State-of-art. Cons: 1) Slower
          training (10× longer), 2) Slower serving (5-10ms), 3) Black box. Decision:
          Startup/small scale: Logistic regression (simpler, faster iteration). Large
          scale (Google/Facebook): DeepFM (accuracy worth complexity, have infrastructure).
          Hybrid: LR for baseline, DeepFM for accuracy-critical campaigns.'
      scenarios:
      - question: Design a real-time ad CTR prediction system for Google Ads scale
          (1M QPS). Discuss architecture, features, and latency.
        answer: 'Scale: 1M QPS, <10ms latency, 1B impressions/day. Architecture: 1)
          Feature extraction (2ms): a) User features: Lookup from Redis (demographics,
          history), b) Ad features: Lookup from cache (creative, landing page), c)
          Context: Parse from request (time, device, page). 2) Model serving (5ms):
          a) Model: DeepFM (optimized with TensorRT), b) Deployment: 1000× model servers
          (c5.2xlarge), c) Load balancing: Route by user_id, d) Batching: Micro-batches
          of 32 (5ms latency limit). 3) Calibration (1ms): Platt scaling on model
          output. 4) Total: 8ms. Throughput: 1000 QPS per server → need 1000 servers.
          Cost: $0.35/hour × 1000 = $350/hour = $250K/month. Optimizations: Model
          quantization (INT8), feature caching (Redis), ANN for similar ads.'
    time_estimate: 75
    video_resources:
    - title: 'ByteByteGo: Ad Serving System Design'
      url: https://www.youtube.com/@ByteByteGo
      duration: 16 min
      description: CTR prediction and ad auction mechanics
      priority: high
    - title: 'Chip Huyen: Real-Time ML at Scale'
      url: https://www.youtube.com/@chiphuyen
      duration: 22 min
      description: Low-latency prediction systems
      priority: high
  - day: 5
    topic: Computer Vision Systems - Image Classification and Object Detection
    activity: Design production CV systems including transfer learning, model optimization,
      and edge deployment
    detailed_content: "Image Classification System:\nExample: Content moderation (detect\
      \ inappropriate images)\n\nModel Selection:\n- Transfer learning from ImageNet:\n\
      \  * ResNet-50: 25M params, 4B FLOPs, good accuracy\n  * EfficientNet-B0: 5M\
      \ params, 0.4B FLOPs, best efficiency\n  * Vision Transformer: State-of-art\
      \ but slower\n- Fine-tuning strategy:\n  * <1K labels: Feature extraction only\
      \ (freeze backbone)\n  * 1K-10K: Fine-tune last few layers\n  * >10K: Full fine-tuning\
      \ with low LR\n\nData Pipeline:\n- Data augmentation: Random crop, flip, color\
      \ jitter\n- Class imbalance: Oversample minority, weighted loss\n- Active learning:\
      \ Label hard examples (low confidence)\n\nTraining Infrastructure:\n- Distributed\
      \ training: Data parallelism across GPUs\n- Mixed precision (FP16): 2× faster,\
      \ same accuracy\n- Gradient accumulation: Simulate large batch size\n\nModel\
      \ Optimization for Production:\n- Quantization: FP32 → INT8 (4× smaller, 4×\
      \ faster)\n- Pruning: Remove 50-90% of weights\n- Distillation: Train small\
      \ student from large teacher\n- ONNX: Framework-agnostic deployment\n\nServing:\n\
      - Batch inference: Group requests for efficiency\n- GPU serving: TensorRT, TensorFlow\
      \ Serving\n- Caching: Cache predictions for popular images\n- Latency: <100ms\
      \ for API, <50ms for real-time\n\nObject Detection:\nExample: Self-driving cars,\
      \ surveillance\n\nTwo-Stage Detectors:\n- Faster R-CNN: Region proposals → classify\
      \ + localize\n- High accuracy but slow (100-500ms per image)\n\nOne-Stage Detectors:\n\
      - YOLO, SSD: Single pass, predict class + bbox directly\n- Faster (10-50ms)\
      \ but slightly lower accuracy\n\nTrade-offs:\n- Accuracy vs Speed: Faster R-CNN\
      \ vs YOLO\n- Resolution: Higher = better accuracy but slower\n- Backbone: ResNet\
      \ vs EfficientNet vs MobileNet\n\nEdge Deployment:\n- Model compression: Quantization\
      \ + pruning + distillation\n- Target: <50MB model, <100ms inference on mobile\
      \ CPU\n- Frameworks: TensorFlow Lite, Core ML, ONNX Runtime\n- Hardware acceleration:\
      \ Neural Engine (iPhone), EdgeTPU\n"
    practice_questions:
      concepts:
      - question: Explain transfer learning for image classification. Why does it
          work and when would you use it?
        answer: 'Transfer learning: Pretrain on large dataset (ImageNet: 1M images,
          1000 classes), fine-tune on target task. Why it works: Low-level features
          (edges, textures) are general, transfer across domains. High-level features
          (object parts) partially transfer. Fine-tuning adapts to new domain. When
          to use: 1) Limited labeled data (<10K samples) - training from scratch would
          overfit. 2) Related domain - ImageNet photos → medical images (some transfer).
          3) Need quick baseline - pretrained models widely available. Not when: 1)
          Very different domain (photos → X-rays: limited transfer), 2) Massive data
          (>1M samples: can train from scratch).'
      - question: How does model quantization (FP32 → INT8) work? What's the accuracy
          vs speed trade-off?
        answer: 'Quantization: Map FP32 weights/activations to INT8 (256 discrete
          values). Process: 1) Determine range: max_val, min_val from calibration
          data. 2) Scale: scale = (max_val - min_val) / 255. 3) Quantize: w_int8 =
          round((w_fp32 - min_val) / scale). 4) Dequantize: w_fp32 ≈ w_int8 * scale
          + min_val. Benefits: 4× memory reduction (4 bytes → 1 byte), 4× faster inference
          (INT8 ops faster). Accuracy trade-off: Post-training quantization: 1-2%
          accuracy drop. Quantization-aware training: <0.5% drop. Why acceptable:
          Neural networks overparameterized, robust to small perturbations.'
      - question: Compare Faster R-CNN vs YOLO for object detection. When would you
          choose each?
        answer: 'Faster R-CNN (Two-stage): 1) Region Proposal Network generates ~2K
          proposals, 2) Classify + refine each. Accuracy: Higher (mAP ~40-45% on COCO).
          Speed: Slow (100-500ms per image). YOLO (One-stage): Single CNN predicts
          class + bbox for all grid cells. Accuracy: Slightly lower (mAP ~35-40%).
          Speed: Fast (10-50ms). Use Faster R-CNN: Accuracy critical, offline processing
          (e.g., medical imaging). Use YOLO: Real-time required (self-driving, video
          surveillance), mobile/edge deployment. Trend: One-stage detectors closing
          accuracy gap (YOLOv8, EfficientDet).'
      tradeoffs:
      - question: For Instagram content moderation (detect inappropriate images),
          design a CV system. Balance accuracy, latency, and scale (1M images/day).
        answer: 'Requirements: High recall (catch all violations), <500ms latency
          (not user-blocking), 1M images/day. Design: 1) Model: EfficientNet-B3 (fine-tuned
          on NSFW dataset). Accuracy: 95% recall, 90% precision. Latency: 50ms on
          GPU. 2) Serving: a) 10 GPU servers (P3.2xlarge, $3/hour), b) Batch size
          32 (optimize throughput), c) Load balancer distributes requests. 3) Throughput:
          1M images/day = 12 images/sec. 1 GPU handles 640 images/sec (batch 32, 50ms).
          Need < 1 GPU, but use 10 for redundancy. 4) Post-processing: Low-confidence
          images (0.4-0.6) sent to human review queue. 5) Cost: 10 × $3/hour × 24
          hours = $720/day. Optimizations: Model quantization (INT8) → 25ms latency,
          5 GPUs → $360/day.'
      estimation:
      - question: Estimate training time and cost for fine-tuning ResNet-50 on 100K
          images for 10 epochs on AWS.
        answer: 'ResNet-50: 25M params, 4B FLOPs per forward pass. Training: Forward
          (4B) + Backward (8B) = 12B FLOPs per image. Total: 100K images × 10 epochs
          × 12B FLOPs = 12P FLOPs. Hardware: P3.2xlarge (V100, 125 TFLOPS, $3/hour).
          Compute time: 12P / 125T = 96K seconds = 27 hours. But: Data loading, batch
          processing, overhead → 2× = 54 hours. Cost: 54 hours × $3 = $162. Memory:
          Batch size 32, activations ~2GB, model+optimizer ~400MB → need 4GB. V100
          has 16GB → sufficient. Reality check: Practitioners report 1-2 days for
          this scale, matches estimate.'
    time_estimate: 75
    video_resources:
    - title: 'Two Minute Papers: Object Detection Overview'
      url: https://www.youtube.com/@TwoMinutePapers
      duration: 8 min
      description: YOLO, Faster R-CNN architectures
      priority: high
    - title: 'Andrej Karpathy: Computer Vision in Production'
      url: https://www.youtube.com/@AndrejKarpathy
      duration: 30 min
      description: Building CV systems at scale
      priority: high
  week2:
  - day: 6
    topic: NLP Systems - Text Classification and Question Answering
    activity: Design chatbots, spam detection, and QA systems using BERT, fine-tuning,
      and RAG
    detailed_content: "Text Classification:\nExamples: Spam detection, sentiment analysis,\
      \ topic classification\n\nClassical Approach:\n- TF-IDF + Logistic Regression:\n\
      \  * Fast, interpretable, works with <10K samples\n  * Baseline: 80-85% accuracy\
      \ on sentiment\n\nDeep Learning Approach:\n- BERT fine-tuning:\n  * Add classification\
      \ head on [CLS] token\n  * Fine-tune last few layers (or all)\n  * Accuracy:\
      \ 90-95% on sentiment\n  * Latency: 50-100ms on CPU\n\nModel Selection:\n- Small\
      \ data (<1K): TF-IDF + LR or pretrained embeddings + LSTM\n- Medium (1K-10K):\
      \ DistilBERT (faster, 97% of BERT accuracy)\n- Large (>10K): BERT-base or RoBERTa\n\
      - Latency critical: TinyBERT (4 layers, 20ms latency)\n\nQuestion Answering\
      \ Systems:\nTwo types:\n1. Extractive QA: Extract answer span from passage\n\
      \   - Example: SQuAD dataset\n   - Model: BERT with span prediction heads\n\
      \   - Output: Start and end positions in text\n\n2. Generative QA: Generate\
      \ answer from knowledge\n   - Example: Open-domain QA\n   - Model: T5, GPT\n\
      \   - Output: Free-form text\n\nRetrieval-Augmented Generation (RAG):\n- Combine\
      \ retrieval + generation\n- Architecture:\n  1. Query → retrieve relevant documents\
      \ (BM25 or dense retrieval)\n  2. Concatenate query + documents\n  3. Generate\
      \ answer with T5 or GPT\n- Benefits: Grounded in documents, up-to-date information\n\
      - Use case: Customer support chatbot with knowledge base\n\nChatbot Design:\n\
      Example: Customer support chatbot\n\nComponents:\n1. Intent Classification:\
      \ Identify user intent\n   - Training data: <intent, example utterances>\n \
      \  - Model: BERT fine-tuned for classification\n\n2. Entity Extraction: Extract\
      \ key information\n   - NER: Named Entity Recognition\n   - Model: BERT + CRF\
      \ layer\n\n3. Dialogue Management: Track conversation state\n   - Rule-based\
      \ or RL-based policy\n\n4. Response Generation:\n   - Retrieval-based: Select\
      \ from templates\n   - Generative: GPT-based generation\n\n5. Knowledge Base:\
      \ FAQ, product docs\n   - Vector database for semantic search\n\nProduction\
      \ Considerations:\n- Latency: <500ms for chatbot response\n- Fallback: Human\
      \ handoff for low confidence\n- Monitoring: Track intent accuracy, resolution\
      \ rate\n- Continuous learning: Retrain on new conversations\n"
    practice_questions:
      concepts:
      - question: Explain how BERT is fine-tuned for text classification. What happens
          to the [CLS] token?
        answer: 'BERT pretraining: Learns contextualized embeddings via MLM + NSP.
          Fine-tuning for classification: 1) Add linear layer on [CLS] token output,
          2) [CLS] embedding (768-dim) → FC layer → num_classes logits, 3) Loss: Cross-entropy,
          4) Backprop through entire BERT (or freeze early layers). [CLS] token: Special
          token at start of sequence, its embedding aggregates entire sequence information
          via self-attention. Why it works: During pretraining, [CLS] learns to represent
          sentence-level semantics (NSP task). Fine-tuning adapts this representation
          to classification task. Alternatives: Average all token embeddings, max
          pooling (but [CLS] works best empirically).'
      - question: How does Retrieval-Augmented Generation (RAG) work? Why is it better
          than pure generation for QA?
        answer: 'RAG: 1) Retrieve: Query → dense retrieval (BERT embeddings) → top-K
          relevant documents. 2) Augment: Concatenate query + retrieved docs as context.
          3) Generate: Pass to T5/GPT → generate answer. Why better: 1) Grounded:
          Answers are based on retrieved docs (not hallucinated). 2) Up-to-date: Update
          document index without retraining model. 3) Interpretable: Can show source
          documents. 4) Efficient: Don''t need to store all knowledge in model parameters.
          Pure generation (GPT): Stores knowledge in parameters, can hallucinate,
          hard to update. Use RAG: Customer support (knowledge base), medical QA (research
          papers). Use pure gen: Creative tasks, general conversation.'
      - question: For a spam detection system, compare TF-IDF+LR vs BERT. Consider
          data size, latency, and interpretability.
        answer: 'TF-IDF+LR: Pros: 1) Fast training (minutes on 100K emails), 2) Fast
          inference (1ms), 3) Interpretable (can see which words indicate spam). Cons:
          1) Bag-of-words (ignores order), 2) Lower accuracy (~85%). BERT: Pros: 1)
          Understands context (''free money'' vs ''money-free vacation''), 2) Higher
          accuracy (~92%). Cons: 1) Slow training (hours), 2) Slow inference (50ms),
          3) Black box. Decision: <10K samples + need interpretability: TF-IDF+LR.
          >10K samples + accuracy critical: DistilBERT (40ms). Latency critical (<10ms):
          TF-IDF+LR. Real systems: Use TF-IDF+LR for first-stage (fast, filters 90%),
          BERT for second-stage (accurate, filters remaining 10%).'
      tradeoffs:
      - question: Design a customer support chatbot with 1000 FAQs. Compare retrieval-based
          vs generative approaches.
        answer: 'Retrieval-based: 1) Encode 1000 FAQs with BERT → embeddings, 2) User
          query → encode → ANN search → top-3 FAQs, 3) Return FAQ answers. Pros: Controlled
          responses (no hallucination), fast (<100ms), interpretable. Cons: Limited
          to FAQs, can''t handle variations. Generative (GPT): 1) Encode FAQs as context,
          2) User query + context → GPT, 3) Generate custom answer. Pros: Flexible,
          natural responses, handles follow-ups. Cons: May hallucinate, slow (500ms),
          expensive ($0.01 per query). Hybrid (best): 1) Retrieval: Find top-3 FAQs,
          2) Rerank: BERT cross-encoder for better ranking, 3) Generate: GPT to rephrase
          answer naturally. Pros: Grounded + flexible. Cons: More complex. For 1000
          FAQs: Start with retrieval (simple, fast), add generative if user feedback
          requests more flexibility.'
      scenarios:
      - question: You're building a chatbot for a bank (10K daily users). Estimate
          infrastructure costs for BERT-based intent classification and response generation.
        answer: 'Scale: 10K users/day, avg 5 messages/user = 50K messages/day = 0.6
          QPS average, 10 QPS peak. Intent classification: DistilBERT (40ms on CPU).
          Peak: 10 QPS × 40ms = 400ms total → need 1 CPU core. Response: Retrieval
          (FAQ search, 10ms) + GPT for rephrasing (200ms). Total latency: 250ms (acceptable).
          Infrastructure: 1) API servers: 2× c5.xlarge (4 vCPUs, $0.17/hour) = $8/day.
          2) GPT API: 50K messages × $0.001 = $50/day. 3) Vector DB (FAISS): 1× r5.large
          (16GB RAM, $0.13/hour) = $3/day. Total: $61/day = $1,830/month. Optimizations:
          Cache popular intents (reduce GPT calls 50%) → $900/month. Scale to 100K
          users: 10× → ~$9K/month.'
    time_estimate: 75
    video_resources:
    - title: 'Andrej Karpathy: NLP Systems Overview'
      url: https://www.youtube.com/@AndrejKarpathy
      duration: 25 min
      description: Text classification and QA architectures
      priority: high
    - title: 'Chip Huyen: Large Language Models in Production'
      url: https://www.youtube.com/@chiphuyen
      duration: 20 min
      description: Deploying and serving LLMs
      priority: medium
  - day: 7
    topic: Fraud Detection and Anomaly Detection Systems
    activity: Design real-time fraud detection for payments, fake accounts, and bot
      detection with graph features
    detailed_content: "Fraud Detection Overview:\n- Types: Credit card fraud, payment\
      \ fraud, fake accounts, bot traffic\n- Characteristics: Rare events (<0.1%),\
      \ adversarial (fraudsters adapt)\n- Requirements: Real-time scoring (<100ms),\
      \ high precision (minimize false positives)\n\nFeature Engineering:\n1. Transaction\
      \ Features:\n   - Amount, merchant, location, time\n   - Velocity: # transactions\
      \ in last 1h, 24h, 7d\n   - Deviation: Compare to user's historical patterns\n\
      \n2. User Features:\n   - Account age, verification status\n   - Historical\
      \ fraud rate\n   - Device fingerprint\n\n3. Graph Features:\n   - User-merchant\
      \ bipartite graph\n   - User-device graph\n   - PageRank, clustering coefficient\n\
      \   - Fraud rings: Connected components\n\n4. Behavioral Features:\n   - Typing\
      \ speed, mouse movements (bot detection)\n   - Session duration, click patterns\n\
      \nModel Selection:\n- XGBoost / LightGBM:\n  * Pros: Great for tabular data,\
      \ handles categorical features\n  * Fast inference (<10ms)\n  * Feature importance\
      \ for interpretability\n- Graph Neural Networks:\n  * For fraud rings (connected\
      \ fraudulent accounts)\n  * Learn from graph structure\n  * Slower but more\
      \ powerful\n\nClass Imbalance:\n- Fraud rate: 0.01-0.1% (1 in 1000-10000)\n\
      - Techniques:\n  * Downsampling: Keep all positives, sample negatives\n  * Class\
      \ weights: Weight positives 100-1000×\n  * Focal loss: Focus on hard examples\n\
      \  * Precision@K: Optimize for top K predictions\n\nReal-Time Scoring:\n- Latency\
      \ budget: <100ms (user waiting for transaction approval)\n- Architecture:\n\
      \  1. Feature extraction (30ms): Lookup from Redis, compute velocity\n  2. Model\
      \ inference (20ms): XGBoost on CPU\n  3. Rule engine (10ms): Hard rules (e.g.,\
      \ amount > $10K → flag)\n  4. Total: 60ms\n- Serving: 1000 QPS → need 10 servers\
      \ (100 QPS each)\n\nPrecision-Recall Trade-off:\n- High precision (minimize\
      \ false positives):\n  * Legitimate transactions flagged → customer frustration\n\
      \  * Set high threshold (predict fraud if score > 0.9)\n- High recall (catch\
      \ all fraud):\n  * Minimize losses to fraud\n  * Set low threshold (score >\
      \ 0.1)\n- Typical: Optimize for precision (few false alarms), accept some fraud\n\
      \nEvaluation:\n- Offline: Precision@1%, Precision@0.1%, AUC-PR (not AUC-ROC\
      \ for imbalanced)\n- Online: Dollar loss prevented, false positive rate, customer\
      \ complaints\n- A/B testing: Random buckets, measure fraud loss\n\nContinuous\
      \ Learning:\n- Fraudsters adapt → model degrades\n- Retrain daily/weekly on\
      \ recent data\n- Monitor: Drift detection (feature distributions, model performance)\n\
      - Feedback loop: Confirmed fraud → add to training\n\nExplainability:\n- Regulators\
      \ require explanations for declined transactions\n- SHAP values: Top features\
      \ contributing to fraud score\n- Rules: Hard rules (e.g., \"Location mismatch\
      \ with billing address\")\n"
    practice_questions:
      concepts:
      - question: Why are graph features important for fraud detection? Give an example
          of a fraud ring.
        answer: 'Fraud rings: Multiple fake accounts controlled by same fraudster.
          Individual accounts look normal, but graph reveals connections. Example:
          10 accounts share same device ID, IP address, or phone number. Graph features:
          1) Degree: # connections to other accounts (fraud ring has high degree).
          2) Clustering coefficient: Fraud ring is densely connected. 3) PageRank:
          Fraudulent accounts link to each other (low PageRank). Detection: Build
          user-device-IP graph, run community detection, flag dense subgraphs. GNNs:
          Learn embeddings that capture graph structure, predict fraud on nodes. Why
          effective: Fraudsters can fake user features but harder to fake graph structure.'
      - question: Explain the precision-recall trade-off in fraud detection. How do
          you choose the operating point?
        answer: 'Precision: Of flagged transactions, % that are actually fraud. Recall:
          Of all fraud, % that we caught. Trade-off: Increasing threshold → higher
          precision, lower recall. Decreasing → higher recall, lower precision. Fraud
          detection: Prioritize precision (minimize false positives). Why: False positive
          = legitimate transaction declined → customer frustration, lost sale. Missing
          fraud = financial loss but less immediate impact. Choosing operating point:
          1) Business decision: How much fraud loss is acceptable vs customer complaints?
          2) Cost: False positive costs $X (lost sale), false negative costs $Y (fraud
          loss). 3) Optimize: Precision subject to recall ≥ threshold (e.g., catch
          80% of fraud). Typical: Set threshold at 95% precision, accept 50% recall.'
      - question: How do you handle model drift in fraud detection? Why is continuous
          retraining important?
        answer: 'Drift types: 1) Data drift: Feature distributions change (new fraud
          tactics). 2) Concept drift: Relationship between features and fraud changes.
          Why critical: Fraudsters adapt (if model blocks tactic A, they switch to
          B). Model trained on old tactics becomes ineffective. Detection: 1) Monitor
          feature distributions (KL divergence from training), 2) Monitor model performance
          (precision drops), 3) A/B test new model vs old. Continuous retraining:
          1) Retrain daily/weekly on recent data (30-day window), 2) Use online learning
          (FTRL) for real-time updates, 3) Feedback loop: Confirmed fraud → add to
          training immediately. Guardrails: Shadow mode new model (don''t affect production),
          gradual rollout, rollback if metrics drop.'
      tradeoffs:
      - question: For payment fraud detection, compare XGBoost vs Graph Neural Networks.
          When to use each?
        answer: 'XGBoost: Pros: 1) Fast inference (<10ms), 2) Works well on tabular
          features (amount, merchant, velocity), 3) Interpretable (SHAP values), 4)
          Mature libraries. Cons: 1) Doesn''t capture graph structure (fraud rings),
          2) Treats transactions independently. GNN: Pros: 1) Captures graph structure
          (user-merchant, user-device), 2) Detects fraud rings (connected accounts),
          3) Better recall for sophisticated fraud. Cons: 1) Slower inference (50-100ms),
          2) Complex, harder to debug, 3) Needs graph data (edges). Decision: Solo
          fraudsters (single transactions): XGBoost. Fraud rings (coordinated attacks):
          GNN. Hybrid: XGBoost for real-time scoring (fast), GNN for batch analysis
          (find rings), combine scores.'
      scenarios:
      - question: Design a real-time fraud detection system for Stripe (100K transactions/second).
          Estimate infrastructure and latency.
        answer: 'Scale: 100K TPS, <100ms latency, 0.1% fraud rate. Architecture: 1)
          Feature Store (Redis): a) User features: Account age, fraud history (10ms
          lookup), b) Velocity: # transactions in 1h (real-time aggregation), c) Graph
          features: Precomputed PageRank (10ms lookup). 2) Model Serving (XGBoost):
          a) Input: 100 features, b) XGBoost inference: 10ms on CPU, c) Deployment:
          1000 servers (c5.2xlarge, 8 vCPUs), d) Each server: 100 QPS (10ms per inference).
          3) Rule Engine: Hard rules (amount > $50K, location mismatch) - 5ms. Total
          latency: 10ms (feature) + 10ms (model) + 5ms (rules) = 25ms. Infrastructure:
          1000 servers × $0.40/hour = $400/hour = $290K/month. Optimizations: Model
          caching (cache scores for repeat patterns), feature caching (popular merchants),
          reduce to 500 servers → $145K/month.'
    time_estimate: 75
    video_resources:
    - title: 'Chip Huyen: Anomaly Detection Systems'
      url: https://www.youtube.com/@chiphuyen
      duration: 18 min
      description: Fraud detection architecture
      priority: high
    - title: 'ByteByteGo: Real-Time Fraud Detection'
      url: https://www.youtube.com/@ByteByteGo
      duration: 12 min
      description: Low-latency anomaly detection
      priority: medium
  week3:
  - day: 8
    topic: Feed Ranking Systems - Multi-Objective Learning and Diversity
    activity: Design social media feed ranking with engagement prediction, multi-task
      learning, and diversity constraints
    detailed_content: "Feed Ranking Overview:\n- Examples: Facebook News Feed, Twitter\
      \ Timeline, Instagram Feed\n- Goal: Show most engaging content to each user\n\
      - Challenges: Multiple objectives, diversity, freshness\n\nMulti-Objective Optimization:\n\
      Conflicting objectives:\n- User engagement: Clicks, likes, shares, time spent\n\
      - Business revenue: Ad clicks, conversions\n- Platform health: Reduce misinformation,\
      \ toxic content\n- Creator satisfaction: Fair distribution of impressions\n\n\
      Approaches:\n1. Weighted Sum: Score = w1*p(click) + w2*p(like) + w3*p(share)\n\
      \   - Simple but hard to tune weights\n\n2. Multi-Task Learning:\n   - Shared\
      \ bottom layers\n   - Task-specific towers for each objective\n   - Benefits:\
      \ Transfer learning, single model deployment\n\n3. Pareto Frontier:\n   - Explore\
      \ trade-offs by varying weights\n   - Business chooses operating point\n\n4.\
      \ Constraint-Based:\n   - Maximize engagement subject to revenue ≥ threshold\n\
      \nFeatures:\n- User features: Demographics, interests, activity level\n- Content\
      \ features: Type (photo/video/link), creator, topic\n- User-content: Has user\
      \ engaged with creator before?\n- Context: Time of day, device, location\n-\
      \ Social: Friends' interactions (likes, shares)\n\nModel Architecture:\n- Two-tower:\
      \ User tower + content tower → dot product\n- Deep & Wide: Wide (cross features)\
      \ + Deep (embeddings)\n- Multi-task: Shared layers → towers for click, like,\
      \ share, time spent\n\nDiversity Constraints:\n- Problem: Top-K by score may\
      \ be all similar content (filter bubble)\n- MMR (Maximal Marginal Relevance):\
      \ Balance relevance and diversity\n- DPP (Determinantal Point Process): Probabilistic\
      \ diverse subset\n- Position-based: Force different types in top-10 (1 video,\
      \ 2 photos, etc.)\n\nFreshness:\n- Problem: Popular old posts dominate new posts\n\
      - Time decay: Score = base_score × e^(-λ × age)\n- Recency boost: Add bonus\
      \ for posts < 1 hour old\n- Balance: Ensure new creators get visibility\n\n\
      Exploration:\n- Bandits: Epsilon-greedy, Thompson sampling\n- Why: Discover\
      \ new content users might like\n- Trade-off: Exploitation (show known good content)\
      \ vs exploration (try new)\n\nEvaluation:\n- Offline: AUC for engagement prediction,\
      \ NDCG for ranking\n- Online: Time spent, engagement rate, retention\n- Guardrail:\
      \ Ensure no harm to platform health metrics\n"
    practice_questions:
      concepts:
      - question: How does multi-task learning help feed ranking? Explain the architecture
          and benefits.
        answer: 'Feed ranking optimizes multiple objectives: click, like, share, time
          spent. Multi-task: Shared bottom layers learn general user/content representations,
          task-specific towers predict each objective. Architecture: User/content
          features → shared layers (512→256 dims) → 4 towers (128→1 each). Benefits:
          1) Transfer learning: Shared layers learn features useful for all tasks
          (e.g., user interests), 2) Efficiency: Single model deployment, 3) Flexibility:
          Adjust weights at serving time (score = 0.3*click + 0.2*like + 0.3*share
          + 0.2*time). Training: Joint loss = w1*L_click + w2*L_like + w3*L_share
          + w4*L_time. Empirically: Multi-task outperforms separate models by 2-5%
          on each metric.'
      - question: Explain the filter bubble problem in feed ranking. How do diversity
          techniques address it?
        answer: 'Filter bubble: If rank purely by predicted engagement, user sees
          only similar content (e.g., all political posts). Causes echo chamber, poor
          user experience. Why happens: Top-K by score selects most similar to user''s
          past behavior. Diversity techniques: 1) MMR: Select items that are relevant
          AND dissimilar to already-selected. Formula: MMR = λ*relevance - (1-λ)*max_similarity.
          2) DPP: Sample diverse subset, kernel = quality × diversity. 3) Position-based:
          Force mix (1 news, 1 photo, 1 video in top-3). Trade-off: Diversity vs engagement
          (showing diverse content may reduce short-term engagement but improve long-term
          retention).'
      - question: How do you balance freshness vs relevance in feed ranking? Explain
          time decay.
        answer: 'Problem: Popular old posts have high engagement score (many likes),
          dominate new posts. But users want fresh content. Time decay: Reduce score
          of old posts. Formula: Score_final = Score_base × e^(-λ×age) where age in
          hours, λ controls decay rate. Example: Post with score=10 at 1 hour old:
          10×e^(-0.1×1) ≈ 9. Same post at 24 hours: 10×e^(-0.1×24) ≈ 0.9. Choosing
          λ: Small λ (0.01): Slow decay, old posts remain. Large λ (0.5): Fast decay,
          prioritize fresh. Typical: λ=0.05-0.1 (half-life ~7-14 hours). Balance:
          Ensure viral content can rise while promoting fresh posts.'
      tradeoffs:
      - question: For Facebook News Feed, how would you balance engagement (time spent)
          vs platform health (reduce misinformation)? Discuss multi-objective optimization.
        answer: 'Conflict: High engagement content (clickbait, sensational) may be
          misinformation. Business must balance. Approaches: 1) Weighted sum: Score
          = 0.7×engagement - 0.3×misinformation_score. Cons: Hard to tune weights.
          2) Constraint-based: Maximize engagement subject to misinformation_rate
          < 1%. Ensures minimum health. 3) Multi-task learning: Predict both engagement
          and misinformation, combine at serving. 4) Pareto frontier: Test multiple
          weight combinations, A/B test, show trade-off curve to leadership. Recommendation:
          Start with constraint (maximize engagement, cap misinformation at acceptable
          level). A/B test: Vary constraint threshold (0.5%, 1%, 2%), measure: time
          spent, retention (long-term health), user complaints. Choose: Threshold
          that maximizes engagement while maintaining retention.'
      scenarios:
      - question: Design Instagram feed ranking (500M DAU, 100M posts/day). Estimate
          compute for ranking model serving.
        answer: 'Scale: 500M users, each opens app 5× per day = 2.5B sessions/day
          = 29K QPS average, 100K QPS peak. Per session: Show 50 posts (rank top-50
          from 1000 candidates). Features: 200 features (user, content, cross). Model:
          Multi-task neural network, 2M params. Inference: 200 features × 1000 candidates
          = 200K features to process. Batch inference (1000 candidates): 10ms on GPU.
          Peak: 100K QPS × 10ms = 1000 GPU-seconds per second → need 1000 GPUs. Infrastructure:
          1000× P3.2xlarge (V100, $3/hour) = $3K/hour = $2.2M/month. Optimizations:
          1) Two-stage: Retrieve 1000 candidates (fast), rank 50 with complex model
          (slow). 2) Caching: Cache scores for popular posts (50% reduction). 3) Model
          quantization (INT8): 4× faster → 250 GPUs → $550K/month. Final: ~$500K/month
          for feed ranking.'
    time_estimate: 75
    video_resources:
    - title: 'ByteByteGo: Design Instagram Feed'
      url: https://www.youtube.com/@ByteByteGo
      duration: 15 min
      description: Multi-objective feed ranking
      priority: high
    - title: 'Chip Huyen: Personalized Feeds at Scale'
      url: https://www.youtube.com/@chiphuyen
      duration: 22 min
      description: Balancing engagement and diversity
      priority: high
  - day: 9
    topic: Training Infrastructure - Distributed Training and Experiment Tracking
    activity: Design scalable training pipelines with data parallelism, hyperparameter
      tuning, and experiment management
    detailed_content: "Distributed Training:\nWhy: Single GPU training too slow for\
      \ large datasets/models\n\nData Parallelism:\n- Replicate model on N GPUs\n\
      - Split batch across GPUs (each processes batch_size/N)\n- Each GPU computes\
      \ gradients on its data\n- Synchronize: Average gradients across GPUs\n- Update\
      \ model weights\n- Linear speedup (ideally): 8 GPUs → 8× faster\n\nSynchronization:\n\
      - Synchronous: Wait for all GPUs before updating (slow)\n- Asynchronous: GPUs\
      \ update independently (faster but less stable)\n- Gradient accumulation: Simulate\
      \ large batch with small batches\n\nModel Parallelism:\n- Split model across\
      \ GPUs (for very large models)\n- Each GPU holds part of model\n- Example: GPT-3\
      \ (175B params) doesn't fit on one GPU\n- Pipeline parallelism: Split into stages\n\
      \nMixed Precision Training:\n- Use FP16 for forward/backward, FP32 for weights\n\
      - Benefits: 2× speedup, 2× memory reduction\n- Loss scaling: Prevent underflow\
      \ in FP16\n- Automatic Mixed Precision (AMP): PyTorch/TensorFlow\n\nHyperparameter\
      \ Tuning:\n- Grid search: Try all combinations (expensive)\n- Random search:\
      \ Sample combinations (better than grid)\n- Bayesian optimization: Model performance,\
      \ suggest next trial\n  * Tools: Optuna, Hyperopt\n- Early stopping: Stop bad\
      \ trials early\n\nExperiment Tracking:\n- Problem: 100s of experiments, lose\
      \ track of hyperparameters, results\n- Tools: MLflow, Weights & Biases, TensorBoard\n\
      - Log: Hyperparameters, metrics, model checkpoints, training curves\n- Compare:\
      \ Visualize experiments side-by-side\n\nTraining Pipeline:\n1. Data loading:\
      \ tf.data, PyTorch DataLoader\n   - Prefetching: Load next batch while training\n\
      \   - Parallelism: Multi-threaded data loading\n2. Training loop: Forward, loss,\
      \ backward, update\n3. Checkpointing: Save model every N steps\n4. Evaluation:\
      \ Validate every epoch\n5. Monitoring: Log metrics to experiment tracker\n\n\
      Resource Estimation:\n- Training time: (dataset_size × epochs × FLOPs_per_sample)\
      \ / (GPU_FLOPS × num_GPUs)\n- Memory: Model params + gradients + activations\
      \ + optimizer state\n  * BERT-base: 110M params × 4 bytes × 4 (params + grads\
      \ + optimizer) = 1.7GB\n  * Add activations: batch_size × seq_len × hidden_dim\
      \ × num_layers\n- Cost: GPU_hours × GPU_price\n\nBest Practices:\n- Start small:\
      \ Overfit on tiny dataset to debug\n- Baseline: Train simple model first\n-\
      \ Monitor: Track training curves (loss, validation accuracy)\n- Checkpoint:\
      \ Save frequently in case of crashes\n- Reproducibility: Set random seeds, log\
      \ hyperparameters\n"
    practice_questions:
      concepts:
      - question: Explain data parallelism for distributed training. How does gradient
          synchronization work?
        answer: 'Data parallelism: Replicate model on N GPUs. Split batch (size B)
          into N parts (B/N each). Each GPU: 1) Forward pass on its B/N samples, 2)
          Compute loss, 3) Backward pass → gradients. Synchronization: All-reduce
          operation averages gradients across GPUs. GPU1 has ∇L1, GPU2 has ∇L2, ...
          → Compute ∇L_avg = (∇L1 + ∇L2 + ... + ∇LN) / N. Update: All GPUs update
          with ∇L_avg (stay synchronized). Speedup: Ideal N× (N GPUs). Reality: 0.8-0.9×
          N due to communication overhead. Tools: PyTorch DistributedDataParallel
          (DDP), Horovod.'
      - question: Why use mixed precision training (FP16)? What problems does it solve
          and what new challenges does it introduce?
        answer: 'Benefits: 1) Speed: FP16 ops 2× faster on Tensor Cores (V100, A100),
          2) Memory: 2× less memory (2 bytes vs 4 bytes), fits larger batches. Challenges:
          1) Underflow: FP16 range is smaller (6e-8 to 65K) vs FP32 (1e-38 to 3e38).
          Small gradients → 0 in FP16. Solution: Loss scaling: Multiply loss by scale
          factor (e.g., 1024) before backward → gradients larger → no underflow. Then
          divide gradients by scale before update. 2) Precision: FP32 weights accumulator
          (update in FP32, store in FP16). Tools: Automatic Mixed Precision (AMP)
          handles this automatically.'
      - question: Explain Bayesian optimization for hyperparameter tuning. Why is
          it better than random search?
        answer: 'Random search: Sample hyperparameters randomly, train, evaluate.
          Inefficient: Treats trials independently. Bayesian optimization: 1) Build
          surrogate model (Gaussian Process) of performance f(hyperparams), 2) Acquisition
          function chooses next hyperparams (balance exploration vs exploitation),
          3) Train with chosen hyperparams, observe performance, 4) Update surrogate
          model, repeat. Why better: Uses information from past trials to suggest
          promising hyperparams. Fewer trials needed (10-20 vs 100+ for random). Example:
          After trying learning_rate=0.1 (bad) and 0.001 (good), BO suggests 0.005
          (interpolate), not 0.5 (random might try). Tools: Optuna, Hyperopt.'
      tradeoffs:
      - question: For training a BERT model on 1M samples, compare single GPU vs 8-GPU
          data parallelism. Consider time, cost, and batch size trade-offs.
        answer: 'Single GPU (V100): Batch size 32 (memory limit), 1M samples / 32
          = 31K batches, 100ms per batch → 3100 seconds/epoch = 52 minutes. 10 epochs:
          520 minutes = 8.7 hours. Cost: $3/hour × 8.7 = $26. 8 GPUs (data parallel):
          Batch size 256 (32 × 8), 1M / 256 = 3.9K batches, 100ms per batch (same,
          but parallelized) → 390 seconds/epoch = 6.5 minutes. 10 epochs: 65 minutes
          = 1.1 hours. Cost: $24/hour × 1.1 = $26. Same cost but 8× faster! Caveat:
          Large batch (256) may require tuning (LR warmup, higher LR). Trade-off:
          8 GPUs faster training (better for iteration speed), same cost.'
      estimation:
      - question: Estimate memory requirements for training ResNet-50 with batch size
          128 on a single GPU.
        answer: 'ResNet-50: 25M parameters. Memory breakdown: 1) Model weights: 25M
          × 4 bytes = 100MB. 2) Gradients: 25M × 4 bytes = 100MB. 3) Optimizer state
          (Adam): 2 × 25M × 4 bytes = 200MB (first + second moment). 4) Activations:
          Batch size × feature maps. Estimate: 128 × 2048 (avg feature map size) ×
          50 layers × 4 bytes = 50MB (rough, varies by layer). Total: 100 + 100 +
          200 + 50 = 450MB. Add buffer (intermediate tensors): 2× = 900MB. Reality
          check: Practitioners report ~1-2GB for batch size 128 on ResNet-50 (matches
          estimate). V100 has 16GB → comfortable fit.'
    time_estimate: 60
    video_resources:
    - title: 'Chip Huyen: Distributed Training'
      url: https://www.youtube.com/@chiphuyen
      duration: 25 min
      description: Data and model parallelism
      priority: high
    - title: 'Andrej Karpathy: Training at Scale'
      url: https://www.youtube.com/@AndrejKarpathy
      duration: 30 min
      description: Infrastructure for large models
      priority: high
  - day: 10
    topic: Model Serving Infrastructure - Batching, Caching, and Deployment Strategies
    activity: Design production serving systems with TensorFlow Serving, optimization,
      and deployment strategies
    detailed_content: "Model Serving Overview:\n- Goal: Serve predictions at low latency\
      \ and high throughput\n- Challenges: Latency (<100ms), scale (1000s QPS), cost\n\
      \nServing Frameworks:\n- TensorFlow Serving: Production-ready, model versioning,\
      \ batching\n- TorchServe: PyTorch native\n- Triton Inference Server: Multi-framework\
      \ (TF, PyTorch, ONNX)\n- Cloud: SageMaker, AI Platform, Azure ML\n\nOptimization\
      \ Techniques:\n1. Batching:\n   - Combine multiple requests into batch\n   -\
      \ Amortize GPU overhead\n   - Trade-off: Latency (wait for batch) vs throughput\n\
      \   - Dynamic batching: Wait up to T ms or B requests\n\n2. Model Optimization:\n\
      \   - Quantization: FP32 → INT8 (4× faster, 4× smaller)\n   - Pruning: Remove\
      \ unimportant weights\n   - Distillation: Small model mimics large model\n \
      \  - ONNX: Framework-agnostic, optimized runtime\n\n3. Caching:\n   - Cache\
      \ predictions for popular inputs\n   - Example: Recommendation cache for popular\
      \ items\n   - TTL: Expire after T minutes\n\n4. GPU vs CPU:\n   - GPU: High\
      \ throughput (batch), expensive\n   - CPU: Low latency (single), cheap\n   -\
      \ Decision: Latency critical → CPU, throughput → GPU\n\nDeployment Strategies:\n\
      1. Blue-Green Deployment:\n   - Run old (blue) and new (green) versions\n  \
      \ - Switch traffic to green\n   - Rollback to blue if issues\n\n2. Canary Deployment:\n\
      \   - Route small % (5%) to new version\n   - Monitor metrics\n   - Gradually\
      \ increase % if successful\n\n3. Shadow Deployment:\n   - New model receives\
      \ traffic but doesn't affect users\n   - Compare predictions with old model\n\
      \   - Validate before switching\n\nModel Versioning:\n- Store multiple model\
      \ versions\n- Gradual rollout: Route % of traffic to each version\n- A/B testing:\
      \ Random assignment to versions\n- Rollback: Quick switch to previous version\n\
      \nMonitoring:\n- Latency: p50, p95, p99 percentiles\n- Throughput: QPS\n- Error\
      \ rate: Failed predictions\n- Resource: CPU, GPU, memory usage\n- Model metrics:\
      \ Prediction distribution, drift\n\nAutoscaling:\n- Monitor QPS, latency\n-\
      \ Scale up: Add instances if latency > threshold\n- Scale down: Remove instances\
      \ if underutilized\n- Kubernetes Horizontal Pod Autoscaler\n\nLoad Balancing:\n\
      - Distribute requests across instances\n- Round-robin, least connections, or\
      \ consistent hashing\n- Health checks: Remove unhealthy instances\n\nCold Start:\n\
      - Problem: New instance takes time to load model\n- Solution: Pre-warm instances,\
      \ model caching\n"
    practice_questions:
      concepts:
      - question: Explain dynamic batching in model serving. What's the latency-throughput
          trade-off?
        answer: 'Dynamic batching: Collect requests over time window T (e.g., 50ms)
          or until batch size B (e.g., 32), whichever comes first. Send batch to model
          for inference. Benefits: Batching amortizes GPU overhead → higher throughput.
          Trade-offs: Latency = wait time + inference time. Wait: 0-50ms (avg 25ms).
          Inference: 50ms for batch. Total: 75ms. Without batching: 0ms wait + 50ms
          inference = 50ms. So: Batching adds 25ms latency but increases throughput
          10× (batch 32 vs 1). Decision: Latency critical (<50ms): No batching or
          small T (10ms). Throughput critical: Large T (100ms), large B (128).'
      - question: Compare canary deployment vs blue-green deployment for ML models.
          When to use each?
        answer: 'Blue-Green: Run two full environments (old=blue, new=green). Switch
          100% traffic blue→green instantly. Rollback: Switch back to blue. Pros:
          Fast rollback, simple. Cons: Need 2× capacity (expensive), all-or-nothing
          risk. Canary: Route small % (5%) to new model, rest to old. Monitor: Compare
          metrics (latency, accuracy). Gradually increase % if good. Pros: Risk mitigation
          (only 5% affected), cheaper (no 2× capacity). Cons: Slower rollout, complex
          traffic routing. Use Blue-Green: Small-scale, confident in new model, need
          fast rollback. Use Canary: Large-scale (millions of users), uncertain about
          new model, want gradual validation.'
      - question: How does model caching improve serving performance? What are the
          trade-offs?
        answer: 'Caching: Store prediction for input. On repeat request, return cached
          prediction (1ms) instead of model inference (50ms). Example: Recommendation
          for popular item requested 1000× per second. Benefits: 50× speedup, reduce
          compute cost. Trade-offs: 1) Memory: Store input→output map. If 1M popular
          items × 1KB = 1GB. 2) Staleness: Cached prediction may be outdated (user''s
          interests changed). 3) Cache miss: Unpopular items still need inference.
          TTL: Set expiration (e.g., 1 hour) to balance freshness vs hit rate. Decision:
          Cache when: High QPS, inputs repeat often (search queries, popular items),
          predictions don''t change frequently. Don''t cache: Personalized predictions
          change rapidly.'
      tradeoffs:
      - question: For a recommendation API (1000 QPS, <100ms latency), compare CPU
          vs GPU serving. Consider cost and performance.
        answer: 'CPU serving: Instance: c5.2xlarge (8 vCPUs, $0.34/hour). Inference:
          50ms per request (no batching, sequential). Throughput: 20 QPS per instance.
          1000 QPS needs: 50 instances. Cost: 50 × $0.34 = $17/hour = $12K/month.
          Latency: 50ms (meets requirement). GPU serving: Instance: P3.2xlarge (V100,
          $3/hour). Batching: Batch size 32, inference 20ms per batch. Throughput:
          1600 QPS (32/20ms). 1000 QPS needs: 1 GPU. Cost: $3/hour = $2.2K/month.
          Latency: 25ms wait + 20ms inference = 45ms (meets requirement). Decision:
          GPU is 5× cheaper and faster! Use GPU unless: Very low QPS (<100), no batching
          possible, want simple deployment.'
      scenarios:
      - question: Design model serving infrastructure for Google Ads CTR prediction
          (1M QPS, <10ms latency). Estimate cost and architecture.
        answer: 'Requirements: 1M QPS, <10ms latency, high availability. Architecture:
          1) Model: DeepFM optimized with TensorRT (INT8 quantization). Inference:
          5ms on CPU (batch 32). 2) Batching: Dynamic batching with 5ms timeout, batch
          size 32. 3) Throughput: 32 requests / 5ms = 6400 QPS per instance. 4) Instances:
          1M QPS / 6400 = 157 instances (round to 200 for redundancy). 5) Hardware:
          c5.4xlarge (16 vCPUs, $0.68/hour). 6) Load balancer: Route by user_id (sticky
          routing). Cost: 200 × $0.68 = $136/hour = $100K/month. Optimizations: 1)
          Model caching (popular ads): 50% hit rate → 100 instances → $50K/month.
          2) Regional deployment (lower latency): 5 regions × 40 instances. Monitoring:
          Latency, QPS, error rate per region. Autoscaling: Scale 150-250 instances
          based on traffic.'
    time_estimate: 60
    video_resources:
    - title: 'Chip Huyen: ML Model Serving'
      url: https://www.youtube.com/@chiphuyen
      duration: 20 min
      description: Batching, caching, and deployment
      priority: high
    - title: 'ByteByteGo: Design ML Inference System'
      url: https://www.youtube.com/@ByteByteGo
      duration: 12 min
      description: Low-latency prediction serving
      priority: high
  - day: 11
    topic: Feature Stores and Data Pipelines
    activity: Design feature engineering pipelines with Spark, feature stores for
      training-serving consistency, and data quality
    detailed_content: "Feature Store Overview:\n- Problem: Feature engineering duplicated\
      \ for training and serving\n- Training: Spark batch processing\n- Serving: Real-time\
      \ API lookup\n- Mismatch: Training-serving skew (features computed differently)\n\
      \nSolution: Feature Store\n- Centralized repository for features\n- Offline\
      \ store: Historical features for training (Parquet, Delta Lake)\n- Online store:\
      \ Real-time features for serving (Redis, Cassandra)\n- Consistency: Same feature\
      \ logic for training and serving\n\nFeature Store Tools:\n- Feast: Open source,\
      \ supports offline+online stores\n- Tecton: Managed, supports streaming features\n\
      - SageMaker Feature Store: AWS managed\n- Databricks Feature Store: Integrated\
      \ with Delta Lake\n\nFeature Engineering Pipeline:\n1. Raw data ingestion: Logs,\
      \ events, database\n2. Feature computation: Spark, SQL, Python\n3. Feature validation:\
      \ Check schema, distributions\n4. Feature storage: Write to offline+online stores\n\
      5. Feature serving: Lookup at serving time\n\nBatch Features (Offline):\n- Computed\
      \ daily/hourly on historical data\n- Example: User's avg purchase amount (last\
      \ 30 days)\n- Storage: Parquet files in S3\n- Workflow: Airflow, Spark\n\nReal-Time\
      \ Features (Online):\n- Computed on-the-fly for serving\n- Example: User's last\
      \ 5 clicked items\n- Storage: Redis (key-value, <1ms lookup)\n- Update: Streaming\
      \ (Kafka, Spark Streaming)\n\nStreaming Features:\n- Computed from event streams\n\
      - Example: # clicks in last 1 hour (tumbling window)\n- Tools: Spark Streaming,\
      \ Flink, Kafka Streams\n- Challenge: Late arrivals, out-of-order events\n\n\
      Feature Monitoring:\n- Data drift: Distribution changes (e.g., avg age shifts)\n\
      - Schema drift: Columns added/removed\n- Data quality: Null rates, outliers\n\
      - Alerts: Trigger retraining if drift detected\n\nTraining-Serving Consistency:\n\
      - Problem: Training uses Spark (batch), serving uses Python (real-time)\n  *\
      \ Different implementations → different results\n- Solution: Feature store ensures\
      \ same logic\n  * Define feature once, materialize for both offline+online\n\
      \nPoint-in-Time Correctness:\n- Problem: Training must use features available\
      \ at prediction time\n  * Can't use future information (data leakage)\n- Solution:\
      \ Feature store tracks timestamps\n  * Retrieve features as of time T (historical)\n\
      \nData Labeling:\n- Active learning: Label data where model is uncertain\n-\
      \ Weak supervision: Programmatic labeling (Snorkel)\n- Human labeling: Mechanical\
      \ Turk, internal team\n- Quality control: Multiple labelers, consensus\n"
    practice_questions:
      concepts:
      - question: What is training-serving skew and how does a feature store prevent
          it?
        answer: 'Training-serving skew: Features computed differently for training
          vs serving → model sees different data distributions → poor performance.
          Example: Training: user_avg_purchase = Spark SQL (last 30 days). Serving:
          Python function (buggy, uses 7 days). Model trained on 30-day avg but serves
          with 7-day avg → skew. Feature store solution: Define feature logic once
          (SQL, Python), materialize to offline store (training) and online store
          (serving). Same logic guaranteed. Feast example: @feature_view(user_avg_purchase,
          aggregation=''avg'', window=''30d''). Generates Spark code for training,
          Redis lookup for serving.'
      - question: Explain point-in-time correctness in feature stores. Why is it critical
          for model training?
        answer: 'Point-in-time correctness: Features must reflect what was known at
          prediction time, not future info. Example: Predict fraud on Jan 1. Can use
          user''s purchases up to Jan 1, NOT Jan 2 (data leakage). Without: Join features
          as of ''now'' → accidentally use future data → model overfits → fails in
          production. With: Feature store tracks timestamps. Request: get_features(user_id,
          timestamp=Jan 1) → returns features as of Jan 1. Implementation: Each feature
          has (entity_id, timestamp, value). At serving: Use current time. At training:
          Use label timestamp.'
      - question: Compare batch features vs streaming features. When to use each?
        answer: 'Batch features: Computed periodically (daily, hourly) on historical
          data. Example: User''s avg purchase amount (last 30 days). Compute: Spark
          job runs nightly. Storage: Parquet in S3 (offline), Redis (online). Use
          when: Features change slowly, can tolerate staleness. Streaming features:
          Computed in real-time from event streams. Example: # clicks in last 1 hour.
          Compute: Kafka Streams / Spark Streaming. Storage: Redis. Use when: Features
          change rapidly, need fresh data (fraud detection). Trade-offs: Streaming
          is complex, expensive (always running). Batch is simple, cheap (periodic).
          Most systems use both: Batch for slow-changing (demographics), streaming
          for fast-changing (recent clicks).'
      tradeoffs:
      - question: For a fraud detection system, design a feature store with both batch
          and streaming features. What features go where?
        answer: 'Batch features (daily Spark job): 1) User account age, 2) # transactions
          last 30 days, 3) Avg transaction amount, 4) Historical fraud rate. Storage:
          Parquet (offline), Redis (online, keyed by user_id). Streaming features
          (Kafka + Spark Streaming): 1) # transactions in last 1 hour, 2) # unique
          merchants last 1 hour, 3) Max transaction amount last 1 hour. Storage: Redis
          (sliding window). Feature serving: At transaction time, lookup both batch
          (from Redis) and streaming (from Redis) → combine → model. Trade-offs: Streaming
          adds latency (5-10ms Redis lookup) but critical for fraud (velocity features).
          Batch features cheaper, cover long-term patterns.'
      scenarios:
      - question: You have 1B historical transactions for training and 10K transactions/second
          for serving. Design the feature pipeline.
        answer: 'Offline (Training): 1) Data: 1B transactions in Parquet (S3). 2)
          Features: Spark job computes user/merchant features (avg amount, frequency).
          3) Output: Feature table (user_id, feature1, feature2, ..., timestamp).
          4) Storage: Delta Lake (S3). 5) Training: Join features with labels (fraud/not),
          train XGBoost. Online (Serving): 1) Batch features: Daily Spark job writes
          to Redis (user_id → features). 2) Streaming features: Kafka Streams aggregates
          1-hour windows → Redis. 3) Serving: a) Transaction arrives, b) Lookup user_id
          in Redis (batch features, 1ms), c) Lookup user_id in Redis (streaming features,
          1ms), d) Model inference (10ms), e) Total: 12ms. Throughput: 10K TPS × 2ms
          Redis = 20K Redis QPS. Redis cluster: 10 nodes (2K QPS each).'
    time_estimate: 60
    video_resources:
    - title: 'Chip Huyen: Feature Stores Explained'
      url: https://www.youtube.com/@chiphuyen
      duration: 18 min
      description: Feature engineering at scale
      priority: high
    - title: 'ByteByteGo: Data Pipeline Architecture'
      url: https://www.youtube.com/@ByteByteGo
      duration: 15 min
      description: ETL for ML systems
      priority: medium
  - day: 12
    topic: Monitoring, Drift Detection, and Model Retraining
    activity: Design monitoring systems for data drift, concept drift, and automated
      retraining pipelines
    detailed_content: "ML System Monitoring:\nUnlike traditional software, ML systems\
      \ degrade over time (data drift)\n\nMetrics to Monitor:\n1. Model performance:\n\
      \   - Offline: AUC, precision, recall (on validation set)\n   - Online: CTR,\
      \ conversion rate, revenue\n2. Data quality:\n   - Feature distributions (mean,\
      \ std, quantiles)\n   - Null rates, missing values\n   - Schema changes\n3.\
      \ System health:\n   - Latency (p50, p95, p99)\n   - Throughput (QPS)\n   -\
      \ Error rate\n4. Business metrics:\n   - Revenue, user engagement, retention\n\
      \nData Drift Detection:\n- Data drift: Input distribution changes\n- Example:\
      \ User demographics shift (more young users)\n- Detection:\n  * Compare training\
      \ vs production feature distributions\n  * KL divergence, PSI (Population Stability\
      \ Index)\n  * Two-sample tests (Kolmogorov-Smirnov)\n- Action: Retrain on recent\
      \ data\n\nConcept Drift:\n- Concept drift: Relationship between features and\
      \ target changes\n- Example: User behavior changes (clicks different content)\n\
      - Detection:\n  * Monitor online metrics (AUC, CTR)\n  * Performance drop indicates\
      \ drift\n- Action: Retrain with new data\n\nPrediction Drift:\n- Prediction\
      \ distribution changes\n- Example: Fraud model predicts fraud rate 5% → 20%\n\
      - Detection: Compare prediction distribution over time\n- Action: Investigate\
      \ cause (data drift? model issue?)\n\nMonitoring Tools:\n- Prometheus: Metrics\
      \ collection\n- Grafana: Visualization, dashboards\n- CloudWatch, Datadog: Cloud\
      \ monitoring\n- WhyLabs, Arize: ML-specific monitoring\n\nAlerting:\n- Define\
      \ thresholds: Latency > 200ms, error rate > 1%\n- Alert channels: Email, Slack,\
      \ PagerDuty\n- Severity: P0 (urgent, page on-call), P1 (investigate within hour)\n\
      \nModel Retraining:\nTriggers:\n1. Scheduled: Retrain weekly/monthly\n2. Performance:\
      \ Online metrics drop below threshold\n3. Data drift: KL divergence exceeds\
      \ threshold\n4. Manual: New features, architecture change\n\nRetraining Pipeline:\n\
      1. Data collection: Gather recent labeled data\n2. Feature engineering: Recompute\
      \ features\n3. Training: Train new model\n4. Evaluation: Compare new vs old\
      \ on validation set\n5. Deployment: Canary → full rollout if better\n6. Monitoring:\
      \ Track new model performance\n\nContinuous Training:\n- Automated retraining\
      \ pipeline\n- Triggered daily/weekly\n- Example: Fraud detection retrains daily\
      \ on last 30 days\n- Tools: Airflow, Kubeflow Pipelines\n\nA/B Testing:\n- Old\
      \ model (control) vs new model (treatment)\n- Random assignment (50/50 or 95/5\
      \ canary)\n- Monitor: Online metrics (CTR, revenue, latency)\n- Statistical\
      \ significance: Two-proportion z-test\n- Duration: Run until significance or\
      \ 2 weeks\n"
    practice_questions:
      concepts:
      - question: Explain data drift vs concept drift. Provide examples and detection
          methods for each.
        answer: 'Data drift: Input distribution P(X) changes. Features shift but relationship
          to target stays same. Example: E-commerce user base shifts younger (age
          distribution changes), but young users'' purchase behavior same as historical.
          Detection: Compare feature distributions (training vs production). KL divergence:
          KL(P_prod || P_train). If KL > threshold, drift detected. Concept drift:
          Relationship P(Y|X) changes. Features same but meaning changes. Example:
          User clicks change (e.g., pandemic changes content preferences). Detection:
          Monitor online performance. If CTR drops from 2% to 1%, concept drift. Can''t
          detect offline (distribution of X unchanged). Action: Data drift → Retrain
          on recent data (new distribution). Concept drift → Retrain with new X-Y
          relationships.'
      - question: How do you design an alerting system for ML model degradation? What
          metrics and thresholds?
        answer: 'Metrics: 1) Online performance: CTR, conversion rate, revenue. 2)
          Prediction distribution: Avg fraud score, quantiles. 3) Data quality: Feature
          null rates, outlier rates. 4) System: Latency, error rate. Thresholds: 1)
          Performance: Alert if CTR drops 5% (relative) from baseline. 2) Prediction:
          Alert if fraud rate prediction > 10% (historical 1%). 3) Data: Alert if
          feature null rate > 5% (historical 1%). 4) System: Latency p99 > 200ms,
          error rate > 1%. Severity: P0 (immediate): Error rate > 10%, system down.
          P1 (1 hour): Performance drop 5-10%. P2 (1 day): Data quality issues. Actions:
          P0 → Rollback to old model. P1 → Investigate, prepare new model. P2 → Fix
          data pipeline.'
      - question: Explain continuous training for ML models. When is it necessary
          and what are the trade-offs?
        answer: 'Continuous training: Automated pipeline retrains model regularly
          (daily, weekly) on fresh data. When necessary: 1) Concept drift: User behavior
          changes rapidly (fraud, recommendations), 2) Seasonality: Patterns change
          (holidays, trends), 3) Fresh data: Recent data more predictive (news, finance).
          Pipeline: 1) Daily: Collect last 30 days labeled data, 2) Retrain: Spark
          job, 3) Evaluate: New model vs old on holdout, 4) Deploy: If new > old by
          1%, deploy via canary. Trade-offs: Pros: Stay current, adapt to drift. Cons:
          1) Cost: Daily retraining expensive (GPU hours), 2) Complexity: Automated
          pipeline needs monitoring, 3) Risk: New model may be worse (need safeguards).
          Recommendation: Start with weekly, move to daily only if drift severe.'
      tradeoffs:
      - question: For a recommendation system, how would you decide between retraining
          weekly vs daily? Consider drift, cost, and complexity.
        answer: 'Weekly retraining: Pros: 1) Cheaper (7× less compute), 2) Simpler
          (fewer deployments), 3) More stable (less noise). Cons: 1) Slower adaptation
          to trends, 2) May miss fast-changing preferences (viral content). Daily
          retraining: Pros: 1) Adapts quickly to trends, 2) Better for fast-changing
          platforms (TikTok). Cons: 1) 7× more expensive, 2) Complex (daily deployments),
          3) Risk of instability (noise in 1-day data). Decision factors: 1) Drift
          rate: A/B test weekly vs daily, measure CTR improvement. If daily +0.5%,
          worth it. 2) Platform: Slow-changing (Netflix movies): weekly. Fast-changing
          (TikTok): daily. 3) Cost: Startup with limited budget: weekly. FAANG scale:
          daily. Recommendation: Start weekly, monitor drift. If performance degrades
          mid-week, switch to daily.'
      scenarios:
      - question: You notice your fraud detection model's precision dropped from 90%
          to 70% over 2 weeks. Diagnose and fix.
        answer: 'Diagnosis: 1) Check data drift: Compare feature distributions (training
          vs last 2 weeks). KL divergence, histogram comparison. Likely: Fraudster
          tactics changed (new features). 2) Check prediction drift: Fraud rate jumped
          1% → 5% (model predicting more fraud). 3) Check concept drift: Precision
          drops → more false positives. Model not adapting. Root cause: Fraudsters
          using new tactics (e.g., stolen cards from new bank). Model trained on old
          patterns. Fix: 1) Immediate: Raise threshold (predict fraud if score > 0.9
          instead of 0.5) → fewer false positives, precision recovers to 85%. Accept
          lower recall. 2) Short-term (1 week): Retrain on last 30 days data (includes
          new fraud patterns). 3) Long-term: Continuous training (retrain weekly),
          add features (detect new patterns). Validation: A/B test new model vs old,
          measure precision, recall, fraud loss.'
    time_estimate: 60
    video_resources:
    - title: 'Chip Huyen: ML Monitoring and Observability'
      url: https://www.youtube.com/@chiphuyen
      duration: 22 min
      description: Detecting drift and model degradation
      priority: high
    - title: 'ByteByteGo: ML Ops Best Practices'
      url: https://www.youtube.com/@ByteByteGo
      duration: 14 min
      description: Model retraining pipelines
      priority: high
  week4:
  - day: 13
    topic: Responsible AI - Fairness, Bias, and Explainability
    activity: Address fairness constraints, bias detection, and interpretability for
      ML systems in production
    detailed_content: "Fairness in ML:\nProblem: Models can discriminate against protected\
      \ groups (gender, race)\n\nFairness Metrics:\n1. Demographic Parity: P(ŷ=1|A=a)\
      \ = P(ŷ=1|A=b)\n   - Equal positive prediction rate across groups\n2. Equal\
      \ Opportunity: P(ŷ=1|Y=1,A=a) = P(ŷ=1|Y=1,A=b)\n   - Equal true positive rate\
      \ (recall) across groups\n3. Equalized Odds: Equal TPR and FPR across groups\n\
      4. Calibration: P(Y=1|ŷ=p,A=a) = p for all groups\n   - Predicted probabilities\
      \ are accurate for all groups\n\nBias Sources:\n- Historical bias: Training\
      \ data reflects past discrimination\n- Measurement bias: Features measured differently\
      \ for groups\n- Aggregation bias: Model assumes all groups are same\n- Evaluation\
      \ bias: Metrics don't capture fairness\n\nBias Mitigation:\n1. Pre-processing:\n\
      \   - Reweigh: Weight training samples to balance groups\n   - Sampling: Oversample\
      \ minority group\n   - Remove biased features: Drop gender, race (but proxies\
      \ remain)\n2. In-processing:\n   - Fairness constraints: Add penalty for disparate\
      \ impact\n   - Adversarial debiasing: Train to be fair and accurate\n3. Post-processing:\n\
      \   - Threshold optimization: Different thresholds per group\n   - Equalized\
      \ odds post-processing\n\nExplainability:\nWhy: Regulators, users want to understand\
      \ model decisions\n\nModel-Agnostic Methods:\n1. SHAP (Shapley Additive Explanations):\n\
      \   - Game-theoretic feature attribution\n   - Global: Which features most important\
      \ overall?\n   - Local: Why this prediction for this user?\n2. LIME (Local Interpretable\
      \ Model-agnostic Explanations):\n   - Approximate model locally with linear\
      \ model\n   - Explain individual predictions\n3. Permutation Importance:\n \
      \  - Shuffle feature, measure performance drop\n   - Important features cause\
      \ large drop\n\nModel-Specific Methods:\n- Linear models: Coefficient = feature\
      \ importance\n- Tree models: Gini importance, split frequency\n- Neural networks:\
      \ Attention weights, saliency maps\n\nInterpretable Models:\n- Linear regression,\
      \ logistic regression: Coefficients\n- Decision trees: Rules (max depth 5-7)\n\
      - Rule-based models: If-then rules\n- Generalized Additive Models (GAM): Sum\
      \ of univariate functions\n\nTrade-off: Accuracy vs Interpretability\n- Interpretable:\
      \ Linear, shallow trees (80-85% accuracy)\n- Black box: Neural networks, deep\
      \ trees (90-95% accuracy)\n- Decision: Regulated domain → interpretable\n\n\
      Privacy-Preserving ML:\n- Differential Privacy: Add noise to gradients\n  *\
      \ Guarantee: No single user affects model by more than ε\n- Federated Learning:\
      \ Train on devices, aggregate gradients\n  * Data never leaves devices\n- Secure\
      \ Multi-Party Computation: Encrypted training\n\nResponsible AI Checklist:\n\
      1. Audit training data for bias\n2. Measure fairness metrics (demographic parity,\
      \ equal opportunity)\n3. Mitigate bias (reweighing, constraints, thresholds)\n\
      4. Provide explanations (SHAP, LIME)\n5. Monitor fairness in production\n6.\
      \ Human review for high-stakes decisions\n"
    practice_questions:
      concepts:
      - question: Explain demographic parity vs equalized odds. Which is more appropriate
          for hiring?
        answer: 'Demographic parity: P(ŷ=1|A=a) = P(ŷ=1|A=b). Equal acceptance rate
          regardless of group (gender, race). Example: 10% of women hired, 10% of
          men hired. Equalized odds: P(ŷ=1|Y=1,A=a) = P(ŷ=1|Y=1,A=b). Equal TPR and
          FPR across groups. If qualified, equal chance of being hired. For hiring:
          Equalized odds better. Demographic parity may hire unqualified candidates
          to meet quota. Equalized odds: Qualified candidates treated equally, fair.
          But: Hard to define ''qualified'' (Y=1). Labels may be biased. Best: Combine
          with human review.'
      - question: How does SHAP provide local explanations for model predictions?
          Compare to LIME.
        answer: 'SHAP: Game-theoretic approach. For prediction f(x), compute contribution
          of each feature x_i. Shapley value: φ_i = Avg over all subsets S of features:
          [f(S ∪ {i}) - f(S)]. Measures marginal contribution. Properties: Additive
          (Σφ_i = f(x) - f(∅)), consistent. LIME: Approximate f locally with linear
          model g. Sample points near x, weight by proximity, fit g. Coefficients
          = feature importance. Compare: SHAP: Theoretically grounded, slow (exponential
          subsets), global + local. LIME: Fast, heuristic, local only. Use SHAP: Need
          rigorous explanations (healthcare, finance). Use LIME: Need fast approximations.
          Both: Model-agnostic, work with any black-box model.'
      - question: What is differential privacy and why is it important for ML? Explain
          the ε parameter.
        answer: 'Differential privacy: Guarantee that model trained on dataset D vs
          D'' (differ by one user) produces similar outputs. Formally: P(M(D) ∈ S)
          ≤ e^ε * P(M(D'') ∈ S) for all outputs S. Meaning: Observing model output
          reveals little about individual user. ε controls privacy: Small ε (0.1):
          Strong privacy, large noise. Large ε (10): Weak privacy, small noise. Implementation:
          Add Gaussian noise to gradients during training. Noise ~ N(0, σ²) where
          σ depends on ε. Why important: Protects user data (GDPR, CCPA). Model can''t
          memorize specific users (prevent inference attacks). Trade-off: Privacy
          (low ε) vs accuracy (high ε). Google uses ε=8 for federated learning (moderate
          privacy).'
      tradeoffs:
      - question: For a credit scoring model, compare using logistic regression (interpretable)
          vs XGBoost (accurate). Consider fairness, explainability, and regulations.
        answer: 'Logistic Regression: Accuracy: 75%. Interpretability: Coefficients
          directly show feature importance (income → +0.5, debt → -0.3). Fairness:
          Easy to audit (remove biased features, check coefficients). Regulations:
          FCRA requires explanations for rejections → LR provides clear reasons. XGBoost:
          Accuracy: 82% (+7%). Interpretability: Black box, need SHAP for explanations
          (slower, less clear). Fairness: Harder to audit, may capture bias in complex
          interactions. Regulations: Can provide SHAP values but less transparent.
          Decision: Start with LR (regulatory requirement, interpretable). If 7% accuracy
          is critical (reduces default losses significantly), use XGBoost + SHAP.
          Prepare explanations for regulators. Hybrid: Use LR for most applications,
          XGBoost for high-risk loans (with human review).'
      scenarios:
      - question: You're building a hiring screening model and find it has 20% lower
          acceptance rate for women than men (demographic parity violated). Diagnose
          and fix.
        answer: 'Diagnosis: 1) Check historical data: Was past hiring biased (fewer
          women hired)? If yes, model learned bias. 2) Check features: Do features
          correlate with gender (e.g., ''years experience'' if women take maternity
          leave)? 3) Check labels: Are ''qualified'' labels biased (male interviewers
          rated women lower)? Root cause: Training data reflects historical discrimination.
          Fix: 1) Pre-processing: Reweigh training data (women''s positive samples
          weighted 2×). 2) In-processing: Add fairness constraint to loss: L = L_accuracy
          + λ*L_fairness where L_fairness penalizes disparate impact. 3) Post-processing:
          Set different thresholds (men: 0.5, women: 0.4) to equalize acceptance rates.
          4) Long-term: Relabel data (unbiased review), collect more representative
          data. Validation: Measure demographic parity, equal opportunity. A/B test
          fairness-aware model. Ensure accuracy drop <5%. Human review for borderline
          cases.'
    time_estimate: 60
    video_resources:
    - title: 'Chip Huyen: ML Fairness and Bias'
      url: https://www.youtube.com/@chiphuyen
      duration: 20 min
      description: Responsible AI practices
      priority: high
    - title: 'Two Minute Papers: Explainable AI'
      url: https://www.youtube.com/@TwoMinutePapers
      duration: 8 min
      description: SHAP and interpretability
      priority: medium
  - day: 14
    topic: ML System Design Practice - End-to-End Case Studies
    activity: 'Practice complete ML system designs: YouTube recommendations, Uber
      ETA prediction, Airbnb search ranking'
    detailed_content: "System Design Framework (MADE):\nM - Model: What ML model to\
      \ use?\nA - API: How users interact with system?\nD - Data: What data to collect,\
      \ features, labels?\nE - Evaluation: Offline and online metrics?\n\nCase Study\
      \ 1: YouTube Video Recommendations\n\nRequirements:\n- 2B users, 500M videos\n\
      - Personalized homepage recommendations\n- Latency < 200ms\n\nDesign:\n1. Candidate\
      \ Generation (50ms):\n   - User embedding (128-dim) from watch history\n   -\
      \ ANN search on video embeddings → 1000 candidates\n   - Sources: CF, two-tower,\
      \ trending\n\n2. Ranking (100ms):\n   - Features: User, video, context, cross\n\
      \   - Model: Multi-task (predict watch time, CTR, like)\n   - XGBoost or DNN,\
      \ batch inference\n\n3. Re-ranking (50ms):\n   - Diversity (MMR): Avoid similar\
      \ videos\n   - Freshness: Boost recent uploads\n   - Business rules: Ad frequency\n\
      \nData:\n- Logs: Watch events (video, duration, like/dislike)\n- Features: User\
      \ demographics, video metadata\n- Labels: Watch time, engagement\n\nEvaluation:\n\
      - Offline: Precision@K, NDCG, AUC\n- Online: Watch time per session, retention\n\
      \nCase Study 2: Uber ETA Prediction\n\nRequirements:\n- Predict arrival time\
      \ for ride\n- Latency < 1s, high accuracy (within 2 min)\n\nDesign:\n1. Model:\
      \ Gradient boosting (XGBoost)\n   - Why: Tabular data, need interpretability\n\
      \n2. Features:\n   - Route: Distance, # turns, highway %\n   - Traffic: Real-time\
      \ traffic speed\n   - Historical: Avg time for route at time-of-day\n   - Driver:\
      \ Recent speeds\n   - Weather: Rain, snow\n\n3. Training:\n   - Labels: Actual\
      \ arrival time\n   - Data: Historical rides (1B trips)\n   - Retrain: Daily\
      \ on last 30 days\n\n4. Serving:\n   - Real-time: Lookup features, model inference\n\
      \   - Latency: 50ms\n\nEvaluation:\n- Offline: MAE, RMSE, % within 2 min\n-\
      \ Online: Actual error, user complaints\n\nCase Study 3: Airbnb Search Ranking\n\
      \nRequirements:\n- Search query → rank listings\n- Personalized, diverse results\n\
      \nDesign:\n1. Candidate Generation:\n   - Geo filtering: Listings in location\n\
      \   - Filters: Price, beds, amenities\n   - ~1000 candidates\n\n2. Ranking:\n\
      \   - Features: User-listing affinity, price, reviews\n   - Model: LambdaMART\
      \ (learning to rank)\n   - Objective: Bookings\n\n3. Re-ranking:\n   - Diversity:\
      \ Mix price ranges, neighborhoods\n   - Personalization: Boost user preferences\n\
      \nEvaluation:\n- Offline: NDCG\n- Online: Booking rate, revenue\n\nInterview\
      \ Tips:\n1. Clarify requirements: Scale, latency, metrics\n2. Start high-level:\
      \ Candidate → rank → re-rank\n3. Dive deep on one area (interviewer's interest)\n\
      4. Discuss trade-offs: Accuracy vs latency\n5. Estimate: Compute, storage, cost\n\
      6. Address failure modes: Cold start, drift\n"
    practice_questions:
      concepts:
      - question: Walk through the three-stage funnel for YouTube recommendations.
          Why not use a single end-to-end model?
        answer: 'Three stages: 1) Candidate Generation: 500M videos → 1K candidates
          (ANN search, 50ms). 2) Ranking: 1K → 500 (complex model, 100ms). 3) Re-ranking:
          500 → 50 (diversity, 50ms). Why not end-to-end: Latency. Complex model on
          500M videos = hours. Even simple model (dot product) on 500M = seconds.
          Solution: Stage 1 uses fast methods (ANN), Stage 2 uses accurate methods
          (DNN) on small set. Trade-off: Stage 1 may miss some relevant videos (99.9%
          reduction), but Stage 2 re-ranks well. Empirically: Three stages achieves
          90% of end-to-end quality at 200ms vs 10s latency.'
      - question: For Uber ETA prediction, why use XGBoost instead of a neural network?
          What are the advantages?
        answer: 'XGBoost advantages: 1) Tabular data: Features are structured (distance,
          traffic, weather), not images/text. XGBoost excels on tabular. 2) Interpretability:
          Feature importance (traffic=40%, distance=30%) helps debugging, stakeholder
          trust. 3) Small data (<1B samples): NNs need more data to outperform. 4)
          Fast training: Hours vs days for NN. 5) Fast inference: <10ms on CPU. NN
          disadvantages: Need GPU, hyperparameter tuning complex, black box. When
          to use NN: If >10M samples and accuracy critical, can try. But XGBoost baseline
          is strong. Empirical: XGBoost achieves 90% accuracy, NN 91% (+1%) at 10×
          training cost. Not worth it.'
      - question: How would you handle cold start for new listings in Airbnb search
          ranking?
        answer: 'Cold start: New listing has no bookings → no collaborative signal.
          Solutions: 1) Content-based: Use listing features (price, location, photos,
          description). Similar to popular listings in area. 2) Host history: If host
          has other listings, use their booking patterns. 3) Geographic: Use avg performance
          of listings in same neighborhood. 4) Exploration: Show new listings to small
          % of users (epsilon-greedy), collect data. 5) Hybrid: Combine content +
          CF. Start with content-based (first week), gradually incorporate CF as bookings
          accumulate. Implementation: For listings with <10 bookings, use content-based
          score. For >10 bookings, blend: 0.7*CF + 0.3*content. After 100 bookings,
          use pure CF.'
      scenarios:
      - question: Design a spam detection system for Gmail (1B users, 100M emails/day).
          Discuss model, features, latency, and accuracy trade-offs.
        answer: 'Requirements: 1B users, 100M emails/day = 1.2K emails/sec, <100ms
          latency, 99% precision (minimize false positives). Design: 1) Model: Two-stage.
          Stage 1: TF-IDF + Logistic Regression (10ms, 95% precision). Stage 2: BERT
          fine-tuned (50ms, 99% precision). 2) Features: Email content, sender reputation,
          headers, attachments. 3) Serving: Stage 1 filters 90% (obvious spam). Stage
          2 on remaining 10% (borderline cases). Total latency: 10ms + 5ms (10% ×
          50ms) = 15ms. 4) Scale: 1.2K emails/sec. Stage 1: CPU (c5.2xlarge, 100 QPS)
          → 12 instances. Stage 2: GPU (P3.2xlarge, 200 QPS on 10%) → 1 instance.
          Cost: $8/hour. 5) Data: Labels from user feedback (mark spam). Continuous
          training: Retrain weekly. 6) Evaluation: Offline: Precision, recall, F1.
          Online: User reports, false positive rate.'
      - question: You're building a real-time fraud detection system for Stripe (100K
          TPS). Walk through your design from data collection to serving.
        answer: 'Requirements: 100K transactions/sec, <100ms latency, 0.1% fraud rate.
          Design: 1) Data Collection: Transaction events (amount, merchant, user,
          device) → Kafka. 2) Feature Engineering: a) Real-time: # transactions in
          last 1h (Kafka Streams), b) Batch: User history, merchant risk (daily Spark
          job). 3) Feature Store: Redis for fast lookup (<10ms). 4) Model: XGBoost
          (10ms inference on CPU). Features: 100 (transaction, user, velocity, graph).
          5) Serving: a) Lookup features from Redis (10ms), b) XGBoost inference (10ms),
          c) Rules engine (5ms, flag high-risk). Total: 25ms. 6) Scale: 100K TPS →
          need 2500 QPS per instance → 40 instances (c5.2xlarge). Cost: $16/hour =
          $11K/month. 7) Labels: Feedback from investigations, chargebacks (delayed).
          8) Retraining: Daily on last 30 days. 9) Monitoring: Fraud loss, false positive
          rate, latency.'
    time_estimate: 90
    video_resources:
    - title: 'Chip Huyen: ML System Design Interview'
      url: https://www.youtube.com/@chiphuyen
      duration: 30 min
      description: End-to-end case study walkthrough
      priority: high
    - title: 'ByteByteGo: Design ML-Powered System'
      url: https://www.youtube.com/@ByteByteGo
      duration: 20 min
      description: Comprehensive system design approach
      priority: high
