\documentclass[11pt,letterpaper]{article}

% Packages
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{tcolorbox}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{fontawesome5}

% Colors
\definecolor{robloxred}{RGB}{227,28,42}
\definecolor{highlight}{RGB}{255,245,230}
\definecolor{codegreen}{RGB}{0,128,0}
\definecolor{codegray}{RGB}{128,128,128}

% Header/Footer
\pagestyle{fancy}
\fancyhf{}
\lhead{DataHub Interview Day 2 Prep}
\rhead{Alex Yang}
\cfoot{\thepage}

% Hyperlink setup
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    urlcolor=blue,
    citecolor=blue
}

% Custom boxes
\newtcolorbox{keyinsight}{
    colback=highlight,
    colframe=robloxred,
    fonttitle=\bfseries,
    title=\faLightbulb\ Key Insight
}

\newtcolorbox{criticalpoint}{
    colback=red!5!white,
    colframe=red!75!black,
    fonttitle=\bfseries,
    title=\faExclamationTriangle\ Critical Point
}

\newtcolorbox{actionitem}{
    colback=green!5!white,
    colframe=green!75!black,
    fonttitle=\bfseries,
    title=\faCheckCircle\ Action Item
}

% Title
\title{\textbf{DataHub Day 2 Interview Preparation}\\
\large Project Deep Dive: Game Clickbait Detection System}
\author{Alex Yang\\
Principal Software Engineer, Roblox}
\date{Interview Date: November 20, 2024\\
Prepared: November 19, 2024}

\begin{document}

\maketitle
\tableofcontents
\newpage

% ============================================================
\section{Interview Overview}
% ============================================================

\subsection{Format}
\begin{itemize}
    \item \textbf{Interviewer:} Abe (DataHub)
    \item \textbf{Duration:} 45-60 minutes
    \item \textbf{Part 1:} Project Deep Dive (25-30 min)
    \item \textbf{Part 2:} Search Architecture Discussion (20-25 min)
\end{itemize}

\subsection{Selected Project}
\textbf{Game Clickbait Detection System} (Q1 2024, Roblox)

\textbf{Why this project:}
\begin{itemize}[leftmargin=*]
    \item Complete end-to-end ownership (problem definition -> architecture -> deployment)
    \item High-visibility impact (CEO escalation -> townhall recognition)
    \item Cross-team leadership (5 teams: DSA, Human Eval, Brand Safety, ML Platform, Game Discovery)
    \item Systems thinking (IP protection, search quality, policy framework)
    \item Measurable success (99\%+ precision, real-time detection)
    \item Recent and fresh (2024)
\end{itemize}

% ============================================================
\section{30-Second Executive Summary}
% ============================================================

\begin{criticalpoint}
\textbf{Practice delivering this in 30 seconds:}

"In Q1 2024, our CEO escalated an urgent search quality issue: copycat games with identical names and images polluted results when searching for popular games like 'Brookhaven'. A team had spent 2 months on a CLIP-based image similarity approach, but when I evaluated it, precision was near-random.

I identified the core issue within days, threw away the previous work, and redesigned from first principles. The result: a real-time, multi-modal IP protection system with 99\%+ precision that processes game updates in 10 seconds, protects hundreds of assets across billions of games, and was recognized at company townhall."
\end{criticalpoint}

% ============================================================
\section{Project Narrative (STAR Framework)}
% ============================================================

\subsection{Situation (2-3 minutes)}

\subsubsection{The Problem}
\textbf{CEO Observation (Holiday 2023):}
\begin{itemize}
    \item Searching "Brookhaven" (top Roblox game) returned pages of copycat games
    \item Copycats used identical/similar names and thumbnail images
    \item Search results looked "ugly" and hurt platform trust
    \item Users clicking copycats experienced low-quality, unrelated gameplay
\end{itemize}

\subsubsection{Previous Team's Approach (2 months of work)}
\textbf{Method:} CLIP-based image similarity detection
\begin{enumerate}
    \item Pipeline to get top 100 popular games
    \item Extract game thumbnail images
    \item Generate CLIP embeddings for all game thumbnails (daily/weekly)
    \item Find games with thumbnails semantically close to top 100
    \item Demote these games in ranking
\end{enumerate}

\textbf{My Evaluation (within days):}
\begin{itemize}
    \item Ran human evaluation on their results with different thresholds (0.9, 0.95)
    \item \textbf{Result: Precision near-random ($\sim$50\%)}
    \item Many false positives (legitimate games with similar art styles)
    \item Many false negatives (modified images evaded detection)
    \item \textbf{Critical flaw: Completely ignored title duplication signal}
\end{itemize}

\begin{keyinsight}
\textbf{Why the previous approach failed:}
\begin{itemize}
    \item Treated as pure image similarity problem
    \item CLIP clusters by visual style, not deceptive intent
    \item False positives: Anime-style games, similar genres (tycoon, obby)
    \item False negatives: Color shifts, crops, overlays evade detection
    \item Ignored strongest signal: title text copying
\end{itemize}
\end{keyinsight}

\subsubsection{Problem Reframing (First Principles)}

\textbf{Key question I asked:} "Why is this even a problem?"

If authentic Brookhaven ranks \#1, users can identify it. So what are we \textit{really} solving?

\textbf{Two distinct problems identified:}

\begin{enumerate}
    \item \textbf{IP Protection (Primary)}
    \begin{itemize}
        \item Platform responsibility to protect creator intellectual property
        \item Games stealing name + image = trademark/copyright violation
        \item Roblox's reputation as fair UGC platform at stake
    \end{itemize}
    
    \item \textbf{Search Quality (Secondary)}
    \begin{itemize}
        \item Copycats are typically low-quality (quick cash grabs)
        \item Crowd out higher-quality, diverse alternatives
        \item Users want: original game (\#1) + quality similar games, NOT 50 low-effort clones
    \end{itemize}
\end{enumerate}

\textbf{Design Principle:} Multi-signal detection for IP violations, not just image matching

\newpage
\subsection{Task (1-2 minutes)}

\subsubsection{My Responsibilities}
\begin{itemize}
    \item \textbf{Technical architecture:} Design end-to-end detection system
    \item \textbf{Cross-team coordination:} Align 5 teams on approach and deliverables
    \item \textbf{Policy framework:} Define what constitutes IP violation
    \item \textbf{Implementation:} Build real-time detection service and offline backfill
    \item \textbf{Evaluation:} Establish metrics and validation pipeline
    \item \textbf{Deployment:} Launch to production with A/B testing
\end{itemize}

\subsubsection{Constraints}
\begin{itemize}
    \item \textbf{Time:} CEO urgency - need results in Q1 2024
    \item \textbf{Scale:} Hundreds of millions of existing games to process
    \item \textbf{Latency:} Real-time updates (<1 minute from game publish to ranking impact)
    \item \textbf{Precision:} Must avoid false positives (creator backlash risk)
    \item \textbf{Coverage:} Top 30 games covers 20-30\% of search queries (Pareto principle)
\end{itemize}

\subsubsection{Success Criteria}
\begin{itemize}
    \item 95\%+ precision on clickbait detection
    \item Real-time detection (<1 min latency)
    \item Backfill all existing games
    \item Zero creator backlash (clear, fair policies)
    \item CEO satisfaction (visible problem resolution)
\end{itemize}

\newpage
\subsection{Action (5-7 minutes)}

\subsubsection{Core Innovation: Protected Assets Library (PAL)}

\textbf{Concept:} Centralized registry of IP assets to protect

\textbf{Schema:}
\begin{verbatim}
Field: type        (icon, title, logo, etc.)
Field: creatorId   
Field: universeId  
Field: rootPlaceId
Field: titleMatchPolicy (for title assets: exact, substring, fuzzy)
\end{verbatim}

\textbf{Violation Logic:} Any signal triggered -> demotion in ranking

\textbf{Three Asset Types:}
\begin{itemize}
    \item \textbf{Signal A:} Text (game names)
    \item \textbf{Signal B:} Images (full thumbnails)
    \item \textbf{Signal C:} Logos (brand marks within images)
\end{itemize}

\subsubsection{Signal A: Text Protection (Rule-Based Heuristics)}

\textbf{Why rule-based, not semantic similarity?}
\begin{itemize}
    \item "Microhard" doesn't violate "Microsoft" IP (semantic but not legal violation)
    \item Need legal precision, not fuzzy matching
    \item Semantic similarity failed in evaluation (no practical utility)
    \item Heuristics achieved 99\%+ precision on golden dataset
\end{itemize}

\textbf{Three Match Policies (Flexible Framework):}

\begin{enumerate}
    \item \textbf{exact:} Strict 1:1 match
    \begin{itemize}
        \item Use case: "Evade" (common English word, don't over-protect)
        \item Normalization: lowercase, remove spaces, strip emojis/brackets
    \end{itemize}
    
    \item \textbf{substring:} Contains match
    \begin{itemize}
        \item Use case: "Welcome to Bloxburg" prevents "Welcome to Bloxburg Christmas!"
        \item Protects against title stuffing: "BrookhavenRP Adopt Me! Pet Simulator"
    \end{itemize}
    
    \item \textbf{fuzzy:} Edit distance tolerance
    \begin{itemize}
        \item Use case: "Brookhaven" with fuzzy=2 catches "bro0khaven", "Br00khaven"
        \item Prevents simple evasion tactics
    \end{itemize}
\end{enumerate}

\textbf{Built-in Exceptions (Nuanced Policy):}
\begin{itemize}
    \item \textbf{Fan-made allowed:} "Brookhaven RP [Fan Version]" NOT clickbait
    \item \textbf{Same creator/group:} Can create own spinoffs (ownership check)
    \item \textbf{Typo tolerance:} "Fan Versoin" still triggers (prevents evasion)
\end{itemize}

\textbf{Cross-functional Work:}
\begin{itemize}
    \item Collaborated with Community Program Manager on policy drafting
    \item Community announcement rollout (several weeks)
    \item Clear creator communication through official channels
    \item \textbf{Result:} Zero creator backlash
\end{itemize}

\textbf{Impact:} Solved $\sim$30\% of clickbait cases

\subsubsection{Signal B: Image Protection (CLIP + PAL)}

\textbf{Key Difference from Previous Approach:}
\begin{itemize}
    \item \textbf{Previous:} Compare ALL games to top 100 (noisy, high false positive rate)
    \item \textbf{Mine:} Compare only against curated PAL library (precise, targeted)
\end{itemize}

\textbf{Implementation:}
\begin{itemize}
    \item Human Eval team curates protected assets using Label Studio
    \item CLIP embeddings generated for full thumbnails
    \item Embeddings stored in RSS (Roblox Similarity Search - vector database)
    \item Fine-tuned similarity thresholds per PAL entry
    \item Monthly pipeline reviews top 30 games + ad-hoc updates
\end{itemize}

\textbf{PAL Evolution:}
\begin{itemize}
    \item Started: $\sim$30-100 entries (top games)
    \item Current: Hundreds of entries
    \item Planned: External brands (Disney, Coca-Cola, etc.)
\end{itemize}

\textbf{Precision:} 95\%+ on image-based clickbait

\subsubsection{Signal C: Logo Protection (YOLOv8 + CLIP)}

\textbf{Two-Stage Detection Pipeline:}

\begin{enumerate}
    \item \textbf{Stage 1: YOLOv8 Logo Detection (Fine-tuned by me)}
    \begin{itemize}
        \item Detect logo regions within game thumbnails
        \item Training: Human Eval drew bounding boxes in Label Studio
        \item Fine-tuned YOLOv8 on Roblox-specific logo corpus
        \item Accuracy: 95\%+
    \end{itemize}
    
    \item \textbf{Stage 2: CLIP Logo Comparison}
    \begin{itemize}
        \item Extract detected logo regions
        \item Generate CLIP embeddings for logos
        \item Compare against PAL logo library (vector search)
    \end{itemize}
\end{enumerate}

\textbf{Cross-Team Integration:}
\begin{itemize}
    \item Leveraged Brand Safety team's logo detection endpoint
    \item Established versioning protocol for future model updates:
    \begin{itemize}
        \item Old endpoint maintained
        \item New versioned endpoints for A/B testing
        \item Enables safe model migrations
    \end{itemize}
\end{itemize}

\textbf{Combined Precision:} 99.5\%+ on logo-based clickbait

\subsubsection{Real-Time Detection Architecture}

\textbf{Design Shift:}
\begin{itemize}
    \item \textbf{Previous:} Weekly batch processing (stale, reactive)
    \item \textbf{Mine:} Event-driven, real-time (10-second latency)
\end{itemize}

\textbf{System Architecture:}

\begin{verbatim}
SNS Topics (Game Lifecycle Events):
  - PRODUCTION_PlaceEntityChangeEvents
  - PRODUCTION_UniverseDisplayInformationChangeEvents
  - PRODUCTION_UniverseEntityChangeEvents
          |
    SQS Queue (~10 QPS)
          |
  Queue Processor Service
          |
Clickbait Detection Service (bedev2 microservice)
  |-- Text rule matching (heuristics)
  |-- CLIP embedding generation -> RSS query
  \-- YOLO logo detection -> Brand Safety endpoint
          |
Frost Feature Store
  Table: sdp_production_nonpii.clickbait_games_features_v0
  Columns: universe_id, is_clickbait, metadata (JSON), updated_time_unix
          |
Experience Document Builder (SNS topic)
          |
Elasticsearch Index (for ranking)
          |
Search Retrieval & Ranking Pipeline
\end{verbatim}

\textbf{Key Design Decisions:}
\begin{itemize}
    \item \textbf{Event-driven:} Triggered by game lifecycle, not polling (efficient)
    \item \textbf{SQS buffering:} Handles traffic spikes, decouples services (resilient)
    \item \textbf{Microservice:} Modular CLIP/YOLO inference (scalable)
    \item \textbf{Feature serving:} Signals propagate to ranking in $\sim$10 seconds (fast)
    \item \textbf{Non-blocking:} Doesn't delay game creation/updates (user-friendly)
\end{itemize}

\textbf{API Features:}
\begin{itemize}
    \item REST endpoints for on-demand detection
    \item Manual override API for edge cases (parodies, legitimate references)
    \item JSON metadata field for detailed signal breakdown
\end{itemize}

\subsubsection{Offline Backfill Pipeline}

\textbf{Challenge:} Process hundreds of millions of existing games

\textbf{Solution: Modular MLP Pipelines}

\begin{enumerate}
    \item \textbf{Separate Pipeline per Feature} (title, thumbnail, logo)
    \begin{itemize}
        \item Load games corpus from Hive
        \item Load PAL
        \item Run similarity detection (text heuristics or vector search)
        \item Output to intermediate Hive tables
    \end{itemize}
    
    \item \textbf{Powerhouse Aggregation Pipeline}
    \begin{itemize}
        \item Load input tables: \texttt{sdp\_icon\_similarity}, \texttt{sdp\_title\_similarity}
        \item Aggregate scores into boolean \texttt{is\_clickbait} column
        \item Publish batch to Frost feature store
    \end{itemize}
\end{enumerate}

\textbf{Benefits of Modular Design:}
\begin{itemize}
    \item Scale compute independently per feature
    \item Iterate on one feature without reprocessing all
    \item Recover from failures without full reruns
    \item Clear separation of concerns (detection vs aggregation)
\end{itemize}

\subsubsection{Cross-Team Collaboration}

\textbf{Five Teams Coordinated:}

\begin{enumerate}
    \item \textbf{DSA (Jinlong Ji)}
    \begin{itemize}
        \item Owned initial backfill pipeline architecture
        \item Top-K game collection logic
        \item Hive table management
    \end{itemize}
    
    \item \textbf{Human Eval (Patricia Morales)}
    \begin{itemize}
        \item Created PAL using Label Studio
        \item Drew logo bounding boxes for YOLOv8 training
        \item Ongoing evaluation pipeline (human-in-the-loop)
    \end{itemize}
    
    \item \textbf{Brand Safety Team}
    \begin{itemize}
        \item Provided logo detection endpoint
        \item Versioning protocol for future model updates
    \end{itemize}
    
    \item \textbf{ML Platform Team}
    \begin{itemize}
        \item Inference infrastructure (CLIP/YOLO serving)
        \item Model deployment support
    \end{itemize}
    
    \item \textbf{Game Discovery Team}
    \begin{itemize}
        \item Search ranking integration
        \item Final ownership post-launch
        \item Ongoing policy iteration
    \end{itemize}
\end{enumerate}

\textbf{Key Coordination Challenge:}

\textit{Handling abandonment of 2 months of work:}
\begin{itemize}
    \item Presented evaluation results showing near-random precision
    \item Data spoke for itself - approach wasn't salvageable
    \item Principal ML Engineer moved to other priorities (no friction)
    \item Key: Show problem clearly + propose concrete alternative (not just criticize)
\end{itemize}

\newpage
\subsection{Result (1-2 minutes)}

\subsubsection{Metrics}

\textbf{Precision/Recall:}
\begin{itemize}
    \item \textbf{Previous approach:} $\sim$50\% precision (near-random)
    \item \textbf{Text rules:} 99\%+ precision, solved 30\% of cases
    \item \textbf{CLIP (full image):} 95\%+ precision
    \item \textbf{YOLO + CLIP (logos):} 99.5\%+ precision
\end{itemize}

\textbf{Scale:}
\begin{itemize}
    \item \textbf{PAL:} Hundreds of protected assets
    \item \textbf{Coverage:} 20-30\% of top search queries
    \item \textbf{Backfill:} Hundreds of millions of existing games processed
    \item \textbf{Real-time:} $\sim$10 QPS game updates, $\sim$10-second latency end-to-end
\end{itemize}

\textbf{Business Impact:}
\begin{itemize}
    \item \textbf{CEO satisfaction:} Problem visibly resolved for top queries
    \item \textbf{Townhall recognition:} Company-wide acknowledgment
    \item \textbf{Policy framework:} Established IP protection standards for platform
    \item \textbf{Expandable:} Foundation for other fraud detection (bait-and-switch teleport games)
    \item \textbf{Zero creator backlash:} Clear communication, fair policies
\end{itemize}

\subsubsection{Timeline}

\begin{itemize}
    \item \textbf{Week 1:} Identified previous approach failure, proposed redesign
    \item \textbf{Milestone 1 (March 1):} Title text protection prod launch + A/B
    \item \textbf{Milestone 2 (March 15):} Thumbnail protection prod launch
    \item \textbf{Milestone 3 (March 22):} Logo protection prod launch
    \item \textbf{Q1 2024 End:} Evaluation pipeline, optimization, handoff to Game Discovery
    \item \textbf{Ongoing:} Maintained and evolved system for several quarters
\end{itemize}

\subsubsection{Handoff}

\begin{itemize}
    \item Successfully transferred ownership to Game Discovery team
    \item Modular architecture enabled them to iterate on policies without rewriting core services
    \item Continued evolution: bait-and-switch detection, expanded PAL, etc.
\end{itemize}

\newpage
\subsection{Technical Challenges \& Lessons (2-3 minutes)}

\subsubsection{Challenge 1: Balancing Precision vs Coverage}

\textbf{Problem:} Can't manually curate PAL for millions of games

\textbf{Solution:}
\begin{itemize}
    \item Focus on top 30 games (Pareto principle: 20-30\% query coverage with minimal effort)
    \item Human-in-the-loop for high-profile additions
    \item Scalable pipeline for monthly reviews
\end{itemize}

\textbf{Lesson:} Perfect is the enemy of good - solve 80\% of the problem with 20\% of the effort

\subsubsection{Challenge 2: Real-Time Latency Requirements}

\textbf{Problem:} Game creation can't be blocked; search results must reflect changes quickly

\textbf{Solution:}
\begin{itemize}
    \item Async event processing (SQS decoupling)
    \item Non-blocking: Detection happens post-creation
    \item Acceptable staleness: 10-second propagation vs instant blocking
\end{itemize}

\textbf{Lesson:} Understand business constraints - near real-time (seconds) $\gg$ batch (days)

\subsubsection{Challenge 3: Fine-Tuning YOLOv8 for Logos}

\textbf{Problem:} Off-the-shelf YOLO detects generic objects, not game logos

\textbf{Solution:}
\begin{itemize}
    \item Created custom dataset with Human Eval (bounding boxes in Label Studio)
    \item Fine-tuned YOLOv8 on Roblox-specific logo corpus
    \item Achieved 95\%+ accuracy on domain-specific task
\end{itemize}

\textbf{Lesson:} Domain-specific ML requires domain-specific data - generic models aren't enough

\subsubsection{Challenge 4: Policy Rollout Without Creator Backlash}

\textbf{Problem:} New restrictions could anger creators (platform risk)

\textbf{Solution:}
\begin{itemize}
    \item Clear, fair rules (spinoff patterns allowed)
    \item Transparent communication (official community announcements)
    \item Gradual enforcement (education before penalties)
    \item Explainable heuristics ("Your title matches X with fuzzy=1" vs "Model score 0.87")
\end{itemize}

\textbf{Lesson:} Technical solutions need social/policy layer - engineering alone isn't enough

\subsubsection{Challenge 5: Defining "Clickbait" vs "Poor Experience"}

\textbf{Problem:} Initial discussions blurred clickbait (pre-game deception) with poor in-game quality

\textbf{My Framing:}
\begin{itemize}
    \item \textbf{Clickbait =} misleading \textit{pre-game signals} (title, thumbnail) regardless of in-game content
    \item \textbf{Poor experience =} separate problem requiring in-game scene analysis (future scope with RFM models)
\end{itemize}

\textbf{Why This Mattered:}
\begin{itemize}
    \item If evaluated based on in-game content, pre-game detection algorithms would falsely appear to fail
    \item Example: Game copies "Brookhaven" title/image but has transformative gameplay $\rightarrow$ still clickbait for IP protection
\end{itemize}

\textbf{Solution:}
\begin{itemize}
    \item Created golden dataset focused \textit{only} on pre-game signals
    \item Separated in-game content analysis to "poor experience detection" (out of scope for V1)
    \item Enabled clear algorithm evaluation and explainable enforcement
\end{itemize}

\textbf{Lesson:} Precise problem definition is foundational - ambiguity leads to misaligned evaluation

\subsubsection{What I'd Do Differently}

\begin{enumerate}
    \item \textbf{Earlier evaluation framework}
    \begin{itemize}
        \item Built golden dataset evaluation late
        \item Should've been day-one priority to validate approach faster
    \end{itemize}
    
    \item \textbf{More automated PAL maintenance}
    \begin{itemize}
        \item Top 30 games required manual monthly review
        \item Could've built automated pipeline detecting trending games for Human Eval queue
    \end{itemize}
    
    \item \textbf{Better internal documentation}
    \begin{itemize}
        \item Leadership didn't fully understand project complexity (part of reason for seeking new opportunities)
        \item Should've created exec-level summary decks: problem $\rightarrow$ solution $\rightarrow$ impact
    \end{itemize}
\end{enumerate}

\textbf{What Worked Well:}
\begin{itemize}
    \item Modular architecture paid off - Game Discovery could iterate on policies without rewriting core services
    \item Cross-team coordination was smooth due to clear work streams and ownership
    \item First principles thinking caught fundamental flaw early (saved months of wasted effort)
\end{itemize}

\newpage
% ============================================================
\section{Connection to DataHub}
% ============================================================

\begin{keyinsight}
\textbf{How This Project Maps to DataHub's Challenges:}

"This project taught me that \textbf{metadata quality} - whether it's game titles, thumbnails, or IP assets - is foundational to platform trust. At DataHub, metadata quality is the \textit{core product}.

I see direct parallels:

\begin{itemize}
    \item \textbf{Protected Assets Library (PAL)} $\approx$ \textbf{DataHub's metadata registry}
    \begin{itemize}
        \item Both are centralized sources of truth
        \item Both require curation, governance, and maintenance
        \item Both enable downstream systems (search ranking vs data discovery)
    \end{itemize}
    
    \item \textbf{Cross-team adoption} was key to my success at Roblox
    \begin{itemize}
        \item Got 5 teams (DSA, Human Eval, Brand Safety, ML Platform, Game Discovery) to trust and contribute to PAL
        \item DataHub faces same challenge: getting engineering, data science, and business teams to trust and contribute metadata
    \end{itemize}
    
    \item \textbf{Real-time propagation} (game updates $\rightarrow$ search ranking in 10 seconds)
    \begin{itemize}
        \item DataHub needs similar: schema changes $\rightarrow$ lineage updates $\rightarrow$ downstream awareness
        \item Event-driven architecture patterns directly applicable
    \end{itemize}
    
    \item \textbf{Policy framework} (exact/substring/fuzzy matching policies)
    \begin{itemize}
        \item Flexible policies > one-size-fits-all
        \item DataHub needs governance policies: who can edit what, approval workflows, data classification rules
    \end{itemize}
\end{itemize}

I'm excited about DataHub because it's solving the \textit{same fundamental problem} - metadata quality and governance - at the data infrastructure level rather than search/discovery."
\end{keyinsight}

% ============================================================
\section{Mock Q\&A Scenarios}
% ============================================================

\subsection{Technical Deep Dive Questions}

\subsubsection{Q: Why didn't you use a machine learning model for title detection?}

\textbf{Answer:}

"I evaluated both approaches. For title matching, heuristics significantly outperformed semantic similarity models for three reasons:

\begin{enumerate}
    \item \textbf{Explainability:} We needed to communicate to creators \textit{exactly why} they were flagged for policy compliance. 'Your title matches protected title X with fuzzy distance 1' is clear; 'model confidence 0.87' is not. This was critical for avoiding creator backlash.
    
    \item \textbf{Semantic similarity misalignment:} ML models would flag 'Zombie Night' as similar to 'Halloween Night' based on semantic meaning, but that's not IP violation. Clickbait is about exact/near-exact copying, not conceptual similarity.
    
    \item \textbf{Pattern recognition for evasion:} Common evasion tactics like 'title stuffing' - cramming multiple popular titles like 'BrookhavenRP Adopt Me! Pet Simulator' - are trivial to detect with substring rules but hard for models to learn without massive training data and constant retraining.
    
    \item \textbf{Empirical results:} When we evaluated semantic similarity on our golden dataset, it had 'no practical utility' - near-random precision. Heuristics achieved 99\%+ precision.
\end{enumerate}

We \textit{did} use ML where appropriate: CLIP for image similarity and YOLOv8 for logo detection, where embeddings and object detection excel. The key was choosing the right tool for each signal."

\subsubsection{Q: How did you handle the massive backfill of hundreds of millions of games?}

\textbf{Answer:}

"I designed separate MLP pipelines per feature - title, thumbnail, logo - so we could run them independently and iterate on one without reprocessing all. The architecture:

\begin{enumerate}
    \item \textbf{Per-Feature Pipelines:}
    \begin{itemize}
        \item Each loads games corpus from Hive
        \item Loads PAL for that feature type
        \item Runs similarity detection (text heuristics or vector search)
        \item Outputs to intermediate Hive tables: \texttt{sdp\_title\_similarity}, \texttt{sdp\_icon\_similarity}
    \end{itemize}
    
    \item \textbf{Powerhouse Aggregation Pipeline:}
    \begin{itemize}
        \item Aggregates scores from intermediate tables
        \item Publishes final \texttt{is\_clickbait} boolean to Frost in batch
    \end{itemize}
\end{enumerate}

This modular design had three benefits:

\begin{itemize}
    \item \textbf{Independent scaling:} Could scale compute per feature based on workload (CLIP inference vs simple string matching)
    \item \textbf{Fault tolerance:} If one pipeline failed, didn't need full rerun - just reprocess that feature
    \item \textbf{Iterability:} Could improve title matching heuristics without re-running CLIP embeddings on billions of images
\end{itemize}

We coordinated with ML Platform team to ensure adequate CLIP inference capacity for the backfill burst."

\subsubsection{Q: What metrics did you use to evaluate success?}

\textbf{Answer:}

"We had multi-level metrics across model performance, business impact, and operations:

\textbf{Model Performance (Precision/Recall):}
\begin{itemize}
    \item Golden dataset evaluation on 2000+ labeled examples
    \item Per-signal accuracy: Title (99\%), CLIP (95\%), YOLO+CLIP (99.5\%)
    \item Confusion matrix analysis to identify systematic errors
\end{itemize}

\textbf{Business Impact:}
\begin{itemize}
    \item CEO satisfaction: Problem visibly resolved for top queries like 'Brookhaven'
    \item Query coverage: Top 30 protected games covered 20-30\% of search queries (Pareto principle validated)
    \item PAL growth: Started with $\sim$30 entries, grew to hundreds (system scaling successfully)
    \item Creator feedback: Zero backlash (policy clarity worked)
\end{itemize}

\textbf{Operational Metrics:}
\begin{itemize}
    \item Real-time latency: P50/P99 for end-to-end detection
    \item Throughput: Sustained $\sim$10 QPS game updates without queue buildup
    \item Backfill completion: Time to process all existing games
    \item False positive rate: Manual override API usage as proxy
\end{itemize}

The combination gave us confidence in both technical correctness and business value."

\subsubsection{Q: How did you ensure high precision to avoid false positives?}

\textbf{Answer:}

"False positives were our biggest risk - flagging legitimate games would hurt creators and damage platform trust. I used a multi-layered approach:

\begin{enumerate}
    \item \textbf{Conservative Thresholds:}
    \begin{itemize}
        \item Started with stringent similarity thresholds (high confidence bar)
        \item Fine-tuned per PAL entry based on that asset's visual distinctiveness
        \item Used confusion matrix to optimize precision vs recall trade-off
    \end{itemize}
    
    \item \textbf{Human-in-the-Loop:}
    \begin{itemize}
        \item Label Studio pipeline for ongoing evaluation
        \item High-profile games get human review before PAL addition
        \item Feedback loop: flagged games reviewed weekly, errors fed back to threshold tuning
    \end{itemize}
    
    \item \textbf{Nuanced Policies:}
    \begin{itemize}
        \item Built-in exceptions: fan-made content, same creator ownership
        \item Flexible match policies: 'Evade' uses exact (common word), 'Brookhaven' uses fuzzy=2
        \item Manual override API for edge cases (parodies, legitimate references)
    \end{itemize}
    
    \item \textbf{Ranking Demotion, Not Filtering:}
    \begin{itemize}
        \item Demoted in ranking rather than hard-filtered
        \item Users still have choice; we just deprioritize suspected clickbait
        \item Reduced impact of false positives
    \end{itemize}
\end{enumerate}

The result: 99\%+ precision with zero creator backlash, which validated our approach."

\subsubsection{Q: Walk me through your collaboration with the Brand Safety team on logo detection.}

\textbf{Answer:}

"The Brand Safety team had an existing logo detection model that hadn't been updated since deployment. I needed to integrate it while planning for future model evolution. Here's how I approached it:

\textbf{Discovery Phase:}
\begin{itemize}
    \item Met with Brand Safety team to understand their model (YOLOv8-based)
    \item Identified constraint: model was static, no update cadence
    \item Learned their model detected generic logos, not game-specific ones
\end{itemize}

\textbf{Integration Strategy:}
\begin{itemize}
    \item Used their endpoint for initial logo region detection (bounding boxes)
    \item Fine-tuned my own YOLOv8 on Roblox-specific logo corpus (using Label Studio dataset)
    \item Two-stage pipeline: their model for generic detection $\rightarrow$ my model for game logo classification
\end{itemize}

\textbf{Future-Proofing:}
\begin{itemize}
    \item Established versioning protocol: if they update model, maintain old endpoint
    \item Create new versioned endpoint (\texttt{/v2/detect-logos})
    \item Enables A/B testing before migration
    \item Asked if they could incorporate PAL into their training (potential future improvement)
\end{itemize}

\textbf{Result:}
\begin{itemize}
    \item Smooth integration, 99.5\%+ precision on logo-based clickbait
    \item Reusable pattern for future cross-team ML integrations
    \item Brand Safety team appreciated the versioning protocol (adopted it for other consumers)
\end{itemize}

This taught me that cross-team ML integration requires planning for model evolution, not just initial integration."

\subsection{Systems Design Questions}

\subsubsection{Q: Why event-driven architecture for real-time detection? What are the trade-offs?}

\textbf{Answer:}

"I chose event-driven architecture for three reasons:

\textbf{Benefits:}
\begin{enumerate}
    \item \textbf{Efficiency:} Only process games that changed (vs polling all games periodically)
    \begin{itemize}
        \item Game updates are sparse events ($\sim$10 QPS vs millions of games)
        \item Polling would waste 99.9\% of compute checking unchanged games
    \end{itemize}
    
    \item \textbf{Low Latency:} React immediately to changes (10-second end-to-end)
    \begin{itemize}
        \item Users see updated ranking within seconds of game publish
        \item vs batch: hours/days of staleness
    \end{itemize}
    
    \item \textbf{Decoupling:} SQS buffer isolates detection service from upstream events
    \begin{itemize}
        \item Handles traffic spikes without dropping events
        \item Detection service can scale independently
        \item Failed processing goes to dead letter queue for recovery
    \end{itemize}
\end{enumerate}

\textbf{Trade-Offs:}
\begin{enumerate}
    \item \textbf{Complexity:} More moving parts (SNS, SQS, multiple services)
    \begin{itemize}
        \item Mitigation: Used mature AWS services with high availability
        \item Clear ownership boundaries between teams
    \end{itemize}
    
    \item \textbf{Eventual Consistency:} 10-second delay before ranking reflects changes
    \begin{itemize}
        \item Acceptable: game creation isn't blocked, creators don't see immediate impact anyway
        \item Much better than batch (hours/days)
    \end{itemize}
    
    \item \textbf{Ordering:} SQS doesn't guarantee FIFO (standard queue)
    \begin{itemize}
        \item Not a problem: we only care about final state, not order of updates
        \item Each update is idempotent (upsert to Frost by universe\_id)
    \end{itemize}
\end{enumerate}

For this use case, the benefits far outweighed the trade-offs. Event-driven was the right choice."

\subsubsection{Q: How would you handle a sudden 10x spike in game update traffic?}

\textbf{Answer:}

"The architecture has natural buffers, but I'd implement additional safeguards:

\textbf{Existing Resilience:}
\begin{itemize}
    \item SQS queue absorbs spikes (messages wait until processed)
    \item Detection service can scale horizontally (more bedev2 instances)
    \item Frost writes are async (doesn't block upstream)
\end{itemize}

\textbf{Additional Mitigations for 10x Spike:}

\begin{enumerate}
    \item \textbf{Rate Limiting at SQS:}
    \begin{itemize}
        \item Set max queue depth threshold (e.g., 100K messages)
        \item If exceeded, temporarily drop low-priority events (minor game updates)
        \item Prioritize new game creation events over minor metadata changes
    \end{itemize}
    
    \item \textbf{Auto-Scaling Detection Service:}
    \begin{itemize}
        \item CloudWatch alarm on SQS ApproximateNumberOfMessages
        \item Trigger auto-scaling policy: add bedev2 instances when queue depth > 10K
        \item Scale down when queue drains to save cost
    \end{itemize}
    
    \item \textbf{Batching at Frost:}
    \begin{itemize}
        \item Instead of per-message Frost write, batch 100 updates
        \item Reduces Frost write load by 100x
        \item Trade-off: slightly higher latency (batch flush interval)
    \end{itemize}
    
    \item \textbf{Circuit Breaker for Downstream:}
    \begin{itemize}
        \item If CLIP/YOLO inference services are overloaded, fail fast
        \item Requeue to SQS for retry with exponential backoff
        \item Prevents cascading failures
    \end{itemize}
    
    \item \textbf{Monitoring \& Alerting:}
    \begin{itemize}
        \item P99 latency alerts (if > 60 seconds, something's wrong)
        \item Queue depth alerts (if > 50K, need manual intervention)
        \item Dead letter queue alerts (if messages failing, investigate)
    \end{itemize}
\end{enumerate}

The key principle: \textbf{graceful degradation}. Accept slightly higher latency under extreme load, but never drop data or crash services."

\subsection{Project Management Questions}

\subsubsection{Q: How did you convince stakeholders to abandon 2 months of work?}

\textbf{Answer:}

"This was delicate - I had to show the problem clearly without making the previous team look bad. Here's how I approached it:

\begin{enumerate}
    \item \textbf{Data-First Approach (Week 1):}
    \begin{itemize}
        \item Immediately ran human evaluation on their CLIP results
        \item Tested multiple thresholds (0.9, 0.95) on Brookhaven case
        \item Result: precision $\sim$50\% (near-random)
        \item Documented: false positives (legitimate anime-style games), false negatives (modified images)
    \end{itemize}
    
    \item \textbf{Root Cause Analysis:}
    \begin{itemize}
        \item \textbf{Not:} "Your approach is wrong" (accusatory)
        \item \textbf{Instead:} "We're solving the wrong problem" (reframe)
        \item Showed CLIP clusters by visual style, not deceptive intent
        \item Identified missing signal: title duplication (strongest clickbait indicator)
    \end{itemize}
    
    \item \textbf{Concrete Alternative (Not Just Criticism):}
    \begin{itemize}
        \item Presented multi-signal architecture: text + image + logo
        \item Showed how each signal addresses specific failure modes
        \item Estimated timeline: M1 (title) in 3 weeks, full system in Q1
    \end{itemize}
    
    \item \textbf{Meeting Dynamics:}
    \begin{itemize}
        \item Invited Principal ML Engineer to the discussion (respect)
        \item Framed as "we discovered this together through evaluation"
        \item He acknowledged CLIP-only approach had limitations
        \item Moved to other priorities (no friction)
    \end{itemize}
    
    \item \textbf{Leadership Alignment:}
    \begin{itemize}
        \item CEO urgency gave me leverage (problem needs solving \textit{now})
        \item PM supported pivot after seeing evaluation data
        \item Committed to clear milestones (M1, M2, M3) for accountability
    \end{itemize}
\end{enumerate}

\textbf{Key Lesson:} When proposing to scrap work, lead with \textbf{data}, offer \textbf{concrete alternatives}, and \textbf{avoid blame}. Make it about the problem, not the people."

\subsubsection{Q: What was the biggest challenge in coordinating 5 teams?}

\textbf{Answer:}

"The biggest challenge was \textbf{aligning on scope and deliverables} - each team had different priorities and timelines. Here's what I did:

\textbf{Challenge 1: Diverging Priorities}
\begin{itemize}
    \item DSA team wanted comprehensive backfill (all games, all time)
    \item Human Eval had limited bandwidth (can't label millions of games)
    \item ML Platform focused on inference efficiency (latency optimization)
    \item Game Discovery wanted fast deployment (CEO pressure)
\end{itemize}

\textbf{Solution: Clear Work Streams}
\begin{itemize}
    \item Created detailed milestones doc with team ownership (M1: title by DSA, M2: thumbnail by me + Human Eval, M3: logo by Brand Safety)
    \item Weekly sync meetings (30 min, focused agenda)
    \item Slack channel for async updates (\texttt{\#clickbait-detection-project})
    \item \textbf{Decision-making authority:} I had final call on architecture; PM had final call on launch timing
\end{itemize}

\textbf{Challenge 2: Human Eval Bottleneck}
\begin{itemize}
    \item Label Studio tasks required expert judgment (what is clickbait?)
    \item Human Eval team had competing priorities (safety moderation)
\end{itemize}

\textbf{Solution: Prioritization + Iteration}
\begin{itemize}
    \item Phase 1: Top 30 games only (minimal viable PAL)
    \item Created detailed labeling guidelines to reduce ambiguity
    \item Automated easy cases (exact title match) to reduce human load
    \item Phase 2: Expand PAL based on query volume analysis
\end{itemize}

\textbf{Challenge 3: Brand Safety Model Dependency}
\begin{itemize}
    \item Their logo model was static (no update schedule)
    \item Risk: if model degrades, we're stuck
\end{itemize}

\textbf{Solution: Versioning Protocol + Fine-Tuning}
\begin{itemize}
    \item Agreed on versioned endpoints for future updates
    \item Fine-tuned my own YOLOv8 as backup (not fully dependent)
    \item Offered to share PAL data for their model retraining (mutual benefit)
\end{itemize}

\textbf{Key Lesson:} Cross-team projects need \textbf{clear ownership}, \textbf{explicit dependencies}, and \textbf{fallback plans}. Over-communicate, document decisions, and reduce critical path dependencies where possible."

\subsubsection{Q: You mentioned leadership didn't fully understand the project. What happened there?}

\textbf{Answer (Honest but Diplomatic):}

"This was a learning experience for me about the importance of \textbf{executive communication}, not just technical execution.

\textbf{What Happened:}
\begin{itemize}
    \item Project had high visibility (CEO escalation, townhall recognition)
    \item But my direct leadership didn't fully grasp the technical complexity or business impact
    \item This contributed to my decision to explore new opportunities
\end{itemize}

\textbf{Where I Could've Done Better:}
\begin{enumerate}
    \item \textbf{Executive-Level Artifacts:}
    \begin{itemize}
        \item I wrote detailed technical docs (architecture, APIs, pipelines)
        \item Should've also created 1-pager for leadership: Problem $\rightarrow$ Solution $\rightarrow$ Impact
        \item Metrics dashboard showing business value (query coverage, precision, CEO satisfaction)
    \end{itemize}
    
    \item \textbf{Proactive Updates:}
    \begin{itemize}
        \item I communicated in team meetings and Slack
        \item Should've done monthly 1:1 highlights with skip-level manager
        \item "Here's what we shipped, here's the impact, here's what's next"
    \end{itemize}
    
    \item \textbf{Connecting to Org Goals:}
    \begin{itemize}
        \item I framed as "fixing clickbait problem"
        \item Could've framed as "establishing IP protection framework for platform" (strategic)
        \item Or "improving search quality metrics" (measurable org goal)
    \end{itemize}
\end{enumerate}

\textbf{What I Learned:}
\begin{itemize}
    \item Technical excellence alone doesn't guarantee recognition
    \item Senior/Principal engineers must translate technical work into business value \textit{for leadership}
    \item Documentation, metrics, and storytelling are as important as code
\end{itemize}

\textbf{Looking Forward:}
\begin{itemize}
    \item At DataHub, I'd apply these lessons from day one
    \item Create exec-friendly artifacts alongside technical design docs
    \item Proactively communicate impact, not just features shipped
\end{itemize}

This experience taught me that \textbf{impact without visibility is wasted effort} - something I'm now very conscious of."

\newpage
% ============================================================
\section{Practice Checklist}
% ============================================================

\begin{actionitem}
\textbf{2-Day Prep Plan:}

\subsection*{Day 1 (3-4 hours)}
\begin{itemize}
    \item Morning (2 hours):
    \begin{itemize}
        \item Read this document cover-to-cover
        \item Highlight 5-7 key talking points to memorize
        \item Practice 30-second elevator pitch (time yourself)
    \end{itemize}
    \item Afternoon (2 hours):
    \begin{itemize}
        \item Draw architecture diagram from memory (whiteboard/paper)
        \item Practice answering "Why heuristics?" question out loud
        \item Write down 3 questions to ask Abe about DataHub
    \end{itemize}
\end{itemize}

\subsection*{Day 2 (3-4 hours)}
\begin{itemize}
    \item Morning (2 hours):
    \begin{itemize}
        \item Review Mock Q\&A section
        \item Practice STAR narrative out loud (record yourself, aim for 10-12 min)
        \item Identify weak spots (what questions make you hesitate?)
    \end{itemize}
    \item Afternoon (1-2 hours):
    \begin{itemize}
        \item Mock interview with friend/colleague
        \item Practice whiteboard diagram + verbal walkthrough
        \item Final review: Key Insights and Critical Points boxes
    \end{itemize}
\end{itemize}

\subsection*{Night Before Interview}
\begin{itemize}
    \item Review 30-second pitch (know it cold)
    \item Skim Technical Challenges section (3-5 lessons learned)
    \item Prepare 3 questions for Abe (show genuine interest)
    \item Get good sleep!
\end{itemize}

\subsection*{Interview Day Morning}
\begin{itemize}
    \item Skim Executive Summary (page 2)
    \item Review architecture diagram (can you draw it from memory?)
    \item Practice: "This project taught me..." (Connection to DataHub)
    \item Arrive 10 min early, bring water, have paper/pen ready
\end{itemize}
\end{actionitem}

% ============================================================
\section{Key Talking Points (Memorize These)}
% ============================================================

\begin{enumerate}
    \item \textbf{First Principles Reframing}
    \begin{itemize}
        \item Previous team: image similarity problem
        \item Me: IP protection + search quality problem
        \item Multi-signal detection (text + image + logo), not single model
    \end{itemize}
    
    \item \textbf{Flexible Policy Framework}
    \begin{itemize}
        \item Three match policies: exact, substring, fuzzy
        \item Not one-size-fits-all: "Evade" vs "Brookhaven" need different protection
        \item Built-in exceptions: fan-made, same creator, typo tolerance
    \end{itemize}
    
    \item \textbf{Right Tool for Each Signal}
    \begin{itemize}
        \item Heuristics for text (explainable, 99\% precision)
        \item CLIP for images (semantic similarity works here)
        \item YOLO for logos (object detection + fine-tuning)
    \end{itemize}
    
    \item \textbf{Event-Driven Real-Time Architecture}
    \begin{itemize}
        \item SNS $\rightarrow$ SQS $\rightarrow$ Detection Service $\rightarrow$ Frost $\rightarrow$ Search Ranking
        \item 10-second latency, non-blocking, scalable
        \item vs previous: weekly batch (stale, reactive)
    \end{itemize}
    
    \item \textbf{99\%+ Precision}
    \begin{itemize}
        \item Systematic validation on golden dataset (2000+ examples)
        \item Conservative thresholds, human-in-the-loop, nuanced policies
        \item Zero creator backlash (proof of policy success)
    \end{itemize}
    
    \item \textbf{Cross-Team Leadership}
    \begin{itemize}
        \item Coordinated 5 teams (DSA, Human Eval, Brand Safety, ML Platform, Game Discovery)
        \item Clear work streams, weekly syncs, decision-making authority
        \item Successful handoff to Game Discovery post-launch
    \end{itemize}
    
    \item \textbf{Connection to DataHub}
    \begin{itemize}
        \item PAL $\approx$ DataHub metadata registry
        \item Cross-team adoption challenges (same at DataHub)
        \item Real-time propagation (schema changes $\rightarrow$ lineage updates)
        \item Policy framework (governance at DataHub)
    \end{itemize}
\end{enumerate}

% ============================================================
\section{Questions to Ask Abe}
% ============================================================

\begin{enumerate}
    \item \textbf{Product Strategy:}
    \begin{itemize}
        \item "What's DataHub's biggest challenge in driving enterprise adoption? Is it technical (scale, latency) or organizational (getting teams to contribute metadata)?"
    \end{itemize}
    
    \item \textbf{Technical Architecture:}
    \begin{itemize}
        \item "How does DataHub handle real-time metadata propagation? If a schema changes in production, how quickly do downstream systems see that in lineage graphs?"
    \end{itemize}
    
    \item \textbf{Team Dynamics:}
    \begin{itemize}
        \item "For this role, what's the balance between building new platform features vs enabling customer integrations? How much is greenfield vs existing codebase evolution?"
    \end{itemize}
    
    \item \textbf{Search/Discovery Expertise:}
    \begin{itemize}
        \item "Given my background in search and RAG systems, where do you see opportunities for me to contribute to DataHub's metadata search and discovery experience?"
    \end{itemize}
    
    \item \textbf{Personal Growth:}
    \begin{itemize}
        \item "What does success look like for a Principal Engineer at DataHub in the first 6-12 months? What are the key milestones or projects that would demonstrate high impact?"
    \end{itemize}
\end{enumerate}

\textbf{Strategy:} Pick 2-3 questions that genuinely interest you. Let the conversation flow naturally - you may not need to ask all of them if the discussion covers the topics organically.

% ============================================================
\section{Final Reminders}
% ============================================================

\begin{criticalpoint}
\textbf{Interview Execution Tips:}

\begin{itemize}
    \item \textbf{Start High-Level, Then Drill Down}
    \begin{itemize}
        \item 30-second summary $\rightarrow$ problem reframing $\rightarrow$ architecture $\rightarrow$ technical details
        \item Let Abe guide depth (if he asks about YOLO fine-tuning, dive deep; if not, stay high-level)
    \end{itemize}
    
    \item \textbf{Use Whiteboard/Paper}
    \begin{itemize}
        \item Draw architecture diagram as you explain
        \item Visual aids help both you and interviewer follow along
        \item Shows systems thinking
    \end{itemize}
    
    \item \textbf{Be Honest About Challenges}
    \begin{itemize}
        \item Don't pretend everything was perfect
        \item "What I'd do differently" shows self-awareness and learning
        \item Leadership recognition issue: frame as learning about exec communication
    \end{itemize}
    
    \item \textbf{Show Enthusiasm}
    \begin{itemize}
        \item This was a project you're proud of - let that show!
        \item Technical problem-solving is fun, not drudgery
        \item Connect excitement about this work to excitement about DataHub
    \end{itemize}
    
    \item \textbf{Ask Clarifying Questions}
    \begin{itemize}
        \item If Abe asks something unclear, ask for clarification
        \item Shows careful thinking, not just rushing to answer
        \item "Just to make sure I understand, you're asking about X vs Y?"
    \end{itemize}
    
    \item \textbf{Time Management}
    \begin{itemize}
        \item Aim for 10-12 min STAR narrative (not 20 min monologue)
        \item Leave time for Abe's follow-up questions (the real signal)
        \item If running long, ask: "Should I keep going or would you like to dive deeper into something specific?"
    \end{itemize}
\end{itemize}
\end{criticalpoint}

\vspace{1em}

\begin{keyinsight}
\textbf{You've Got This!}

You have:
\begin{itemize}
    \item 20+ years of engineering experience
    \item A compelling project with measurable impact (99\%+ precision, CEO recognition)
    \item Deep technical expertise (search, ML, distributed systems)
    \item Clear narrative arc (problem $\rightarrow$ insight $\rightarrow$ solution $\rightarrow$ impact)
    \item Strong connection to DataHub's mission (metadata quality, cross-team adoption)
\end{itemize}

Trust your experience. Be yourself. Show your passion for solving hard problems.

\textbf{Good luck!}
\end{keyinsight}

% ============================================================
\section{Part 2: Search Architecture Deep Dive}
% ============================================================

\subsection{Overview}

\begin{keyinsight}
\textbf{Your Search Expertise at Roblox:}

You are uniquely positioned to discuss search architecture because you've worked across the entire search stack:

\begin{itemize}
    \item \textbf{Retrieval Layer:} RSS (billion-scale vector database), semantic search
    \item \textbf{Ranking Layer:} Clickbait detection signal (quality scoring)
    \item \textbf{Query Understanding:} Spelling correction service, autocomplete
    \item \textbf{Search Quality:} Game clickbait detection, evaluation frameworks
    \item \textbf{Platform Infrastructure:} RAG Platform (retrieval + generation)
\end{itemize}

\textbf{Key Message:} You're not just a user of search systems - you're a \textit{builder of search infrastructure} that others depend on.
\end{keyinsight}

\subsection{Roblox Search Architecture: High-Level Overview}

\subsubsection{The Roblox Search Problem Space}

\textbf{What Makes Roblox Search Unique:}

\begin{enumerate}
    \item \textbf{Scale:}
    \begin{itemize}
        \item Millions of user-generated games (experiences)
        \item Billions of search queries annually
        \item Real-time content creation (games published every second)
    \end{itemize}
    
    \item \textbf{User-Generated Content (UGC) Challenges:}
    \begin{itemize}
        \item Noisy metadata (creators gaming SEO with keyword stuffing)
        \item Quality variance (AAA games vs low-effort clones)
        \item IP violations (clickbait, copycats)
        \item Sparse signals for new content (cold start problem)
    \end{itemize}
    
    \item \textbf{Diverse User Base:}
    \begin{itemize}
        \item Age range: 6-60+ years old
        \item Query types: navigational ("Brookhaven"), exploratory ("fun games"), developmental ("obby")
        \item Intent: play now, discover new, find friends' games
    \end{itemize}
\end{enumerate}

\subsubsection{End-to-End Search Flow}

\textbf{Draw this architecture during the interview:}

\begin{verbatim}
User Query: "Brookhaven"
    |
[1] Query Understanding
    |-- Spelling Correction (your service)
    |-- Query Normalization (lowercase, trim)
    |-- Intent Detection (navigational vs exploratory)
    \-- Query Expansion (synonyms, related terms)
    |
[2] Retrieval (Multi-Stage)
    |-- [2a] Keyword Retrieval (Elasticsearch)
    |   |-- Title/Description match (BM25 scoring)
    |   \-- Returns: Top 1000 candidates
    |-- [2b] Semantic Retrieval (RSS - your system)
    |   |-- Query -> CLIP embedding
    |   |-- Vector search against game thumbnails
    |   \-- Returns: Top 500 candidates
    \-- [2c] Hybrid Fusion
        \-- Merge keyword + semantic results -> Top 200 candidates
    |
[3] Ranking
    |-- [3a] Feature Generation (Frost)
    |   |-- Engagement: CTR, play time, favorites
    |   |-- Quality: clickbait score (your system), creator reputation
    |   |-- Freshness: publish date, update recency
    |   |-- Personalization: user history, age appropriateness
    |-- [3b] Learning-to-Rank (LTR) Model
    |   |-- Gradient-boosted trees (XGBoost/LightGBM)
    |   |-- Trained on user feedback (clicks, play sessions)
    |   \-- Outputs: relevance score per candidate
    \-- [3c] Re-Ranking & Filtering
        |-- Diversity: avoid duplicate creators in top 10
        |-- Safety: filter age-inappropriate content
        \-- Policy: demote clickbait (your system)
    |
[4] Serving
    |-- Cache: Redis for popular queries
    |-- Latency: P99 < 500ms
    \-- A/B Testing: gradual rollout of ranking changes
    |
Search Results Displayed to User
\end{verbatim}

\subsection{Your Contributions to Roblox Search}

\subsubsection{1. RSS (Roblox Similarity Search) - Vector Database Infrastructure}

\textbf{Your Role:} Initiator and lead engineer (2022-2023, V1)

\textbf{Problem Solved:}
\begin{itemize}
    \item Roblox needed billion-scale vector search for semantic retrieval
    \item Off-the-shelf solutions (Pinecone, Weaviate) didn't meet requirements:
    \begin{itemize}
        \item Cost at Roblox scale ($\sim$100M+ vectors)
        \item Latency for real-time serving (P99 < 100ms)
        \item Data residency (must run in Roblox data centers)
    \end{itemize}
\end{itemize}

\textbf{Technical Approach:}
\begin{enumerate}
    \item \textbf{Ported Milvus (Open Source) to Roblox Infrastructure}
    \begin{itemize}
        \item Milvus: distributed vector database (built on FAISS)
        \item Adapted for Roblox's Nomad-based microservice environment
        \item Integration with Roblox storage layer (S3, persistent volumes)
    \end{itemize}
    
    \item \textbf{Deployment Architecture}
    \begin{itemize}
        \item Distributed across Roblox data centers
        \item Sharded by collection (games, avatars, assets)
        \item HNSW index for approximate nearest neighbor (ANN) search
        \item Replica sets for high availability
    \end{itemize}
    
    \item \textbf{Scale Metrics}
    \begin{itemize}
        \item Billions of vectors indexed
        \item P99 latency: $\sim$50-100ms for k-NN search (k=100)
        \item $\sim$50 production use cases across Roblox (search, recommendations, safety)
    \end{itemize}
\end{enumerate}

\textbf{First Major Use Case: Avatar Marketplace Semantic Search}
\begin{itemize}
    \item \textbf{Problem:} Keyword search failed for visual queries ("red hoodie", "anime hair")
    \item \textbf{Solution:} 
    \begin{itemize}
        \item Generated CLIP embeddings for all avatar items (images)
        \item Indexed in RSS (multi-modal: text query $\rightarrow$ image results)
        \item Hybrid retrieval: keyword (Elasticsearch) + semantic (RSS)
    \end{itemize}
    \item \textbf{Impact:} Improved discovery for long-tail items (less popular items surfaced)
\end{itemize}

\textbf{Trade-Offs Discussion (Critical for Interview):}

\begin{table}[h]
\centering
\begin{tabular}{|p{3cm}|p{5cm}|p{5cm}|}
\hline
\textbf{Decision} & \textbf{Benefits} & \textbf{Trade-Offs} \\ \hline
Milvus vs Build from Scratch & Faster time-to-market, battle-tested at scale & Less control, vendor lock-in (mitigated by open source) \\ \hline
HNSW vs FLAT index & Sub-100ms latency (ANN) & Lower recall than exhaustive search (acceptable for top-k) \\ \hline
In-house vs SaaS & Cost savings, data residency, customization & Operational burden (maintenance, upgrades) \\ \hline
Sharding by collection & Isolation, independent scaling per use case & Cross-collection queries require multiple searches \\ \hline
\end{tabular}
\end{table}

\subsubsection{2. RAG Platform - Retrieval-Augmented Generation}

\textbf{Your Role:} Lead architect and engineer (2024, V1 MVP)

\textbf{Problem Solved:}
\begin{itemize}
    \item Multiple teams building ad-hoc RAG systems (AI Safety, DevRel, ROS)
    \item Duplicated effort: data ingestion, embedding generation, vector DB integration
    \item No standardized evaluation or monitoring
\end{itemize}

\textbf{Technical Approach:}
\begin{enumerate}
    \item \textbf{Ported AnythingLLM (Open Source) to Roblox BEDEV2}
    \begin{itemize}
        \item AnythingLLM: full-stack RAG platform (document processing, embeddings, LLM integration)
        \item Adapted for Nomad-based microservice deployment
        \item Integration with Roblox auth, data access controls
    \end{itemize}
    
    \item \textbf{Platform Components}
    \begin{itemize}
        \item \textbf{Data Ingestion:} PDF, DOCX, CSV parsers; chunking strategies
        \item \textbf{Embedding Generation:} CLIP, Sentence-BERT, domain-specific models
        \item \textbf{Vector Storage:} RSS integration (reused your vector DB)
        \item \textbf{LLM Orchestration:} Multi-LLM support (GPT-4, Claude, internal models)
        \item \textbf{Agent Framework:} Tool use, memory, prompt templates
    \end{itemize}
    
    \item \textbf{Web Platform (No-Code RAG)}
    \begin{itemize}
        \item UI for non-engineers to build RAG apps
        \item Upload documents $\rightarrow$ configure retrieval $\rightarrow$ deploy chatbot
        \item Reduced development cycle: 3 months $\rightarrow$ 1 week
    \end{itemize}
\end{enumerate}

\textbf{Adoption \& Impact:}
\begin{itemize}
    \item \textbf{10+ production use cases within 6 months}
    \item \textbf{Use Case 1: Moderation Mate (Safety Team)}
    \begin{itemize}
        \item RAG over moderation policies and case histories
        \item Helps moderators make consistent decisions
        \item 5K QPS at peak
    \end{itemize}
    \item \textbf{Use Case 2: Community Insights (DevRel)}
    \begin{itemize}
        \item RAG over creator feedback, forum posts, support tickets
        \item Helps DevRel team understand creator pain points
        \item Research tool, not user-facing
    \end{itemize}
\end{itemize}

\textbf{RAG-Specific Challenges (Discuss if Asked):}

\begin{enumerate}
    \item \textbf{Data Privacy \& Access Control}
    \begin{itemize}
        \item Problem: RAG system has access to sensitive documents
        \item Solution: Row-level security, document-level permissions
        \item Integration with Roblox's RBAC (role-based access control)
    \end{itemize}
    
    \item \textbf{Retrieval Quality (Precision vs Recall)}
    \begin{itemize}
        \item Problem: Retrieving irrelevant chunks hurts LLM output quality
        \item Solution: Hybrid search (keyword + semantic), reranking, chunk overlap
        \item Evaluation: human eval on relevance, LLM-as-judge for answer quality
    \end{itemize}
    
    \item \textbf{Latency (P99 < 5 seconds for user-facing)}
    \begin{itemize}
        \item Breakdown: Retrieval (100ms) + Reranking (200ms) + LLM (3-4s)
        \item Optimization: parallel retrieval, streaming LLM responses
    \end{itemize}
\end{enumerate}

\subsubsection{3. Autocomplete Services (Avatar, Studio, Brand)}

\textbf{Your Role:} Sole engineer (Avatar: 2020, Studio: 2021, Brand: 2025 as mentor)

\textbf{Problem Solved:}
\begin{itemize}
    \item Users need instant query suggestions as they type
    \item Reduce typos, guide users to popular content
    \item Different domains: Avatar Marketplace, Studio (creator tools), Brand partnerships
\end{itemize}

\textbf{Technical Approach:}

\begin{enumerate}
    \item \textbf{Trie-Based Data Structure}
    \begin{itemize}
        \item Prefix tree for fast prefix matching (O(k) for query length k)
        \item Precomputed suggestions for top 10K prefixes
        \item In-memory (Redis) for P99 < 10ms latency
    \end{itemize}
    
    \item \textbf{Suggestion Ranking}
    \begin{itemize}
        \item Signals: query frequency, result CTR, recency
        \item Personalization: user history, age group
        \item A/B testing framework for ranking changes
    \end{itemize}
    
    \item \textbf{Offline Pipeline (Daily)}
    \begin{itemize}
        \item Aggregate query logs (Hive)
        \item Filter: remove PII, profanity, low-frequency queries
        \item Build trie, compute scores, publish to Redis
    \end{itemize}
\end{enumerate}

\textbf{Scale Metrics:}
\begin{itemize}
    \item Avatar Autocomplete: All Roblox users ($\sim$70M DAU)
    \item Studio Autocomplete: Roblox creators ($\sim$2M+ monthly)
    \item P99 latency: 10ms (critical for user experience)
    \item Update frequency: Daily (captures trending queries)
\end{itemize}

\subsubsection{4. Spelling Correction Service}

\textbf{Your Role:} Sole engineer (2022)

\textbf{Problem Solved:}
\begin{itemize}
    \item User queries contain typos ("Brookhven" $\rightarrow$ "Brookhaven")
    \item Children are primary users (higher typo rate)
    \item Poor spelling = zero results = bad user experience
\end{itemize}

\textbf{Technical Approach:}

\begin{enumerate}
    \item \textbf{Dictionary-Based Correction}
    \begin{itemize}
        \item Dictionary: all valid game titles, popular queries
        \item Algorithm: Edit distance (Levenshtein), phonetic similarity (Soundex)
        \item Candidate generation: words within edit distance 2
    \end{itemize}
    
    \item \textbf{Context-Aware Ranking}
    \begin{itemize}
        \item Given "Brookhven", candidates: ["Brookhaven", "Brookhaven RP", "Brook Haven"]
        \item Rank by: query frequency, result availability, user context
    \end{itemize}
    
    \item \textbf{Production Deployment}
    \begin{itemize}
        \item Microservice (bedev2), REST API
        \item Integration: Search (query rewriting), Autocomplete (suggestion generation)
        \item 3 production adoptions: Avatar Marketplace, Studio, Community
    \end{itemize}
\end{enumerate}

\subsubsection{5. Game Clickbait Detection (Covered in Part 1)}

\textbf{Search Quality Signal:}
\begin{itemize}
    \item Clickbait score (0-1) stored in Frost feature table
    \item Used by ranking model as quality signal (demote low-quality copycats)
    \item Real-time updates (10-second latency from game publish to ranking)
\end{itemize}

\subsection{Key Search Topics: Deep Dive}

\subsubsection{Indexing Pipeline}

\textbf{Roblox Game Indexing Flow:}

\begin{verbatim}
Game Created/Updated Event (SNS)
    |
[1] Document Builder Service
    |-- Fetch metadata: title, description, creator, genre, etc.
    |-- Generate features: engagement stats, quality scores
    |-- Enrich: clickbait score (your system), safety flags
    \-- Output: JSON document
    |
[2] Elasticsearch Indexing
    |-- Index mapping: title (text), description (text), genre (keyword)
    |-- Sharding: by universe_id (game ID)
    |-- Replica sets: 3x for availability
    \-- Refresh interval: 1 second (near real-time)
    |
[3] Vector Indexing (RSS)
    |-- Generate CLIP embedding for game thumbnail
    |-- Index in RSS (separate collection: "game_thumbnails")
    \-- Async (doesn't block Elasticsearch indexing)
    |
[4] Feature Store (Frost)
    |-- Compute ranking features (engagement, quality, freshness)
    |-- Publish to Frost tables
    \-- Used by ranking service at query time
\end{verbatim}

\textbf{Indexing Challenges:}

\begin{enumerate}
    \item \textbf{Freshness vs Consistency}
    \begin{itemize}
        \item Problem: Game metadata changes frequently (title updates, thumbnail swaps)
        \item Solution: Near real-time indexing (1-second refresh) vs eventual consistency
        \item Trade-off: Slight staleness acceptable for better performance
    \end{itemize}
    
    \item \textbf{Schema Evolution}
    \begin{itemize}
        \item Problem: Adding new fields (e.g., clickbait\_score) requires reindexing
        \item Solution: Backward-compatible mappings, gradual rollout
        \item Zero-downtime reindexing: alias swap pattern
    \end{itemize}
    
    \item \textbf{Scale (Millions of Games)}
    \begin{itemize}
        \item Problem: Full reindex takes hours/days
        \item Solution: Incremental indexing (only changed documents)
        \item Backfill: parallel workers, checkpointing for recovery
    \end{itemize}
\end{enumerate}

\subsubsection{Query Understanding}

\textbf{Roblox Query Understanding Pipeline:}

\begin{enumerate}
    \item \textbf{Normalization}
    \begin{itemize}
        \item Lowercase, trim whitespace
        \item Remove special characters (emojis, punctuation)
        \item Unicode normalization (\'e $\rightarrow$ e)
    \end{itemize}
    
    \item \textbf{Spelling Correction (Your Service)}
    \begin{itemize}
        \item Detect misspellings: edit distance, phonetic similarity
        \item Suggest corrections: "Brookhven" $\rightarrow$ "Brookhaven"
        \item User experience: "Did you mean..." vs auto-correct
    \end{itemize}
    
    \item \textbf{Intent Detection}
    \begin{itemize}
        \item \textbf{Navigational:} User wants specific game ("Adopt Me")
        \item \textbf{Exploratory:} User wants category ("scary games", "obby")
        \item \textbf{Transactional:} User wants to create/edit ("new game template")
    \end{itemize}
    
    \item \textbf{Query Expansion}
    \begin{itemize}
        \item Synonyms: "scary" $\rightarrow$ ["horror", "spooky", "creepy"]
        \item Related terms: "obby" $\rightarrow$ ["obstacle course", "parkour"]
        \item Learned from user behavior (query reformulations)
    \end{itemize}
\end{enumerate}

\textbf{Challenge: Roblox-Specific Terminology}
\begin{itemize}
    \item "Obby" = obstacle course (Roblox slang, not in standard dictionaries)
    \item "Brookhaven RP" = roleplay (RP is domain-specific abbreviation)
    \item Solution: Custom dictionary, query log mining, creator tagging
\end{itemize}

\subsubsection{Ranking \& Relevance}

\textbf{Ranking Signal Categories:}

\begin{enumerate}
    \item \textbf{Textual Relevance (Traditional IR)}
    \begin{itemize}
        \item BM25 score (keyword match in title, description)
        \item TF-IDF (term frequency, inverse document frequency)
        \item Exact match boost (query = title $\rightarrow$ higher score)
    \end{itemize}
    
    \item \textbf{Engagement Signals}
    \begin{itemize}
        \item CTR (click-through rate): clicks / impressions
        \item Play time: average session duration
        \item Favorites: user saves to favorites list
        \item Retention: users return to game repeatedly
    \end{itemize}
    
    \item \textbf{Quality Signals}
    \begin{itemize}
        \item Clickbait score: 0-1 (your system, lower = higher quality)
        \item Creator reputation: established vs new creator
        \item Safety flags: age-appropriate, policy violations
        \item User ratings: thumbs up/down
    \end{itemize}
    
    \item \textbf{Freshness Signals}
    \begin{itemize}
        \item Publish date: new games get temporary boost
        \item Update recency: recently updated games prioritized
        \item Decay function: boost diminishes over time
    \end{itemize}
    
    \item \textbf{Personalization Signals}
    \begin{itemize}
        \item User history: games played, genres preferred
        \item Social: friends' games, group memberships
        \item Age appropriateness: filter by user's age group
        \item Language: localization, regional preferences
    \end{itemize}
\end{enumerate}

\textbf{Learning-to-Rank (LTR) Model:}

\begin{itemize}
    \item \textbf{Model Type:} Gradient-boosted decision trees (XGBoost/LightGBM)
    \item \textbf{Training Data:} User interactions (clicks, play sessions, favorites)
    \item \textbf{Features:} $\sim$50-100 features across categories above
    \item \textbf{Objective:} Maximize engagement (play time, retention) while maintaining quality
    \item \textbf{Evaluation:} Offline (NDCG, MRR) + Online (A/B tests on CTR, play time)
\end{itemize}

\textbf{Trade-Off: Engagement vs Quality}

\begin{itemize}
    \item \textbf{Problem:} Clickbait games have high CTR (by design) but low quality
    \item \textbf{Naive approach:} Rank purely by CTR $\rightarrow$ clickbait dominates
    \item \textbf{Solution:} Multi-objective optimization
    \begin{itemize}
        \item Objective 1: Maximize engagement (CTR, play time)
        \item Objective 2: Maximize quality (low clickbait score, high ratings)
        \item Weighted combination: $\alpha \cdot engagement + (1-\alpha) \cdot quality$
    \end{itemize}
    \item \textbf{Your contribution:} Clickbait score as quality signal (part of Objective 2)
\end{itemize}

\subsubsection{Semantic Search (CLIP + RSS)}

\textbf{Why Semantic Search?}

\begin{itemize}
    \item Keyword search fails for visual/conceptual queries:
    \begin{itemize}
        \item "red hoodie" (visual attribute, not in text metadata)
        \item "anime style" (artistic style, hard to describe in keywords)
        \item "games like Brookhaven" (conceptual similarity, not keyword match)
    \end{itemize}
\end{itemize}

\textbf{Semantic Search Pipeline:}

\begin{verbatim}
User Query: "anime style games"
    |
[1] Query Embedding
    |-- CLIP text encoder: query -> 512-dim vector
    |-- Normalize: L2 norm = 1
    \-- Output: query embedding
    |
[2] Vector Search (RSS)
    |-- k-NN search: find top-k nearest game thumbnail embeddings
    |-- Distance metric: cosine similarity (dot product for normalized vectors)
    |-- Index: HNSW (approximate nearest neighbor)
    \-- Returns: Top 500 candidates with similarity scores
    |
[3] Hybrid Fusion
    |-- Keyword results (Elasticsearch): Top 1000
    |-- Semantic results (RSS): Top 500
    |-- Fusion method: Reciprocal Rank Fusion (RRF)
    \-- Output: Top 200 candidates for ranking
\end{verbatim}

\textbf{Hybrid Search Trade-Offs:}

\begin{table}[h]
\centering
\small
\begin{tabular}{|l|p{4cm}|p{4cm}|}
\hline
\textbf{Approach} & \textbf{Strengths} & \textbf{Weaknesses} \\ \hline
Keyword Only & Precise for exact matches, fast & Misses conceptual similarity \\ \hline
Semantic Only & Finds conceptual matches & Misses exact matches, slower \\ \hline
Hybrid (Keyword + Semantic) & Best of both worlds & Increased complexity, tuning needed \\ \hline
\end{tabular}
\end{table}

\textbf{CLIP Model Details:}
\begin{itemize}
    \item \textbf{Base model:} OpenAI CLIP (ViT-B/32 or ViT-L/14)
    \item \textbf{Fine-tuning:} Considered but not done in V1 (off-the-shelf worked well)
    \item \textbf{Future:} Fine-tune on Roblox-specific data (game thumbnails + user clicks)
\end{itemize}

\subsubsection{Personalization}

\textbf{Cold Start Problem (New Users):}

\begin{itemize}
    \item \textbf{Problem:} No user history $\rightarrow$ can't personalize
    \item \textbf{Solution:} 
    \begin{itemize}
        \item Default to popularity (most played games globally)
        \item Age-based filtering (show age-appropriate content)
        \item Gradual learning: collect clicks/plays, update profile
    \end{itemize}
\end{itemize}

\textbf{Cold Start Problem (New Games):}

\begin{itemize}
    \item \textbf{Problem:} No engagement data $\rightarrow$ can't rank well
    \item \textbf{Solution:}
    \begin{itemize}
        \item Freshness boost: new games get temporary ranking boost
        \item Creator reputation: leverage creator's past game quality
        \item Content-based features: genre, description, thumbnail quality
    \end{itemize}
\end{itemize}

\textbf{Privacy Considerations:}

\begin{itemize}
    \item User history stored with consent (privacy policy)
    \item Anonymized for model training (no PII)
    \item User control: "Clear history" option
\end{itemize}

\subsubsection{Evaluation \& Metrics}

\textbf{Offline Metrics (Pre-Deployment):}

\begin{enumerate}
    \item \textbf{NDCG (Normalized Discounted Cumulative Gain)}
    \begin{itemize}
        \item Measures ranking quality (higher-ranked relevant results = better)
        \item NDCG@10: focus on top 10 results (most important for users)
    \end{itemize}
    
    \item \textbf{MRR (Mean Reciprocal Rank)}
    \begin{itemize}
        \item Position of first relevant result (1/rank)
        \item Good for navigational queries ("Adopt Me" $\rightarrow$ should be \#1)
    \end{itemize}
    
    \item \textbf{Precision@k / Recall@k}
    \begin{itemize}
        \item Precision@10: \% of top 10 results that are relevant
        \item Recall@10: \% of relevant results found in top 10
    \end{itemize}
\end{enumerate}

\textbf{Online Metrics (A/B Testing):}

\begin{enumerate}
    \item \textbf{CTR (Click-Through Rate)}
    \begin{itemize}
        \item Clicks / Impressions (how often users click results)
        \item Broken down by position (position 1 vs position 10)
    \end{itemize}
    
    \item \textbf{Play Time}
    \begin{itemize}
        \item Average session duration after search
        \item Measures result quality (did user find what they wanted?)
    \end{itemize}
    
    \item \textbf{Zero Result Rate}
    \begin{itemize}
        \item \% of queries with no results
        \item Target: <5\% (spelling correction helps)
    \end{itemize}
    
    \item \textbf{Query Reformulation Rate}
    \begin{itemize}
        \item \% of users who search again after initial query
        \item High rate = initial results unsatisfactory
    \end{itemize}
\end{enumerate}

\textbf{Human Evaluation:}

\begin{itemize}
    \item Periodic manual review of top queries (top 1000)
    \item Raters assess: relevance, quality, diversity
    \item Identify edge cases, systematic errors
    \item Feed back into model training, feature engineering
\end{itemize}

\subsection{Search Architecture Trade-Offs}

\subsubsection{Latency vs Quality}

\begin{table}[h]
\centering
\small
\begin{tabular}{|p{3cm}|p{5cm}|p{5cm}|}
\hline
\textbf{Component} & \textbf{Latency Impact} & \textbf{Quality Impact} \\ \hline
Retrieval Depth & More candidates = slower & More candidates = better recall \\ \hline
Ranking Model Complexity & Deeper trees = slower & More features = better accuracy \\ \hline
Semantic Search & Vector search adds 50-100ms & Improves conceptual matches \\ \hline
Personalization & User profile lookup adds 20ms & Improves relevance \\ \hline
\end{tabular}
\end{table}

\textbf{Roblox's Choice:}
\begin{itemize}
    \item Target P99 latency: 500ms (acceptable for search)
    \item Budget breakdown: Retrieval (100ms) + Ranking (200ms) + Overhead (200ms)
    \item Trade-off: Accept slightly slower for better quality (user retention > speed)
\end{itemize}

\subsubsection{Freshness vs Consistency}

\begin{itemize}
    \item \textbf{Real-time indexing:} Game available in search within seconds
    \item \textbf{Eventual consistency:} Ranking features (engagement) update hourly/daily
    \item \textbf{Acceptable staleness:} Users don't expect instant engagement stats
\end{itemize}

\subsubsection{Recall vs Precision}

\begin{itemize}
    \item \textbf{High recall (cast wide net):} Retrieve 1000 candidates, risk irrelevant results
    \item \textbf{High precision (narrow focus):} Retrieve 100 candidates, risk missing relevant results
    \item \textbf{Roblox's approach:} High recall in retrieval (1000), precision in ranking (filter to top 20)
\end{itemize}

\subsubsection{Personalization vs Diversity}

\begin{itemize}
    \item \textbf{Problem:} Over-personalization creates filter bubbles
    \begin{itemize}
        \item User only sees games similar to past plays
        \item Misses discovery of new genres, creators
    \end{itemize}
    \item \textbf{Solution:} Inject diversity
    \begin{itemize}
        \item 70\% personalized results + 30\% diverse/popular results
        \item Diversity constraints: no more than 2 games from same creator in top 10
        \item Exploration: occasionally show random high-quality games
    \end{itemize}
\end{itemize}

\subsection{Potential Interview Questions \& Answers}

\subsubsection{Q: Walk me through what happens when a user types a search query.}

\textbf{Answer (End-to-End Flow):}

"Let me walk through a concrete example: user searches 'scary games'.

\textbf{Stage 1: Query Understanding (50ms)}
\begin{itemize}
    \item Normalize: lowercase, trim spaces
    \item Check spelling: 'scarry' $\rightarrow$ 'scary' (my spelling service)
    \item Detect intent: exploratory (not navigational)
    \item Expand: add synonyms ['horror', 'spooky', 'creepy']
\end{itemize}

\textbf{Stage 2: Retrieval (100ms)}
\begin{itemize}
    \item Keyword retrieval (Elasticsearch):
    \begin{itemize}
        \item Match 'scary' in title, description, tags
        \item BM25 scoring: games with 'scary' in title rank higher
        \item Returns: Top 1000 candidates
    \end{itemize}
    \item Semantic retrieval (RSS - my system):
    \begin{itemize}
        \item Generate CLIP embedding for 'scary games' text
        \item Vector search: find games with similar thumbnail embeddings
        \item Returns: Top 500 candidates
    \end{itemize}
    \item Hybrid fusion: Merge results using Reciprocal Rank Fusion
    \begin{itemize}
        \item Combines keyword + semantic scores
        \item Output: Top 200 candidates for ranking
    \end{itemize}
\end{itemize}

\textbf{Stage 3: Ranking (200ms)}
\begin{itemize}
    \item Feature generation (Frost):
    \begin{itemize}
        \item Engagement: CTR, play time, favorites
        \item Quality: clickbait score (my system), creator reputation
        \item Freshness: publish date, update recency
        \item Personalization: user's age group, past scary game plays
    \end{itemize}
    \item LTR model inference:
    \begin{itemize}
        \item XGBoost model with $\sim$100 features
        \item Outputs relevance score per candidate
    \end{itemize}
    \item Re-ranking \& filtering:
    \begin{itemize}
        \item Diversity: max 2 games from same creator in top 10
        \item Safety: filter age-inappropriate content for this user
        \item Policy: demote clickbait (score > 0.8 threshold)
    \end{itemize}
\end{itemize}

\textbf{Stage 4: Serving (50ms)}
\begin{itemize}
    \item Check cache (Redis): popular queries cached for 5 minutes
    \item Format results: thumbnails, titles, metadata
    \item Log: query, user, results for offline analysis
    \item Return: Top 20 results to client
\end{itemize}

\textbf{Total Latency:} P99 $\sim$ 400-500ms (within budget)"

\subsubsection{Q: How do you handle very popular queries that could overwhelm the system?}

\textbf{Answer:}

"Popular queries like 'Adopt Me' or 'Brookhaven' can spike to thousands of QPS. We use multi-layer caching:

\textbf{Layer 1: CDN Edge Cache}
\begin{itemize}
    \item Results cached at edge nodes (close to users geographically)
    \item TTL: 5 minutes (balance freshness vs load)
    \item Invalidation: on ranking model updates or game policy changes
    \item Benefit: Offloads 80-90\% of popular query traffic
\end{itemize}

\textbf{Layer 2: Application Cache (Redis)}
\begin{itemize}
    \item Full result set cached in Redis (including ranking scores)
    \item TTL: 1 minute (more aggressive, needs fresher data)
    \item Personalization: cache key includes user age group (coarse-grained)
    \item Benefit: Handles cache misses from Layer 1
\end{itemize}

\textbf{Layer 3: Partial Result Cache}
\begin{itemize}
    \item Cache retrieval candidates (top 200) separately from ranking
    \item Ranking still computed per-request (for personalization)
    \item Benefit: Saves expensive retrieval step, allows real-time personalization
\end{itemize}

\textbf{Graceful Degradation:}
\begin{itemize}
    \item If ranking service is overloaded (P99 > 1s):
    \begin{itemize}
        \item Fall back to simpler ranking (BM25 only, skip LTR model)
        \item Still functional, just less optimal
    \end{itemize}
    \item If Elasticsearch is overloaded:
    \begin{itemize}
        \item Serve stale cache (extend TTL to 10 minutes)
        \item Better than showing errors to users
    \end{itemize}
\end{itemize}

\textbf{Rate Limiting:}
\begin{itemize}
    \item Per-user rate limit: 10 queries/second (prevent abuse)
    \item Per-IP rate limit: 100 queries/second (prevent DDoS)
\end{itemize}

This multi-layer approach ensures we can handle traffic spikes without degrading user experience."

\subsubsection{Q: What's your strategy for ranking results for a brand new user?}

\textbf{Answer:}

"New users have no history, so we use a combination of defaults and rapid learning:

\textbf{Initial Strategy (Session 1):}
\begin{itemize}
    \item \textbf{Age-based filtering:}
    \begin{itemize}
        \item User age (from account signup) determines content appropriateness
        \item Age 6-12: family-friendly games only
        \item Age 13+: broader content, still filtered by safety policies
    \end{itemize}
    \item \textbf{Popularity as proxy:}
    \begin{itemize}
        \item Default to globally popular games (high play time, favorites)
        \item Assumption: popular games are high-quality, good introduction
    \end{itemize}
    \item \textbf{Diversity:}
    \begin{itemize}
        \item Show diverse genres (roleplay, obby, tycoon, simulator)
        \item Goal: learn user preferences quickly through exploration
    \end{itemize}
\end{itemize}

\textbf{Rapid Learning (Sessions 2-10):}
\begin{itemize}
    \item \textbf{Implicit feedback:}
    \begin{itemize}
        \item Track clicks: which games did user click?
        \item Track play time: which games did user play for > 5 minutes?
        \item Track favorites: which games did user save?
    \end{itemize}
    \item \textbf{Genre preference detection:}
    \begin{itemize}
        \item If user plays 3+ roleplay games $\rightarrow$ boost roleplay genre
        \item If user plays 0 scary games $\rightarrow$ reduce scary genre
    \end{itemize}
    \item \textbf{Creator affinity:}
    \begin{itemize}
        \item If user plays multiple games from Creator X $\rightarrow$ boost Creator X's other games
    \end{itemize}
\end{itemize}

\textbf{Long-Term Personalization (Sessions 10+):}
\begin{itemize}
    \item Full personalization model (user embedding, collaborative filtering)
    \item Social signals: friends' games, group memberships
    \item Temporal patterns: play more tycoon games on weekends
\end{itemize}

\textbf{Balancing Exploration vs Exploitation:}
\begin{itemize}
    \item \textbf{Exploitation:} Show games similar to past plays (80\%)
    \item \textbf{Exploration:} Inject random high-quality games (20\%)
    \item Prevents filter bubble, allows discovering new interests
\end{itemize}

The key is \textbf{gradual personalization}: start with safe defaults, learn quickly from early interactions, then fully personalize."

\subsubsection{Q: How do you detect and fix search quality issues?}

\textbf{Answer:}

"Search quality degrades over time (user behavior changes, new content, model drift). We use multiple detection mechanisms:

\textbf{Detection Layer 1: Automated Monitoring}
\begin{itemize}
    \item \textbf{Query-level metrics:}
    \begin{itemize}
        \item Zero result rate (alert if > 10\%)
        \item CTR drop (alert if drops > 5\% week-over-week)
        \item Query reformulation rate (alert if > 30\%)
    \end{itemize}
    \item \textbf{System-level metrics:}
    \begin{itemize}
        \item Latency (alert if P99 > 1s)
        \item Error rate (alert if > 1\%)
    \end{itemize}
\end{itemize}

\textbf{Detection Layer 2: Human Evaluation}
\begin{itemize}
    \item \textbf{Monthly audits:}
    \begin{itemize}
        \item Sample 100 top queries (cover 50\%+ of search volume)
        \item Human raters assess: relevance (1-5 scale), quality, diversity
        \item Compare to baseline (previous month, competitor platforms)
    \end{itemize}
    \item \textbf{Issue categorization:}
    \begin{itemize}
        \item Retrieval issues: relevant games not retrieved
        \item Ranking issues: relevant games retrieved but ranked poorly
        \item Quality issues: low-quality games ranked highly
    \end{itemize}
\end{itemize}

\textbf{Detection Layer 3: User Feedback}
\begin{itemize}
    \item "Report search result" button (crowdsourced quality)
    \item Support tickets mentioning search problems
    \item Social media monitoring (Twitter, Discord complaints)
\end{itemize}

\textbf{Root Cause Analysis (Example: Clickbait Problem):}
\begin{enumerate}
    \item \textbf{Symptom:} CEO reports poor results for "Brookhaven"
    \item \textbf{Hypothesis generation:}
    \begin{itemize}
        \item Retrieval issue? (relevant games not found)
        \item Ranking issue? (copycat games ranked too high)
        \item Data quality issue? (games have misleading metadata)
    \end{itemize}
    \item \textbf{Investigation:}
    \begin{itemize}
        \item Manual inspection: top 20 results for "Brookhaven"
        \item Finding: 15/20 are copycats with identical names/images
        \item Root cause: \textbf{Ranking issue} (engagement signals favor clickbait)
    \end{itemize}
    \item \textbf{Fix:} Clickbait detection system (my project)
    \begin{itemize}
        \item Add quality signal to ranking (clickbait score)
        \item Demote games with high clickbait scores
    \end{itemize}
    \item \textbf{Validation:}
    \begin{itemize}
        \item A/B test: treatment group sees demoted clickbait
        \item Metrics: CTR, play time, user satisfaction surveys
        \item Result: CTR slightly down (clickbait has high CTR by design), but play time up (users find quality games)
    \end{itemize}
\end{enumerate}

\textbf{Continuous Improvement Loop:}
\begin{itemize}
    \item Detect $\rightarrow$ Diagnose $\rightarrow$ Fix $\rightarrow$ Validate $\rightarrow$ Repeat
    \item Quarterly model retraining (new data, new features)
    \item Annual architecture review (e.g., switch to transformer-based ranking)
\end{itemize}

The key is \textbf{multi-signal detection} (automated + human + user feedback) and \textbf{rigorous validation} (A/B tests, not just intuition)."

\subsubsection{Q: How would you add a new ranking signal to the system?}

\textbf{Answer:}

"I'll walk through this using the clickbait score as a concrete example, since I actually did this:

\textbf{Step 1: Signal Definition \& Feasibility (Week 1)}
\begin{itemize}
    \item \textbf{Define signal:} Clickbait score (0-1, higher = more likely clickbait)
    \item \textbf{Hypothesis:} Demoting clickbait will improve search quality (play time, user satisfaction)
    \item \textbf{Feasibility check:}
    \begin{itemize}
        \item Can we compute it? (Yes: text rules + CLIP + YOLO)
        \item At what latency? (Real-time: 10 seconds for new games)
        \item Coverage? (Top 30 games initially, expandable)
    \end{itemize}
\end{itemize}

\textbf{Step 2: Offline Pipeline Development (Weeks 2-4)}
\begin{itemize}
    \item Build detection service (covered in Part 1)
    \item Backfill existing games (hundreds of millions)
    \item Store in Frost feature table: \texttt{clickbait\_games\_features\_v0}
    \item Validate: human eval on sample (95\%+ precision)
\end{itemize}

\textbf{Step 3: Feature Integration (Week 5)}
\begin{itemize}
    \item \textbf{Frost schema update:}
    \begin{itemize}
        \item Add \texttt{clickbait\_score} column to ranking features table
        \item Ensure backward compatibility (default 0.0 for missing values)
    \end{itemize}
    \item \textbf{Ranking service integration:}
    \begin{itemize}
        \item Modify feature generation code to fetch \texttt{clickbait\_score}
        \item Add to LTR model input (new feature column)
    \end{itemize}
\end{itemize}

\textbf{Step 4: Model Training (Week 6)}
\begin{itemize}
    \item \textbf{Offline training:}
    \begin{itemize}
        \item Retrain XGBoost model with new feature (100 $\rightarrow$ 101 features)
        \item Use historical data (query logs, user interactions)
        \item Objective: maximize play time, minimize clickbait score
    \end{itemize}
    \item \textbf{Offline evaluation:}
    \begin{itemize}
        \item Test set: held-out queries from last month
        \item Metrics: NDCG@10, MRR, clickbait in top 10 (should decrease)
        \item Result: NDCG improves +2\%, clickbait in top 10 drops -40\%
    \end{itemize}
\end{itemize}

\textbf{Step 5: Online A/B Testing (Weeks 7-9)}
\begin{itemize}
    \item \textbf{Experiment setup:}
    \begin{itemize}
        \item Control: Current ranking (no clickbait signal)
        \item Treatment: New ranking (with clickbait signal)
        \item Traffic split: 95\% control, 5\% treatment (safe rollout)
    \end{itemize}
    \item \textbf{Metrics tracked:}
    \begin{itemize}
        \item Primary: Play time (should increase)
        \item Secondary: CTR (may decrease, clickbait has high CTR)
        \item Guardrail: Zero result rate, latency (should not regress)
    \end{itemize}
    \item \textbf{Analysis:}
    \begin{itemize}
        \item Play time: +5\% (statistically significant)
        \item CTR: -2\% (acceptable trade-off)
        \item User surveys: satisfaction up +3\%
    \end{itemize}
    \item \textbf{Decision:} Ship to 100\%
\end{itemize}

\textbf{Step 6: Gradual Rollout (Week 10)}
\begin{itemize}
    \item Ramp: 5\% $\rightarrow$ 25\% $\rightarrow$ 50\% $\rightarrow$ 100\% over 2 weeks
    \item Monitor for issues (latency spikes, error rate)
    \item Rollback plan: instant revert to control if critical issue
\end{itemize}

\textbf{Step 7: Post-Launch Monitoring (Ongoing)}
\begin{itemize}
    \item Weekly metrics review (play time, clickbait in top 10)
    \item Monthly model retraining (incorporate new data)
    \item Iterate on clickbait detection (expand PAL, improve precision)
\end{itemize}

\textbf{Key Lessons:}
\begin{itemize}
    \item \textbf{Validate early:} Offline eval before A/B (avoids wasted user exposure)
    \item \textbf{Gradual rollout:} Catch issues before full launch
    \item \textbf{Multiple metrics:} Don't optimize for single metric (CTR alone would favor clickbait)
    \item \textbf{Cross-team coordination:} Needed DSA for backfill, Game Discovery for ranking integration
\end{itemize}"

\subsubsection{Q: What's your biggest challenge with search at Roblox's scale?}

\textbf{Answer:}

"The biggest challenge is the \textbf{tension between freshness and quality signals}.

\textbf{The Problem:}
\begin{itemize}
    \item \textbf{New games} (created every second):
    \begin{itemize}
        \item Need to surface quickly (creators expect instant visibility)
        \item But have no engagement data (no CTR, play time, favorites)
        \item Can't rely on quality signals (clickbait detection requires metadata)
    \end{itemize}
    \item \textbf{Quality signals} (engagement, ratings):
    \begin{itemize}
        \item Take time to accumulate (days/weeks)
        \item New games start at a disadvantage (low ranking)
        \item Creates barrier for new creators (can't break through)
    \end{itemize}
\end{itemize}

\textbf{Specific Example (Illustrative):}
\begin{itemize}
    \item Creator X publishes high-quality game at 10:00 AM
    \item Game is indexed in Elasticsearch within seconds (real-time)
    \item But ranking model sees:
    \begin{itemize}
        \item Engagement: 0 clicks, 0 play time (no data yet)
        \item Quality: 0 favorites, no clickbait score yet (processing delay)
        \item Creator reputation: unknown (new creator)
    \end{itemize}
    \item Result: Game ranks poorly despite being high-quality
    \item Creator frustrated: "My game doesn't show up in search!"
\end{itemize}

\textbf{Our Mitigations (Partial Solutions):}

\begin{enumerate}
    \item \textbf{Freshness Boost:}
    \begin{itemize}
        \item New games (published < 24 hours) get temporary ranking boost
        \item Decays exponentially over time (day 1: +10\%, day 7: +1\%)
        \item Gives new games a chance to accumulate engagement
    \end{itemize}
    
    \item \textbf{Content-Based Signals (No Engagement Required):}
    \begin{itemize}
        \item Thumbnail quality (CLIP embedding $\rightarrow$ aesthetic score)
        \item Description completeness (has detailed description?)
        \item Genre tagging (properly categorized?)
        \item These signals available immediately
    \end{itemize}
    
    \item \textbf{Creator Reputation Transfer:}
    \begin{itemize}
        \item If Creator X has 3 high-quality games previously, new game inherits reputation
        \item Assumption: good creators make good games
        \item Helps established creators, but not new ones
    \end{itemize}
    
    \item \textbf{Rapid Engagement Acceleration:}
    \begin{itemize}
        \item Show new games in "New \& Trending" section (separate from main search)
        \item Users browsing this section provide early engagement signals
        \item Signals feed back into main search ranking within hours
    \end{itemize}
\end{enumerate}

\textbf{Why This Remains Hard:}
\begin{itemize}
    \item \textbf{Cold start is unsolvable:} Can't predict quality without data
    \item \textbf{Gaming the system:} Freshness boost incentivizes spam (publish many low-quality games)
    \item \textbf{Balance:} Too much freshness boost $\rightarrow$ low-quality games surface; too little $\rightarrow$ new creators can't break through
\end{itemize}

\textbf{Future Directions I'd Explore:}
\begin{itemize}
    \item \textbf{Pre-launch quality prediction:}
    \begin{itemize}
        \item Analyze game before publish (in Studio editor)
        \item Predict quality from: code complexity, asset quality, playtesting data
        \item Give creators feedback: "Add more detail to your description to improve searchability"
    \end{itemize}
    \item \textbf{Multi-armed bandit:}
    \begin{itemize}
        \item Treat new game ranking as exploration-exploitation problem
        \item Dynamically adjust freshness boost based on early engagement
        \item If game gets high CTR in first hour $\rightarrow$ increase boost; if low CTR $\rightarrow$ decrease boost
    \end{itemize}
\end{itemize}

This challenge touches multiple areas: ranking algorithms, creator incentives, platform economics - exactly the kind of \textbf{cross-functional systems problem} I enjoy solving."

\subsection{Connection to DataHub (Search Context)}

\begin{keyinsight}
\textbf{How Your Search Experience Applies to DataHub:}

"DataHub's core product is \textbf{metadata search and discovery} - finding the right dataset, understanding lineage, discovering related data. This maps directly to my search experience:

\begin{itemize}
    \item \textbf{Roblox Game Search} $\leftrightarrow$ \textbf{DataHub Dataset Search}
    \begin{itemize}
        \item Games have metadata (title, description, creator) $\leftrightarrow$ Datasets have metadata (schema, owner, tags)
        \item Users search for games (keywords, concepts) $\leftrightarrow$ Data scientists search for datasets (column names, business context)
        \item Quality signals (engagement, clickbait) $\leftrightarrow$ Quality signals (data quality scores, usage frequency)
    \end{itemize}
    
    \item \textbf{RSS (Vector Database)} $\leftrightarrow$ \textbf{Semantic Dataset Search}
    \begin{itemize}
        \item CLIP embeddings for game thumbnails $\leftrightarrow$ Embeddings for schema/column descriptions
        \item Semantic search: "anime style games" $\leftrightarrow$ "customer transaction data"
        \item Both solve: keywords alone don't capture conceptual similarity
    \end{itemize}
    
    \item \textbf{RAG Platform} $\leftrightarrow$ \textbf{DataHub Documentation/Lineage}
    \begin{itemize}
        \item RAG over Roblox docs (policies, guides) $\leftrightarrow$ RAG over dataset documentation (data dictionaries, ETL logic)
        \item Retrieval: find relevant context $\leftrightarrow$ Lineage: find upstream/downstream dependencies
        \item Generation: answer questions $\leftrightarrow$ Explain: "How is this field computed?"
    \end{itemize}
    
    \item \textbf{Quality Signals (Clickbait Detection)} $\leftrightarrow$ \textbf{Data Quality Signals}
    \begin{itemize}
        \item Game quality: clickbait score, engagement, ratings $\leftrightarrow$ Dataset quality: completeness, freshness, usage
        \item Both require: real-time signal generation, integration with ranking
        \item Both face: trade-offs between quality and discoverability
    \end{itemize}
\end{itemize}

I'm excited about DataHub because it's the \textbf{same core problem} - search, discovery, quality - but in the data infrastructure domain. The techniques I've built (vector search, RAG, quality signals) directly transfer."
\end{keyinsight}

% ============================================================
% Appendix: Quick Reference
% ============================================================
\newpage
\appendix
\section{Quick Reference: Key Numbers}

\begin{table}[h]
\centering
\begin{tabular}{|l|l|}
\hline
\textbf{Metric} & \textbf{Value} \\ \hline
Previous Approach Precision & $\sim$50\% (near-random) \\ \hline
Title Heuristics Precision & 99\%+ \\ \hline
CLIP Image Similarity Precision & 95\%+ \\ \hline
YOLO + CLIP Logo Precision & 99.5\%+ \\ \hline
Real-Time Latency & $\sim$10 seconds end-to-end \\ \hline
SQS Throughput & $\sim$10 QPS game updates \\ \hline
PAL Size (Initial) & $\sim$30-100 entries \\ \hline
PAL Size (Current) & Hundreds of entries \\ \hline
Query Coverage & 20-30\% of top queries \\ \hline
Protected Games (Top) & 30 games (expandable) \\ \hline
Backfill Scale & Hundreds of millions of games \\ \hline
Project Timeline & Q1 2024 ($\sim$3 months) \\ \hline
Teams Coordinated & 5 (DSA, Human Eval, Brand Safety, ML Platform, Game Discovery) \\ \hline
Creator Backlash & Zero (policy success) \\ \hline
\end{tabular}
\end{table}

\section{Quick Reference: Architecture Components}

\begin{itemize}
    \item \textbf{PAL (Protected Assets Library):} Centralized registry (text, image, logo)
    \item \textbf{SNS Topics:} PlaceEntityChangeEvents, UniverseDisplayInformationChangeEvents
    \item \textbf{SQS Queue:} Buffer for game update events ($\sim$10 QPS)
    \item \textbf{Queue Processor:} Consumes SQS, calls Detection Service
    \item \textbf{Detection Service (bedev2):} Multi-signal analysis (text, CLIP, YOLO)
    \item \textbf{RSS (Vector DB):} CLIP embeddings storage and similarity search
    \item \textbf{Frost Feature Store:} Table \texttt{clickbait\_games\_features\_v0}
    \item \textbf{Elasticsearch:} Indexed for search ranking integration
    \item \textbf{MLP Pipelines:} Offline backfill (per-feature: title, thumbnail, logo)
    \item \textbf{Powerhouse:} Aggregates scores, publishes to Frost in batch
\end{itemize}

\section{Quick Reference: Contact Info}

\begin{itemize}
    \item \textbf{Interviewer:} Abe (DataHub)
    \item \textbf{Interview Type:} Day 2 Onsite - Project Deep Dive + Search Architecture
    \item \textbf{Format:} 45-60 minutes
    \item \textbf{Your Role:} Principal Software Engineer, Roblox
    \item \textbf{Project:} Game Clickbait Detection System (Q1 2024)
\end{itemize}

\end{document}
