\documentclass[10pt]{article}
\usepackage[margin=0.75in]{geometry}
\usepackage{amsmath,amsthm,amsfonts,amssymb}
\usepackage{color,graphicx}
\usepackage{hyperref}
\usepackage{longtable}
\usepackage{array}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{colortbl}
\usepackage{listings}

% Define colors
\definecolor{priority1}{RGB}{255,200,200}
\definecolor{priority2}{RGB}{255,235,200}
\definecolor{priority3}{RGB}{200,255,200}

% Configure listings for code blocks
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{lightgray!20},
    basicstyle=\ttfamily\small,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2
}

\lstset{style=mystyle}

\title{\textbf{ML System Design Study Guide} \\ \large Production ML Infrastructure for Staff/Principal Interviews}
\author{}
\date{}

\begin{document}
\maketitle

\section*{Overview}

This study guide accompanies the \textit{ML System Design Templates} cheatsheet. Use this document to:
\begin{itemize}
\item Master production ML system design (serving, training, monitoring)
\item Understand real-world ML infrastructure (feature stores, model registries)
\item Practice end-to-end ML system architecture for common use cases
\item Prepare for 45-60 minute ML system design rounds at Staff/Principal level
\end{itemize}

\textbf{What is ML System Design?}

Different from algorithm interviews, ML system design tests your ability to:
\begin{enumerate}
\item \textbf{Build end-to-end ML systems} - From data ingestion to production serving
\item \textbf{Scale ML infrastructure} - Handle millions of users, petabytes of data
\item \textbf{Make trade-offs} - Accuracy vs latency, cost vs performance
\item \textbf{Operate in production} - Monitoring, retraining, A/B testing
\end{enumerate}

\section{Study Strategy: System-First Approach}

\subsection{Why Start with Common Systems?}

Unlike algorithm interviews (pattern-based), ML system design is \textbf{use-case driven}:
\begin{itemize}
\item 80\% of questions fall into 7-8 common patterns (recommendation, search, fraud detection, etc.)
\item Each pattern has a \textbf{canonical architecture} with proven solutions
\item Interviewers expect you to know industry best practices
\end{itemize}

\textbf{Wrong approach}: Study infrastructure components in isolation \\
\textbf{Right approach}: Learn complete systems end-to-end, then generalize

\subsection{The 28-Day Preparation Plan}

\textbf{Week 1: Core Systems (Days 1-7)}

Master these 3 systems - they cover 60\% of questions:

\textbf{Day 1-2: Recommendation System}
\begin{itemize}
\item \textbf{Examples}: Netflix, YouTube, Spotify, TikTok
\item \textbf{Study}: Collaborative filtering, two-tower models, cold start
\item \textbf{Practice}: Design "YouTube recommendations" from scratch
\item \textbf{Key concepts}: ANN search, embedding tables, real-time serving
\end{itemize}

\textbf{Day 3-4: Search \& Ranking}
\begin{itemize}
\item \textbf{Examples}: Google Search, Amazon product search
\item \textbf{Study}: Candidate generation → Ranking → Re-ranking
\item \textbf{Practice}: Design "Amazon product search"
\item \textbf{Key concepts}: Inverted index, NDCG, query understanding
\end{itemize}

\textbf{Day 5-7: Ad Click Prediction (CTR)}
\begin{itemize}
\item \textbf{Examples}: Google Ads, Facebook Ads
\item \textbf{Study}: Feature engineering, DeepFM, calibration
\item \textbf{Practice}: Design "Google Ads CTR prediction"
\item \textbf{Key concepts}: Factorization machines, negative sampling, auction
\end{itemize}

\textbf{Week 2: Additional Systems (Days 8-14)}

\textbf{Day 8-9: Computer Vision Systems}
\begin{itemize}
\item \textbf{Image classification}: Content moderation, medical diagnosis
\item \textbf{Object detection}: Self-driving cars, surveillance
\item \textbf{Practice}: Design "content moderation for Instagram"
\item \textbf{Key concepts}: Transfer learning, model quantization, edge deployment
\end{itemize}

\textbf{Day 10-11: NLP Systems}
\begin{itemize}
\item \textbf{Text classification}: Spam detection, sentiment analysis
\item \textbf{Question answering}: Chatbots, search
\item \textbf{Practice}: Design "chatbot for customer support"
\item \textbf{Key concepts}: BERT fine-tuning, RAG, prompt engineering
\end{itemize}

\textbf{Day 12-13: Fraud Detection}
\begin{itemize}
\item \textbf{Examples}: Credit card fraud, fake accounts, bot detection
\item \textbf{Practice}: Design "payment fraud detection system"
\item \textbf{Key concepts}: Real-time scoring, graph features, precision-recall trade-off
\end{itemize}

\textbf{Day 14: Feed Ranking}
\begin{itemize}
\item \textbf{Examples}: Facebook News Feed, Twitter Timeline
\item \textbf{Practice}: Design "social media feed ranking"
\item \textbf{Key concepts}: Multi-task learning, diversity, engagement prediction
\end{itemize}

\textbf{Week 3: Infrastructure Deep Dive (Days 15-21)}

Now that you know the systems, understand the components:

\textbf{Day 15-16: Training Infrastructure}
\begin{itemize}
\item Distributed training (data parallelism, model parallelism)
\item Hyperparameter tuning (Bayesian optimization, early stopping)
\item Experiment tracking (MLflow, Weights \& Biases)
\end{itemize}

\textbf{Day 17-18: Serving Infrastructure}
\begin{itemize}
\item Model serving (TensorFlow Serving, TorchServe, ONNX)
\item Optimization (batching, quantization, distillation)
\item Deployment strategies (blue-green, canary, shadow)
\end{itemize}

\textbf{Day 19-20: Data Infrastructure}
\begin{itemize}
\item Feature stores (Feast, Tecton)
\item Feature engineering pipelines (Spark, Airflow)
\item Data labeling (active learning, weak supervision)
\end{itemize}

\textbf{Day 21: Monitoring \& Evaluation}
\begin{itemize}
\item Metrics (offline vs online, guardrails)
\item Drift detection (data drift, concept drift)
\item A/B testing, multi-armed bandits
\end{itemize}

\textbf{Week 4: Practice \& Polish (Days 22-28)}

\textbf{Day 22-24: Mock Interviews}
\begin{itemize}
\item Practice 3-4 full system designs (45 min each)
\item Use MADE framework: Model → API → Data → Evaluation
\item Record yourself, identify gaps
\end{itemize}

\textbf{Day 25-26: Trade-off Mastery}
\begin{itemize}
\item Review all systems, focus on justifying choices
\item Practice explaining: "Why XGBoost over neural network?"
\item Prepare for follow-up: "How would you reduce latency by 10×?"
\end{itemize}

\textbf{Day 27-28: Final Review}
\begin{itemize}
\item Review all 7 core systems (30 min each)
\item Memorize capacity estimation formulas
\item Prepare questions for interviewer
\end{itemize}

\subsection{Spaced Repetition for Systems}

For each ML system (e.g., recommendation), use this schedule:

\begin{itemize}
\item \textbf{Day 1-2}: Deep study (3-4 hours total)
\begin{itemize}
\item Read system design from cheatsheet
\item Watch tech talks (YouTube, engineering blogs)
\item Sketch architecture on whiteboard
\item Practice designing from scratch
\end{itemize}

\item \textbf{Day 4}: Quick review (30 min)
\begin{itemize}
\item Redraw architecture from memory
\item Verify all components present
\end{itemize}

\item \textbf{Day 7}: Re-practice (45 min)
\begin{itemize}
\item Do full mock interview for this system
\item Time yourself (45 minutes)
\end{itemize}

\item \textbf{Day 14}: Final review (20 min)
\begin{itemize}
\item Quick whiteboard sketch
\item Review trade-offs and alternatives
\end{itemize}
\end{itemize}

\section{Common Weak Spots}

Most candidates struggle with these areas - prioritize if time-limited:

\subsection{Conceptual Weak Spots}

\textbf{1. Feature Engineering for ML Systems}
\begin{itemize}
\item \textbf{Problem}: Don't know how to design features for recommendation/ranking
\item \textbf{Symptoms}: Suggest "use all available data" without specificity
\item \textbf{Fix}: Study feature categories (user, item, context, cross-features)
\item \textbf{Practice}: For each system, list 10-15 specific features
\end{itemize}

\textbf{2. Offline vs Online Metrics}
\begin{itemize}
\item \textbf{Problem}: Only discuss accuracy, ignore business metrics
\item \textbf{Symptoms}: "We'll use AUC" for ad CTR prediction
\item \textbf{Fix}: Offline (AUC, NDCG) trains model; Online (CTR, revenue) measures success
\item \textbf{Practice}: For each system, identify both offline and online metrics
\end{itemize}

\textbf{3. Cold Start Problem}
\begin{itemize}
\item \textbf{Problem}: Don't have solution for new users/items
\item \textbf{Symptoms}: Assume collaborative filtering works for everyone
\item \textbf{Fix}: Hybrid approach (content-based + collaborative filtering)
\item \textbf{Practice}: Design onboarding flow for new Netflix user
\end{itemize}

\textbf{4. Real-time vs Batch Serving}
\begin{itemize}
\item \textbf{Problem}: Don't understand when to use each
\item \textbf{Symptoms}: Propose real-time serving for email recommendations
\item \textbf{Fix}: Real-time = user blocking (search, ads); Batch = precompute (email)
\item \textbf{Practice}: Classify 10 use cases as real-time or batch
\end{itemize}

\textbf{5. Model Deployment \& Rollback}
\begin{itemize}
\item \textbf{Problem}: "Just deploy new model to production"
\item \textbf{Symptoms}: No canary, A/B test, or rollback strategy
\item \textbf{Fix}: Always use canary (5\% traffic) → monitor → gradual rollout
\item \textbf{Practice}: Design deployment pipeline with rollback trigger
\end{itemize}

\subsection{Infrastructure Weak Spots}

\textbf{1. Feature Store Purpose}
\begin{itemize}
\item \textbf{Problem}: Don't understand why feature store is needed
\item \textbf{Symptoms}: Propose duplicating feature logic for training \& serving
\item \textbf{Fix}: Feature store ensures training-serving consistency
\item \textbf{Practice}: Explain how feature store prevents data skew
\end{itemize}

\textbf{2. Embedding Table Scaling}
\begin{itemize}
\item \textbf{Problem}: Don't know how to handle 1B user embeddings
\item \textbf{Symptoms}: "Store in Redis" (crashes with OOM)
\item \textbf{Fix}: Sharding (hash user\_id mod N), parameter server, ANN index
\item \textbf{Practice}: Calculate memory for 1B users × 128-dim embeddings
\end{itemize}

\textbf{3. Model Serving Optimization}
\begin{itemize}
\item \textbf{Problem}: Model too slow, don't know how to optimize
\item \textbf{Symptoms}: "Use bigger GPU"
\item \textbf{Fix}: Batching, quantization (FP32→INT8), distillation, caching
\item \textbf{Practice}: List 5 ways to reduce inference latency by 10×
\end{itemize}

\textbf{4. Data Drift Detection}
\begin{itemize}
\item \textbf{Problem}: Model degrades over time, don't monitor
\item \textbf{Symptoms}: "Model works in production, we're done"
\item \textbf{Fix}: Monitor feature distributions, prediction distributions, online metrics
\item \textbf{Practice}: Design drift detection pipeline with auto-retrain trigger
\end{itemize}

\section{Staff/Principal Level Expectations}

\subsection{Beyond Basic Design}

At Staff/Principal level, interviewers expect you to:

\textbf{1. Justify Every Choice}

Don't just say "use BERT for text classification" - explain:
\begin{itemize}
\item \textbf{Why BERT?} → High accuracy, pretrained on large corpus, fine-tune on domain
\item \textbf{Why not DistilBERT?} → If latency is critical (< 50ms), use DistilBERT
\item \textbf{Why not TF-IDF + Logistic Regression?} → For small dataset, this might be better
\item \textbf{Trade-offs}: Accuracy (BERT > DistilBERT > LR), Latency (opposite order)
\end{itemize}

\textbf{2. Quantitative Analysis}

\textbf{Bad answer}: "We'll use GPUs for training" \\
\textbf{Good answer}: "Training 100M samples, 10 epochs, ResNet-50 (4B FLOPs/image) on V100 (125 TFLOPS) with batch size 256 → approximately 8 GPU-days → \$2,500 on AWS"

Practice calculating:
\begin{itemize}
\item Training time \& cost
\item Serving QPS per instance
\item Storage for embeddings
\item Bandwidth for data transfer
\end{itemize}

\textbf{3. Failure Mode Analysis}

For each component, discuss what can go wrong:
\begin{itemize}
\item \textbf{Model server crashes} → Load balancer + multiple replicas, health checks
\item \textbf{Feature missing at serving} → Default value strategy, fallback model
\item \textbf{Data drift detected} → Auto-retrain pipeline, human review before deploy
\item \textbf{Model too slow} → Circuit breaker, fallback to simple model
\end{itemize}

\textbf{4. Real-World Constraints}

Always consider:
\begin{itemize}
\item \textbf{Privacy}: GDPR, CCPA → federated learning, differential privacy
\item \textbf{Fairness}: Avoid bias in gender, race → fairness constraints, debiasing
\item \textbf{Explainability}: Regulators want interpretability → SHAP, LIME
\item \textbf{Cost}: Limited budget → choose cost-effective solution
\end{itemize}

\subsection{Trade-off Discussions}

Prepare to discuss these for every system:

\textbf{Accuracy vs Latency}
\begin{itemize}
\item \textbf{High accuracy}: BERT-large, ensemble models, deep networks
\item \textbf{Low latency}: DistilBERT, quantization, model pruning, caching
\item \textbf{Decision point}: Search ranking (100ms OK) vs ad serving (10ms critical)
\end{itemize}

\textbf{Complexity vs Interpretability}
\begin{itemize}
\item \textbf{Complex (black box)}: Neural networks, gradient boosting
\item \textbf{Interpretable}: Linear regression, decision trees
\item \textbf{Decision point}: Fraud detection (need to explain) vs image classification (accuracy matters)
\end{itemize}

\textbf{Real-time vs Batch}
\begin{itemize}
\item \textbf{Real-time}: Higher cost, lower latency, user-blocking
\item \textbf{Batch}: Lower cost, higher latency, precompute overnight
\item \textbf{Decision point}: Search (real-time) vs email recommendations (batch)
\end{itemize}

\textbf{Precision vs Recall}
\begin{itemize}
\item \textbf{High precision}: Minimize false positives (fraud alerts annoy users)
\item \textbf{High recall}: Catch all positives (cancer screening, safety-critical)
\item \textbf{Decision point}: Fraud (balance both) vs medical (recall critical)
\end{itemize}

\textbf{Training Data Size}
\begin{itemize}
\item \textbf{Small data (< 10K)}: Classical ML, transfer learning, data augmentation
\item \textbf{Large data (> 1M)}: Deep learning, large models
\item \textbf{Decision point}: Startup (small) vs FAANG (large)
\end{itemize}

\subsection{Responsible AI: Fairness, Bias \& Explainability}

At Staff/Principal level, you're expected to proactively address responsible AI concerns. This separates strategic thinkers from implementers.

\textbf{1. Fairness \& Bias Detection}

\textbf{Key Metrics to Monitor:}
\begin{itemize}
\item \textbf{Demographic Parity}: $P(\hat{Y}=1|A=a) = P(\hat{Y}=1|A=b)$ for sensitive attribute $A$ (gender, race)
\item \textbf{Equal Opportunity}: $P(\hat{Y}=1|Y=1,A=a) = P(\hat{Y}=1|Y=1,A=b)$ (equal TPR across groups)
\item \textbf{Equalized Odds}: Equal TPR and FPR across groups
\item \textbf{Calibration}: $P(Y=1|\hat{p}=0.7,A=a) = 0.7$ for all groups
\end{itemize}

\textbf{Interview Scenario:} Design a hiring screening system

\textbf{Bad answer}: "Train XGBoost on historical data, maximize AUC"

\textbf{Good answer}:
\begin{enumerate}
\item \textbf{Identify bias sources}: Historical hiring data may reflect past discrimination → need to audit
\item \textbf{Measure disparate impact}: Check if acceptance rate differs by gender/race → 80\% rule (EEOC)
\item \textbf{Mitigation strategies}:
\begin{itemize}
\item \textit{Pre-processing}: Reweigh training data, remove biased features
\item \textit{In-processing}: Add fairness constraints to loss function
\item \textit{Post-processing}: Adjust decision thresholds per group
\end{itemize}
\item \textbf{Trade-off}: Might sacrifice 2-3\% accuracy for fairness → discuss with stakeholders
\item \textbf{Monitoring}: Track fairness metrics in production, alert if demographic parity drops
\end{enumerate}

\textbf{Code Pattern for Bias Detection:}
\begin{verbatim}
# Calculate disparate impact
def disparate_impact(y_pred, sensitive_attr):
    groups = np.unique(sensitive_attr)
    approval_rates = {}

    for group in groups:
        mask = sensitive_attr == group
        approval_rate = np.mean(y_pred[mask])
        approval_rates[group] = approval_rate

    # 80% rule: min_rate / max_rate >= 0.8
    min_rate = min(approval_rates.values())
    max_rate = max(approval_rates.values())
    di_ratio = min_rate / max_rate

    return di_ratio, approval_rates
\end{verbatim}

\textbf{2. Explainability \& Interpretability}

\textbf{When interpretability matters:}
\begin{itemize}
\item \textbf{Regulated industries}: Healthcare (FDA approval), finance (credit decisions)
\item \textbf{High-stakes decisions}: Loan approvals, hiring, criminal justice
\item \textbf{Debugging}: Understanding model failures, feature engineering
\item \textbf{User trust}: Why was I rejected? What can I do differently?
\end{itemize}

\textbf{Techniques by Model Type:}

\textit{Inherently Interpretable Models:}
\begin{itemize}
\item \textbf{Linear models}: Feature weights directly interpretable
\item \textbf{Decision trees}: Human-readable rules, max depth 5-7
\item \textbf{Rule-based models}: If-then rules
\end{itemize}

\textit{Post-hoc Explanations (for black-box models):}
\begin{itemize}
\item \textbf{SHAP (SHapley Additive exPlanations)}: Game-theoretic feature attribution
\begin{itemize}
\item \textit{Global}: Which features are most important overall?
\item \textit{Local}: Why did this specific user get rejected?
\end{itemize}
\item \textbf{LIME (Local Interpretable Model-agnostic Explanations)}: Approximate locally with linear model
\item \textbf{Attention weights}: For transformers, visualize which tokens matter
\item \textbf{Saliency maps}: For CNNs, highlight important image regions
\end{itemize}

\textbf{Interview Pattern:} Design a credit scoring system

\textbf{Strategic answer}:
\begin{enumerate}
\item \textbf{Model choice}: Start with interpretable model (logistic regression, shallow tree)
\item \textbf{Benchmark}: If accuracy gap is small (< 5\%), stick with interpretable model
\item \textbf{If complex model needed}: Use XGBoost + SHAP for explanations
\item \textbf{Production serving}:
\begin{itemize}
\item Store top-5 SHAP values per prediction in database
\item API returns: Score + "Top factors: income (+15 pts), debt ratio (-8 pts)"
\item Legal review approved explanation templates
\end{itemize}
\item \textbf{Adverse action notices}: Must provide reasons for rejection (FCRA requirement)
\end{enumerate}

\textbf{3. Privacy-Preserving ML}

\textbf{Techniques for Scale:}
\begin{itemize}
\item \textbf{Differential Privacy}: Add calibrated noise to gradients/outputs
\begin{itemize}
\item \textit{Use case}: "Guarantee no single user affects model by more than $\epsilon$"
\item \textit{Trade-off}: Privacy ($\epsilon$ smaller) vs accuracy
\item \textit{Example}: Google's federated learning with $\epsilon=8$ DP
\end{itemize}
\item \textbf{Federated Learning}: Train on-device, aggregate gradients (no raw data transfer)
\begin{itemize}
\item \textit{Use case}: Keyboard prediction on phones (Gboard)
\item \textit{Challenges}: Non-IID data, stragglers, communication cost
\end{itemize}
\item \textbf{Homomorphic Encryption}: Compute on encrypted data
\begin{itemize}
\item \textit{Use case}: Hospital collaboration without sharing patient data
\item \textit{Trade-off}: 100-1000x slower than plaintext
\end{itemize}
\end{itemize}

\textbf{Interview Pattern:} Design personalized health recommendations

\textbf{Privacy-first answer}:
\begin{enumerate}
\item \textbf{Data minimization}: Only collect what's necessary, anonymize where possible
\item \textbf{Secure aggregation}: Train federated model on user devices
\item \textbf{Differential privacy}: Add noise to published model updates ($\epsilon=1.0$)
\item \textbf{Access controls}: Encrypt PII, role-based access, audit logs
\item \textbf{Right to be forgotten (GDPR)}: Implement model unlearning or retrain without user data
\end{enumerate}

\textbf{4. Model Governance at Scale}

\textbf{What to discuss:}
\begin{itemize}
\item \textbf{Model cards}: Document training data, performance metrics by demographic
\item \textbf{Bias dashboards}: Real-time fairness monitoring in production
\item \textbf{Human-in-the-loop}: High-risk predictions flagged for review
\item \textbf{Audit trails}: Log every prediction for compliance
\item \textbf{Model versioning}: Track which model version made each decision
\item \textbf{Red teaming}: Adversarial testing for bias and robustness
\end{itemize}

\textbf{Production Architecture Example:}
\begin{verbatim}
Request -> Model Server -> Prediction + SHAP values
                        -> Log to audit DB
                        -> Check fairness metrics
                        -> If high-risk, send to human review queue
                        -> Return prediction + explanation
\end{verbatim}

\textbf{Interview Talking Points:}

When designing ANY ML system, proactively mention:
\begin{enumerate}
\item \textbf{"Let me think about fairness..."} → Shows you consider bias, not just accuracy
\item \textbf{"For explainability..."} → SHAP for complex models, simpler models if possible
\item \textbf{"To protect privacy..."} → Anonymization, federated learning, differential privacy
\item \textbf{"For compliance..."} → GDPR right to deletion, FCRA adverse action notices
\end{enumerate}

Don't wait for interviewer to ask - Staff/Principal engineers proactively design responsible systems.

\subsection{Advanced A/B Testing \& Experimentation}

Beyond basic A/B testing, Staff/Principal engineers need to understand sophisticated experimentation techniques.

\textbf{1. Interleaving (for Ranking Systems)}

\textbf{Problem with traditional A/B testing for ranking:}
\begin{itemize}
\item Need huge sample size to detect small ranking improvements
\item User experience inconsistent (sees only variant A or B, not both)
\item Slow to converge (weeks to months)
\end{itemize}

\textbf{Interleaving solution:}
\begin{itemize}
\item Show results from \textbf{both} ranking models in same SERP
\item Track which results user clicks (implicit preference)
\item Much more sensitive → 10-100x faster than A/B test
\end{itemize}

\textbf{Team Draft Interleaving Algorithm:}
\begin{verbatim}
# Given two ranking models A and B
def team_draft_interleave(results_A, results_B, k=10):
    interleaved = []
    seen = set()
    teams = {'A': [], 'B': []}

    # Alternately pick from A and B
    for i in range(k):
        if i % 2 == 0:  # A's turn
            for doc in results_A:
                if doc not in seen:
                    interleaved.append(doc)
                    teams['A'].append(doc)
                    seen.add(doc)
                    break
        else:  # B's turn
            for doc in results_B:
                if doc not in seen:
                    interleaved.append(doc)
                    teams['B'].append(doc)
                    seen.add(doc)
                    break

    return interleaved, teams

# Attribution: Count clicks per team
def evaluate_interleaving(clicks, teams):
    score_A = sum(1 for doc in clicks if doc in teams['A'])
    score_B = sum(1 for doc in clicks if doc in teams['B'])

    # Variant B wins if score_B > score_A
    return "B wins" if score_B > score_A else "A wins"
\end{verbatim}

\textbf{Interview talking point:} "For search ranking improvements, I'd use team-draft interleaving instead of A/B testing → 100x fewer users needed, results in days not months"

\textbf{2. CUPED (Variance Reduction)}

\textbf{Problem:} A/B tests need large samples because of high variance in user behavior

\textbf{CUPED (Controlled-experiment Using Pre-Experiment Data):}
\begin{itemize}
\item Use pre-experiment metric (e.g., last week's clicks) as covariate
\item Reduce variance → smaller sample size needed
\item Microsoft reported 50\% variance reduction in practice
\end{itemize}

\textbf{Formula:}
$$\hat{Y}_{\text{CUPED}} = Y - \theta(X - E[X])$$

Where:
\begin{itemize}
\item $Y$ = Post-experiment metric (e.g., revenue)
\item $X$ = Pre-experiment metric (e.g., historical revenue)
\item $\theta = \text{Cov}(X,Y) / \text{Var}(X)$ (optimal coefficient)
\end{itemize}

\textbf{Implementation:}
\begin{verbatim}
import numpy as np

def cuped_adjustment(y_control, y_treatment,
                    x_control, x_treatment):
    """
    y_control/treatment: Post-experiment metric
    x_control/treatment: Pre-experiment metric (covariate)
    """
    # Combine both groups for theta calculation
    x_combined = np.concatenate([x_control, x_treatment])
    y_combined = np.concatenate([y_control, y_treatment])

    # Calculate optimal theta
    theta = np.cov(x_combined, y_combined)[0,1] / np.var(x_combined)

    # Adjust metrics
    x_mean = np.mean(x_combined)
    y_control_adj = y_control - theta * (x_control - x_mean)
    y_treatment_adj = y_treatment - theta * (x_treatment - x_mean)

    # T-test on adjusted metrics
    from scipy import stats
    t_stat, p_value = stats.ttest_ind(y_treatment_adj,
                                       y_control_adj)

    effect_size = np.mean(y_treatment_adj) - np.mean(y_control_adj)
    variance_reduction = 1 - np.var(y_treatment_adj) / np.var(y_treatment)

    return {
        'effect_size': effect_size,
        'p_value': p_value,
        'variance_reduction': variance_reduction
    }
\end{verbatim}

\textbf{Interview scenario:} "Our A/B test needs 1M users to detect 1\% revenue lift"

\textbf{Strategic answer:} "Use CUPED with last month's revenue as covariate → reduce variance by 40-50\% → only need 500K users → launch experiment 2x faster"

\textbf{3. Sequential Testing (Early Stopping)}

\textbf{Problem:} Fixed-horizon A/B tests waste time if result is already significant

\textbf{Solution:} Sequential probability ratio test (SPRT) or mSPRT (mixture SPRT)
\begin{itemize}
\item Check for significance continuously
\item Stop early if clear winner detected
\item Control false positive rate (still $\alpha = 0.05$)
\end{itemize}

\textbf{When to use:}
\begin{itemize}
\item High-velocity testing (e.g., optimizing ad CTR)
\item Cost of running experiment is high
\item Need results quickly for business decision
\end{itemize}

\textbf{Trade-off:}
\begin{itemize}
\item \textbf{Pro}: 30-50\% faster on average
\item \textbf{Con}: Slightly higher variance in estimate
\item \textbf{Risk}: Can't peek at fixed-horizon test (inflates false positives)
\end{itemize}

\textbf{4. Multi-Armed Bandits (Adaptive Allocation)}

\textbf{A/B testing limitation:} 50/50 traffic split wastes impressions on losing variant

\textbf{Bandit solution:} Adaptively allocate more traffic to winning variant

\textbf{Comparison:}

\begin{tabular}{|l|c|c|}
\hline
\textbf{Method} & \textbf{Regret} & \textbf{Statistical Power} \\
\hline
A/B Test & High (50\% on loser) & High (clear winner) \\
Epsilon-Greedy & Medium & Medium \\
Thompson Sampling & Low (95\% on winner) & Low (harder to detect) \\
\hline
\end{tabular}

\textbf{When to use bandits vs A/B:}
\begin{itemize}
\item \textbf{Bandits}: Many variants (> 5), optimize cumulative reward, traffic is expensive
\item \textbf{A/B test}: Need clean statistical inference, regulatory requirement, high-stakes decision
\end{itemize}

\textbf{Interview pattern:} "Netflix thumbnail personalization: 10 thumbnails per show"

\textbf{Bandit answer:}
\begin{enumerate}
\item \textbf{Exploration phase}: Show each thumbnail to 1000 users (uniform random)
\item \textbf{Exploitation phase}: Thompson Sampling bandit
\item \textbf{Monitoring}: Track CTR, watch time, regret
\item \textbf{Periodically reset}: Concept drift → thumbnails that worked last month may not work now
\end{enumerate}

\textbf{5. Stratified Sampling \& Matching}

\textbf{Problem:} Randomization might create imbalanced groups (e.g., more iOS users in treatment)

\textbf{Solutions:}
\begin{itemize}
\item \textbf{Stratified randomization}: Randomize within strata (platform, country, etc.)
\item \textbf{Matched pairs}: Match users by propensity score, then randomize within pairs
\item \textbf{Re-randomization}: Reject randomizations with large covariate imbalance
\end{itemize}

\textbf{Interview talking point:} "I'd stratify randomization by platform (iOS/Android) and country to ensure balance, then check covariate balance before launching"

\textbf{6. Network Effects \& Interference}

\textbf{Problem:} Users in control group affected by users in treatment (violates SUTVA)

\textbf{Examples:}
\begin{itemize}
\item Social networks: Friend in treatment shares content to friend in control
\item Marketplaces: Surge pricing affects both riders and drivers
\item Recommendations: More clicks on treatment → less supply for control
\end{itemize}

\textbf{Solutions:}
\begin{itemize}
\item \textbf{Cluster randomization}: Randomize by city/region, not individual user
\item \textbf{Ego-network randomization}: Assign user + friends to same variant
\item \textbf{Switchback experiments}: Alternate time periods (A/B/A/B)
\item \textbf{Synthetic controls}: Model counterfactual using untreated units
\end{itemize}

\textbf{Interview scenario:} "Design A/B test for Uber driver incentive program"

\textbf{Network-aware answer:}
\begin{enumerate}
\item \textbf{Unit of randomization}: City (not driver) → prevents spillover
\item \textbf{Matched pairs}: Pair similar cities, randomize within pair
\item \textbf{Duration}: 4 weeks to measure equilibrium effects
\item \textbf{Metrics}: Driver retention, utilization, rider wait time (check for negative impact)
\end{enumerate}

\textbf{Key Interview Talking Points:}

When discussing A/B testing, demonstrate Staff/Principal depth:
\begin{enumerate}
\item \textbf{For ranking systems}: "Use interleaving, not A/B testing → 100x faster"
\item \textbf{For variance reduction}: "Apply CUPED with pre-experiment data → 2x sample efficiency"
\item \textbf{For many variants}: "Use Thompson Sampling bandit → minimize regret"
\item \textbf{For network effects}: "Cluster randomization by city/region"
\item \textbf{Sample size}: Calculate upfront using power analysis ($\alpha=0.05, \beta=0.2$, MDE)
\end{enumerate}

These advanced techniques show you've run experiments at scale, not just textbook A/B tests.

\section{The Principal Engineer's Perspective: Beyond the System}

At Principal level, you're not just designing systems—you're making **strategic investment decisions** that affect the entire organization for years. This section covers the questions that separate Staff engineers (who execute excellently) from Principal engineers (who set direction).

\subsection{ML Platforms vs. Bespoke Solutions}

\textbf{The Core Question:} Should we build a one-off solution for this product team, or invest in a platform that serves many teams?

This is arguably the **most important architectural decision** a Principal ML engineer makes, yet it's rarely discussed in interview prep materials.

\textbf{Bespoke Solution Approach:}

\textbf{Pros:}
\begin{itemize}
\item \textbf{Fast time-to-market}: Build exactly what one team needs, ship in weeks
\item \textbf{Highly optimized}: Can hardcode product-specific logic, squeeze every bit of performance
\item \textbf{Lower upfront cost}: No abstraction overhead, no multi-tenant complexity
\item \textbf{Prove value fast}: Show business impact before making larger investment
\end{itemize}

\textbf{Cons:}
\begin{itemize}
\item \textbf{Technical debt accumulates}: Every team builds their own ML stack → 10 different training pipelines
\item \textbf{Knowledge silos}: Each team's expertise trapped in their codebase
\item \textbf{Operational burden}: 10 teams × 3 models each = 30 separate systems to maintain
\item \textbf{Inconsistent quality}: Some teams build robust systems, others cut corners
\item \textbf{Duplicate effort}: Everyone reinvents feature engineering, model serving, monitoring
\end{itemize}

\textbf{ML Platform Approach:}

\textbf{Example platforms:}
\begin{itemize}
\item \textbf{Ranking-as-a-Service}: Unified ranking platform for Feed, Search, Ads (used by Meta, LinkedIn)
\item \textbf{Feature Store}: Centralized feature computation and serving (Uber Michelangelo, Airbnb Zipline)
\item \textbf{Model Registry + Serving}: Deploy any model via standard API (Netflix, Spotify)
\item \textbf{AutoML Platform}: Self-service ML for non-experts (Google Cloud AutoML, H2O.ai)
\end{itemize}

\textbf{Pros:}
\begin{itemize}
\item \textbf{Enforces best practices}: Every team gets monitoring, A/B testing, bias detection "for free"
\item \textbf{Accelerates future projects}: Second model ships in days, not months
\item \textbf{Economies of scale}: Shared infrastructure → lower per-model cost
\item \textbf{Consistent governance}: Centralized compliance, privacy, security controls
\item \textbf{Knowledge sharing}: Platform team becomes center of ML excellence
\end{itemize}

\textbf{Cons:}
\begin{itemize}
\item \textbf{Slower initial development}: Must design for generality, build abstractions
\item \textbf{Higher upfront cost}: Need 3-5 engineers for 6-12 months before first customer
\item \textbf{Risk of over-engineering}: Might build features no one uses
\item \textbf{Platform team becomes bottleneck}: Product teams wait for platform features
\item \textbf{Harder to optimize}: Abstractions can hide performance opportunities
\end{itemize}

\textbf{The Principal-Level Decision Framework:}

\begin{enumerate}
\item \textbf{For the FIRST use case}: Build bespoke
\begin{itemize}
\item Prove the business value exists
\item Learn domain-specific requirements
\item Move fast, iterate, show impact
\item BUT: Design clean interfaces, avoid hard-coding where possible
\end{itemize}

\item \textbf{When you see the SECOND similar request}: Evaluate platform investment
\begin{itemize}
\item If 3+ teams will need similar capability → strong platform case
\item If problems are truly different → bespoke might still be right
\end{itemize}

\item \textbf{For mature orgs with 10+ ML use cases}: Platform is almost always right
\begin{itemize}
\item Maintenance cost of bespoke solutions becomes unsustainable
\item Competitive advantage shifts from "having ML" to "velocity of ML iteration"
\end{itemize}
\end{enumerate}

\textbf{Interview Gold - The Hybrid Answer:}

\textit{"For the first version of the feed ranker, I'd build a bespoke solution to ship in 6 weeks and prove the 10\% engagement lift. However, I would design the system with clean interfaces—separating feature computation, model serving, and ranking logic—anticipating that we'll extract a generalized ranking platform in V2.}

\textit{Once we see similar needs from the Search and Ads teams (likely within 6 months), I'd propose investing in 'Ranking-as-a-Service.' This would cost 4 engineers for 6 months upfront, but would reduce the time-to-market for future ranking projects from 3 months to 2 weeks, and cut our operational overhead by 60\% within a year.}

\textit{The key metric I'd track is: Time from 'new ranking use case identified' to 'model in production.' If this doesn't drop from 12 weeks to < 3 weeks within a year of platform launch, the investment wasn't worth it."}

This answer shows:
\begin{itemize}
\item You balance short-term delivery with long-term vision
\item You think in terms of organizational ROI, not just technical elegance
\item You know how to derisk platform investments (prove value first)
\item You define success metrics for platform initiatives
\end{itemize}

\subsection{Managing the ML Project Lifecycle \& ROI}

Principals are expected to guide projects from **conception to deprecation** and justify their cost to senior leadership.

\textbf{1. The "0 to 1" vs. "1 to N" Problem}

ML systems evolve through distinct phases, each requiring different strategy:

\textbf{Phase 1: "0 to 1" - Proving Value (Months 1-6)}

\textbf{Goal:} Prove the ML solution delivers measurable business impact

\textbf{Strategy:}
\begin{itemize}
\item \textbf{Start with simple baselines}: Logistic regression often beats no ML by 50\%
\item \textbf{Focus on data quality}: Garbage in, garbage out—spend 70\% of time here
\item \textbf{Fast iteration}: Ship weekly, A/B test everything, learn from real users
\item \textbf{Manual processes OK}: Human labeling, rule-based fallbacks, small-scale serving
\item \textbf{Metrics that matter}: Business KPIs (revenue, retention), not model metrics (AUC)
\end{itemize}

\textbf{Success looks like:} "Our fraud detection model saved \$2M in 3 months, but it's hacky and requires daily manual fixes"

\textbf{Phase 2: "1 to N" - Scaling \& Operationalizing (Months 6-18)}

\textbf{Goal:} Make the system reliable, efficient, and low-maintenance

\textbf{Strategy:}
\begin{itemize}
\item \textbf{Automate everything}: Retraining pipelines, monitoring, alerting, rollback
\item \textbf{Optimize for cost}: Model compression, caching, batch processing
\item \textbf{Invest in reliability}: 99.9\% uptime, graceful degradation, circuit breakers
\item \textbf{Complex models now justified}: XGBoost → Deep learning if business case exists
\item \textbf{Governance \& compliance}: Bias monitoring, explainability, audit trails
\end{itemize}

\textbf{Success looks like:} "Fraud detection now runs fully automated, saves \$10M/year, costs \$500K to operate, requires 0.2 engineer oncall time"

\textbf{Phase 3: Mature System - Incremental Gains (Year 2+)}

\textbf{Goal:} Extract remaining value while minimizing maintenance cost

\textbf{Strategy:}
\begin{itemize}
\item \textbf{Diminishing returns}: Going from 90\% → 92\% accuracy might cost 6 engineer-months
\item \textbf{Focus on long tail}: Edge cases, new fraud patterns, concept drift
\item \textbf{Cost optimization}: Can we use smaller model? Reduce serving cost by 50\%?
\item \textbf{Consider deprecation}: Is this model still needed? Can we merge it with another system?
\end{itemize}

\textbf{Interview Pattern:} Design a new recommendation system

\textbf{Principal-level answer structure:}
\begin{enumerate}
\item \textbf{"For V1 (first 3 months)..."} → Collaborative filtering, batch recommendations, manual curation for cold start
\item \textbf{"Once we prove 15\% engagement lift (Month 6)..."} → Invest in real-time serving, two-tower model, hire ML infra engineer
\item \textbf{"At scale (Year 2)..."} → Deep learning models, multi-objective optimization, automated retraining
\end{enumerate}

This shows you think in **phases**, not just "final state."

\textbf{2. The Hidden Costs of ML}

Most candidates discuss model accuracy. Principals discuss **total cost of ownership (TCO).**

\textbf{Direct Costs:}
\begin{itemize}
\item \textbf{Training compute}: \$2K-\$50K per model iteration (GPUs expensive!)
\item \textbf{Inference serving}: 10M predictions/day → \$5K-\$20K/month in cloud costs
\item \textbf{Storage}: 100TB training data + feature store → \$3K/month
\item \textbf{Data labeling}: \$0.10-\$5 per label → \$50K for 10K labeled examples
\end{itemize}

\textbf{Hidden Costs (often 3-5x larger):}
\begin{itemize}
\item \textbf{Engineering time}: 2 engineers × \$200K/year = \$400K/year
\item \textbf{Opportunity cost}: These engineers can't work on other projects
\item \textbf{Maintenance overhead}: Oncall rotation, incident response, model retraining
\item \textbf{Technical debt}: Future refactoring, migrations, deprecation
\item \textbf{Organizational friction}: Coordinating with data, infra, product teams
\end{itemize}

\textbf{Back-of-envelope ROI Calculation:}

\begin{verbatim}
Example: Fraud Detection Model

Annual Benefits:
- Fraud prevented: $10M/year
- Manual review time saved: 2 FTE × $150K = $300K/year
Total Benefit: $10.3M/year

Annual Costs:
- Cloud infrastructure: $120K/year
- 1.5 ML engineers: $300K/year
- Data labeling: $50K/year
- Oncall overhead: $30K/year
Total Cost: $500K/year

ROI = ($10.3M - $500K) / $500K = 19.6x

Payback period: ~2 weeks of operation
\end{verbatim}

\textbf{Interview talking point:} "Before proposing a deep learning solution, I'd ask: What's the business value of improving AUC from 0.85 to 0.90? If it's \$1M/year but costs \$500K in engineering time and \$200K in compute, the ROI is only 1.4x. I might stick with the simpler model."

\textbf{3. Model Deprecation Strategy}

Most engineers never think about **when to turn off a model**. Principals do.

\textbf{When to deprecate:}
\begin{itemize}
\item \textbf{Maintenance cost > incremental value}: Old fraud model catches 2\% of fraud, new model catches 95\%. Maintaining both isn't worth it.
\item \textbf{Business need disappeared}: COVID prediction models in 2025? Probably not needed.
\item \textbf{Replaced by better solution}: Rule-based system → ML model → platform-provided model
\item \textbf{Data dependencies broken}: Model relies on deprecated API, cost to fix > value
\end{itemize}

\textbf{Deprecation process:}
\begin{enumerate}
\item \textbf{Shadow mode}: Run old + new model side-by-side, compare outputs for 2 weeks
\item \textbf{Measure impact}: Does new model cover all old model's use cases?
\item \textbf{Gradual rolloff}: 90\% new / 10\% old → 100\% new
\item \textbf{Kill switch ready}: Can we rollback instantly if something breaks?
\item \textbf{Sunset timeline}: Announce "Model X will be deprecated on Date Y", give teams 6 months
\item \textbf{Delete code \& data}: Don't let zombie models accumulate
\end{enumerate}

\textbf{Interview Gold:} "I'd track the 'cost per incremental unit of value' for each model. When Model V1's maintenance cost (\$50K/year) exceeds its incremental value over Model V2 (\$20K/year in fraud caught that V2 misses), it's time to deprecate V1."

\textbf{Key Metrics for ML Project Management:}

\begin{tabular}{|l|l|}
\hline
\textbf{Metric} & \textbf{Why It Matters} \\
\hline
Time to first value & How fast can we ship V1? \\
Cost per prediction & Are we spending \$0.001 or \$1 per prediction? \\
Engineering time per model & Can 1 engineer maintain 10 models? \\
Model development velocity & Weeks from idea to production? \\
Incident rate & How often do models break? \\
Business impact per dollar spent & ROI of ML investment \\
\hline
\end{tabular}

\textbf{Principal-Level Summary:}

When discussing ANY ML system design, demonstrate strategic thinking:
\begin{enumerate}
\item \textbf{"For V1, I'd build bespoke to prove value in 6 weeks..."} → Shows you prioritize speed
\item \textbf{"Once we see 3+ similar use cases, invest in platform..."} → Shows you think about leverage
\item \textbf{"The ROI is X, payback period is Y..."} → Shows you speak the language of leadership
\item \textbf{"We'd deprecate Model V1 when maintenance cost exceeds incremental value..."} → Shows you manage full lifecycle
\end{enumerate}

This is what separates a Staff engineer who builds great systems from a Principal engineer who builds great **businesses** with ML.

\section{System Design Practice Problems}

\subsection{Core Systems (Must Practice)}

\textbf{1. Design YouTube Video Recommendations}
\begin{itemize}
\item \textbf{Clarify}: 2B users, 500M videos, personalized homepage
\item \textbf{Challenges}: Cold start, diversity, watch time optimization
\item \textbf{Key components}: Two-tower model, ANN search, re-ranking
\end{itemize}

\textbf{2. Design Google Search Ranking}
\begin{itemize}
\item \textbf{Clarify}: Billions of queries/day, < 100ms latency
\item \textbf{Challenges}: Query understanding, candidate generation, relevance
\item \textbf{Key components}: Inverted index, BERT for query embedding, LambdaMART
\end{itemize}

\textbf{3. Design Google Ads CTR Prediction}
\begin{itemize}
\item \textbf{Clarify}: 100K QPS, < 10ms latency, optimize revenue
\item \textbf{Challenges}: Sparse features, calibration, auction integration
\item \textbf{Key components}: DeepFM, negative sampling, real-time serving
\end{itemize}

\textbf{4. Design Instagram Content Moderation}
\begin{itemize}
\item \textbf{Clarify}: 100M photos/day, detect NSFW/violence
\item \textbf{Challenges}: High recall (catch all bad content), low false positive
\item \textbf{Key components}: CNN (ResNet), human review pipeline, active learning
\end{itemize}

\textbf{5. Design Credit Card Fraud Detection}
\begin{itemize}
\item \textbf{Clarify}: 1M transactions/sec, < 100ms latency
\item \textbf{Challenges}: Extreme imbalance (0.1\% fraud), real-time scoring
\item \textbf{Key components}: XGBoost, graph features, rule engine
\end{itemize}

\subsection{Advanced Systems (Bonus)}

\textbf{6. Design Uber ETA Prediction}
\begin{itemize}
\item \textbf{Challenges}: Time series, traffic patterns, real-time updates
\item \textbf{Key components}: LSTM/GRU, historical data, map features
\end{itemize}

\textbf{7. Design Netflix Thumbnail Personalization}
\begin{itemize}
\item \textbf{Challenges}: Multi-armed bandit, image selection, A/B testing
\item \textbf{Key components}: Contextual bandits, Thompson sampling, CNN for image embeddings
\end{itemize}

\textbf{8. Design Spotify Playlist Generation}
\begin{itemize}
\item \textbf{Challenges}: Sequence modeling, diversity, transitions
\item \textbf{Key components}: RNN for sequence, collaborative filtering, audio features
\end{itemize}

\textbf{9. Design Airbnb Search Ranking}
\begin{itemize}
\item \textbf{Challenges}: Geo-spatial, multi-objective (relevance + host earnings)
\item \textbf{Key components}: Geo-search, XGBoost, Pareto optimization
\end{itemize}

\textbf{10. Design Amazon Product Categorization}
\begin{itemize}
\item \textbf{Challenges}: Hierarchical classification, 10K+ categories
\item \textbf{Key components}: BERT for text, hierarchical softmax, image + text fusion
\end{itemize}

\section{The MADE Framework (Deep Dive)}

Use this framework for every ML system design interview:

\subsection{M - Model Selection \& Training}

\textbf{Questions to Address:}
\begin{enumerate}
\item \textbf{What model architecture?}
\begin{itemize}
\item Justify choice based on data type (images → CNN, text → Transformer)
\item Discuss alternatives and trade-offs
\end{itemize}

\item \textbf{How to train?}
\begin{itemize}
\item Training data size \& labeling strategy
\item Loss function (cross-entropy, MSE, custom)
\item Hyperparameters (learning rate, batch size, epochs)
\item Regularization (dropout, L2, early stopping)
\end{itemize}

\item \textbf{Infrastructure?}
\begin{itemize}
\item Single GPU, distributed training, or CPU?
\item Training time estimate
\item Experiment tracking (MLflow, Weights \& Biases)
\end{itemize}
\end{enumerate}

\textbf{Example (YouTube Recommendations):}
\begin{itemize}
\item \textbf{Model}: Two-tower neural network (user tower + video tower)
\item \textbf{Loss}: Softmax cross-entropy over watched videos
\item \textbf{Training}: 100M user-video pairs, distributed training on 10 GPUs, 1 week
\end{itemize}

\subsection{A - API Design \& Serving}

\textbf{Questions to Address:}
\begin{enumerate}
\item \textbf{What is the API?}
\begin{itemize}
\item Input: User ID, context (device, time, location)
\item Output: List of recommendations, scores, explanations
\end{itemize}

\item \textbf{Real-time or batch?}
\begin{itemize}
\item Real-time: User-blocking (search, ads, fraud detection)
\item Batch: Precompute overnight (email recommendations)
\end{itemize}

\item \textbf{Latency requirements?}
\begin{itemize}
\item < 10ms: Ad serving (need caching, simple models)
\item < 100ms: Search ranking (GPU serving, batching)
\item < 1s: Image classification (can use larger models)
\end{itemize}

\item \textbf{Serving infrastructure?}
\begin{itemize}
\item TensorFlow Serving, TorchServe, or custom Flask API
\item Load balancing, autoscaling, health checks
\item Optimization: batching, quantization, caching
\end{itemize}
\end{enumerate}

\textbf{Example (Ad CTR Prediction):}
\begin{itemize}
\item \textbf{API}: \texttt{predict\_ctr(user\_id, ad\_id, context)} → float (0-1)
\item \textbf{Serving}: Real-time (< 10ms), TensorFlow Serving on CPU
\item \textbf{Optimization}: Feature caching (Redis), model quantization (INT8)
\end{itemize}

\subsection{D - Data Pipeline \& Features}

\textbf{Questions to Address:}
\begin{enumerate}
\item \textbf{What features?}
\begin{itemize}
\item User features: Demographics, behavior, history
\item Item features: Content, metadata, popularity
\item Context features: Time, device, location
\item Cross features: User × item interactions
\end{itemize}

\item \textbf{How to compute features?}
\begin{itemize}
\item Batch: Spark jobs, Airflow pipelines
\item Real-time: Kafka + Flink streaming
\item Feature store: Feast, Tecton for consistency
\end{itemize}

\item \textbf{Training data?}
\begin{itemize}
\item Where to get labels? (User clicks, human annotations)
\item How to handle imbalance? (Negative sampling, weighted loss)
\item Train/val/test split? (Temporal split for time series)
\end{itemize}

\item \textbf{Data quality?}
\begin{itemize}
\item Missing values: Imputation, default values
\item Outliers: Clipping, robust scaling
\item Data drift: Monitor feature distributions
\end{itemize}
\end{enumerate}

\textbf{Example (Search Ranking):}
\begin{itemize}
\item \textbf{Features}: Query-document BERT similarity, click-through rate, document quality score, user search history
\item \textbf{Labels}: Clicks (positive), impressions without clicks (negative)
\item \textbf{Pipeline}: Spark job to compute features daily, store in Hive
\end{itemize}

\subsection{E - Evaluation \& Monitoring}

\textbf{Questions to Address:}
\begin{enumerate}
\item \textbf{Offline metrics?}
\begin{itemize}
\item Classification: AUC, Precision, Recall, F1
\item Ranking: NDCG@K, MRR, Precision@K
\item Regression: RMSE, MAE, R²
\end{itemize}

\item \textbf{Online metrics?}
\begin{itemize}
\item Business: CTR, conversion rate, revenue
\item Engagement: Time spent, likes, shares
\item Retention: DAU, session duration
\end{itemize}

\item \textbf{A/B testing?}
\begin{itemize}
\item Control vs treatment (50/50 split)
\item Statistical significance (p < 0.05)
\item Duration (1-2 weeks minimum)
\item Guardrails: Latency, error rate
\end{itemize}

\item \textbf{Monitoring?}
\begin{itemize}
\item Model performance: Accuracy, AUC over time
\item Data drift: Feature distribution shifts
\item Prediction drift: Output distribution changes
\item System health: Latency, QPS, error rate
\end{itemize}

\item \textbf{Retraining?}
\begin{itemize}
\item Periodic: Daily, weekly, monthly
\item Triggered: When metrics degrade by > 5\%
\item Online learning: Incremental updates
\end{itemize}
\end{enumerate}

\textbf{Example (Fraud Detection):}
\begin{itemize}
\item \textbf{Offline}: Precision-Recall AUC (imbalanced data)
\item \textbf{Online}: \% fraud caught, false positive rate, customer complaints
\item \textbf{Monitoring}: Alert if precision drops below 80\%
\item \textbf{Retraining}: Daily (fraud patterns evolve quickly)
\end{itemize}

\section{Capacity Estimation Cheatsheet}

Memorize these formulas for quick calculations:

\subsection{Training Cost}

\textbf{GPU Hours:}
\begin{verbatim}
Time = (Samples × Epochs × FLOPs_per_sample) / (GPU_TFLOPS × Batch_size)
\end{verbatim}

\textbf{Example}: Train ResNet-50 on ImageNet
\begin{itemize}
\item 1.2M images, 100 epochs, 4B FLOPs/image
\item V100 (125 TFLOPS FP16), batch size 256
\item Time = $(1.2M \times 100 \times 4B) / (125T \times 256)$ ≈ 10 days on 8 GPUs
\item Cost = 240 hours × \$12/hour (p3.8xlarge) = \textbf{\$2,880}
\end{itemize}

\subsection{Serving Cost}

\textbf{Instances Needed:}
\begin{verbatim}
Instances = (Target_QPS × Latency) / (Batch_size × GPU_throughput)
\end{verbatim}

\textbf{Example}: Serve 1000 images/sec with ResNet-50
\begin{itemize}
\item Latency = 50ms, Batch size = 32
\item GPU throughput = 640 images/sec (32 × 20 batches/sec)
\item Instances = $(1000 \times 0.05) / (32 \times 0.05)$ = 2 GPUs
\item Cost = 2 × 730 hours × \$0.53/hour (g4dn.xlarge) = \textbf{\$770/month}
\end{itemize}

\subsection{Storage}

\textbf{Embeddings:}
\begin{verbatim}
Size = Num_items × Embedding_dim × 4 bytes (FP32)
\end{verbatim}

\textbf{Example}: 1B user embeddings, 128-dim
\begin{itemize}
\item Size = $1B \times 128 \times 4$ bytes = \textbf{512 GB}
\item Store in sharded Redis cluster (64 shards × 8GB each)
\end{itemize}

\textbf{Feature Store:}
\begin{verbatim}
Size = Num_users × Features_per_user × Bytes_per_feature
\end{verbatim}

\textbf{Example}: 100M users, 500 features (avg 4 bytes)
\begin{itemize}
\item Size = $100M \times 500 \times 4$ = \textbf{200 GB}
\item Store in S3 for training, Redis for online serving
\end{itemize}

\section{Interview Execution Strategy}

\subsection{Time Management (45 minutes)}

\textbf{Phase 1: Clarify Requirements (5 min)}
\begin{itemize}
\item Scale: How many users? QPS?
\item Latency: Real-time (< 100ms) or batch?
\item Data: Is labeled data available? How much?
\item Constraints: Budget, privacy, explainability?
\end{itemize}

\textbf{Phase 2: High-Level Design (5 min)}
\begin{itemize}
\item Draw 3-stage pipeline: Data → Model → Serving
\item Identify key components (feature store, model server, monitoring)
\item Get interviewer buy-in before diving deep
\end{itemize}

\textbf{Phase 3: Deep Dive (25 min)}

Spend time on what interviewer cares about (ask!):
\begin{itemize}
\item \textbf{Model (10 min)}: Architecture, training, loss function
\item \textbf{Features (5 min)}: Feature engineering, data pipeline
\item \textbf{Serving (5 min)}: API design, latency optimization
\item \textbf{Evaluation (5 min)}: Metrics, A/B testing, monitoring
\end{itemize}

\textbf{Phase 4: Trade-offs \& Alternatives (10 min)}
\begin{itemize}
\item Discuss alternative approaches
\item Justify your design choices
\item Address edge cases, failure modes
\item Scale considerations (10× more users)
\end{itemize}

\subsection{Communication Tips}

\textbf{Think Out Loud:}
\begin{itemize}
\item "I'm thinking we need real-time serving because users are waiting for search results..."
\item "Let me calculate if CPU is sufficient or if we need GPU..."
\end{itemize}

\textbf{Draw Diagrams:}
\begin{itemize}
\item Data flow: User → API → Model → Response
\item System components: Feature store, model server, cache
\item Use boxes and arrows clearly
\end{itemize}

\textbf{Ask Clarifying Questions:}
\begin{itemize}
\item "Is 100ms latency acceptable?"
\item "Do we have labeled data or do we need to collect it?"
\item "Should we prioritize precision or recall?"
\end{itemize}

\textbf{State Assumptions:}
\begin{itemize}
\item "I'm assuming we have 1M labeled examples..."
\item "Let's say we can tolerate 5\% false positive rate..."
\end{itemize}

\subsection{Common Mistakes to Avoid}

\begin{itemize}
\item \textbf{Jumping to model too fast} → Clarify requirements first!
\item \textbf{Ignoring data pipeline} → Most work is data engineering, not modeling
\item \textbf{Over-engineering} → Start simple (logistic regression), then optimize
\item \textbf{Not discussing metrics} → Always define success criteria
\item \textbf{Forgetting monitoring} → Production models degrade, need monitoring
\item \textbf{Not justifying choices} → Explain why this model, not others
\item \textbf{Missing trade-offs} → Accuracy vs latency, cost vs performance
\item \textbf{No failure handling} → What if model server crashes?
\end{itemize}

\section{Recommended Resources}

\subsection{Must-Read Blog Posts}

\begin{enumerate}
\item \textbf{Netflix Recommendations} - Personalized ranking
\item \textbf{Uber Michelangelo} - End-to-end ML platform
\item \textbf{Facebook DeepText} - NLP at scale
\item \textbf{Google TFX} - Production ML pipelines
\item \textbf{Instagram Explore} - Candidate generation \& ranking
\item \textbf{Twitter Timeline Ranking} - Real-time feed ranking
\item \textbf{Airbnb Search Ranking} - Multi-objective optimization
\item \textbf{DoorDash ETA Prediction} - Time series forecasting
\end{enumerate}

\subsection{Books}

\begin{itemize}
\item \textbf{Designing Machine Learning Systems} - Chip Huyen (2022) ← Best overall
\item \textbf{Machine Learning Design Patterns} - Lakshmanan et al.
\item \textbf{Reliable Machine Learning} - Todd Underwood (O'Reilly)
\end{itemize}

\subsection{Online Courses}

\begin{itemize}
\item \textbf{Full Stack Deep Learning} - Free, production ML focus
\item \textbf{MLOps Specialization} - DeepLearning.AI
\item \textbf{Grokking the ML Interview} - Educative (paid, worth it)
\end{itemize}

\section{Final Checklist}

\subsection{Before Your Interview}

$\square$ Practiced 5-7 complete system designs (45 min each)

$\square$ Can draw architecture for:
\begin{itemize}
\item Recommendation system
\item Search ranking
\item Ad CTR prediction
\item Fraud detection
\item Computer vision system
\end{itemize}

$\square$ Memorized capacity estimation formulas

$\square$ Can explain 3-5 deployment strategies (blue-green, canary, shadow)

$\square$ Know difference between offline and online metrics

$\square$ Prepared questions for interviewer about their ML stack

\subsection{During the Interview}

$\square$ Clarified all requirements before diving into design

$\square$ Drew high-level architecture first (get interviewer buy-in)

$\square$ Discussed trade-offs for every major decision

$\square$ Stated assumptions explicitly

$\square$ Asked for feedback periodically ("Does this make sense?")

$\square$ Covered MADE framework: Model, API, Data, Evaluation

$\square$ Left 5-10 minutes for questions

\section{Conclusion}

ML system design interviews test your ability to build \textbf{production-ready ML systems}, not just algorithms. The key to success:

\begin{enumerate}
\item \textbf{Master 7 core systems} - 80\% of questions fall into these patterns
\item \textbf{Use MADE framework} - Ensures you cover all critical components
\item \textbf{Justify every choice} - Trade-offs, alternatives, quantitative analysis
\item \textbf{Think end-to-end} - Data → Model → Serving → Monitoring
\item \textbf{Practice, practice, practice} - Do 10+ mock interviews
\end{enumerate}

Remember: Interviewers want to see how you \textbf{think}, not just what you know. Explain your reasoning, discuss trade-offs, and show you understand real-world constraints.

Good luck with your Staff/Principal ML Engineer interviews!

\end{document}
