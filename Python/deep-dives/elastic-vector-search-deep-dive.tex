\documentclass[10pt]{article}
\usepackage[margin=0.75in]{geometry}
\usepackage{amsmath,amsthm,amsfonts,amssymb}
\usepackage{color,graphicx}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{enumitem}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{lightgray!20},
    basicstyle=\ttfamily\small,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2
}

\lstset{style=mystyle}

\title{\textbf{Elasticsearch Vector Search \& Relevance Deep Dive} \\ \large Interview Preparation Guide for Elastic Search Team}
\author{}
\date{}

\begin{document}
\maketitle

\section*{Overview}

This guide provides comprehensive coverage of Elasticsearch vector search, relevance ranking, and neural search for engineers interviewing with Elastic's search relevance team. It covers modern semantic search, embedding-based retrieval, hybrid search strategies, and production optimization techniques.

\textbf{Key Focus Areas for Elastic Interview:}
\begin{itemize}
\item \textbf{Vector Search}: Dense vectors, k-NN, HNSW, similarity metrics
\item \textbf{Relevance}: BM25, TF-IDF, scoring algorithms, query-time boosting
\item \textbf{Neural Search}: Embeddings, ELSER, semantic search, RAG
\item \textbf{Hybrid Search}: Combining keyword + vector search with RRF
\item \textbf{Production}: Performance tuning, memory optimization, BBQ quantization
\end{itemize}

\section{Recommended Reading List}

\subsection{Priority 1: Vector Search Specific}

\textbf{``Vector Search for Practitioners with Elastic''} \\
\textit{Authors: Bahaaldine Azarmi, Jeff Vestal, Shay Banon (2023-2024)}

\textbf{Why This Book:}
\begin{itemize}[leftmargin=*]
\item \textbf{Most relevant} - Written specifically for Elasticsearch 8+ vector capabilities
\item Foreword by Elastic's founder (Shay Banon)
\item Covers production deployment at scale
\item Modern techniques: ELSER, RAG, RRF, BBQ quantization
\end{itemize}

\textbf{Key Chapters to Focus On:}
\begin{enumerate}
\item \textbf{Chapters 1-3}: NLP fundamentals, embeddings, dense vectors
\item \textbf{Chapter 4}: Dense vector storage in Elasticsearch
\item \textbf{Chapter 5-6}: k-NN search, HNSW algorithm, performance
\item \textbf{Chapter 7}: ELSER (Elastic Learned Sparse EncodeR) - Elastic's secret sauce
\item \textbf{Chapter 8}: Neural search and semantic search patterns
\item \textbf{Chapter 9}: Hybrid search with RRF (Reciprocal Rank Fusion)
\item \textbf{Chapter 10}: RAG (Retrieval-Augmented Generation) with ChatGPT
\item \textbf{Chapter 11-12}: Production tuning, node scaling, memory optimization
\end{enumerate}

\subsection{Priority 2: Relevance Theory}

\textbf{``Relevant Search: With Applications for Solr and Elasticsearch''} \\
\textit{Authors: Doug Turnbull, John Berryman}

\textbf{Why This Book:}
\begin{itemize}[leftmargin=*]
\item \textbf{The bible} of search relevance - explains the ``why'' behind scoring
\item Deep dive into Lucene internals (Elasticsearch's foundation)
\item Practical relevance tuning strategies
\item Covers both keyword and semantic approaches
\end{itemize}

\textbf{Essential Chapters:}
\begin{enumerate}
\item \textbf{Chapter 1-2}: Relevance fundamentals, user intent
\item \textbf{Chapter 3}: Lucene scoring (TF-IDF, BM25)
\item \textbf{Chapter 4-5}: Query DSL for relevance, boosting strategies
\item \textbf{Chapter 8}: Debugging relevance (explain API)
\item \textbf{Chapter 10}: Learning to Rank (LTR)
\end{enumerate}

\subsection{Priority 3: Elasticsearch Foundations}

\textbf{``Elasticsearch in Action, Second Edition''} \\
\textit{Author: Madhusudhan Konda (2023)}

\textbf{Use Case:} Refresh on core Elasticsearch concepts if rusty

\textbf{Focus Areas:}
\begin{itemize}
\item Cluster architecture, sharding, replicas
\item Index mappings, analyzers, tokenizers
\item Query DSL mastery
\item Aggregations and analytics
\item Performance optimization
\end{itemize}

\section{Vector Search Fundamentals}

\subsection{What is Vector Search?}

\textbf{Traditional Keyword Search (BM25):}
\begin{verbatim}
Query: "best pizza restaurant"
Matches documents with: best, pizza, restaurant
Score based on: term frequency, inverse document frequency
\end{verbatim}

\textbf{Vector Search (Semantic Search):}
\begin{verbatim}
Query: "best pizza restaurant"
Embedding: [0.23, -0.45, 0.67, ..., 0.12] (768-dim vector)
Matches documents with SIMILAR embeddings (even without exact words)
Can match: "top Italian eatery", "great pizzeria"
Score based on: cosine similarity, dot product, or L2 distance
\end{verbatim}

\textbf{Key Insight:} Vector search captures \textit{semantic meaning}, not just \textit{keyword overlap}.

\subsection{Dense Vectors vs Sparse Vectors}

\textbf{Dense Vectors (Traditional Embeddings):}
\begin{itemize}
\item \textbf{Dimensions}: 384, 768, 1024 (every dimension has a value)
\item \textbf{Models}: BERT, Sentence Transformers, OpenAI embeddings
\item \textbf{Storage}: 768 floats Ã— 4 bytes = 3KB per document
\item \textbf{Use Case}: General semantic search, cross-lingual search
\item \textbf{Pros}: Captures deep semantic meaning
\item \textbf{Cons}: High memory usage, slower indexing
\end{itemize}

\textbf{Sparse Vectors (ELSER):}
\begin{itemize}
\item \textbf{Dimensions}: 30,000+ (but only ~200 non-zero values)
\item \textbf{Elastic's ELSER}: Learned sparse representations
\item \textbf{Storage}: Much smaller due to sparsity
\item \textbf{Use Case}: Domain-specific search, better explainability
\item \textbf{Pros}: Faster, more interpretable, better for rare terms
\item \textbf{Cons}: Requires training, less universal than dense vectors
\end{itemize}

\textbf{When to Use Which:}
\begin{itemize}
\item \textbf{Dense}: Cross-domain search, multilingual, semantic similarity
\item \textbf{Sparse (ELSER)}: Domain-specific, explainable results, exact term matching important
\item \textbf{Hybrid}: Combine both with RRF for best results!
\end{itemize}

\subsection{k-NN Search in Elasticsearch}

\textbf{Exact k-NN (Brute Force):}
\begin{lstlisting}
POST /my-index/_search
{
  "query": {
    "script_score": {
      "query": {"match_all": {}},
      "script": {
        "source": "cosineSimilarity(params.query_vector, 'vector_field') + 1.0",
        "params": {"query_vector": [0.2, 0.4, ...]}
      }
    }
  }
}
\end{lstlisting}

\textbf{Time Complexity:} O(N Ã— D) where N = docs, D = dimensions

\textbf{Approximate k-NN (HNSW):}
\begin{lstlisting}
POST /my-index/_search
{
  "knn": {
    "field": "vector_field",
    "query_vector": [0.2, 0.4, ...],
    "k": 10,
    "num_candidates": 100
  }
}
\end{lstlisting}

\textbf{HNSW (Hierarchical Navigable Small World):}
\begin{itemize}
\item \textbf{Time Complexity:} O(log N) - much faster!
\item \textbf{Tradeoff:} Approximate results (may miss some neighbors)
\item \textbf{Parameters:}
  \begin{itemize}
  \item \texttt{m}: Max connections per layer (default: 16, higher = better recall, more memory)
  \item \texttt{ef\_construction}: Build-time search depth (default: 100, higher = better graph quality)
  \item \texttt{num\_candidates}: Query-time candidates (trade recall vs latency)
  \end{itemize}
\item \textbf{Memory:} ~8-10 bytes per dimension per document (significant!)
\end{itemize}

\section{Similarity Metrics}

\subsection{Cosine Similarity}

\textbf{Formula:}
\[
\text{cosine\_similarity}(\mathbf{a}, \mathbf{b}) = \frac{\mathbf{a} \cdot \mathbf{b}}{\|\mathbf{a}\| \|\mathbf{b}\|}
\]

\textbf{Range:} [-1, 1] where 1 = identical direction, 0 = orthogonal, -1 = opposite

\textbf{Properties:}
\begin{itemize}
\item Measures angle, not magnitude
\item Normalized - good for comparing docs of different lengths
\item Most common for text embeddings (BERT, Sentence Transformers)
\end{itemize}

\textbf{Elasticsearch:} Converts to score: \texttt{(1 + cosine) / 2} â†’ [0, 1]

\subsection{Dot Product}

\textbf{Formula:}
\[
\text{dot\_product}(\mathbf{a}, \mathbf{b}) = \sum_{i=1}^{n} a_i \times b_i
\]

\textbf{Range:} Unbounded (depends on vector magnitudes)

\textbf{Properties:}
\begin{itemize}
\item Faster than cosine (no normalization)
\item Magnitude matters - longer vectors score higher
\item Good when vector length is meaningful
\end{itemize}

\textbf{Use Case:} OpenAI embeddings (pre-normalized), product recommendations

\subsection{L2 Distance (Euclidean)}

\textbf{Formula:}
\[
\text{L2\_distance}(\mathbf{a}, \mathbf{b}) = \sqrt{\sum_{i=1}^{n} (a_i - b_i)^2}
\]

\textbf{Range:} [0, âˆž] where 0 = identical, higher = more different

\textbf{Properties:}
\begin{itemize}
\item Measures absolute distance in vector space
\item Sensitive to magnitude
\item Elasticsearch inverts for scoring: \texttt{1 / (1 + L2)}
\end{itemize}

\textbf{Use Case:} Image embeddings, face recognition

\subsection{Which Metric to Choose?}

\begin{tabular}{|l|l|l|}
\hline
\textbf{Metric} & \textbf{Best For} & \textbf{Training Model} \\
\hline
Cosine & Text embeddings (BERT, ST) & Normalize vectors \\
Dot Product & Pre-normalized (OpenAI) & Unit vectors \\
L2 & Image, face recognition & Euclidean space \\
\hline
\end{tabular}

\textbf{Pro Tip:} Match the metric to how your embedding model was trained!

\section{Elasticsearch Relevance Scoring}

\subsection{BM25 (Best Match 25)}

\textbf{The default scoring algorithm in Elasticsearch 7+}

\textbf{Formula:}
\[
\text{score}(D, Q) = \sum_{t \in Q} \text{IDF}(t) \times \frac{f(t, D) \times (k_1 + 1)}{f(t, D) + k_1 \times (1 - b + b \times \frac{|D|}{\text{avgdl}})}
\]

\textbf{Components:}
\begin{itemize}
\item \textbf{IDF(t)}: Inverse Document Frequency - rare terms score higher
\item \textbf{f(t, D)}: Term frequency in document
\item \textbf{|D|}: Document length
\item \textbf{avgdl}: Average document length in corpus
\item \textbf{k1}: Term saturation parameter (default: 1.2)
\item \textbf{b}: Length normalization (default: 0.75)
\end{itemize}

\textbf{Key Properties:}
\begin{itemize}
\item \textbf{Term Saturation}: Diminishing returns for repeated terms
\item \textbf{Length Normalization}: Penalizes very long documents
\item \textbf{Better than TF-IDF}: Handles term frequency saturation
\end{itemize}

\textbf{Tuning Parameters:}
\begin{lstlisting}
PUT /my-index
{
  "settings": {
    "index": {
      "similarity": {
        "custom_bm25": {
          "type": "BM25",
          "k1": 1.5,     // Higher = more term freq influence
          "b": 0.5       // Lower = less length normalization
        }
      }
    }
  }
}
\end{lstlisting}

\subsection{Query-Time Boosting}

\textbf{Field Boosting:}
\begin{lstlisting}
{
  "query": {
    "multi_match": {
      "query": "search query",
      "fields": ["title^3", "body^1", "tags^2"]
    }
  }
}
\end{lstlisting}

\textbf{Function Score:}
\begin{lstlisting}
{
  "query": {
    "function_score": {
      "query": {"match": {"title": "pizza"}},
      "functions": [
        {"filter": {"term": {"featured": true}}, "weight": 2},
        {"field_value_factor": {"field": "rating", "modifier": "log1p"}},
        {"gauss": {"location": {"origin": "40.7,-74.0", "scale": "10km"}}}
      ],
      "score_mode": "sum",
      "boost_mode": "multiply"
    }
  }
}
\end{lstlisting}

\textbf{Boosting Strategies:}
\begin{itemize}
\item \textbf{Field Boost}: Title > Body > Tags
\item \textbf{Recency}: Decay function for time-sensitive content
\item \textbf{Popularity}: Log(views), log(ratings) to prevent dominance
\item \textbf{Geo-Proximity}: Distance-based boosting
\item \textbf{Business Rules}: Featured items, paid placements
\end{itemize}

\section{Hybrid Search with RRF}

\subsection{Why Hybrid Search?}

\textbf{Problem:} Keyword search and vector search have different strengths

\begin{itemize}
\item \textbf{Keyword (BM25)}: Exact matches, rare terms, technical jargon
\item \textbf{Vector}: Semantic meaning, synonyms, paraphrases
\end{itemize}

\textbf{Example:}
\begin{itemize}
\item Query: ``CPU overheating solutions''
\item \textbf{BM25 misses}: ``processor temperature fixes'' (no keyword overlap)
\item \textbf{Vector misses}: ``Core i9-13900K thermal throttling'' (too specific)
\item \textbf{Hybrid}: Captures both semantic meaning AND specific terms
\end{itemize}

\subsection{Reciprocal Rank Fusion (RRF)}

\textbf{Algorithm:}
\[
\text{RRF\_score}(d) = \sum_{r \in R} \frac{1}{k + \text{rank}_r(d)}
\]

Where:
\begin{itemize}
\item \textbf{R}: Set of retrieval methods (e.g., BM25, vector search)
\item \textbf{rank\_r(d)}: Rank of document d in retrieval r
\item \textbf{k}: Constant (default: 60) to smooth rankings
\end{itemize}

\textbf{Example:}
\begin{verbatim}
Document A: BM25 rank=1, Vector rank=5
  RRF_score(A) = 1/(60+1) + 1/(60+5) = 0.0164 + 0.0154 = 0.0318

Document B: BM25 rank=10, Vector rank=2
  RRF_score(B) = 1/(60+10) + 1/(60+2) = 0.0143 + 0.0161 = 0.0304

Winner: Document A (balanced high ranks beats one very high)
\end{verbatim}

\textbf{Elasticsearch RRF Query:}
\begin{lstlisting}
POST /my-index/_search
{
  "retriever": {
    "rrf": {
      "retrievers": [
        {
          "standard": {
            "query": {"match": {"title": "pizza restaurant"}}
          }
        },
        {
          "knn": {
            "field": "title_vector",
            "query_vector": [0.2, 0.4, ...],
            "k": 10,
            "num_candidates": 100
          }
        }
      ],
      "rank_constant": 60,
      "rank_window_size": 100
    }
  }
}
\end{lstlisting}

\textbf{RRF Advantages:}
\begin{itemize}
\item \textbf{No training required} - parameter-free fusion
\item \textbf{Robust} - handles different score scales automatically
\item \textbf{Balanced} - rewards documents that rank well in multiple retrievers
\item \textbf{Production-ready} - fast, deterministic
\end{itemize}

\section{ELSER: Elastic's Secret Sauce}

\subsection{What is ELSER?}

\textbf{ELSER = Elastic Learned Sparse EncodeR}

\textbf{Key Idea:} Learned sparse vectors that combine:
\begin{itemize}
\item Semantic understanding (like dense vectors)
\item Interpretability (like keyword search)
\item Efficiency (sparse = less memory)
\end{itemize}

\textbf{How It Works:}
\begin{enumerate}
\item Train a model to predict ``expansion terms'' for a query/document
\item Generate sparse vector: 30,000 dimensions, ~200 non-zero
\item Non-zero dimensions = relevant terms with weights
\item Match using efficient inverted index (like BM25)
\end{enumerate}

\subsection{ELSER vs Dense Vectors}

\begin{tabular}{|l|l|l|}
\hline
\textbf{Aspect} & \textbf{Dense (BERT)} & \textbf{Sparse (ELSER)} \\
\hline
Dimensions & 768 (all non-zero) & 30k (200 non-zero) \\
Storage & 3KB per doc & 800 bytes per doc \\
Indexing & Slow (HNSW build) & Fast (inverted index) \\
Query Speed & Medium (ANN) & Very Fast (inverted index) \\
Explainability & Opaque & Transparent (see terms) \\
Domain Adaptation & Hard (retrain) & Easier (fine-tune) \\
\hline
\end{tabular}

\textbf{When to Use ELSER:}
\begin{itemize}
\item Domain-specific search (legal, medical, e-commerce)
\item Need explainable results (why did this match?)
\item Memory/cost constrained
\item Exact term matching still important
\item Elasticsearch 8.8+ available
\end{itemize}

\subsection{ELSER Setup in Elasticsearch}

\textbf{1. Deploy ELSER Model:}
\begin{lstlisting}
PUT _ml/trained_models/.elser_model_2
{
  "input": {"field_names": ["text_field"]}
}

POST _ml/trained_models/.elser_model_2/deployment/_start
{
  "number_of_allocations": 2
}
\end{lstlisting}

\textbf{2. Create Inference Pipeline:}
\begin{lstlisting}
PUT _ingest/pipeline/elser-ingest
{
  "processors": [
    {
      "inference": {
        "model_id": ".elser_model_2",
        "input_output": [
          {"input_field": "content", "output_field": "content_sparse"}
        ]
      }
    }
  ]
}
\end{lstlisting}

\textbf{3. Index with ELSER:}
\begin{lstlisting}
PUT /my-elser-index/_doc/1?pipeline=elser-ingest
{
  "content": "Best pizza restaurants in New York"
}
\end{lstlisting}

\textbf{4. Query with ELSER:}
\begin{lstlisting}
POST /my-elser-index/_search
{
  "query": {
    "text_expansion": {
      "content_sparse": {
        "model_id": ".elser_model_2",
        "model_text": "top Italian eateries NYC"
      }
    }
  }
}
\end{lstlisting}

\section{Production Optimization}

\subsection{Memory Management for Vector Search}

\textbf{Memory Calculation:}
\begin{verbatim}
Dense Vector Memory = N_docs Ã— Dimensions Ã— 4 bytes Ã— (1 + HNSW_overhead)
HNSW_overhead â‰ˆ 2-3x (depending on m parameter)

Example:
10M docs Ã— 768 dims Ã— 4 bytes Ã— 3 = 92 GB
\end{verbatim}

\textbf{Optimization Strategies:}

\textbf{1. Reduce Dimensions (PCA/Quantization):}
\begin{itemize}
\item 768 dims â†’ 384 dims = 50\% memory savings
\item Use \texttt{element\_type: byte} for 8-bit quantization (75\% savings)
\item Tradeoff: 5-10\% recall loss for 4x memory reduction
\end{itemize}

\textbf{2. BBQ Quantization (Elasticsearch 8.11+):}
\begin{lstlisting}
PUT /my-index
{
  "mappings": {
    "properties": {
      "vector_field": {
        "type": "dense_vector",
        "dims": 768,
        "index": true,
        "similarity": "cosine",
        "index_options": {
          "type": "hnsw",
          "m": 16,
          "ef_construction": 100
        },
        "element_type": "byte"  // BBQ quantization
      }
    }
  }
}
\end{lstlisting}

\textbf{BBQ (Byte-quantized Binary Quantization):}
\begin{itemize}
\item Quantizes float32 â†’ int8 (1 byte instead of 4)
\item 4x memory reduction
\item Minimal recall loss (~2-3\%)
\item Faster query time (integer arithmetic)
\end{itemize}

\subsection{Indexing Performance}

\textbf{Bulk Indexing with Vectors:}
\begin{lstlisting}
POST _bulk
{"index": {"_index": "my-index", "_id": "1"}}
{"title": "Doc 1", "vector": [0.1, 0.2, ...]}
{"index": {"_index": "my-index", "_id": "2"}}
{"title": "Doc 2", "vector": [0.3, 0.4, ...]}
\end{lstlisting}

\textbf{Performance Tips:}
\begin{enumerate}
\item \textbf{Disable Refresh During Bulk}: \texttt{index.refresh\_interval: -1}
\item \textbf{Increase Bulk Size}: 1000-5000 docs per request
\item \textbf{Use Multiple Shards}: Parallelize HNSW graph building
\item \textbf{Allocate More Heap}: Vector indexing is memory-intensive
\item \textbf{SSD Storage}: HNSW graph access is I/O intensive
\end{enumerate}

\subsection{Query Performance Tuning}

\textbf{k-NN Parameters:}
\begin{lstlisting}
{
  "knn": {
    "field": "vector",
    "query_vector": [...],
    "k": 10,              // Top-k results
    "num_candidates": 100 // Search space (higher = better recall, slower)
  }
}
\end{lstlisting}

\textbf{Tuning Guidelines:}
\begin{itemize}
\item \textbf{num\_candidates}: Start with 10x k, increase if recall poor
\item \textbf{Trade-off}: 100 candidates = 10ms, 1000 candidates = 50ms
\item \textbf{Latency Budget}: Allocate 20-50ms for vector search in p95
\end{itemize}

\textbf{Filtering with k-NN:}
\begin{lstlisting}
{
  "knn": {
    "field": "vector",
    "query_vector": [...],
    "k": 10,
    "num_candidates": 100,
    "filter": {
      "term": {"category": "restaurants"}
    }
  }
}
\end{lstlisting}

\textbf{Warning:} Filters are applied \textit{after} k-NN retrieval!
\begin{itemize}
\item Bad: Retrieve 100 candidates, 95 filtered out â†’ only 5 results
\item Fix: Increase \texttt{num\_candidates} when using filters
\item Rule of Thumb: \texttt{num\_candidates = k / selectivity}
\end{itemize}

\subsection{Node Sizing for Vector Workloads}

\textbf{Data Node Sizing:}
\begin{itemize}
\item \textbf{CPU}: Vector search is CPU-intensive (similarity calculations)
  \begin{itemize}
  \item Recommended: 8-16 cores per node
  \item Higher clock speed > more cores
  \end{itemize}
\item \textbf{RAM}: Vectors + HNSW graphs must fit in memory
  \begin{itemize}
  \item Calculate: (Index size Ã— 3) + 4GB heap
  \item Example: 90GB vectors â†’ 270GB RAM + 4GB heap = 274GB
  \end{itemize}
\item \textbf{Storage}: SSD mandatory for HNSW graph access
  \begin{itemize}
  \item NVMe preferred for low-latency p99
  \end{itemize}
\end{itemize}

\textbf{ML Node Sizing (for ELSER/embeddings):}
\begin{itemize}
\item \textbf{CPU}: Inference is CPU-heavy (or GPU if available)
\item \textbf{RAM}: Model size Ã— number of allocations
  \begin{itemize}
  \item ELSER v2: ~1GB per allocation
  \item Sentence Transformer: ~500MB per allocation
  \end{itemize}
\item \textbf{Throughput}: Scale allocations based on QPS
  \begin{itemize}
  \item 1 allocation â‰ˆ 10-20 inferences/sec
  \item 100 QPS â†’ 5-10 allocations
  \end{itemize}
\end{itemize}

\section{Retrieval-Augmented Generation (RAG)}

\subsection{What is RAG?}

\textbf{Problem:} LLMs have outdated knowledge, hallucinate, no private data

\textbf{Solution:} RAG = Retrieve relevant docs + Generate answer with context

\textbf{RAG Pipeline:}
\begin{enumerate}
\item \textbf{User Query}: ``What's our return policy for electronics?''
\item \textbf{Retrieve}: Use Elasticsearch (vector/hybrid) to find top-k docs
\item \textbf{Augment}: Inject retrieved docs into LLM prompt
\item \textbf{Generate}: LLM generates answer grounded in retrieved context
\end{enumerate}

\subsection{RAG with Elasticsearch}

\textbf{Architecture:}
\begin{verbatim}
User Query
    â†“
[Embedding Model] â†’ Query Vector
    â†“
[Elasticsearch] â†’ Hybrid Search (BM25 + Vector + ELSER)
    â†“
Top-K Documents
    â†“
[Prompt Template] â†’ Context: {retrieved_docs}\nQuestion: {query}
    â†“
[LLM: GPT-4, Claude] â†’ Generated Answer
    â†“
User
\end{verbatim}

\textbf{Elasticsearch Query for RAG:}
\begin{lstlisting}
POST /knowledge-base/_search
{
  "retriever": {
    "rrf": {
      "retrievers": [
        {
          "standard": {
            "query": {"match": {"content": "return policy electronics"}}
          }
        },
        {
          "knn": {
            "field": "content_vector",
            "query_vector": [...],  // Embedded query
            "k": 5,
            "num_candidates": 50
          }
        }
      ]
    }
  },
  "size": 5,
  "_source": ["title", "content", "url"]
}
\end{lstlisting}

\textbf{Prompt Construction:}
\begin{lstlisting}[language=Python]
results = es.search(index="kb", body=query)

context = "\n\n".join([
    f"[{i+1}] {doc['_source']['title']}\n{doc['_source']['content']}"
    for i, doc in enumerate(results['hits']['hits'])
])

prompt = f"""Use the following context to answer the question.
If the answer is not in the context, say "I don't know."

Context:
{context}

Question: {user_query}

Answer:"""

answer = openai.ChatCompletion.create(
    model="gpt-4",
    messages=[{"role": "user", "content": prompt}]
)
\end{lstlisting}

\subsection{RAG Optimization Strategies}

\textbf{1. Chunking Strategy:}
\begin{itemize}
\item \textbf{Chunk Size}: 256-512 tokens (balance context vs precision)
\item \textbf{Overlap}: 50-100 tokens to avoid boundary issues
\item \textbf{Metadata}: Store parent doc ID, section headers
\end{itemize}

\textbf{2. Re-ranking After Retrieval:}
\begin{verbatim}
Elasticsearch (k=50) â†’ Re-ranker Model (top-5) â†’ LLM
\end{verbatim}
\begin{itemize}
\item Use cross-encoder (BERT-based) for fine-grained relevance
\item 10x more compute than retrieval, but 10x better precision
\end{itemize}

\textbf{3. Hybrid Retrieval:}
\begin{itemize}
\item Combine BM25 (exact matches) + Vector (semantic) + ELSER (learned sparse)
\item Use RRF to fuse results
\item Higher recall â†’ better RAG quality
\end{itemize}

\textbf{4. Query Expansion:}
\begin{itemize}
\item Generate multiple variations of user query
\item Retrieve for each variation, merge results
\item Example: ``return policy'' â†’ [``refund policy'', ``exchange policy'', ``money back guarantee'']
\end{itemize}

\textbf{5. Prompt Engineering:}
\begin{itemize}
\item \textbf{Citation}: Ask LLM to cite source document [1], [2]
\item \textbf{Confidence}: Ask LLM to indicate confidence level
\item \textbf{Brevity}: Limit answer length to avoid hallucination
\end{itemize}

\section{Interview Preparation Strategy}

\subsection{Week 1: Deep Dive (7 days before)}

\textbf{Reading Plan:}
\begin{enumerate}
\item \textbf{Days 1-3}: ``Vector Search for Practitioners'' chapters 1-7
  \begin{itemize}
  \item Focus: Dense vectors, k-NN, HNSW, ELSER
  \item Take notes on production considerations
  \end{itemize}
\item \textbf{Days 4-5}: ``Relevant Search'' chapters 1-5
  \begin{itemize}
  \item Focus: BM25, scoring, relevance tuning
  \item Practice explain API examples
  \end{itemize}
\item \textbf{Days 6-7}: Hands-on lab (see below)
\end{enumerate}

\subsection{Hands-On Lab (2-3 days before)}

\textbf{Setup:}
\begin{lstlisting}[language=bash]
# Start Elasticsearch 8.x locally
docker run -p 9200:9200 -e "discovery.type=single-node" \
  docker.elastic.co/elasticsearch/elasticsearch:8.11.0
\end{lstlisting}

\textbf{Lab Exercises:}
\begin{enumerate}
\item \textbf{Index Documents with Vectors}:
  \begin{itemize}
  \item Generate embeddings with Sentence Transformers
  \item Index 1000+ documents with dense\_vector field
  \item Test different similarity metrics (cosine, dot\_product, L2)
  \end{itemize}

\item \textbf{k-NN Queries}:
  \begin{itemize}
  \item Run exact k-NN (script\_score)
  \item Run approximate k-NN (HNSW)
  \item Compare latency and recall
  \item Test different num\_candidates values
  \end{itemize}

\item \textbf{Hybrid Search with RRF}:
  \begin{itemize}
  \item Create BM25 query
  \item Create k-NN query
  \item Combine with RRF retriever
  \item Compare results vs individual methods
  \end{itemize}

\item \textbf{ELSER (if time permits)}:
  \begin{itemize}
  \item Deploy ELSER model
  \item Create inference pipeline
  \item Index documents with ELSER
  \item Query with text\_expansion
  \end{itemize}

\item \textbf{Performance Tuning}:
  \begin{itemize}
  \item Benchmark query latency
  \item Test BBQ quantization (element\_type: byte)
  \item Measure memory usage
  \item Tune HNSW parameters (m, ef\_construction)
  \end{itemize}
\end{enumerate}

\subsection{Day Before Interview: Review \& Mock}

\textbf{Elasticsearch Docs to Review:}
\begin{itemize}
\item k-NN search: \url{https://www.elastic.co/guide/en/elasticsearch/reference/current/knn-search.html}
\item Dense vector field: \url{https://www.elastic.co/guide/en/elasticsearch/reference/current/dense-vector.html}
\item RRF retriever: \url{https://www.elastic.co/guide/en/elasticsearch/reference/current/rrf.html}
\item ELSER: \url{https://www.elastic.co/guide/en/machine-learning/current/ml-nlp-elser.html}
\end{itemize}

\textbf{Recent Elastic Blogs (2024):}
\begin{itemize}
\item BBQ quantization for memory optimization
\item Semantic reranking improvements
\item ELSER v2 updates
\item Vector search performance benchmarks
\end{itemize}

\textbf{Mock Interview Topics:}
\begin{enumerate}
\item \textbf{System Design}: ``Design a semantic search system for 100M documents''
  \begin{itemize}
  \item Discuss indexing pipeline, embedding generation, k-NN configuration
  \item Talk through latency, memory, cost tradeoffs
  \end{itemize}

\item \textbf{Relevance Debugging}: ``BM25 scores are too low for short docs''
  \begin{itemize}
  \item Explain BM25 formula, length normalization
  \item Suggest tuning b parameter, field boosting
  \end{itemize}

\item \textbf{Vector Search Optimization}: ``k-NN queries are slow at p99''
  \begin{itemize}
  \item Discuss HNSW tuning, num\_candidates
  \item Suggest BBQ quantization, dimension reduction
  \end{itemize}

\item \textbf{Hybrid Search}: ``When to use keyword vs vector vs hybrid?''
  \begin{itemize}
  \item Compare use cases, strengths, weaknesses
  \item Explain RRF fusion strategy
  \end{itemize}
\end{enumerate}

\section{Key Discussion Points for Interview}

\subsection{Vector Search Tradeoffs}

\textbf{Be Prepared to Discuss:}

\textbf{1. Dimensionality Tradeoffs:}
\begin{itemize}
\item \textbf{Higher dims (768, 1024)}: Better semantic representation, more memory
\item \textbf{Lower dims (384, 256)}: Less memory, faster, slight quality loss
\item \textbf{When to reduce}: Cost-constrained, latency-sensitive, large corpus
\end{itemize}

\textbf{2. Exact vs Approximate k-NN:}
\begin{itemize}
\item \textbf{Exact (Brute Force)}: 100\% recall, O(N) time, too slow for >10k docs
\item \textbf{Approximate (HNSW)}: 90-95\% recall, O(log N) time, production-ready
\item \textbf{When to use exact}: Small corpus, offline batch jobs, quality critical
\end{itemize}

\textbf{3. Dense vs Sparse Vectors:}
\begin{itemize}
\item \textbf{Dense}: General-purpose, cross-domain, semantic understanding
\item \textbf{Sparse (ELSER)}: Domain-specific, explainable, efficient
\item \textbf{Hybrid}: Combine both with RRF for best results
\end{itemize}

\textbf{4. Memory vs Latency vs Cost:}
\begin{itemize}
\item \textbf{High memory}: Store full-precision vectors, fast queries
\item \textbf{Low memory}: BBQ quantization, slower indexing, slight recall loss
\item \textbf{Tradeoff}: 4x memory savings vs 2-3\% recall drop
\end{itemize}

\subsection{Production Considerations}

\textbf{Scaling Vector Search:}
\begin{enumerate}
\item \textbf{Indexing Throughput}:
  \begin{itemize}
  \item Bottleneck: HNSW graph construction
  \item Solution: Multiple shards, parallel indexing, SSD storage
  \item Benchmark: 1000 docs/sec with 768-dim vectors on 8-core node
  \end{itemize}

\item \textbf{Query Latency}:
  \begin{itemize}
  \item Target: p50 < 20ms, p95 < 50ms, p99 < 100ms
  \item Tuning: num\_candidates, HNSW m/ef\_construction, BBQ quantization
  \item Monitoring: Track slow queries, cache hit rates
  \end{itemize}

\item \textbf{Memory Management}:
  \begin{itemize}
  \item Calculate: (N\_docs Ã— dims Ã— 4 bytes Ã— 3) for HNSW
  \item Mitigation: BBQ quantization, dimension reduction, tiered storage
  \end{itemize}

\item \textbf{Model Deployment}:
  \begin{itemize}
  \item ELSER/embedding models on ML nodes
  \item Allocations based on QPS: 1 allocation = 10-20 req/sec
  \item Failover: Multiple allocations for HA
  \end{itemize}
\end{enumerate}

\subsection{Recent Elasticsearch Innovations (2024)}

\textbf{Highlight These in Interview:}

\textbf{1. BBQ Quantization:}
\begin{itemize}
\item Float32 â†’ Int8 (1 byte) for 4x memory savings
\item Minimal recall loss (~2-3\%)
\item Faster query time (integer arithmetic)
\end{itemize}

\textbf{2. RRF (Reciprocal Rank Fusion):}
\begin{itemize}
\item Parameter-free fusion of multiple retrievers
\item Combines BM25 + Vector + ELSER seamlessly
\item Better than simple score combination
\end{itemize}

\textbf{3. ELSER v2:}
\begin{itemize}
\item Improved sparse representations
\item Better domain adaptation
\item Lower latency than dense vectors
\end{itemize}

\textbf{4. Semantic Reranking:}
\begin{itemize}
\item Two-stage retrieval: Fast k-NN â†’ Slow cross-encoder
\item 10x quality improvement for top-k results
\item Use for RAG, question answering
\end{itemize}

\section{Common Interview Questions}

\subsection{Technical Deep Dives}

\textbf{Q1: How does HNSW work? Why is it faster than brute-force k-NN?}

\textbf{Answer:}
\begin{itemize}
\item HNSW builds a hierarchical graph with multiple layers
\item Top layer: Sparse, long-range connections (navigation)
\item Bottom layer: Dense, short-range connections (refinement)
\item Search starts at top, greedily navigates to bottom
\item Time complexity: O(log N) vs O(N) for brute-force
\item Tradeoff: Approximate results, memory overhead (2-3x)
\item Parameters: m (connections per layer), ef\_construction (build quality)
\end{itemize}

\textbf{Q2: Explain BM25. Why is it better than TF-IDF?}

\textbf{Answer:}
\begin{itemize}
\item BM25 = Best Match 25, improved TF-IDF with saturation
\item Key improvement: Term frequency saturation (diminishing returns)
  \begin{itemize}
  \item TF-IDF: Linear growth with term freq â†’ spam vulnerability
  \item BM25: Logarithmic saturation â†’ natural text favored
  \end{itemize}
\item Length normalization with parameter b:
  \begin{itemize}
  \item b=1: Full normalization (penalize long docs)
  \item b=0: No normalization (favor long docs)
  \item Default b=0.75: Balanced
  \end{itemize}
\item k1 parameter: Term saturation rate (default 1.2)
\end{itemize}

\textbf{Q3: How would you debug poor search relevance?}

\textbf{Answer:}
\begin{enumerate}
\item \textbf{Use Explain API}:
  \begin{lstlisting}
  GET /index/_explain/doc_id
  {
    "query": {"match": {"field": "query"}}
  }
  \end{lstlisting}
  \begin{itemize}
  \item Shows scoring breakdown: TF, IDF, field norms
  \item Identify which component is off
  \end{itemize}

\item \textbf{Check Analyzers}:
  \begin{lstlisting}
  POST /_analyze
  {
    "analyzer": "standard",
    "text": "query text"
  }
  \end{lstlisting}
  \begin{itemize}
  \item Verify tokenization, stemming, stopwords
  \item Mismatch between index and query analysis
  \end{itemize}

\item \textbf{Review Mapping}:
  \begin{itemize}
  \item Check field types, analyzers, similarity settings
  \item Verify boosting, multi-field setup
  \end{itemize}

\item \textbf{Tune BM25 Parameters}:
  \begin{itemize}
  \item Adjust k1 (term saturation)
  \item Adjust b (length normalization)
  \end{itemize}

\item \textbf{Add Field Boosting}:
  \begin{itemize}
  \item Boost title > body > tags
  \item Use function\_score for business logic
  \end{itemize}
\end{enumerate}

\textbf{Q4: When would you use cosine vs dot product vs L2 distance?}

\textbf{Answer:}
\begin{itemize}
\item \textbf{Cosine Similarity}:
  \begin{itemize}
  \item Use for: Text embeddings (BERT, Sentence Transformers)
  \item Why: Normalized, focuses on direction not magnitude
  \item Training: Models trained with cosine loss
  \end{itemize}
\item \textbf{Dot Product}:
  \begin{itemize}
  \item Use for: Pre-normalized embeddings (OpenAI, Cohere)
  \item Why: Faster (no normalization), magnitude meaningful
  \item Training: Models output unit vectors
  \end{itemize}
\item \textbf{L2 Distance}:
  \begin{itemize}
  \item Use for: Image embeddings, face recognition
  \item Why: Euclidean distance in latent space
  \item Training: Triplet loss, contrastive loss
  \end{itemize}
\item \textbf{Key Rule}: Match the metric to how the model was trained!
\end{itemize}

\subsection{System Design Questions}

\textbf{Q5: Design a semantic search system for 100M documents.}

\textbf{Answer Outline:}

\textbf{1. Requirements Clarification:}
\begin{itemize}
\item QPS: 1000 queries/sec
\item Latency: p95 < 100ms
\item Index updates: Real-time or batch?
\item Budget: Memory/cost constraints?
\end{itemize}

\textbf{2. High-Level Architecture:}
\begin{verbatim}
User Query â†’ Embedding API â†’ Elasticsearch Cluster
                               â†“
                      [Hybrid Search: BM25 + k-NN]
                               â†“
                          Top-K Results
\end{verbatim}

\textbf{3. Embedding Generation:}
\begin{itemize}
\item \textbf{Model}: Sentence Transformers (all-MiniLM-L6-v2, 384 dims)
\item \textbf{Deployment}: Dedicated embedding service (5-10 instances)
\item \textbf{Caching}: Cache query embeddings (Redis, 1hr TTL)
\end{itemize}

\textbf{4. Elasticsearch Setup:}
\begin{itemize}
\item \textbf{Cluster}: 10 data nodes, 3 master nodes
\item \textbf{Shards}: 50 primary shards (2M docs per shard)
\item \textbf{Replicas}: 1 replica for HA
\item \textbf{Node Specs}: 32 cores, 256GB RAM, NVMe SSD
\end{itemize}

\textbf{5. Memory Calculation:}
\begin{verbatim}
Vector memory = 100M docs Ã— 384 dims Ã— 4 bytes Ã— 3 (HNSW) = 461 GB
BM25 index = 100M docs Ã— 1KB avg = 100 GB
Total per shard = (461 + 100) / 50 = 11 GB
Node capacity = 256GB RAM / 11GB = 23 shards per node
\end{verbatim}

\textbf{6. Query Strategy:}
\begin{itemize}
\item \textbf{Hybrid Search}: RRF fusion of BM25 + k-NN
\item \textbf{k-NN Params}: k=10, num\_candidates=100
\item \textbf{Optimization}: BBQ quantization for 4x memory savings
\end{itemize}

\textbf{7. Indexing Pipeline:}
\begin{itemize}
\item \textbf{Batch Processing}: Kafka â†’ Embedding Service â†’ Elasticsearch
\item \textbf{Throughput}: 10k docs/sec (100 indexing clients)
\item \textbf{Refresh Interval}: 30s for near-real-time
\end{itemize}

\textbf{8. Monitoring:}
\begin{itemize}
\item Query latency (p50, p95, p99)
\item Indexing lag, backlog size
\item Memory usage, GC pauses
\item k-NN recall metrics (offline)
\end{itemize}

\textbf{Q6: How would you improve search relevance for e-commerce?}

\textbf{Answer:}

\textbf{1. Hybrid Search Foundation:}
\begin{itemize}
\item \textbf{BM25}: Exact product names, SKUs, brands
\item \textbf{Vector}: Semantic similarity (``running shoes'' â†’ ``sneakers'')
\item \textbf{RRF}: Fuse both for balanced results
\end{itemize}

\textbf{2. Query Understanding:}
\begin{itemize}
\item \textbf{Query Expansion}: Synonyms (``laptop'' â†’ ``notebook'')
\item \textbf{Query Correction}: Spell check (``iphon'' â†’ ``iphone'')
\item \textbf{Intent Detection}: Navigate (``nike'') vs Browse (``running shoes'')
\end{itemize}

\textbf{3. Feature Engineering:}
\begin{itemize}
\item \textbf{Popularity}: log(sales), log(views) as boost
\item \textbf{Recency}: Decay for seasonal products
\item \textbf{Price}: Match user's price range
\item \textbf{Ratings}: Boost high-rated products
\item \textbf{Availability}: Penalize out-of-stock
\end{itemize}

\textbf{4. Personalization:}
\begin{itemize}
\item \textbf{User History}: Boost categories user browses
\item \textbf{Collaborative}: ``Users who bought X also bought Y''
\item \textbf{A/B Testing}: Test personalized vs generic
\end{itemize}

\textbf{5. Business Rules:}
\begin{itemize}
\item \textbf{Promoted Products}: Boost sponsored items
\item \textbf{Diversity}: Don't show 10 variants of same product
\item \textbf{Merchandising}: Seasonal campaigns, new arrivals
\end{itemize}

\textbf{6. Evaluation:}
\begin{itemize}
\item \textbf{Offline}: NDCG, MRR on human-labeled data
\item \textbf{Online}: CTR, conversion rate, revenue per search
\item \textbf{A/B Testing}: Compare relevance improvements vs baseline
\end{itemize}

\section{Summary Checklist}

\subsection{Must-Know Concepts}

\textbf{Vector Search:}
\begin{itemize}[label=$\square$]
\item Dense vs sparse vectors (ELSER)
\item k-NN: Exact (brute-force) vs Approximate (HNSW)
\item Similarity metrics: Cosine, Dot Product, L2
\item HNSW algorithm, parameters (m, ef\_construction)
\item Memory calculation for vector indices
\item BBQ quantization for memory optimization
\end{itemize}

\textbf{Relevance:}
\begin{itemize}[label=$\square$]
\item BM25 formula, parameters (k1, b)
\item TF-IDF vs BM25 differences
\item Field boosting, function\_score
\item Explain API for debugging
\item Analyzers, tokenizers, filters
\end{itemize}

\textbf{Hybrid Search:}
\begin{itemize}[label=$\square$]
\item Why hybrid search? (keyword + semantic)
\item RRF (Reciprocal Rank Fusion) algorithm
\item Elasticsearch RRF retriever syntax
\item When to use hybrid vs pure vector
\end{itemize}

\textbf{Production:}
\begin{itemize}[label=$\square$]
\item Node sizing (CPU, RAM, storage)
\item Indexing performance tuning
\item Query latency optimization (num\_candidates)
\item Filtering with k-NN (selectivity issues)
\item Monitoring and alerting
\end{itemize}

\textbf{Advanced:}
\begin{itemize}[label=$\square$]
\item ELSER setup and querying
\item RAG architecture with Elasticsearch
\item Semantic reranking strategies
\item Recent Elastic innovations (2024)
\end{itemize}

\subsection{Hands-On Lab Checklist}

\begin{itemize}[label=$\square$]
\item Setup Elasticsearch 8.x locally
\item Generate embeddings with Sentence Transformers
\item Index documents with dense\_vector field
\item Run k-NN queries (exact and approximate)
\item Test different similarity metrics
\item Implement hybrid search with RRF
\item Benchmark query latency
\item Test BBQ quantization
\item (Optional) Deploy and test ELSER
\item (Optional) Build simple RAG pipeline
\end{itemize}

\subsection{Interview Day Checklist}

\textbf{Morning Of:}
\begin{itemize}[label=$\square$]
\item Review BM25 formula and parameters
\item Review HNSW algorithm intuition
\item Review RRF fusion formula
\item Skim Elasticsearch vector search docs
\item Practice explaining concepts out loud
\end{itemize}

\textbf{During Interview:}
\begin{itemize}[label=$\square$]
\item Ask clarifying questions (requirements, constraints)
\item Think out loud (show reasoning process)
\item Discuss tradeoffs (memory vs latency vs quality)
\item Mention recent Elastic innovations (BBQ, ELSER, RRF)
\item Draw diagrams (architecture, pipelines)
\item Be honest about what you don't know
\item Show enthusiasm for vector search/relevance work!
\end{itemize}

\section{Final Thoughts}

\textbf{Key Strengths to Highlight:}
\begin{itemize}
\item Experience with search systems (even if not recent)
\item Strong ML fundamentals (embeddings, neural networks)
\item Production systems thinking (scaling, monitoring, tradeoffs)
\item Curiosity about modern techniques (ELSER, RAG, hybrid search)
\end{itemize}

\textbf{Topics to Show Depth:}
\begin{itemize}
\item Vector search is not magic - understand HNSW, memory, latency
\item Hybrid search > pure vector - leverage strengths of both
\item Production is hard - discuss real-world constraints
\item Evaluation matters - offline metrics, online A/B tests
\end{itemize}

\textbf{Questions to Ask Interviewer:}
\begin{itemize}
\item What are the biggest challenges in Elastic's vector search today?
\item How is ELSER adoption vs dense vectors in production?
\item What's the roadmap for hybrid search improvements?
\item How does the team evaluate relevance quality?
\item What ML/NLP techniques is the team exploring next?
\end{itemize}

\vspace{1em}
\noindent\textbf{Good luck with your interview! You've got this! ðŸš€}

\end{document}
